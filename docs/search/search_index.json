{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#_","title":"_","text":"TRADE AND BET ON BETFAIR USING OUR API OR CHOOSE FROM A CATALOGUE OF THIRD PARTY BETTING TOOLS              The Betfair Exchange is a unique platform for peer-to-peer trading and wagering.             We welcome and support winning clients and provide tools for your automation journey.          Join Now Get Your API Key Not in Australia or New Zealand? Trade and bet on Betfair using our API or choose from a catalogue of third party betting tools Join Now Get Your API Key Not in Australia or New Zealand? Trade and bet on Betfair using our API or choose from a catalogue of third party betting tools Join Now Get Your API Key Not in Australia or New Zealand? <p>REAL-TIME DATA</p> <p>Get access to up-to-date data on the Betfair Exchange, including prices, volumes, and market trends.</p> <p>LIGHTNING FASTEXECUTION</p> <p>Place bets and trades quickly and efficiently, with sub-second response times.</p> <p>AUTOMATION</p> <p>Automate your betting strategies and reduce manual intervention, by using our API to place bets and trades automatically.</p> <p>CUSTOMISATION</p> <p>Develop your own trading and betting applications, tailored to your specific needs and preferences.</p> <p>INTEGRATION</p> <p>Integrate Betfair data into your existing systems and workflows and enhance your trading and betting capabilities.</p> <p>REAL-TIME DATA</p> <p>Get access to up-to-date data on the Betfair Exchange, including prices, volumes, and market trends.</p> <p>LIGHTNING FASTEXECUTION</p> <p>Place bets and trades quickly and efficiently, with sub-second response times.</p> <p>AUTOMATION</p> <p>Automate your betting strategies and reduce manual intervention, by using our API to place bets and trades automatically.</p> <p>CUSTOMISATION</p> <p>Develop your own trading and betting applications, tailored to your specific needs and preferences.</p> <p>INTEGRATION</p> <p>Integrate Betfair data into your existing systems and workflows and enhance your trading and betting capabilities.</p> REAL TIME AND HISTORICAL DATA In addition to a real-time data streaming service, we provide access to a database of historical market data. The Betfair database currently holds almost 1.5TB of raw and preprocessed market data available in JSON format.  Our Data Availability Listing REAL TIME AND HISTORICAL DATA In addition to a real-time data streaming service, we provide access to a database of historical market data. The Betfair database currently holds almost 1.5TB of raw and preprocessed market data available in JSON format.  Our Data Availability Listing DEDICATED TEAM We are a service-oriented team dedicated to supporting those looking to create and automate a betting or trading strategy. Whether you're a seasoned quant or brand new to Betfair, we're here to help. 12.4% 76K 50ms 1450GB ADVANTAGE FOR BSP AGAINST BEST TOTE AUSTRALIAN RACING EVENTS PER YEAR STREAM UPDATE FREQUENCY OF HISTORICAL PRICING DATA 12.4% ADVANTAGE FOR BSP AGAINST BEST TOTE 76K AUSTRALIAN RACING EVENTS PER YEAR 50ms STREAM UPDATE FREQUENCY 1450GB OF HISTORICAL PRICING DATA Connect to the API in your language of choice              Helper libraries are available for most programming languages, provided either by Betfair or supported by the community so you can focus on the most important aspects instead of wasting time connecting the pieces together.              Connect to the API in your language of choice              Helper libraries are available for most programming languages, provided either by Betfair or supported by the community so you can focus on the most important aspects instead of wasting time connecting the pieces together.              C++ Clojure Node.js Perl Scala Python C# PHP Ruby Java Javascript R C++ Clojure Node.js Perl Scala Python C# PHP Ruby Java Javascript R RESOURCE CATALOGUE API Tutorials <p>Betfair has a set of customer-facing transactional APIs to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software setups or tapping existing libraries.</p> Learn More Historic Pricing Data <p>We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport. Australian and New Zealand customers can reach out directly for access to our curated database.</p> Learn More Data Modelling <p>Many of our most successful customers use predictive models as the basis for their betting strategies. We have a series of modelling tutorials created by community members ranging from racing to sports. We hold regular datathons where cash prizes can be won.</p> Learn More Automation Tools <p>There are many applications that have been built by external developers which offer exciting features including one-click bets or tools for fully automating your strategies on the exchange. We've developed reviews and tutorials for some of the most popular tools.</p> Learn More Inspiration and Information <p>There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy. We're always looking for new and interesting content.</p> Learn More Quants Community <p>betfair quants\u00a0is really active Betfair-owned Discord server for people interested in modelling and automation on the Exchange. Our resident experts are always willing and able to share their expertise and experience. Please\u00a0reach out\u00a0if you'd like an invitation.</p> Learn More API Tutorials <p>Betfair has a set of customer-facing transactional APIs to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software setups or tapping existing libraries.</p> Learn More Historic Pricing Data <p>We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport. Australian and New Zealand customers can reach out directly for access to our curated database.</p> Learn More Data Modelling <p>Many of our most successful customers use predictive models as the basis for their betting strategies. We have a series of modelling tutorials created by community members ranging from racing to sports. We hold regular datathons where cash prizes can be won.</p> Learn More Automation Tools <p>There are many applications that have been built by external developers which offer exciting features including one-click bets or tools for fully automating your strategies on the exchange. We've developed reviews and tutorials for some of the most popular tools.</p> Learn More Inspiration &amp; Info <p>There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy. We're always looking for new and interesting content.</p> Learn More Quants Community <p>betfair quants\u00a0is really active Betfair-owned Discord server for people interested in modelling and automation on the Exchange. Our resident experts are always willing and able to share their expertise and experience. Please\u00a0reach out\u00a0if you'd like an invitation.</p> Learn More <p>For one-on-one support, AUS and NZ customers can contact us</p> Contact us Get Your API Key <p></p>"},{"location":"api/apiPythontutorial/","title":"Betfair API tutorial in Python","text":"<p>This tutorial will walk you through the process of connecting to Betfair's API, grabbing data and placing a bet in Python. It will utilise the <code>betfairlightweight</code> Python library.</p>"},{"location":"api/apiPythontutorial/#requirements","title":"Requirements","text":"<p>This tutorial will assume that you have an API app key. If you don't, please follow the steps outlined here.</p> <p>This tutorial will also assume that you have a basic understanding of what an API is. For a summary in layman's terms, read this article.</p>"},{"location":"api/apiPythontutorial/#new-zealand-customers","title":"New Zealand customers","text":"<p>All requests to Betfair sites from IP addresses located in New Zealand must now call endpoints ending in '.com.au' (requests to '.com' endpoints will be blocked).</p> <p>All sites containing the url 'developer.betfair.com' do not currently have an active alternative 'developer.betfair.com.au' endpoint. This is currently being worked on being rectified by developers at Betfair UK. In the meantime, New Zealand customers are advised to utilise a VPN or VPS with an Australian IP address to access these sites until further advised. </p> <p>Betfair apologises for any inconvenience caused.</p>"},{"location":"api/apiPythontutorial/#quick-links","title":"Quick Links","text":"<p>Here are some other useful links for accessing our API:</p> <ul> <li>How to create an API app key</li> <li>Developer Docs - the official dev docs for Betfair's API</li> <li>Sports API Visualiser - Useful for exploring what the API has to offer</li> <li>Account API Visualiser</li> <li>Examples using <code>betfairlightweight</code></li> <li>There's a more complete list of resources here</li> </ul>"},{"location":"api/apiPythontutorial/#getting-started","title":"Getting Started","text":""},{"location":"api/apiPythontutorial/#setting-up-your-certificates","title":"Setting Up Your Certificates","text":"<p>To use the API securely, Betfair recommends generating certificates. The <code>betfairlightweight</code> package requires this to login non-interactively. For detailed instructions on how to generate certificates on a windows machine, follow the instructions outlined here. For alternate instructions for Windows, or for Mac/Linux machines, follow the instructions outlined here. You should then create a folder for your certs, perhaps named 'certs' and grab the path location.</p>"},{"location":"api/apiPythontutorial/#installing-betfairlightweight","title":"Installing <code>betfairlightweight</code>","text":"<p>We also need to install <code>betfairlightweight</code>. To do this, simply use pip install <code>betfairlightweight</code> in the cmd prompt/terminal. If this doesn't work, you will have to Google your error. If you're just starting out with Python, you may have to add Python to your environment variables.</p>"},{"location":"api/apiPythontutorial/#sending-requests-to-the-api","title":"Sending Requests to the API","text":""},{"location":"api/apiPythontutorial/#log-into-the-api-client","title":"Log into the API Client","text":"<p>Now we're finally ready to log in and use the API. First, we create an APIClient object and then log in. To log in, we'll need to specify where we put our certs. In this example, I'll put them in a folder named 'certs', on my desktop.</p> <p>You'll also need to change the <code>username</code>, <code>password</code> and <code>app_key</code> variables to your own.</p> <p><code>In [206]:</code></p> <pre><code># Import libraries\nimport betfairlightweight\nfrom betfairlightweight import filters\nimport pandas as pd\nimport numpy as np\nimport os\nimport datetime\nimport json\n\n# Change this certs path to wherever you're storing your certificates\n\n# Your credentials.json file should look like this:\n\n# {\n#     \"username\" : \"johnsmith123\",\n#     \"password\" : \"guest\",\n#     \"app_key\" : \"****************\"\n# }\n\nwith open('credentials.json') as f:\n    cred = json.load(f)\n    my_username = cred['username']\n    my_password = cred['password']\n    my_app_key = cred['app_key']\n\ntrading = betfairlightweight.APIClient(username=my_username,\n                                       password=my_password,\n                                       app_key=my_app_key,\n                                       certs=certs_path)\n\ntrading.login()\n\n# if you're having issues with certs, you can login this way without using certificates (at your own risk)\n\n# trading = betfairlightweight.APIClient(username=my_username,\n#                                        password=my_password,\n#                                        app_key=my_app_key\n#                                        )\n\n# trading.login_interactive()\n</code></pre> <p><code>Out[206]:</code></p> <pre><code>&lt;LoginResource&gt;\n</code></pre>"},{"location":"api/apiPythontutorial/#get-event-ids","title":"Get Event IDs","text":"<p>Betfair's API has a number of operations. For example, if you want to list the market book for a market, you would use the listMarketBook operation. These endpoints are shown in the Sports API Visualiser and in the docs. They are also listed below:</p>"},{"location":"api/apiPythontutorial/#sports-api","title":"Sports API","text":"<ul> <li>listEventTypes</li> <li>listCompetitions</li> <li>listTimeRanges</li> <li>listEvents</li> <li>listMarketTypes</li> <li>listCountries</li> <li>listVenues</li> <li>listMarketCatalogue</li> <li>listMarketBook</li> <li>listRunnerBook</li> <li>placeOrders</li> <li>cancelOrders</li> <li>updateOrders</li> <li>replaceOrders</li> <li>listCurrentOrders</li> <li>listClearedOrders</li> <li>listMarketProfitAndLoss</li> </ul> <p>The Account Operations API operations/endpoints can be found here.</p> <p>First we need to grab the 'Event Type Id'. Each sport has a different ID. Below we will find the ids for all sports by requesting the event_type_ids without a filter.</p> <p><code>In [43]:</code></p> <pre><code># Grab all event type ids. This will return a list which we will iterate over to print out the id and the name of the sport\nevent_types = trading.betting.list_event_types()\n\nsport_ids = pd.DataFrame({\n    'Sport': [event_type_object.event_type.name for event_type_object in event_types],\n    'ID': [event_type_object.event_type.id for event_type_object in event_types]\n}).set_index('Sport').sort_index()\n\nsport_ids\n</code></pre> <p><code>Out[43]:</code></p> Sport ID American Football 6423 Athletics 3988 Australian Rules 61420 Baseball 7511 Basketball 7522 Boxing 6 Chess 136332 Cricket 4 Cycling 11 Darts 3503 Esports 27454571 Financial Bets 6231 Gaelic Games 2152880 Golf 3 Greyhound Racing 4339 Handball 468328 Horse Racing 7 Ice Hockey 7524 Mixed Martial Arts 26420387 Motor Sport 8 Netball 606611 Politics 2378961 Rugby League 1477 Rugby Union 5 Snooker 6422 Soccer 1 Special Bets 10 Tennis 2 Volleyball 998917 <p>If we just wanted to get the event id for horse racing, we could use the filter function from <code>betfairlightweight</code> as shown in the examples and below.</p> <p><code>In [50]:</code></p> <pre><code># Filter for just horse racing\nhorse_racing_filter = betfairlightweight.filters.market_filter(text_query='Horse Racing')\n\n# This returns a list\nhorse_racing_event_type = trading.betting.list_event_types(\n    filter=horse_racing_filter)\n\n# Get the first element of the list\nhorse_racing_event_type = horse_racing_event_type[0]\n\nhorse_racing_event_type_id = horse_racing_event_type.event_type.id\nprint(f\"The event type id for horse racing is {horse_racing_event_type_id}\")\n\n# The event type id for horse racing is 7\n</code></pre>"},{"location":"api/apiPythontutorial/#get-competition-ids","title":"Get Competition IDs","text":"<p>Sometimes you may want to get markets based on the competition. An example may be the Brownlow medal, or the EPL. Let's have a look at all the soccer competitions over the next week and filter to only get the EPL Competition ID.</p> <p><code>In [90]:</code></p> <pre><code># Get a datetime object in a week and convert to string\ndatetime_in_a_week = (datetime.datetime.utcnow() + datetime.timedelta(weeks=1)).strftime(\"%Y-%m-%dT%TZ\")\n\n# Create a competition filter\ncompetition_filter = betfairlightweight.filters.market_filter(\n    event_type_ids=[1], # Soccer's event type id is 1\n    market_start_time={\n        'to': datetime_in_a_week\n    })\n\n# Get a list of competitions for soccer\ncompetitions = trading.betting.list_competitions(\n    filter=competition_filter\n)\n\n# Iterate over the competitions and create a dataframe of competitions and competition ids\nsoccer_competitions = pd.DataFrame({\n    'Competition': [competition_object.competition.name for competition_object in competitions],\n    'ID': [competition_object.competition.id for competition_object in competitions]\n})\n</code></pre> <p><code>In [94]:</code></p> <pre><code># Get the English Premier League Competition ID\nsoccer_competitions[soccer_competitions.Competition.str.contains('English Premier')]\n</code></pre> <p><code>Out[94]:</code></p> Competition ID 116 English Premier League 10932509"},{"location":"api/apiPythontutorial/#get-upcoming-events","title":"Get Upcoming Events","text":"<p>Say you want to get all the upcoming events for Thoroughbreads for the next 24 hours. We will use the <code>listEvents</code> operation for this. First, as before, we define a market filter, and then using the betting method from our trading object which we defined earlier.</p> <p><code>In [207]:</code></p> <pre><code># Define a market filter\nthoroughbreds_event_filter = betfairlightweight.filters.market_filter(\n    event_type_ids=[horse_racing_event_type_id],\n    market_countries=['AU'],\n    market_start_time={\n        'to': (datetime.datetime.utcnow() + datetime.timedelta(days=1)).strftime(\"%Y-%m-%dT%TZ\")\n    }\n)\n\n# Print the filter\nthoroughbreds_event_filter\n</code></pre> <p><code>Out[207]:</code></p> <pre><code>{'eventTypeIds': ['7'],\n 'marketCountries': ['AU'],\n 'marketStartTime': {'to': '2018-10-26T22:25:00Z'}}\n</code></pre> <p><code>In [208]:</code></p> <pre><code># Get a list of all thoroughbred events as objects\naus_thoroughbred_events = trading.betting.list_events(\n    filter=thoroughbreds_event_filter\n)\n\n# Create a DataFrame with all the events by iterating over each event object\naus_thoroughbred_events_today = pd.DataFrame({\n    'Event Name': [event_object.event.name for event_object in aus_thoroughbred_events],\n    'Event ID': [event_object.event.id for event_object in aus_thoroughbred_events],\n    'Event Venue': [event_object.event.venue for event_object in aus_thoroughbred_events],\n    'Country Code': [event_object.event.country_code for event_object in aus_thoroughbred_events],\n    'Time Zone': [event_object.event.time_zone for event_object in aus_thoroughbred_events],\n    'Open Date': [event_object.event.open_date for event_object in aus_thoroughbred_events],\n    'Market Count': [event_object.market_count for event_object in aus_thoroughbred_events]\n})\n\naus_thoroughbred_events_today\n</code></pre> <p><code>Out[208]:</code></p> Event Name Event ID Event Venue Country Code Time Zone Open Date Market Count 0 MVal (AUS) 26th Oct 28971066 Moonee Valley AU Australia/Sydney 2018-10-26 07:30:00 24 1 Newc (AUS) 26th Oct 28974559 Newcastle AU Australia/Sydney 2018-10-26 07:07:00 20 2 Bath (AUS) 26th Oct 28974547 Bathurst AU Australia/Sydney 2018-10-26 02:43:00 16 3 Cant (AUS) 26th Oct 28974545 Canterbury AU Australia/Sydney 2018-10-26 07:15:00 16 4 Scne (AUS) 26th Oct 28973942 Scone AU Australia/Sydney 2018-10-26 02:25:00 16 5 Gawl (AUS) 26th Oct 28974550 Gawler AU Australia/Adelaide 2018-10-26 04:00:00 16 6 Gatt (AUS) 26th Oct 28974549 Gatton AU Australia/Queensland 2018-10-26 01:55:00 16 7 GlPk (AUS) 26th Oct 28974562 Gloucester Park AU Australia/Perth 2018-10-26 09:10:00 20 8 Hoba (AUS) 26th Oct 28974563 Hobart AU Australia/Sydney 2018-10-26 05:23:00 18 9 Echu (AUS) 26th Oct 28974016 Echuca AU Australia/Sydney 2018-10-26 01:30:00 18 10 Melt (AUS) 26th Oct 28974560 Melton AU Australia/Sydney 2018-10-26 07:18:00 18 11 MVal (AUS) 26th Oct 28921730 None AU Australia/Sydney 2018-10-26 11:00:00 1 12 Redc (AUS) 26th Oct 28974561 Redcliffe AU Australia/Queensland 2018-10-26 02:17:00 16 13 SCst (AUS) 26th Oct 28974149 Sunshine Coast AU Australia/Queensland 2018-10-26 06:42:00 20"},{"location":"api/apiPythontutorial/#get-market-types","title":"Get Market Types","text":"<p>Say we want to know what market types a certain event is offering. To do this, we use the <code>listMarketTypes</code> operation. Let's take the Moonee Valley event from above (ID: 28971066). As this is a horse race we would expect that it would have Win and Place markets.</p> <p><code>In [209]:</code></p> <pre><code># Define a market filter\nmarket_types_filter = betfairlightweight.filters.market_filter(event_ids=['28971066'])\n\n# Request market types\nmarket_types = trading.betting.list_market_types(\n        filter=market_types_filter\n)\n\n# Create a DataFrame of market types\nmarket_types_mooney_valley = pd.DataFrame({\n    'Market Type': [market_type_object.market_type for market_type_object in market_types],\n})\n\nmarket_types_mooney_valley\n</code></pre> <p><code>Out[209]:</code></p> Market Type 0 OTHER_PLACE 1 PLACE 2 WIN"},{"location":"api/apiPythontutorial/#get-market-catalogues","title":"Get Market Catalogues","text":"<p>If we want to know the various market names that there are for a particular event, as well as how much has been matched on each market, we want to request data from the <code>listMarketCatalogue</code> operation. We can provide a number of filters, including the Competition ID, the Event ID, the Venue etc. to the filter.</p> <p>We must also specify the maximum number of results, and if we want additional data like the event data or runner data, we can also request that.</p> <p>For a more comprehensive understanding of the options for filters and what we can request, please have a look at the Sports API Visualiser. The options listed under market filter should be put into a filter, whilst the others should be arguments to the relevant operation function in <code>betfairlightweight</code>.</p> <p>For example, if we want all the markets for Moonee Valley, we should use the following filters and arguments.</p> <p><code>In [210]:</code></p> <pre><code>market_catalogue_filter = betfairlightweight.filters.market_filter(event_ids=['28971066'])\n\nmarket_catalogues = trading.betting.list_market_catalogue(\n    filter=market_catalogue_filter,\n    max_results='100',\n    sort='FIRST_TO_START'\n)\n\n# Create a DataFrame for each market catalogue\nmarket_types_mooney_valley = pd.DataFrame({\n    'Market Name': [market_cat_object.market_name for market_cat_object in market_catalogues],\n    'Market ID': [market_cat_object.market_id for market_cat_object in market_catalogues],\n    'Total Matched': [market_cat_object.total_matched for market_cat_object in market_catalogues],\n})\n\nmarket_types_mooney_valley\n</code></pre> <p><code>Out[210]:</code></p> Market Name Market ID Total Matched 0 4 TBP 1.150090094 0.000000 1 To Be Placed 1.150090092 0.000000 2 R1 1000m 3yo 1.150090091 2250.188360 3 4 TBP 1.150090101 0.000000 4 To Be Placed 1.150090099 141.775816 5 R2 2040m Hcap 1.150090098 1093.481760 6 To Be Placed 1.150090106 0.000000 7 R3 1500m Hcap 1.150090105 1499.642480 8 4 TBP 1.150090108 0.000000 9 To Be Placed 1.150090113 19.855136 10 R4 2040m Hcap 1.150090112 588.190288 11 4 TBP 1.150090115 0.000000 12 4 TBP 1.150090122 0.000000 13 R5 955m Hcap 1.150090119 545.762616 14 To Be Placed 1.150090120 91.920584 15 4 TBP 1.150090129 48.623344 16 To Be Placed 1.150090127 65.616152 17 R6 1200m Hcap 1.150090126 506.342200 18 R7 1200m Grp1 1.150038686 34480.834976 19 4 TBP 1.150038689 701.052968 20 To Be Placed 1.150038687 1504.823656 21 R8 1500m Hcap 1.150090140 232.971760 22 4 TBP 1.150090143 0.000000 23 To Be Placed 1.150090141 73.768352"},{"location":"api/apiPythontutorial/#get-market-books","title":"Get Market Books","text":"<p>If we then want to get the prices available/last traded for a market, we should use the <code>listMarketBook</code> operation. Let's Look at the market book for Moonee Valley R7. We will need to define a function which processes the runner books and collates the data into a DataFrame.</p> <p><code>In [212]:</code></p> <pre><code>def process_runner_books(runner_books):\n    '''\n    This function processes the runner books and returns a DataFrame with the best back/lay prices + vol for each runner\n    :param runner_books:\n    :return:\n    '''\n    best_back_prices = [runner_book.ex.available_to_back[0]['price']\n        if runner_book.ex.available_to_back\n        else 1.01\n        for runner_book\n        in runner_books]\n    best_back_sizes = [runner_book.ex.available_to_back[0]['size']\n        if runner_book.ex.available_to_back\n        else 1.01\n        for runner_book\n        in runner_books]\n\n    best_lay_prices = [runner_book.ex.available_to_lay[0]['price']\n        if runner_book.ex.available_to_lay\n        else 1000.0\n        for runner_book\n        in runner_books]\n    best_lay_sizes = [runner_book.ex.available_to_lay[0]['size']\n        if runner_book.ex.available_to_lay\n        else 1.01\n        for runner_book\n        in runner_books]\n\n    selection_ids = [runner_book.selection_id for runner_book in runner_books]\n    last_prices_traded = [runner_book.last_price_traded for runner_book in runner_books]\n    total_matched = [runner_book.total_matched for runner_book in runner_books]\n    statuses = [runner_book.status for runner_book in runner_books]\n    scratching_datetimes = [runner_book.removal_date for runner_book in runner_books]\n    adjustment_factors = [runner_book.adjustment_factor for runner_book in runner_books]\n\n    df = pd.DataFrame({\n        'Selection ID': selection_ids,\n        'Best Back Price': best_back_prices,\n        'Best Back Size': best_back_sizes,\n        'Best Lay Price': best_lay_prices,\n        'Best Lay Size': best_lay_sizes,\n        'Last Price Traded': last_prices_traded,\n        'Total Matched': total_matched,\n        'Status': statuses,\n        'Removal Date': scratching_datetimes,\n        'Adjustment Factor': adjustment_factors\n    })\n    return df\n</code></pre> <p><code>In [213]:</code></p> <pre><code># Create a price filter. Get all traded and offer data\nprice_filter = betfairlightweight.filters.price_projection(\n    price_data=['EX_BEST_OFFERS']\n)\n\n# Request market books\nmarket_books = trading.betting.list_market_book(\n    market_ids=['1.150038686'],\n    price_projection=price_filter\n)\n\n# Grab the first market book from the returned list as we only requested one market \nmarket_book = market_books[0]\n\nrunners_df = process_runner_books(market_book.runners)\n\nrunners_df\n</code></pre> <p><code>Out[213]:</code></p> Selection ID Best Back Price Best Back Size Best Lay Price Best Lay Size Last Price Traded Total Matched Status Removal Date Adjustment Factor 0 16905731 12.0 65.54 13.0 33.09 12.0 1226.67 ACTIVE None 8.333 1 15815968 6.6 96.64 7.0 9.00 6.6 5858.61 ACTIVE None 14.286 2 9384677 14.0 114.71 15.0 76.71 14.0 964.80 ACTIVE None 6.667 3 8198751 17.5 14.67 19.0 33.02 17.5 940.56 ACTIVE None 5.556 4 9507057 38.0 53.13 100.0 40.22 46.0 224.72 ACTIVE None 3.125 5 21283266 15.0 121.46 19.5 5.56 19.5 1102.37 ACTIVE None 7.692 6 21283267 80.0 37.58 760.0 9.70 760.0 125.30 ACTIVE None 1.087 7 21063807 6.4 1503.62 7.2 50.00 6.6 8011.44 ACTIVE None 13.333 8 21283268 48.0 54.57 60.0 51.93 50.0 150.22 ACTIVE None 2.381 9 21283269 8.8 235.77 9.4 30.40 8.8 1729.96 ACTIVE None 11.111 10 4883975 46.0 33.42 55.0 5.00 46.0 208.45 ACTIVE None 2.381 11 202351 25.0 20.00 30.0 6.00 24.0 658.09 ACTIVE None 2.632 12 21283270 19.5 69.33 22.0 20.00 19.5 825.59 ACTIVE None 4.545 13 21283271 5.3 96.14 5.7 5.03 5.3 12654.32 ACTIVE None 16.871"},{"location":"api/apiPythontutorial/#orderbook-workflow","title":"Orderbook Workflow","text":"<p>Now that we have the market book in an easy to read DataFrame, we can go ahead and start placing orders based on the market book. Although it is a simple (and probably not profitable) strategy, in the next few sections we will be backing the favourite and adjusting our orders.</p>"},{"location":"api/apiPythontutorial/#placing-orders","title":"Placing Orders","text":"<p>To place an order we use the <code>placeOrders</code> operation. A handy component of <code>placeOrders</code> is that you can send your strategy along with the runner that you want to back, so it is extremely easy to analyse how your strategy performed later. Let's place a 5 dollar back bet on the favourite at $7 call this strategy <code>'back_the_fav'</code>.</p> <p>Note that if you are placing a limit order you must specify a price which is allowed by Betfair. For example, the price 6.3 isn't allowed, whereas 6.4 is, as prices go up by 20c increments at that price range. You can read about tick points here.</p> <p><code>In [232]:</code></p> <pre><code># Get the favourite's price and selection id\nfav_selection_id = runners_df.loc[runners_df['Best Back Price'].idxmin(), 'Selection ID']\nfav_price = runners_df.loc[runners_df['Best Back Price'].idxmin(), 'Best Back Price']\n</code></pre> <p><code>In [276]:</code></p> <pre><code># Define a limit order filter\nlimit_order_filter = betfairlightweight.filters.limit_order(\n    size=5, \n    price=7,\n    persistence_type='LAPSE'\n)\n\n# Define an instructions filter\ninstructions_filter = betfairlightweight.filters.place_instruction(\n    selection_id=str(fav_selection_id),\n    order_type=\"LIMIT\",\n    side=\"BACK\",\n    limit_order=limit_order_filter\n)\n\ninstructions_filter\n</code></pre> <p><code>Out[276]:</code></p> <pre><code>{'limitOrder': {'persistenceType': 'LAPSE', 'price': 7, 'size': 5},\n 'orderType': 'LIMIT',\n 'selectionId': '21283271',\n 'side': 'BACK'}\n</code></pre> <p><code>In [277]:</code></p> <pre><code># Place the order\norder = trading.betting.place_orders(\n    market_id='1.150038686', # The market id we obtained from before\n    customer_strategy_ref='back_the_fav',\n    instructions=[instructions_filter] # This must be a list\n)\n</code></pre> <p>Now that we've placed the other, we can check if the order placing was a success and if any has been matched.</p> <p><code>In [306]:</code></p> <pre><code>order.__dict__\n</code></pre> <p><code>Out[306]:</code></p> <pre><code>{'_data': {'instructionReports': [{'averagePriceMatched': 0.0,\n    'betId': '142384852665',\n    'instruction': {'limitOrder': {'persistenceType': 'LAPSE',\n      'price': 7.0,\n      'size': 5.0},\n     'orderType': 'LIMIT',\n     'selectionId': 21283271,\n     'side': 'BACK'},\n    'orderStatus': 'EXECUTABLE',\n    'placedDate': '2018-10-26T00:46:46.000Z',\n    'sizeMatched': 0.0,\n    'status': 'SUCCESS'}],\n  'marketId': '1.150038686',\n  'status': 'SUCCESS'},\n '_datetime_created': datetime.datetime(2018, 10, 26, 0, 46, 46, 455349),\n '_datetime_updated': datetime.datetime(2018, 10, 26, 0, 46, 46, 455349),\n 'customer_ref': None,\n 'elapsed_time': 1.484069,\n 'error_code': None,\n 'market_id': '1.150038686',\n 'place_instruction_reports': [&lt;betfairlightweight.resources.bettingresources.PlaceOrderInstructionReports at 0x23e0f7952e8&gt;],\n 'status': 'SUCCESS'}\n</code></pre> <p>As we can see, the status is <code>'SUCCESS'</code>, whilst the <code>sizeMatched</code> is 0. Let's now look at our current orders.</p>"},{"location":"api/apiPythontutorial/#get-current-orders","title":"Get Current Orders","text":"<p>To get our current orders, we need to use the <code>listCurrentOrders</code> operation. We can then use either the bet id, the market id, or the bet strategy to filter our orders.</p> <p><code>In [311]:</code></p> <pre><code>trading.betting.list_current_orders(customer_strategy_refs=['back_the_fav']).__dict__\nOut[311]:\n{'_data': {'currentOrders': [{'averagePriceMatched': 0.0,\n    'betId': '142384852665',\n    'bspLiability': 0.0,\n    'customerStrategyRef': 'back_the_fav',\n    'handicap': 0.0,\n    'marketId': '1.150038686',\n    'orderType': 'LIMIT',\n    'persistenceType': 'LAPSE',\n    'placedDate': '2018-10-26T00:46:46.000Z',\n    'priceSize': {'price': 7.0, 'size': 5.0},\n    'regulatorCode': 'MALTA LOTTERIES AND GAMBLING AUTHORITY',\n    'selectionId': 21283271,\n    'side': 'BACK',\n    'sizeCancelled': 0.0,\n    'sizeLapsed': 0.0,\n    'sizeMatched': 0.0,\n    'sizeRemaining': 5.0,\n    'sizeVoided': 0.0,\n    'status': 'EXECUTABLE'}],\n  'moreAvailable': False},\n '_datetime_created': datetime.datetime(2018, 10, 26, 2, 14, 56, 84036),\n '_datetime_updated': datetime.datetime(2018, 10, 26, 2, 14, 56, 84036),\n 'elapsed_time': 1.327456,\n 'more_available': False,\n 'orders': [&lt;betfairlightweight.resources.bettingresources.CurrentOrder at 0x23e0e7acd30&gt;],\n 'publish_time': None,\n 'streaming_unique_id': None,\n 'streaming_update': None}\n</code></pre> <p>As we can see, we have one order which is unmatched for our strategy <code>'back_the_fav'</code></p>"},{"location":"api/apiPythontutorial/#cancelling-orders","title":"Cancelling Orders","text":"<p>Let's now cancel this bet. To do this, we will use the <code>cancelOrders</code> operation. If you pass in a market ID it will cancel all orders for that specific market ID, like you can do on the website.</p> <p><code>In [312]:</code></p> <pre><code>cancelled_order = trading.betting.cancel_orders(market_id='1.150038686')\n</code></pre> <p><code>In [328]:</code></p> <pre><code># Create a DataFrame to view the instruction report\npd.Series(cancelled_order.cancel_instruction_reports[0].__dict__).to_frame().T\n</code></pre> <p><code>Out[328]:</code></p> status size_cancelled cancelled_date instruction error_code 0 SUCCESS 5 2018-10-26 06:01:26 betfairlightweight.resources.bettingresources... None"},{"location":"api/apiPythontutorial/#get-past-orders-and-results","title":"Get Past Orders and Results","text":"<p>If we want to go back and look at past orders we have made, there are two main operations for this:</p> <ul> <li><code>listClearedOrders</code> - this operation takes a range of data down to the individual selection ID level, and returns a summary of those specific orders</li> <li><code>listMarketProfitAndLoss</code> - this operation is more specific, and only takes Market IDs to return the Profit/Loss for that market Alternatively, we can use the <code>getAccountStatement</code> operation from the Account Operations API.</li> </ul> <p>Let's now use both Sports API operations based on our previous orders and then compare it to the <code>getAccountStatement</code> operation.</p>"},{"location":"api/apiPythontutorial/#get-cleared-orders","title":"Get Cleared Orders","text":"<p><code>In [346]:</code></p> <pre><code># listClearedOrders\ncleared_orders = trading.betting.list_cleared_orders(bet_status=\"SETTLED\",\n                                                    market_ids=[\"1.150038686\"])\n</code></pre> <p><code>In [371]:</code></p> <pre><code># Create a DataFrame from the orders\npd.DataFrame(cleared_orders._data['clearedOrders'])\n</code></pre> <p><code>Out[371]:</code></p> betCount betId betOutcome eventId eventTypeId handicap lastMatchedDate marketId orderType persistenceType placedDate priceMatched priceReduced priceRequested profit selectionId settledDate side sizeSettled 0 1 142383373022 LOST 28971066 7 0.0 2018-10-26T10:31:53.000Z 1.150038686 MARKET_ON_CLOSE LAPSE 2018-10-26T00:12:03.000Z 5.74 False 5.74 -5.0 21283271 2018-10-26T10:34:39.000Z BACK 5.0 1 1 142383570640 WON 28971066 7 0.0 2018-10-26T00:16:32.000Z 1.150038686 LIMIT LAPSE 2018-10-26T00:16:31.000Z 5.40 False 5.50 5.0 21283271 2018-10-26T10:34:39.000Z LAY 5.0 <p>Note that we can also filter for certain dates, bet ids, event ids, selection ids etc. We can also group by the event type, the event, the market, the runner, the side, the bet and the strategy, which is extremely useful if you're looking for a quick summary of how your strategy is performing.</p>"},{"location":"api/apiPythontutorial/#get-market-profit-and-loss","title":"Get Market Profit and Loss","text":"<p>Now let's find the Profit and Loss for the market. To do this we will use the <code>listMarketProfitAndLoss</code> operation. Note that this function only works with market IDs, and once the website clears the market, the operation will no longer work. However the market is generally up for about a minute after the race, so if your strategy is automated, you can check once if your bet is settled and if it is, hit the <code>getMarketProfitAndLoss</code> endpoint.</p> <p>Because of this, we will check a different market ID to the example above.</p> <p><code>In [406]:</code></p> <pre><code># Get the profit/loss - this returns a list\npl = trading.betting.list_market_profit_and_loss(market_ids=[\"1.150318913\"], \n                                                 include_bsp_bets='true', \n                                                 include_settled_bets='true')\n</code></pre> <p><code>In [410]:</code></p> <pre><code># Create a profit/loss DataFrame\npl_df = pd.DataFrame(pl[0]._data['profitAndLosses']).assign(marketId=pl[0].market_id)\npl_df\n</code></pre> <p><code>Out[410]:</code></p> ifWin selectionId marketId 0 -5.0 10065177 1.150318913 1 14.0 17029506 1.150318913 2 -5.0 5390339 1.150318913 3 -5.0 13771011 1.150318913 4 -5.0 138209 1.150318913 5 -5.0 10503541 1.150318913 6 -5.0 12165809 1.150318913"},{"location":"api/apiPythontutorial/#get-account-statement","title":"Get Account Statement","text":"<p>Another method is to use the <code>getAccountStatement</code>, which provides an overview of all your bets over a certain time period. You can then filter this for specific dates if you wish.</p> <p><code>In [428]:</code></p> <pre><code># Define a date filter - get all bets for the past 4 days\nfour_days_ago = (datetime.datetime.utcnow() - datetime.timedelta(days=4)).strftime(\"%Y-%m-%dT%TZ\")\nacct_statement_date_filter = betfairlightweight.filters.time_range(from_=four_days_ago)\n\n# Request account statement\naccount_statement = trading.account.get_account_statement(item_date_range=acct_statement_date_filter)\n</code></pre> <p><code>In [450]:</code></p> <pre><code># Create df of recent transactions\nrecent_transactions = pd.DataFrame(account_statement._data['accountStatement'])\nrecent_transactions\n</code></pre> <p><code>Out[450]:</code></p> amount balance itemClass itemClassData itemDate legacyData refId 0 -5.0 256.74 UNKNOWN   {'unknownStatementItem': '{\"avgPrice\":3.8,\"bet... 2018-10-28T23:14:28.000Z {'avgPrice': 3.8, 'betSize': 5.0, 'betType': '... 142845441633 1 5.0 261.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.4,\"bet... 2018-10-26T10:34:39.000Z {'avgPrice': 5.4, 'betSize': 5.0, 'betType': '... 142383570640 2 -5.0 256.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.74,\"be... 2018-10-26T10:34:39.000Z {'avgPrice': 5.74, 'betSize': 5.0, 'betType': ... 142383373022 <p><code>In [468]:</code></p> <pre><code># Create df of itemClassData - iterate over the account statement list and convert to json so that the DataFrame function\n# can read it correctly\nclass_data = [json.loads(account_statement.account_statement[i].item_class_data['unknownStatementItem']) \n              for i in range(len(account_statement.account_statement))]\n</code></pre> <p><code>In [471]:</code></p> <pre><code>class_df = pd.DataFrame(class_data)\nclass_df\n</code></pre> <p><code>Out [471]:</code></p> avgPrice betCategoryType betSize betType commissionRate eventId eventTypeId fullMarketName grossBetAmount marketName marketType placedDate selectionId selectionName startDate transactionId transactionType winLose 0 3.80 M 5.0   B None 150318913 7 USA / TPara (US) 28th Oct/ 16:06 R8 1m Allw Claim 0.0 R8 1m Allw Claim O 2018-10-28T23:02:28.000Z 17029506 Gato Guapo 2018-10-28T23:06:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST 1 5.40 E 5.0 L None 150038686 7 AUS / MVal (AUS) 26th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:16:31.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_CREDIT 2 5.74 M 5.0 B None 150038686 7 AUS / MVal (AUS) 26th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:12:03.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST <p>As we can see, this DataFrame provides a much more comprehensive view of each of our bets. However, it lacks the ability to filter by strategy like the <code>listClearedOrders</code> operation in the Sports API.</p>"},{"location":"api/apiResources/","title":"API Resources","text":"<p>There are a wide range of resources available to help make it easier to interact with the Betfair API, including those created by Betfair as well as the wider community.</p> <p>Here are some of the resources we'd recommend taking a look at if you're building a program to interact with either the polling and/or Stream APIs. </p>"},{"location":"api/apiResources/#the-basics","title":"The basics","text":"<ul> <li>Creating &amp; activating your app key</li> <li>Dev Docs</li> <li>FAQ</li> <li>Developer Program knowledge base</li> <li>Developer Forum where you can share your experiences and find out what's worked for other clients</li> <li>Stream API where you can find an intro to the stream api</li> </ul> <p>API access</p> <p>Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support.</p>"},{"location":"api/apiResources/#github","title":"Github","text":"<ul> <li>Our Datascientists' repos for using R and Python to access the API</li> <li>There's an ANZ Betfair Down Under community GitHub repo where you can find sample code, libraries, tutorials and other resources for automating and modelling on the Exchange, including an Awesome List and Knowledge Share with helper functions and guidance on best practice shared by the ANZ automation community. </li> <li>The UK\u2019s Github repo including libraries for other languages</li> </ul>"},{"location":"api/apiResources/#visualisers","title":"Visualisers","text":"<ul> <li>Exchange Sports API visualiser for testing market-related queries</li> <li>Exchange Account API visualiser for testing account-related queries</li> </ul>"},{"location":"api/apiResources/#other-resources","title":"Other resources","text":"<ul> <li>Towards Data Science provide a decent basic walk through of how to log in to the polling API and pull market data, with some interesting commentary along the way.</li> </ul> <p>Betfair Quants Discord Group</p> <p><code>betfair quants</code> is really active Betfair-owned Discord group for people interested in modelling and automation on the Exchange. Please reach out if you'd like an invitation. </p>"},{"location":"api/apiResources/#new-zealand-customers","title":"New Zealand customers","text":"<p>All requests to Betfair sites from IP addresses located in New Zealand must now call endpoints ending in '.com.au' (requests to '.com' endpoints will be blocked).</p> <p>All sites containing the url 'developer.betfair.com' do not currently have an active alternative 'developer.betfair.com.au' endpoint. This is currently being worked on being rectified by developers at Betfair UK. In the meantime, New Zealand customers are advised to utilise a VPN or VPS with an Australian IP address to access these sites until further advised. </p> <p>Betfair apologises for any inconvenience caused.</p>"},{"location":"api/apiRtutorial/","title":"Betfair API tutorials in R","text":"<p>Betfair's API can be easily traversed in R. It allows you to retrieve market information, create/cancel bets and manage your account. Here's a collection of easy to follow API tutorials in R:</p> <ul> <li>Accessing the API using R</li> <li>Get Worldcup Odds</li> <li>AFL Odds PulleR Tutorial</li> </ul>"},{"location":"api/apiRtutorial/#accessing-the-api-using-r","title":"Accessing the API using R","text":""},{"location":"api/apiRtutorial/#set-up-r","title":"Set up R","text":"<ul> <li>What is R?</li> <li>Download and install R \u2013 get the language set up on your computer</li> <li>Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R</li> </ul>"},{"location":"api/apiRtutorial/#new-zealand-customers","title":"New Zealand customers","text":"<p>All requests to Betfair sites from IP addresses located in New Zealand must now call endpoints ending in '.com.au' (requests to '.com' endpoints will be blocked).</p> <p>All sites containing the url 'developer.betfair.com' do not currently have an active alternative 'developer.betfair.com.au' endpoint. This is currently being worked on being rectified by developers at Betfair UK. In the meantime, New Zealand customers are advised to utilise a VPN or VPS with an Australian IP address to access these sites until further advised. </p> <p>Betfair apologises for any inconvenience caused.</p>"},{"location":"api/apiRtutorial/#required-packages","title":"Required Packages","text":"<p>Two R packages are required:</p> <pre><code>library(tidyverse)\nlibrary(abettor)\n</code></pre> <p>The abettor package can be downloaded here. For an in-depth understanding of the package, have a read of the documentation. Instructions are also provided in the sample code.</p>"},{"location":"api/apiRtutorial/#login-to-betfair","title":"Login to Betfair","text":"<p>To login to Betfair, replace the following dummy username, password and app key with your own.</p> <pre><code>abettor::loginBF(username = \"betfair_username\",\n                 password = \"betfair_password\",\n                 applicationKey = \"betfair_app_key\")\n</code></pre> <p>If you don't have a live app key for the API yet take a look at this page.</p>"},{"location":"api/apiRtutorial/#finding-event-ids","title":"Finding Event IDs","text":"<p>In order to find data for specific markets, you will first need to know the event ID. This is easily achieved with the abettor package. </p> <p>To find the event IDs of events in the next 60 days:</p> <pre><code>abettor::listEventTypes(toDate = (format(Sys.time() + 86400 * 60, \"%Y-%m-%dT%TZ\")))\n</code></pre> <p>This will return a DataFrame of the following structure:</p> <pre><code>eventType.id    eventType.name  marketCount\n1   Soccer  1193\n2   Tennis  2184\n7522    Basketball  1\n4   Cricket 37\n7   Horse Racing    509\n61420   Australian Rules    31\n4339    Greyhound Racing    527\n</code></pre>"},{"location":"api/apiRtutorial/#finding-competition-ids","title":"Finding Competition IDs","text":"<p>Once you have the event ID, the next logical step is to find the competition IDs for the event you want to get data for. For example, if you want to find the competition IDs for Australian Rules, you would use the following</p> <pre><code>abettor::listCompetitions(\n  eventTypeIds = 61420,  ## AFL is eventTypeId 61420,\n  toDate = (format(Sys.time() + 86400 * 180, \"%Y-%m-%dT%TZ\")) ## Look ahead until the next 180 days\n)\n</code></pre> <p>This will return the following structured DataFrame:</p> <pre><code>competition.id  competition.name    marketCount competitionRegion\n11516633    Brownlow Medal 2018 3   AUS\n11897406    AFL 78  AUS\n</code></pre>"},{"location":"api/apiRtutorial/#finding-specific-markets","title":"Finding Specific Markets","text":"<p>The next logical step is to find the market that you are interested in. Furthering our example above, if you want the Match Odds for all Australian Rules games over the next 60 days, simply use the Competition ID from above in the following.</p> <pre><code>abettor::listMarketCatalogue(\n  eventTypeIds = 61420,\n  marketTypeCodes = \"MATCH_ODDS\", ## Restrict our search to Match Odds only, not other markets for the same match\n  competitionIds = 11897406,\n  toDate = (format(Sys.time() + 86400 * 60, \"%Y-%m-%dT%TZ\"))\n</code></pre> <p>This returns a large DataFrame object with each market, participants and associated odds.</p>"},{"location":"api/apiRtutorial/#get-world-cup-odds-tutorial","title":"Get World Cup Odds Tutorial","text":"<p>This tutorial walks you through the process of retrieving exchange odds for all the matches from the 2018 FIFA World Cup 2018. This can be modified for other sports and uses.</p> <p>You can run this script in R.</p> <pre><code>###################################################\n### FIFA World Cup Datathon\n### Betfair API Tutorial\n###\n### This script allows you to access the Betfair\n### API and retrive exchange odds for all the \n### matches from the upcoming FIFA World Cup 2018\n###################################################\n\n###################################################\n### Setup\n###################################################\n\n## Loading required packages\nlibrary(tidyverse) ## package for general data manipulation - https://www.tidyverse.org/\nlibrary(abettor) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor\n\n## Enter your Betfair API Credentials below\nbetfair_username &lt;- \"\"\nbetfair_password &lt;- \"\"\nbetfair_app_key &lt;- \"\"\n\n## Login to Betfair - should return \"SUCCESS:\" on successful login\nbetfair_login &lt;- abettor::loginBF(username = betfair_username,\n                                  password = betfair_password,\n                                  applicationKey = betfair_app_key)\n\n###################################################\n## Retrieving all soccer competitions for \n## which markets are currently alive\n## on the Betfair Exchange\n###################################################\n\nall_soccer_markets &lt;- abettor::listCompetitions(\n  eventTypeIds = 1,  ## Soccer is eventTypeId 1,\n  toDate = (format(Sys.time() + 86400 * 60, \"%Y-%m-%dT%TZ\")) ## Look ahead until the next 60 days\n  )\n###################################################\n## Retrieving the competition id\n## for the 2018 World Cup \n###################################################\n\nworld_cup_competition_id &lt;- all_soccer_markets %&gt;%\n  dplyr::pull(competition) %&gt;% ## Extracting the variable competition which is a nested data frame\n  dplyr::filter(name == \"2018 FIFA World Cup\") %&gt;% ## Filtering for the competition we need\n  dplyr::pull(id) ## Extracting the id for the competition we need\n\n###################################################\n## Obtaining all markets that are currently \n## alive on the Betfair Exchange that belong to\n## Competition ID that is mapped to the World Cup \n###################################################\n\nall_world_cup_markets &lt;- abettor::listMarketCatalogue(\n  eventTypeIds = 1, ## Soccer is eventTypeId 1\n  marketTypeCodes = \"MATCH_ODDS\", ## Restrict our search to Match Odds only, not other markets for the same match\n  competitionIds = world_cup_competition_id, ## Restrict our search to World Cup matches only\n  toDate = (format(Sys.time() + 86400 * 60, \"%Y-%m-%dT%TZ\")) ## Look ahead until the next 60 days\n)\n\n###################################################\n## Obtaining the current odds on the Betfair\n## Exchange for all the markets that were \n## obtained in the previous step (World Cup Matches)\n###################################################\n\n## Creating a vector/array of all market ids\nall_world_cup_markets_market_ids &lt;- all_world_cup_markets %&gt;%\n  pull(marketId)\n\n## This function takes in a single market id and returns \n## the current live odds on the Betfair Exchange for that market\nfetch_odds &lt;- function(market_id) {\n\n  ## Retrieving market odds for a single market\n  odds &lt;- abettor::listMarketBook(marketIds = market_id, ##Runs listMarketBook for given market_id\n                                  priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices\n                                  ) %&gt;%\n    pull(runners) %&gt;% ## Extracting the runners field which has details of odds\n    as.data.frame() %&gt;% ## Converting to data frame from list\n    select(lastPriceTraded) %&gt;% ## Extracting team and last matched odds\n    mutate(market_id = market_id) %&gt;% ## Padding market id to the data to make it unique to this match\n    bind_cols(data.frame(outcome = c(\"o_1\",\"o_2\",\"o_3\"))) %&gt;% ## Creating outcome order to maintain consistency\n    spread(outcome, lastPriceTraded) %&gt;% ## Reshaping data to make it 1 row per match\n    rename(team_1_odds = o_1,\n           team_2_odds = o_2,\n           draw_odds = o_3) %&gt;% ##Renaming columns such that all matches can be combined into one data frame\n    select(market_id, team_1_odds, draw_odds, team_2_odds) ## Ordering columns in the right order\n\n  return(odds)\n}\n\n## The code below maps (or loops) each market id in the vector we created\n## above through the fetch_odds function and retrives the market odds \n## into a single data frame\nworld_cup_market_odds &lt;- map_dfr(.x = all_world_cup_markets_market_ids, ##Iterate over market ids\n                                .f = fetch_odds ## through function fetch_odds\n                                ) %&gt;%\n  bind_cols(all_world_cup_markets %&gt;% ## Merge with event names to identify which match odds it is\n              pull(event) %&gt;%\n              select(name)) %&gt;% \n  mutate(team_1 = gsub(\" v .*\",\"\",name), ## Extracting team 1 from match name\n         team_2 = gsub(\".* v \", \"\", name)) %&gt;% ## Extracing team 2 from match name\n  select(team_1, team_2, team_1_odds, draw_odds, team_2_odds) ## Extracting columns that we need\n\n## Writing output to csv file\nwrite_csv(world_cup_market_odds, \"world_cup_market_odds.csv\")\n</code></pre>"},{"location":"api/apiRtutorial/#afl-odds-puller-tutorial","title":"AFL Odds PulleR Tutorial","text":"<p>This tutorial walks you through the process of retrieving exchange odds for the the next round of Australian Rules.</p> <p>You can run this script in R.</p> <pre><code>###################################################\n### AFL Model\n### Betfair API Odds GrabbR\n###\n### This script allows you to access the Betfair\n### API and retrive exchange odds for all the \n### matches for the upcoming round of AFL games\n###################################################\n\n## Loading required packages\nlibrary(tidyverse) ## package for general data manipulation - https://www.tidyverse.org/\nlibrary(abettor) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor\n\n## Login to Betfair - should return \"SUCCESS:\" on successful login\nbetfair_login &lt;- abettor::loginBF(username = 'your_username',\n                                  password = 'your_password',\n                                  applicationKey = \"your_betfair_app_key\")\n\n###################################################\n## Retrieving all AFL competitions for \n## which markets are currently alive\n## on the Betfair Exchange\n###################################################\n\nall_afl_markets &lt;- abettor::listCompetitions(\n  eventTypeIds = 61420,  ## AFL is eventTypeId 61420,\n  toDate = (format(Sys.time() + 86400 * 180, \"%Y-%m-%dT%TZ\")) ## Look ahead until the next 180 days\n)\n###################################################\n## Retrieving the competition id\n## for the regular AFL season\n###################################################\n\nafl_competition_id &lt;- all_afl_markets %&gt;%\n  dplyr::pull(competition) %&gt;% ## Extracting the variable competition which is a nested data frame\n  dplyr::filter(name == \"AFL\") %&gt;% ## Filtering for the competition we need\n  dplyr::pull(id) ## Extracting the id for the competition we need\n\n###################################################\n## Obtaining all markets that are currently \n## alive on the Betfair Exchange that belong to\n## Competition ID that is mapped to the AFL\n###################################################\n\nall_afl_markets &lt;- abettor::listMarketCatalogue(\n  eventTypeIds = 61420, ## AFL is eventTypeId 61420\n  marketTypeCodes = \"MATCH_ODDS\", ## Restrict our search to Match Odds only, not other markets for the same match\n  competitionIds = afl_competition_id, ## Restrict our search to AFL Matches Only\n  toDate = (format(Sys.time() + 86400 * 60, \"%Y-%m-%dT%TZ\")) ## Look ahead until the next 60 days\n)\n\n###################################################\n## Obtaining the current odds on the Betfair\n## Exchange for all the markets that were \n## obtained in the previous step\n###################################################\n\n## Creating a vector/array of all market ids\nall_afl_markets_market_ids &lt;- all_afl_markets %&gt;%\n  pull(marketId)\n\n## This function takes in a single market id and returns \n## the current live odds on the Betfair Exchange for that market\nfetch_odds &lt;- function(market_id) {\n\n  ## Retrieving market odds for a single market\n  odds &lt;- abettor::listMarketBook(marketIds = market_id, ##Runs listMarketBook for given market_id\n                                  priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices\n  ) %&gt;%\n    pull(runners) %&gt;% ## Extracting the runners field which has details of odds\n    as.data.frame() %&gt;% ## Converting to data frame from list\n    select(lastPriceTraded) %&gt;% ## Extracting team and last matched odds\n    mutate(market_id = market_id) %&gt;% ## Padding market id to the data to make it unique to this match\n    bind_cols(data.frame(outcome = c(\"o_1\",\"o_2\"))) %&gt;% ## Creating outcome order to maintain consistency\n    spread(outcome, lastPriceTraded) %&gt;% ## Reshaping data to make it 1 row per match\n    rename(team_1_odds = o_1,\n           team_2_odds = o_2) %&gt;% ##Renaming columns such that all matches can be combined into one data frame\n    select(market_id, team_1_odds, team_2_odds) ## Ordering columns in the right order\n\n  return(odds)\n}\n\n## The code below maps (or loops) each market id in the vector we created\n## above through the fetch_odds function and retrives the market odds \n## into a single data frame\nafl_market_odds &lt;- map_dfr(.x = all_afl_markets_market_ids, ##Iterate over market ids\n                                .f = fetch_odds ## through function fetch_odds\n) %&gt;%\n  bind_cols(all_afl_markets %&gt;% ## Merge with event names to identify which match odds it is\n              pull(event) %&gt;%\n              select(name)) %&gt;% \n  mutate(team_1 = gsub(\" v .*\",\"\",name), ## Extracting team 1 from match name\n         team_2 = gsub(\".* v \", \"\", name)) %&gt;% ## Extracing team 2 from match name\n  select(team_1, team_2, team_1_odds, team_2_odds) ## Extracting columns that we need\n\n## Writing output to csv file\nwrite_csv(afl_market_odds, \"weekly_afl_odds.csv\")\n</code></pre>"},{"location":"api/apiappkey/","title":"How to access the Betfair API","text":"<p>Betfair has it\u2019s own Exchange API. You can use it to programmatically retrieve live markets, automate successful trading strategies or create your own customised trading interface. Professional punters use it for these functions and many more.</p> <p>This guide helps Australian and New Zealand customers with obtaining their Betfair API Key. If you\u2019re outside of these two regions please go to the UK's Developer Program website.</p> <p>There are four steps involved in getting access to our API</p> <ul> <li>Obtain an SSOID token</li> <li>Register your application</li> <li>Obtain your app key</li> <li>Activate your app key</li> </ul> <p>API access</p> <p>Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support.</p>"},{"location":"api/apiappkey/#new-zealand-customers","title":"New Zealand customers","text":"<p>All requests to Betfair sites from IP addresses located in New Zealand must now call endpoints ending in '.com.au' (requests to '.com' endpoints will be blocked).</p> <p>All sites containing the url 'developer.betfair.com' do not currently have an active alternative 'developer.betfair.com.au' endpoint. This is currently being worked on being rectified by developers at Betfair UK. In the meantime, New Zealand customers are advised to utilise a VPN or VPS with an Australian IP address to access these sites until further advised. </p> <p>Betfair apologises for any inconvenience caused.</p>"},{"location":"api/apiappkey/#find-your-ssoid-token","title":"Find your SSOID token","text":"<p>The simplest way to setup your browser with the SSOID is to follow this link and log in - this will allow for the SSOID to be automatically populated in the next step. </p> <p>After logging in, you\u2019ll be sent to the main Betfair website. Note: it may not show that you\u2019re logged in on the site. You can ignore that. Proceed to step two.</p>"},{"location":"api/apiappkey/#register-your-application","title":"Register your application","text":"<p>Navigate to the API-NG accounts visualiser.</p> <p>If you\u2019ve followed step 1 correctly, your SSOID token should be automatically populated in the visualiser.</p> <p></p> <p>Next click on <code>createDeveloperAppKeys</code> in the left hand navigation.</p> <p>Type in an application name (this is your app key name, so make sure this is unique), then click \u2018Execute\u2019 down the bottom of the page.</p> <ul> <li>Common errors when creating your app key are if the Application Name you\u2019re using isn\u2019t unique (no Betfair customers can have the same Application Name) or if you\u2019re Application Name contains your account username</li> </ul> <p>If you receive an error message saying that your app key couldn\u2019t be created, it\u2019s most likely because you already have one. Use the <code>getDeveloperAppKeys</code> method in the left hand menu to check whether there\u2019s already an app key associated with your account.</p>"},{"location":"api/apiappkey/#find-your-app-key","title":"Find your app key","text":"<p>After your key is created, you should see in the right hand panel your application:</p> <p></p> <p>You\u2019ll notice that two application keys have been created;</p> <ul> <li> <p>Version \u2013 1.0-Delay: is a delayed app key for development purposes</p> </li> <li> <p>Version \u2013 1.0: is the live pricing app key; on yours it should have a status \u2018No\u2019 in Active.</p> </li> </ul> <p>Grab the application key listed for the live price one - for the example above, that is \u2018MkcBqyZrD53V6A..\u2019</p>"},{"location":"api/apiappkey/#activate-your-app-key","title":"Activate your app key","text":"<p>This process will generate two app keys: </p> <ul> <li> <p>A developer key which is designed for development purposes. This has a variable delay of between 1 and 180 seconds, doesn\u2019t show matched volume and doesn\u2019t need to be activated prior to use.</p> </li> <li> <p>A live app key is intended for transacting on the Exchange and should only be used when you\u2019re ready to start placing bets or can no longer test your strategy effectively using the developer key. </p> </li> </ul> <p>Please note that if the live key is used to pull data from the Exchange without corresponding bets being placed a delay may be automatically applied to the live key.</p> <p>If you\u2019re ready to start testing your strategy or placing bets, please contact api@betfair.com.au and we will be happy to assist with activating the live key and implementing your strategy. </p>"},{"location":"automation/","title":"Index","text":"<p>Betfair is one of the only betting platforms in the world that demands winning clients. Unlike bookies, we don\u2019t ban you when you succeed. We need you, and we want you to be able to keep improving your strategies so you win more. </p> <p>We're here to help you in your automation journey, and this site is dedicated to sharing the tools and resources you need to succeed in this journey. </p>"},{"location":"automation/#accessing-our-api","title":"Accessing our API","text":"<p>As you may already know, Betfair has its own API to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software.</p> <p>There are lots of resources available to support you in accessing the API effectively:</p> <ul> <li>Creating &amp; activating your app key</li> <li>Developer Program knowledge base</li> <li>Dev Docs</li> <li>Developer Forum where you can share your experiences and find out what's worked for other clients</li> <li>Exchange Sports API visualiser for testing market-related queries</li> <li>Exchange Account API visualiser for testing account-related queries</li> <li>Our Datascientists' repos for using R and Python to access the API</li> <li>The UK\u2019s Github repo including libraries for other languages</li> </ul> <p>API access</p> <p>Customers are able to access our API to embed it into their programs and automate their strategies If you're a programmer there are lots of resources around to help</p>"},{"location":"automation/#historic-data","title":"Historic Data","text":"<p>We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport \u2013 you can take a look at our explanation of the different data sources if you\u2019re not quite sure where to start. We\u2019ve also shared some tips on learning to create predictive models using this data, which link in with the models shared in the modelling section.</p> <ul> <li>Betfair data sources</li> <li>Accessing the official Historic Data site</li> <li>Historic Data FAQs &amp; sample data</li> <li>Historic Data Specifications</li> <li>API for downloading historic data files (quicker than manually downloading)</li> <li>Sample code for using the historic data download API</li> <li>The Stream API dev docs are the best source of information for interpreting the data from the Historic Data site</li> <li>Historic BSP csv files</li> </ul> <p>Historic Betfair data</p> <p>There is a lot of historical price data available for all makrets offered on the Exchange, ranging from aggregate, market-level csv files to complete JSON recreations of API Stream data</p>"},{"location":"automation/#using-third-party-tools-for-automation","title":"Using third party tools for automation","text":"<p>Whilst the following tools are not custom built for your approach, they do allow you to automate your betting strategies. You just set up specific betting conditions and let the third party application do the work for you. Bet Angel and Gruss Betting Assistant are the most popular third party tools. </p> <p>We\u2019re putting together a collection of articles on how to use some of these third party tools to automate basic strategies, to give you a starting point that you can then work from.</p> <ul> <li>Bet Angel Overview<ul> <li>Ratings automation</li> <li>Market favourite automation </li> <li>Tipping automation</li> <li>Automating multiple simultaneous markets</li> </ul> </li> <li> <p>Gruss</p> <ul> <li>Ratings automation</li> <li>Market favourite automation</li> <li>Automating multiple simultaneous markets</li> </ul> </li> <li> <p>Cymatic Trader</p> <ul> <li>Ratings automation</li> </ul> </li> <li>BF Bot Manager<ul> <li>Double or Bust</li> </ul> </li> </ul>"},{"location":"automation/#data-modelling","title":"Data modelling","text":"<ul> <li>An intro to building a predictive model</li> <li>Open source predictive models built by our in-house Data Scientists<ul> <li>Modelling the Aus Open</li> <li>EPL modelling series</li> <li>AFL modelling series</li> <li>Brownlow modelling tutorial</li> </ul> </li> </ul> <p>Predictive modelling</p> <p>Many of our most successful customers use predictive models as the basis for their betting strategies </p>"},{"location":"automation/#inspiration-information","title":"Inspiration &amp; information","text":"<ul> <li>'Back and Lay' is a subreddit dedicated to discussing trading techniques</li> <li>Our Twitter community is really active </li> <li>Racing Stategy</li> </ul> <p>Some extra info</p> <p>There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy. </p>"},{"location":"automation/#need-extra-help","title":"Need extra help?","text":"<p>If you\u2019re looking for bespoke advice or have extra questions, please contact us at bdp@betfair.com.au. We have a dedicated in-house resource that is here to automate your betting strategies.</p>"},{"location":"automation/EloModelIntro/","title":"Bet Angel Pro: Ratings automation","text":""},{"location":"automation/EloModelIntro/#automating-a-thoroughbred-ratings-strategy-across-multiple-markets-using-bet-angel-pro","title":"Automating a thoroughbred ratings strategy across multiple markets using Bet Angel Pro","text":"<p>Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to some of our other Betfair automation tutorials, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Bet Angel. We've also added some functionality to the spreadsheet to allow you to automate three simultaneous markets in the event that a market is delayed and race times for multiple markets overlap. </p> <p>Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. </p>"},{"location":"automation/EloModelIntro/#-the-plan","title":"- The plan","text":"<p>Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here, incorporating the thoroughbred racing ratings into the auotmation. </p> <p>We'll step through how we went about getting Bet Angel Pro to place bets using the Betfair's Data Scientists' thoroughbred ratings model. Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. </p> <p></p> <p>Resources</p> <ul> <li>Ratings: Betfair's Data Scientists' thoroughbred ratings model</li> <li>Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings </li> <li>Tool: Bet Angel Pro</li> <li>Assigning multiple markets to your Excel worksheets in Bet Angel so you dont miss a race: Betfair automating simultaneous markets tutorial</li> </ul>"},{"location":"automation/EloModelIntro/#-set-up","title":"- Set up","text":"<p>Make sure you've downloaded and installed Bet Angel Pro, and signed in.</p> <p>Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. </p> <p></p>"},{"location":"automation/EloModelIntro/#-downloading-formatting-ratings","title":"- Downloading &amp; formatting ratings","text":"<p>Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. </p> <p></p> <p>Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. </p> <p></p> <p>Copy the ratings data over to the customised Bet Angel template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated.</p> <p></p>"},{"location":"automation/EloModelIntro/#-writing-your-rules","title":"- Writing your rules","text":"<p>As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. </p> <p>We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. </p> <p>This is how we used Excel to implement our set of rules. </p>"},{"location":"automation/EloModelIntro/#-trigger-to-place-bet","title":"- Trigger to place bet","text":"<p>In short, we want to back or lay runners when:</p> <ul> <li>The available to back price is greater than the rating for that runner, then we will back the runner</li> <li>The available to back price is less than the rating for that runner, then we will lay the runner</li> <li>Back market percentage is less than a certain value that we choose</li> <li>The scheduled event start time is less than a certain number of seconds that we choose</li> <li>The event isn't in play </li> </ul>"},{"location":"automation/EloModelIntro/#-using-cell-references-to-simplify-formulas","title":"- Using cell references to simplify formulas","text":"<p>Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement.</p> <p>Cell names used in this tutorial</p> <ul> <li> <p>Ratings refers to the entire Column I in the 'RATINGS' worksheet</p> </li> <li> <p>RunnerName refers to the entire column H in the 'RATINGS' worksheet</p> </li> <li> <p>Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. </p> </li> <li> <p>UserOverround refers to cell H4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. </p> </li> <li> <p>UserTimeTillJump refers to cell H3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market</p> </li> <li> <p>BACKLAY refers to cell H5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners</p> </li> </ul> <p>This is our trigger for the 'BET ANGEL' worksheet:</p> <p>``` excel tab=\"Multi line\" =IF(     AND(         OR(              AND(BACKLAY=\"BACK\",(G9&gt;(INDEX(Ratings,MATCH(B9,RunnerName,0))))),              AND(BACKLAY=\"LAY\",(G9&lt;(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),         Overrounds1&lt;UserOverround,         TimeTillJump1&lt;UserTimeTillJump,         ISBLANK(InPlay1)),         BACKLAY,     \"\" )</p> <pre><code>``` excel tab=\"Single line\"\n=IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(B9,RunnerName,0)))))),Overrounds1&lt;UserOverround,TimeTillJump1&lt;UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\")\n</code></pre> <p>Stepping through each step:</p> <ul> <li>Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell I5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings.</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY=\"BACK\",(G9&gt;(INDEX(Ratings,MATCH(B9,RunnerName,0))))),\n             AND(BACKLAY=\"LAY\",(G9&lt;(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),\n        Overrounds1&lt;UserOverround,\n        TimeTillJump1&lt;UserTimeTillJump,\n        ISBLANK(InPlay1)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found on the Hub.</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY=\"BACK\",(G9&gt;(INDEX(Ratings,MATCH(B9,RunnerName,0))))),\n             AND(BACKLAY=\"LAY\",(G9&lt;(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),\n        Overrounds1&lt;UserOverround,\n        TimeTillJump1&lt;UserTimeTillJump,\n        ISBLANK(InPlay1)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell F3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. </li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY=\"BACK\",(G9&gt;(INDEX(Ratings,MATCH(B9,RunnerName,0))))),\n             AND(BACKLAY=\"LAY\",(G9&lt;(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),\n        Overrounds1&lt;UserOverround,\n        TimeTillJump1&lt;UserTimeTillJump,\n        ISBLANK(InPlay1)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <p>Calculating the time until the jump for multiple markets at the same time</p> <p>One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E.</p> <p>In the 'BET ANGEL' worksheet, the formulas will be written <code>TimeTillJump1&lt;UserTimeTillJump,</code> while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written <code>TimeTillJump2&lt;UserTimeTillJump,</code> and <code>TimeTillJump3&lt;UserTimeTillJump,</code> respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market.  </p> <ul> <li>Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. </li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY=\"BACK\",(G9&gt;(INDEX(Ratings,MATCH(B9,RunnerName,0))))),\n             AND(BACKLAY=\"LAY\",(G9&lt;(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),\n        Overrounds1&lt;UserOverround,\n        TimeTillJump1&lt;UserTimeTillJump,\n        ISBLANK(InPlay1)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed.</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY=\"BACK\",(G9&gt;(INDEX(Ratings,MATCH(B9,RunnerName,0))))),\n             AND(BACKLAY=\"LAY\",(G9&lt;(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),\n        Overrounds1&lt;UserOverround,\n        TimeTillJump1&lt;UserTimeTillJump,\n        ISBLANK(InPlay1)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <p>updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets</p> <p>You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market.  </p> <ul> <li>Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(B9,RunnerName,0))))),\n             AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(B9,RunnerName,0)))))),\n        Overrounds2&lt;UserOverround,\n        TimeTillJump2&lt;UserTimeTillJump,\n        ISBLANK(InPlay2)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(B9,RunnerName,0))))),\n             AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(B9,RunnerName,0)))))),\n        Overrounds3&lt;UserOverround,\n        TimeTillJump3&lt;UserTimeTillJump,\n        ISBLANK(InPlay3)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <p>Excel functions</p> <ul> <li>IF function: IF(if this is true, do this, else do this)</li> <li>AND function: AND(this is true, and so is this, and so is this) - returns true or false</li> <li>And Or function: checks that the statement meets more than one condition. If this OR that, then do the following. </li> <li>Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. </li> </ul>"},{"location":"automation/EloModelIntro/#-preparing-the-spreadsheet","title":"- Preparing the spreadsheet","text":"<p>You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight. </p> <ul> <li>Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner).</li> </ul> <p>``` excel tab=\"Multi line\" =IF(     AND(         OR(              AND(BACKLAY=\"BACK\",(G9&gt;(INDEX(Ratings,MATCH(B9,RunnerName,0))))),              AND(BACKLAY=\"LAY\",(G9&lt;(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),         Overrounds1&lt;UserOverround,         TimeTillJump1&lt;UserTimeTillJump,         ISBLANK(InPlay1)),         BACKLAY,     \"\" ) <pre><code>``` excel tab=\"Single line\"\n=IF(AND(OR(AND(BACKLAY=\"BACK\",(G9&gt;(INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY=\"LAY\",(G9&lt;(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),Overrounds1&lt;UserOverround,TimeTillJump1&lt;UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\")\n</code></pre></p> <p></p> <ul> <li>Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks'. For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. </li> </ul> <p>Note:</p> <p>The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. </p> <p><code>=IF(B9=\"\",\"\",G9)</code></p> <p></p> <ul> <li>Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. </li> </ul> <p><code>=IF(B9=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(G9-1),stake*(H9/(H9-1))-stake))</code></p> <p></p>"},{"location":"automation/EloModelIntro/#-connecting-to-bet-angel","title":"- Connecting to Bet Angel","text":""},{"location":"automation/EloModelIntro/#video-walk-through","title":"Video walk through","text":"<p>We've put together a litte video walk through to help make this process easier. </p>"},{"location":"automation/EloModelIntro/#-selecting-markets","title":"- Selecting markets","text":"<p>We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets.</p> <p>Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. </p> <p>Make sure you sort the races by start time, so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. </p> <p>You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets.</p> <p></p> <p>The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature. </p>"},{"location":"automation/EloModelIntro/#-linking-the-spreadsheet","title":"- Linking the spreadsheet","text":"<p>Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. </p> <p></p>"},{"location":"automation/EloModelIntro/#and-youre-set","title":"And you're set!","text":"<p>Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off.</p> <p>Note:</p> <p>You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.</p>"},{"location":"automation/EloModelIntro/#bet-angel-features","title":"Bet Angel features","text":"<p>Here are some Bet Angel features that you'll need to consider.</p>"},{"location":"automation/EloModelIntro/#-multiple-betsclearing-status-cells","title":"- Multiple bets/clearing status cells","text":"<p>The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. </p> <p>As we want to use each worksheet over and over again for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel. If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs. </p>"},{"location":"automation/EloModelIntro/#-turning-off-bet-confirmation","title":"- Turning off bet confirmation","text":"<p>Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. Bet Angel allows you to save different settings configurations as profiles. Depending what you are wanting to use Bet Angel for each time you open it up, you can select the appropriate setting profile to suit your needs without having to go through and change them every time. </p> <p></p>"},{"location":"automation/EloModelIntro/#-editing-the-spreadsheet","title":"- Editing the spreadsheet","text":"<p>The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits. </p>"},{"location":"automation/EloModelIntro/#areas-for-improvement","title":"Areas for improvement","text":"<p>There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. </p>"},{"location":"automation/EloModelIntro/#what-next","title":"What next?","text":"<p>We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.</p>"},{"location":"automation/EloModelIntro/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/GeeksToybasicmarketviewstakingandoneclickbetting/","title":"Geeks Toy: Setup basic market view and one click betting","text":""},{"location":"automation/GeeksToybasicmarketviewstakingandoneclickbetting/#setup-basic-market-view","title":"Setup basic market view","text":"<ol> <li> <p>Select market type and location\u00a0</p> <p> </p> </li> <li> <p>Expand Track/Meet/Game and select Market\u00a0</p> <p> </p> </li> <li> <p>This will open the market in a new window\u00a0</p> <p> </p> </li> </ol> <p>Setting Default Stake </p> <p>Click Staking and Tools\u00a0</p> <p> </p> <p>Set the desired default stake.\u00a0</p> <p>Laying for default liability </p> <p>Click the coloured button to the right of the stake and select \u201cLiability\u201d\u00a0</p> <p> </p> <p>Set the desired default Liability.\u00a0</p>"},{"location":"automation/GeeksToybasicmarketviewstakingandoneclickbetting/#setup-one-click-betting","title":"Setup one click betting","text":"<p>Make sure you are happy to not confirm bets before turning this off!</p> <p>Open a Market and right click the top then go to:</p> <ul> <li>Advanced Betting \u2013 Available Stake Click \u2013 One Click\u00a0</li> <li>And\u00a0Advanced Betting \u2013 Price Click \u2013 One Click\u00a0</li> </ul> <p> </p> <p>This will allow you to instantly place a bet with one click with your default Stake/Liability by clicking on a runner in the market.\u00a0</p>"},{"location":"automation/GeeksToybasicmarketviewstakingandoneclickbetting/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/GeeksToyinstallationandsetup/","title":"Geeks Toy: Installation and setup","text":"<p>Geeks Toy is a 3rd\u00a0party program that\u00a0uses the Betfair API. Essentially, it is a dedicated platform that can\u00a0utilise\u00a0the\u00a0Exchange in ways the website and mobile app cannot. It is built to be able to enhance the experience of the\u00a0Exchange for the punters and potentially increase the amount the punters benefit from the Exchange. This guide aims to help get a better look into Geeks Toy and how to set it up and take a further look into the features that it offers.\u00a0</p>"},{"location":"automation/GeeksToyinstallationandsetup/#installation","title":"Installation","text":"<p>So, Geeks Toy sounds like something to check out, but how is it installed on my device? It is worth noting that Geeks Toy is only available on a computer or laptop and is not available for mobile devices.\u00a0With that being said, let\u00a0us look into how to install Geeks Toy.\u00a0</p> <ol> <li>Open your browser of\u00a0choice.\u00a0</li> <li> <p>Either type \u201cGeeks Toy\u201d into your browser or in your URL bar, type in\u00a0www.geekstoy.com </p> <p> View of the Search engine results and the correct link to click on. </p> </li> <li> <p>Welcome to Geeks Toy! Feel free to look around and explore the website and when ready, click on the download button.\u00a0</p> <p></p> <p>The main screen of Geeks Toy. Download button highlighted. </p> </li> <li> <p>Next, click the \u201cDownload\u00a0for Betfair\u201d button, this will start a download for the program installer (commonly called the install wizard).\u00a0</p> <p></p> <p>Download page and install wizard download highlighted. </p> </li> <li> <p>Follow the prompts on your install wizard and once complete, open Geeks Toy.\u00a0</p> </li> <li>Sign in using your Betfair login details to access Geeks Toy. One thing to be aware of, you can set the program to operate under a training mode, so you can get a better understanding of Geeks Toy before betting from your actual Betfair account. You can change it from this initial login screen under \u201cOperation Mode.\u201d\u00a0Once logged in, you have successfully installed\u00a0and are now able to use\u00a0Geeks Toy.</li> </ol> <p> </p>"},{"location":"automation/GeeksToyinstallationandsetup/#basic-set-up-and-settings","title":"Basic set up and settings","text":"<p>Congratulations, now you have access to one of Betfair\u2019s most popular 3rd\u00a0party apps.\u00a0But where do you go from here? Next, we will look at the basic set up and settings that Geeks toy has to offer so that you may be able to\u00a0maximise\u00a0your experience using Geeks Toy.\u00a0</p>"},{"location":"automation/GeeksToyinstallationandsetup/#market-navigator","title":"Market Navigator","text":"<p>The first screen you will see\u00a0once logged in is the \u201cMarket Navigator\u201d screen.\u00a0 </p> <p>Market Navigator Screen </p> <p>This is the main screen\u00a0you will see when logging into Geeks Toy. From here You will be able to access all the markets that are available via the Betfair Exchange. By navigating through the menus, you will be able to find any horse race, any soccer or AFL match that you would normally find through\u00a0the Betfair website.\u00a0From this screen, you will also be able to select which market you would like to be involved in the event. You can also access a whole host of other options from this screen by right clicking.\u00a0  Market Navigator Screen with more options by right clicking.</p>"},{"location":"automation/GeeksToyinstallationandsetup/#customizable-widgets-and-interface","title":"Customizable widgets and interface","text":"<p>A list of potential widgets ready to customise. </p> <p>A unique and\u00a0attractive feature of Geeks Toy is its fully customisable interface in that you can pick and choose what you would like to see, make it as big or small as you like. This allows\u00a0for an experience and for punting that suits your needs.\u00a0</p>"},{"location":"automation/GeeksToyinstallationandsetup/#profiles-can-be-saved-and-opened-after-creation","title":"Profiles can be saved and opened after creation","text":"<p>Once you have gotten your ideal view or setup, you may realise that this is ideal for horse racing, but not so much for a soccer game. You can then choose to save your horse racing \u201cprofile\u201d so that you may create another view that is better for soccer, which when created you can save as your soccer \u201cprofile\u201d. The profile in this case is the way you have manipulated the widgets to get the specific experience you are wanting to achieve. The handiness of this feature is it allows quick and easy access between different profiles, and more importantly, not having to construct the profile each time. If you are having troubles figuring out a profile to begin with, click\u00a0here\u00a0to watch pro Geeks Toy trader,\u00a0Caan\u00a0Berry, do a tutorial on setting up a profile.\u00a0</p> <p></p> <p>A view of where to save and load profiles menu.</p>"},{"location":"automation/GeeksToyinstallationandsetup/#ladder-view-of-the-markets","title":"Ladder\u00a0view of the markets","text":"<p>One of the other features that Geeks Toy offers is the \u201cLadder view\u201d of the markets. Essentially, the ladder view is beneficial to punters as it can show not only the best back and lay prices, as well as the next 2 best prices on both sides, but it goes into greater depth in that it is able to go back further and still show how much money is available. This then helps give a better idea of how the selection has traded thus far and where the money is currently, so that you may be able to get those better odds, all refreshing with Geeks Toy's impressive 200 millisecond refresh rate.\u00a0\u00a0</p> <p></p>"},{"location":"automation/GeeksToyinstallationandsetup/#market-selection","title":"Market Selection","text":"<p>Now that you have\u00a0set up some profiles and played with the settings, now it is time to select the market you wish to bet into.\u00a0</p> <ol> <li> <p>On the Market Navigator screen, select which Sport or Race type (Horse Racing or Greyhounds)\u00a0</p> <p></p> </li> <li> <p>Once you have selected the Sport or Race type you would like, now it is time to choose a league or racecourse.\u00a0</p> <p> </p> <p>Horse Racing -&gt; Great Britain Races -&gt; Newbury Races, 13th\u00a0May. </p> </li> <li> <p>Lastly, is to select the Match or Race that you are wanting to bet into. Also, when doing this making sure you are in the correct market within that Match or Race, for example, Winners Market or Place Market for races and Match Odds or Handicap Market for sports.\u00a0</p> <p> </p> <p>Newbury Races, 13th\u00a0May -&gt; Select race number. </p> </li> <li> <p>Now that you have selected your desired market, you are able to place the bet you want and start trading on the Exchange.\u00a0\u00a0</p> <p> </p> <p>The\u00a0Race Market Grid View \u2013 Alternative to the Ladder View. </p> </li> </ol>"},{"location":"automation/GeeksToyinstallationandsetup/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/GoldenRulesofAutomation/","title":"Golden Rules of Automation","text":"<p>| Some rules of thumb to help on your modelling &amp; automation journey</p> <p>So, you're interested in modelling and automation? That's great to hear, but between having that interest and actually developing a fully working automated betting strategy is often a lot of effort, mistakes and iterative learning. Here we share some of our internal 'golden rules' of automation  - those lessons we've learnt ourselves or heard from others in the community  - in the hope that this might help to expedite your automation journey and help you avoid making some of the same mistakes we have along the way! As a high level philosophy, we suggest taking a risk averse approach to modelling and automation, avoiding bias in your model and back testing, being conservative in your staking and setting your automation up with fail safes and, if in doubt, to simply not bet. Hopefully this philosophy will stand you in good stead on your automation journey!</p>"},{"location":"automation/GoldenRulesofAutomation/#modelling","title":"Modelling","text":"<p>Obviously before you can have an automation you need a model, of one form or another. Although this isn't the main focus of this article we do have several tutorials written by our in-house data scientists and modellers from the wider community that might be valuable:</p> <ul> <li>Greyhound modelling using form data in Python</li> <li>Modelling the Aus Open</li> <li>EPL ML walk through in Python</li> <li>AFL modelling in Python</li> </ul>"},{"location":"automation/GoldenRulesofAutomation/#back-testing","title":"Back testing","text":"<p>Once you've developed the first cut of your model you're going to want to make sure you test your strategy using historical data before you start betting with real money. Although you can never know exactly how a strategy is going to play out in the real world, particularly in terms of the 'butterfly effect' that your activity might have on the wider market, there is a lot you can do to test your theories before putting them into practice.</p> <ul> <li>We've put together several tutorials on how to use the JSON historical Betfair pricing data which is available back to 2016, including one specifically focused on back testing: Back testing ratings in Python</li> <li>To prevent data leakage it's important to make sure to only use data that is available before the outcome of the event you are modelling begins e.g. the BSP is only known after a Horse Racing market goes in play, therefore make sure you don't include the BSP of the race you're modelling, as you won't have it before the jump when you might be looking to bet. It sounds simple but it's one that catches the best of us more often than we'd like to admit!</li> <li>When back testing make sure to partition your dataset:<ul> <li>It is common in data science to split your dataset into training, testing and validation sets, this way you can create and train your strategy on the training dataset and test and validate your strategy on separate datasets.</li> <li>Strategies can often become overfitted to the dataset they were trained on, leading to strategies that may not be generalizable or don't hold in real life. Making sure you separate your testing and validation datasets will help mitigate this risk by back testing your strategy on an out of sample data set.</li> </ul> </li> <li>If you don't have the data you need to back test your strategy reach out to Data@betfair.com.au and we'll see what we can do!</li> </ul>"},{"location":"automation/GoldenRulesofAutomation/#staking","title":"Staking","text":"<p>Hand in hand with your back testing you need to develop a staking strategy. Different people can bet the same model and get very different results, and those differences come down to the betting and staking strategies you use to implement your model. Here are some things to consider when developing your staking and bank management strategy: </p> <ul> <li>The staking approach that you use should mirror the staking approach used when back testing your strategy</li> <li>If you want to change your staking you should consider re-running your back testing before going live with any changes.</li> <li>Is there a maximum stake size you want to have on any given bet/selection? <ul> <li>If you are using a pure Kelly staking approach you may end up putting a large portion of your account onto a single selection - capped Kelly is a potential option here. </li> </ul> </li> <li>Minimum bet sizes<ul> <li>If you are using Kelly or some variable staking measure it's important to note that Betfair has minimum bet sizes (AUD5 unless you have minimum bets removed from your app key for testing, when it's then 1p), and lay bets have minimum bet liabilities. </li> </ul> </li> <li>If your strategy is creating ratings, it might be worth checking the difference between your rated price and the market price; if your ratings are significantly different to the market odds close to the jump then a general rule of thumb is that your model is probably missing something as the market is (generally speaking), the best source of truth. This is particularly important to consider when your staking strategies increase your stake size based on the size of your edge e.g. Kelly. There are a couple of ways of handling this, one way of doing so is to set a maximum stake size, effectively capping the size of your bets, and another to stay out of the market if the difference is too significant. </li> <li>What impact might your strategy have on the market? For example, if you're trying to place large stakes on longer prices you may crunch the price in or not get fully matched, either of which would not be in keeping with your back testing and lead to unexpected variance.</li> </ul>"},{"location":"automation/GoldenRulesofAutomation/#automation","title":"Automation","text":""},{"location":"automation/GoldenRulesofAutomation/#choose-a-tool-that-suits-the-job","title":"Choose a tool that suits the job","text":"<p>Depending on the complexity of your strategy and your skillset. If you can achieve what you need using one of the many tools that exist for automation on the Exchange, great! If not, and you have some coding skills, probably the next best option is to see whether there's a code library around in a language that you're familiar with. We've put together a collection of some of the most popular libraries and repos for interacting with the Exchange APIs that might help you decide where to start. </p>"},{"location":"automation/GoldenRulesofAutomation/#code-modularization","title":"Code modularization","text":"<p>It's easy to dive straight into it when you're building a bot, but being intentional in your design can save you grief in the long term. Writing modular code will help create a more robust pipeline that makes it easier to diagnose errors when they occur and deploy fixes and is much easier to maintain. It's also worth commenting your code as you go (we know, do as we say, not as we do!), but it really makes it easier to come back to your code down the track and understand what your logic is doing. The same principle also applies to using existing tools.</p>"},{"location":"automation/GoldenRulesofAutomation/#logging","title":"Logging","text":"<p>Your bot will need debugging, guaranteed, and having a decent logging functionality in place from the beginning will save you grief during the process. Consider implementing a function to log important information as you need to check it, such as the ratings/predictions generated by your model, bet placement attempts from your bot and other relevant fields. This will be really helpful for:</p> <ul> <li>Diagnosing where and how things are going wrong </li> <li>often you can see there's an issue, but you don't know where or what the issue is.</li> <li>Optimizing your automation program in the future </li> <li>Logging your model's predictions can also be useful to further evaluate your strategy once you go live so it's worth saving them for later use (more on this under the Monitoring section).</li> </ul>"},{"location":"automation/GoldenRulesofAutomation/#automatic-stop-manual-override","title":"Automatic Stop &amp; Manual Override","text":"<p>The best and worst thing about automated bots is that they work autonomously, but that also means that at times they can do things you don't want them to do when you're not actively monitoring them. To this end it's worth considering implementing a function that stops your automation from placing bets if a certain situation arises, for example if the same bet is placed multiple times in a short amount of time. This can help to minimise the potential risks and damage of automation bugs. Similarly, creating a manual override function that you can use at any time in case of emergencies to instantly stop all bet placement and/or to cancel all unmatched bets may also be worth considering. Worst case this can be a kill switch, but you need to be prepared in advance for these situations, and they will happen, so, you want to make sure you can resolve them with as little negative impact as possible. </p>"},{"location":"automation/GoldenRulesofAutomation/#going-live","title":"Going Live","text":""},{"location":"automation/GoldenRulesofAutomation/#start-small","title":"Start Small","text":"<p>If you have an edge that's worth having it's unlikely that it will disappear overnight, and you really should consider starting with small stakes when you're implementing a new model or strategy to ensure that reality matches theory before you stake up. There are a few reasons to start small before considering scaling up:</p> <ul> <li>This allows you to evaluate your strategies live without too much skin in the game. It's really easy to have a live bot that doesn't track against the back testing performance, often because there was a flaw in the logic used to back test or there was too much biasing in the model. </li> <li>This also ensures your bet placement system is working correctly with minimal risk; there are too many stories of large multimillion dollar, financial trading companies who have fallen to trading errors which have wiped them out e.g., Knight Capital - you don't want to make the same mistake!</li> <li>Starting small also allows you to gain confidence that your program and strategies are behaving as expected before you start putting more substantial stakes through the system.</li> </ul>"},{"location":"automation/GoldenRulesofAutomation/#testing","title":"Testing","text":"<p>When you first turn on your automation on it can be a good idea to keep an eye on your bot to make sure it's doing what you expect it to do:</p> <ul> <li>This will give you a better understanding of how your automation works in practice, and you can quickly pick up where things work differently to expected.</li> <li>This is especially important for strategies which place multiple bets and cancel bets allowing you to optimise code to trade more or less aggressively.</li> <li>You will also be able to catch if there are any errors in your logic or bet placement before it has a chance to cause you too much grief.</li> </ul>"},{"location":"automation/GoldenRulesofAutomation/#keep-a-small-account-balance","title":"Keep a Small Account Balance","text":"<p>At the end of the day, in the worst-case scenario you can't lose more than the balance that's in your betting account, so it can be worth considering limiting the available balance while you're testing a new strategy or bot just to be on the safe side. There are a few things to consider:</p> <ul> <li>Make sure to do a quick estimate of how much working capital is required to cover your expected liabilities - you equally don't want your bot to run out of available funds when you do want bets to go on.</li> <li>An alternative approach to this is to use the <code>customerStrategyRef</code> field in the API to cap the amount you can have on any given strategy. </li> <li>Once you have tested your system and are happy with how it's working you can increase your bank size.</li> </ul>"},{"location":"automation/GoldenRulesofAutomation/#monitoring","title":"Monitoring","text":"<p>Congratulations, you made it and you've got your model up and running all on its own, fantastic! So, what's next? Despite the temptation, neither model or automated strategies should really be left to their own devices long term without regular monitoring and sense checks, to make sure that your model is still solid and nothing has changed in your data or the wider market to undermine your potential edge. </p>"},{"location":"automation/GoldenRulesofAutomation/#variance","title":"Variance","text":"<p>Any long-term strategy is likely to see variance across time, and it is challenging to know when a period of poor performance is expected variance or whether it's a result of something changing in your data, model efficacy or the wider market. A couple of things to consider: - Regularly compare your back tests against reality, i.e., for the last 30 days what did your simulation say your outcomes should be, and how does that compare with what really happened? If these are significantly different you may want to do some digging.  - Consider what time period you want to review your automation, once it's established. Some people check daily, others weekly, and others again look at it monthly. Whatever your preference, this helps to give you a framework to work against and regular accountability. </p>"},{"location":"automation/GoldenRulesofAutomation/#resources","title":"Resources","text":"<p>If you want to learn more, here are some resources that might be valuable:</p> <ul> <li>Meet ups and workshop recordings</li> <li>API resources</li> <li>Historic pricing data</li> <li>Modelling tutorials</li> </ul>"},{"location":"automation/GoldenRulesofAutomation/#over-to-you","title":"Over to you!","text":"<p>Building and automating a model on the Exchange can be great fun, and hopefully some of this resonates with you or you can apply within your automation journey. If we can help along the way please reach out!</p>"},{"location":"automation/GoldenRulesofAutomation/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/GrussSettingupbasicmarketview/","title":"Gruss Betting Assistant: Setting up basic market view and one click betting","text":""},{"location":"automation/GrussSettingupbasicmarketview/#setting-up-basic-market-view","title":"Setting up basic market view","text":"<p>These steps provided below are a simple way to navigate into the market\u2019s you wish to place bets or trade in.\u00a0When Gruss is opened you will see the market tree on the\u00a0left hand\u00a0side of the program.\u00a0</p> <p>Step 1.\u00a0Select what market you would like to bet in.\u00a0</p> <p> </p> <p>Step 2.\u00a0What event/track you would like to bet in.\u00a0</p> <p> </p> <p>Step 3.\u00a0Select the market you wish to place a bet in.\u00a0</p> <p> </p>"},{"location":"automation/GrussSettingupbasicmarketview/#quick-pick-list","title":"Quick pick list","text":"<p>Quick pick list has been filtered from the main markets to allow quicker access into your markets without having to manually search for them.\u00a0\u00a0</p> <p> </p> <p>There are numerous ways to add markets to your quick picks.\u00a0</p> <ol> <li>Click and drag the market to the quick pick list.\u00a0</li> <li>Click into the market you want to enter and once it is highlighted press Q.\u00a0</li> <li>Once you are in a market, you can\u00a0highlight\u00a0the market selection at the top of Gruss, select add to quick pick list. Then select this market and it will add it to your quick picks.\u00a0</li> </ol> <p> </p> <p>When you have added markets to quick pick they will appear like this:</p> <p> </p> <p>Once the markets are over or you no longer need to place bets or trade you can delete the markets out of your quick picks.\u00a0</p> <ol> <li>Click on the market and press\u00a0the key\u00a0\u2018delete\u2019 when it is highlighted.\u00a0</li> <li>Right click on your mouse once the market is highlighted, this will bring up numerous options to clear your Quick pick list. <code>Delete market</code> will remove the market you wish to remove from the Quick pick list.\u00a0</li> </ol> <p> </p> <p>The overall market view is similar to the Exchange,\u00a0however\u00a0with the added Gruss features\u00a0that enable faster\u00a0betting and\u00a0easier\u00a0market overview\u00a0for live betting.\u00a0</p> <p> </p> <ul> <li><code>Selection name</code> \u2013\u00a0name of the teams or selections you wish to bet or trade on.\u00a0</li> <li><code>Rev</code> (reverse) \u2013 this can be selected to make the lay on the left side of the market and the back on the right side of the market.\u00a0</li> <li><code>Dutch</code> \u2013 this can be selected to back multiple selections to get a potential profit on both or multiple selections\u00a0in\u00a0your selected market.\u00a0</li> <li><code>Price history</code> \u2013 This\u00a0is a graph that shows the movement of the back or lay prices throughout the match.\u00a0</li> <li><code>WOM</code> (Weight of Money) \u2013 This is a\u00a0calculation\u00a0of the three amounts available to back and lay divided by each other. If\u00a0it is\u00a0pink it may indicate that the price is likely to go out so you can lay and back later at a higher price. If its\u00a0blue,\u00a0then you may be able to lay it at a lower price.\u00a0</li> <li><code>Blue arrows</code> \u2013 these can be used to move your odds down or up to place your bets at\u00a0higher or lower odds. Once you change the odds using the arrows it will stay fixed at those odds and will not update. The best price will continue to update as normal.\u00a0</li> <li><code>Percentage of market</code> -\u00a0the book % for the back odds and likewise above the best lay odds. You can right click on these percentages to sort the grid either by the back odds or lay odds in ascending or\u202fdescending\u202forder.\u00a0</li> <li><code>Back stake</code> \u2013 This is how much you are willing to back on the selection.\u00a0</li> <li><code>Lay Stake</code> \u2013 This is how you are willing to lay on the selection.\u00a0</li> <li><code>Profit</code> \u2013 This will show you your potential profit if that selection were to win depending on if you have backed or laid the selection.\u00a0</li> <li><code>Level Profit</code> \u2013 This can also be referred to as a green book or greening up. You can hover the mouse over the level profit column selection to see if there is\u00a0a\u00a0opportunity to lock in profit or minimise your loses. The column will then display the potential that can be taken against each selection in the market.\u00a0</li> </ul>"},{"location":"automation/GrussSettingupbasicmarketview/#one-click-betting","title":"One click betting","text":"<p>Make sure you are happy to not confirm bets before turning this off!</p> <p>By unticking the\u00a0\u2018verify bets\u2019 box\u00a0it\u00a0will allow you to bet into the market without having to verify your bets,\u00a0allowing the one click betting feature\u00a0to be enabled. This can be used to place bets into the market fast and\u00a0efficiently\u00a0especially if there are small amounts of liquidity in the market.\u00a0</p> <p> </p> <p>To stop the one click betting feature all you have to do is click the \u2018verify bets\u2019 box which will now bring up a verify bets page. This feature allows you to verify and ensure you are placing the correct bets into the market.\u00a0</p> <p></p>"},{"location":"automation/GrussSettingupbasicmarketview/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/Kelly/","title":"Kelly Criterion staking","text":""},{"location":"automation/Kelly/#automating-with-kelly-staking-method","title":"Automating with Kelly staking method","text":"<p>In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters.</p> <p>Bet Angel Pro and Gruss have a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Bet Angel and Gruss - we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. </p>"},{"location":"automation/Kelly/#-the-plan","title":"- The plan","text":"<p>We'll be building on the Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model. For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here, as the concepts and underlying trigger based strategy here do build on what we covered previously. </p> <p>Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. </p> <p>If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read about Kelly staking. There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. </p> <p>Resources</p> <ul> <li>Ratings: Betfair's Data Scientists' thoroughbred ratings model</li> <li>Before you start: check out the Bet Angel Ratings tutorial </li> <li>Rules: here's the spreadsheet for Bet Angel we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy</li> <li>Rules: here's the spreadsheet for Gruss</li> <li>Understanding how the Kelly Criterion staking strategy works</li> <li>Tools: Bet Angel Pro and Gruss Betting Assistant</li> </ul>"},{"location":"automation/Kelly/#-recapping-the-strategy-covered-in-the-ratings-automation-tutorial","title":"- Recapping the strategy covered in the Ratings automation tutorial","text":"<p>We'll be using the same trigger strategy that's outlined in the Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub. Whilst the trigger will remain unchanged, we'll need to make small tweaks to the stake column of the 'BET ANGEL' worksheet (column N) for Bet Angel users or 'MARKET' worksheet (column S) for Gruss users. We've added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Ratings tutorial, we highly recommend that you do so as to understand how the bet placement trigger works. The tutorial can be found here.</p>"},{"location":"automation/Kelly/#-set-up","title":"- Set up","text":"<p>Make sure you've downloaded and installed Bet Angel Pro or Gruss, and signed in. For Bet Angel users, Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. </p> <p>Bet Angel</p> <p></p> <p>Gruss</p> <p></p>"},{"location":"automation/Kelly/#-writing-your-rules","title":"- Writing your rules","text":"<p>We're using a customised version of the Ratings tutorial template for Bet Angel Professional and Gruss Betting Assistant to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. </p> <p>This is how we used Excel to implement our set of rules. </p>"},{"location":"automation/Kelly/#-using-cell-references-to-simplify-formulas","title":"- Using cell references to simplify formulas","text":"<p>Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement.</p> <p>Cell names used in this tutorial</p> <ul> <li>Account Balance refers to cell C6 of the 'BET ANGEL' worksheet or cell I2 of the 'MARKET' worksheet for Gruss users</li> <li>UserStake refers to cell D4 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake</li> <li>StakeType refers to cell X1 of the \"SETTINGS' worksheet</li> <li>Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet</li> <li>HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet</li> </ul> <p>Calculating the Kelly stake</p> <p>As explained here, the formula to claculate the Kelly stake is:</p> <p><code>(BP-Q)/B</code> Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P).</p> <p>To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet.  </p> <p></p>"},{"location":"automation/Kelly/#stepping-through-each-step","title":"Stepping through each step:","text":"<ul> <li>Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('BET ANGEL' worksheet) and return the best available back odds from the G column</li> </ul> <p><pre><code>=IFERROR(INDEX('Bet Angel'!$E$9:$J$68,MATCH(H2,'Bet Angel'!$B$9:$B$68,0),3),\"\")\n</code></pre> </p> <ul> <li>Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds)</li> </ul> <p><pre><code>=IFERROR(K2-1,\"\")\n</code></pre> </p> <ul> <li>Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability</li> </ul> <p><pre><code>=IFERROR((1/I2),\"\")\n</code></pre> </p> <ul> <li>Column N - Probability of loss % 1 divided by the probability of a win from column M</li> </ul> <p><pre><code>=IFERROR(1-M2,\"\")\n</code></pre> </p> <ul> <li>Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1)</li> </ul> <p><pre><code>=IFERROR(((L2*M2)-N2)/L2,\"\")\n</code></pre> </p> <ul> <li>Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2</li> </ul> <p><pre><code>=IFERROR(O2/2,\"\")\n</code></pre> </p> <ul> <li>Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Bet Angel populates in cell C6 of the 'BET ANGEL' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0.</li> </ul> <p><pre><code>=IFERROR(IF(AccountBalance*P2&gt;0,AccountBalance*O2,\"0\"),\"\")\n</code></pre> </p> <ul> <li>Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. </li> </ul> <p><pre><code>=IFERROR(IF(AccountBalance*P2&gt;0,AccountBalance*P2,\"0\"),\"\")\n</code></pre> </p> <ul> <li>Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly.</li> </ul> <p></p> <p></p> <p>Excel functions</p> <ul> <li>IF statement: IF(if this is true, do this, else do this)</li> <li>IFERROR: If there is an error that occurs in the cell, display nothing</li> <li>AND statement: AND(this is true, and so is this, and so is this) - returns true or false</li> <li>Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. </li> </ul>"},{"location":"automation/Kelly/#-preparing-the-spreadsheet","title":"- Preparing the spreadsheet","text":"<p>You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight. </p> <ul> <li>Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet.</li> </ul> <p><code>=IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType))</code></p> <p></p>"},{"location":"automation/Kelly/#and-youre-set","title":"And you're set!","text":"<p>Once you've set your Kelly strategy set up with your strategy, it should only take a number of seconds to load your markets for the day. Just make sure you have all of the Bet Angel settings correctly selected before you leave your to run, as some of them reset by default when you turn Bet Angel off.</p>"},{"location":"automation/Kelly/#areas-for-improvement","title":"Areas for improvement","text":"<p>There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. </p> <p>For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - We missed some races because of this. </p>"},{"location":"automation/Kelly/#what-next","title":"What next?","text":"<p>We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.</p>"},{"location":"automation/Kelly/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/betAngelKellyStake/","title":"Bet Angel Pro: Kelly Criterion staking","text":""},{"location":"automation/betAngelKellyStake/#automating-with-kelly-staking-method-and-bet-angel-pro","title":"Automating with Kelly staking method and Bet Angel Pro","text":"<p>In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters.</p> <p>Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. </p>"},{"location":"automation/betAngelKellyStake/#-the-plan","title":"- The plan","text":"<p>We'll be building on the Bet Angel Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model. For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here, as the concepts and underlying trigger based strategy here do build on what we covered previously. </p> <p>Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. </p> <p>If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read  about Kelly staking. There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. </p> <p>Resources</p> <ul> <li>Ratings: Betfair's Data Scientists' thoroughbred ratings model</li> <li>Before you start: check out the Bet Angel Ratings tutorial </li> <li>Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy</li> <li>Understanding how the Kelly Criterion staking strategy works</li> <li>Tool: Bet Angel Pro</li> </ul>"},{"location":"automation/betAngelKellyStake/#-recapping-the-strategy-covered-in-the-bet-angel-ratings-automation-tutorial","title":"- Recapping the strategy covered in the Bet Angel ratings automation tutorial","text":"<p>We'll be using the same trigger strategy that's outlined in the Bet Angel Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub. The trigger has been simplified in this tutorial and we'll need to make small tweaks to the stake column of the 'BET ANGEL' worksheet (column N). We've also added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Bet Angel ratings tutorial, we highly recommend that you do so as to understand how the concept of the bet placement trigger works. The tutorial can be found here.</p>"},{"location":"automation/betAngelKellyStake/#-set-up","title":"- Set up","text":"<p>Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.  </p>"},{"location":"automation/betAngelKellyStake/#-writing-your-rules","title":"- Writing your rules","text":"<p>We're using a customised version of the Bet Angel Ratings tutorial template to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. </p> <p>This is how we used Excel to implement our set of rules. </p>"},{"location":"automation/betAngelKellyStake/#-using-cell-references-to-simplify-formulas","title":"- Using cell references to simplify formulas","text":"<p>Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement.</p> <p>Cell names used in this tutorial</p> <ul> <li>Account Balance refers to cell C6 of the 'BET ANGEL' worksheet</li> <li>StakeType refers to cell I5 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake</li> <li>Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet</li> <li>HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet</li> </ul> <p>Calculating the Kelly stake</p> <p>As explained here, the formula to calculate the Kelly stake is:</p> <p><code>(BP-Q)/B</code> Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P).</p> <p>To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet.  </p> <p></p>"},{"location":"automation/betAngelKellyStake/#stepping-through-each-step","title":"Stepping through each step:","text":"<ul> <li>Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('BET ANGEL' worksheet) and return the best available back odds from the G column</li> </ul> <p><pre><code>=IFERROR(INDEX('Bet Angel'!$E$9:$J$68,MATCH(H2,'Bet Angel'!$B$9:$B$68,0),3),\"\")\n</code></pre> </p> <ul> <li>Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds)</li> </ul> <p><pre><code>=IFERROR(K2-1,\"\")\n</code></pre> </p> <ul> <li>Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability</li> </ul> <p><pre><code>=IFERROR((1/I2),\"\")\n</code></pre> </p> <ul> <li>Column N - Probability of loss % 1 divided by the probability of a win from column M</li> </ul> <p><pre><code>=IFERROR(1-M2,\"\")\n</code></pre> </p> <ul> <li>Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1)</li> </ul> <p><pre><code>=IFERROR(((L2*M2)-N2)/L2,\"\")\n</code></pre> </p> <ul> <li>Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2</li> </ul> <p><pre><code>=IFERROR(O2/2,\"\")\n</code></pre> </p> <ul> <li>Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Bet Angel populates in cell C6 of the 'BET ANGEL' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0.</li> </ul> <p><pre><code>=IFERROR(IF(AccountBalance*P2&gt;0,AccountBalance*O2,\"0\"),\"\")\n</code></pre> </p> <ul> <li>Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. </li> </ul> <p><pre><code>=IFERROR(IF(AccountBalance*P2&gt;0,AccountBalance*P2,\"0\"),\"\")\n</code></pre> </p> <ul> <li>Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly.</li> </ul> <p></p> <p></p> <p>Excel functions</p> <ul> <li>IF statement: IF(if this is true, do this, else do this)</li> <li>IFERROR: If there is an error that occurs in the cell, display nothing</li> <li>AND statement: AND(this is true, and so is this, and so is this) - returns true or false</li> <li>Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. </li> </ul>"},{"location":"automation/betAngelKellyStake/#-preparing-the-spreadsheet","title":"- Preparing the spreadsheet","text":"<p>You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight. </p> <ul> <li>Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet.</li> </ul> <p><code>=IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType))</code></p> <p></p>"},{"location":"automation/betAngelKellyStake/#and-youre-set","title":"And you're set!","text":"<p>Once you've set your Kelly strategy set up with your strategy, it should only take a number of seconds to load your markets for the day. Just make sure you have all of the Bet Angel settings correctly selected before you leave your to run, as some of them reset by default when you turn Bet Angel off.</p>"},{"location":"automation/betAngelKellyStake/#areas-for-improvement","title":"Areas for improvement","text":"<p>There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. </p> <p>For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - We missed some races because of this. </p>"},{"location":"automation/betAngelKellyStake/#what-next","title":"What next?","text":"<p>We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.</p>"},{"location":"automation/betAngelKellyStake/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/betAngelMarketFavouriteAutomation/","title":"Bet Angel Pro: Market favourite automation","text":""},{"location":"automation/betAngelMarketFavouriteAutomation/#automating-a-market-favourite-strategy-using-bet-angel-pro","title":"Automating a market favourite strategy using Bet Angel Pro","text":"<p>Here we explore how to implement an automated strategy to place Betfair Starting Price (BSP) bets on the top two runners in the market. This lets you choose your selections based on market sentiment close to the jump, and not worry about current market price by using BSP to place your bets. You could equally use effectively the same approach if you wanted to lay the favourite(s) instead of backing them.</p> <p>Building on our previous articles, we're using the spreadsheet functionality available in Bet Angel Pro to implement this strategy. If you haven't already we'd recommend going back and having a read of this article, as the concepts here do build on what we covered previously. As we've said before, there are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions. </p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#-the-plan","title":"- The plan","text":"<p>Given that we're simply choosing our selections based on the market we don't need any ratings for this strategy. The plan is to look at the market a couple of minutes before the scheduled jump and place BSP bets based on its formation. </p> <p>Our approach here, and how we've set up the accompanying spreadsheet, backs the top two runners in the market two minutes out from the scheduled start time using the Betfair Starting Price. </p> <p>Resources</p> <ul> <li>Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and approach</li> <li>Tool: Bet Angel Pro</li> </ul>"},{"location":"automation/betAngelMarketFavouriteAutomation/#-set-up","title":"- Set up","text":"<p>Make sure you've downloaded and installed Bet Angel Pro, and signed in.</p> <p>Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. </p> <p></p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#-writing-your-rules","title":"- Writing your rules","text":"<p>As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. </p> <p>We're using a customised version of the default Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on the favourites being shown in the market. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. </p> <p>This is how we used Excel to implement our set of rules. </p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#-trigger-to-place-bet","title":"- Trigger to place bet","text":"<p>In short, we want to back runners when:</p> <ul> <li>the selection's available to back price (Column G) is either the lowest or second lowest in the market - the top two market favourites with the ability to easily change this</li> <li>the scheduled event start time is less and greater than what we specify</li> <li>Back market percentage is less than a certain value that we choose</li> <li>the event isn't in play </li> </ul>"},{"location":"automation/betAngelMarketFavouriteAutomation/#-using-cell-references-to-simplify-formulas","title":"- Using cell references to simplify formulas","text":"<p>Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement.</p> <p>Cell names used in this tutorial</p> <ul> <li> <p>Fav refers to cell C4 in the 'OPTIONS' worksheet</p> </li> <li> <p>Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets</p> </li> <li> <p>UserOverround refers to cell C3 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market</p> </li> <li> <p>MinTime refers to cell C2 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>MaxTime refers to cell E2 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market</p> </li> <li> <p>TakeSP refers to cell C5 in the 'OPTIONS' worksheet which allows you to change a single value more easily.</p> </li> </ul> <p>This is our trigger on Excel formula:</p> <p>``` excel tab=\"Multi line\" =IF(     AND(         (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) &lt; Fav+1,         TimeTillJump1 &lt; MaxTime,         TimeTillJump1 &gt; MinTime,         Overrounds1&lt;UserOverrounds,         ISBLANK(InPlay1)),     \"BACK\", \"\")</p> <pre><code>``` excel tab=\"Single line\"\n=IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) &lt; Fav+1,TimeTillJump1 &lt; MaxTime,TimeTillJump1 &gt; MinTime,Overrounds1&lt;UserOverrounds,ISBLANK(InPlay1)),\"BACK\",\"\")\n</code></pre> <p>Stepping through each step:</p> <ul> <li>Finding the top two selections in the market: check each runner to see if they're one of the two market favourites - We're doing this by going through the best available to back (column G) price for each runner, ranking them in order (which sorts them from highest to lowest - which is the opposite of what we want) then subtracting that rank number from the total number of selections available to inverse the order. Finally, we plus one to the resulting rank - if we didn't do this then you'd have a rank order that started at 0, not 1, and we thought that would just confuse matters!</li> </ul> <p>Once it's established what each selection's rank is, we then check if that rank is less than three, and if it is we know that the runner in question is one of the top two in the market, based on the current available to back prices.</p> <pre><code>=IF(\n    AND(\n        (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) &lt; Fav+1,\n        TimeTillJump1 &lt; MaxTime,\n        TimeTillJump1 &gt; MinTime,\n        Overrounds1&lt;UserOverrounds,\n        ISBLANK(InPlay1)),\n    \"BACK\",\n\"\")\n</code></pre> <ul> <li>TimeTillJump1 &lt; MaxTime and &gt; MinTime: check whether the seconds left on the countdown are smaller than what is defined in cell C2 and greater than cell E2 in the 'OPTIONS' worksheet (named 'MinTime' and 'MaxTime' respectively), as we need to both place the bet and then convert it to a BSP bet before the off (more on this later). This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E4 (named 'TimeTillJump1')in the 'SETTINGS' worksheet, where we've already done the calculations for you.</li> </ul> <pre><code>=IF(\n    AND(\n        (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) &lt; Fav+1,\n        TimeTillJump1 &lt; MaxTime,\n        TimeTillJump1 &gt; MinTime,\n        Overrounds1&lt;UserOverrounds,\n        ISBLANK(InPlay1)),\n    \"BACK\",\n\"\")\n</code></pre> <ul> <li>Overrounds1 &lt; UserOverrounds: checking whether the market overrounds are less than the specific value that is specified in cell C3 of the 'OPTIONS' worksheet</li> </ul> <pre><code>=IF(\n    AND(\n        (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) &lt; Fav+1,\n        TimeTillJump1 &lt; MaxTime,\n        TimeTillJump1 &gt; MinTime,\n        Overrounds1&lt;UserOverrounds,\n        ISBLANK(InPlay1)),\n    \"BACK\",\n\"\")\n</code></pre> <ul> <li>InPlay1: checking whether the event has gone in play, as this is purely a pre-play strategy, though you could certainly take a similar approach to in-play markets. InPlay refers to G1 in the 'BET ANGEL' worksheet, if this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. </li> </ul> <pre><code>=IF(\n    AND(\n        (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) &lt; Fav+1,\n        TimeTillJump1 &lt; MaxTime,\n        TimeTillJump1 &gt; MinTime,\n        Overrounds1&lt;UserOverrounds,\n        ISBLANK(InPlay1)),\n    \"BACK\",\n\"\")\n</code></pre> <ul> <li>Result: if the statement above is true, the formula returns 'BACK', at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed.</li> </ul> <pre><code>=IF(\n    AND(\n        (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) &lt; Fav+1,\n        TimeTillJump1 &lt; MaxTime,\n        TimeTillJump1 &gt; MinTime,\n        Overrounds1&lt;UserOverrounds,\n        ISBLANK(InPlay1)),\n    \"BACK\",\n\"\")\n</code></pre> <p>updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets</p> <p>You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market.  </p> <ul> <li>Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2</li> </ul> <pre><code>=IF(\n    AND(\n        (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) &lt; Fav+1,\n        TimeTillJump2 &lt; MaxTime,\n        TimeTillJump2 &gt; MinTime,\n        Overrounds2&lt;UserOverrounds,\n        ISBLANK(InPlay2)),\n    \"BACK\",\n\"\")\n</code></pre> <ul> <li>Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3</li> </ul> <pre><code>=IF(\n    AND(\n        (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) &lt; Fav+1,\n        TimeTillJump2 &lt; MaxTime,\n        TimeTillJump2 &gt; MinTime,\n        Overrounds2&lt;UserOverrounds,\n        ISBLANK(InPlay2)),\n    \"BACK\",\n\"\")\n</code></pre> <ul> <li>Convert bets to Betfair Starting Price: Bet Angel Pro doesn't offer the option to place straight BSP bets, so  we've got around that here by placing the bets initially at odds of 1000 (which won't get matched for short favourites), and then at certain amount of seconds from the scheduled start using what Bet Angel calls a 'Global Command' to convert all unmatched bets to BSP. The exact number of seconds can be easily changed by updating cell C5 from the 'OPTIONS' worksheet. This formula goes in cell L6 of the 'BET ANGEL' worksheet, and once it's triggered the bets will automatically convert. Remember, just like the main trigger, each worksheet's global command will need to be updated to reference its respective TimeTillJump calculation. </li> </ul> <p>-- 'BET ANGEL' worksheet global command formula     <pre><code>=IF(TimeTillJump1 &lt; TakeSP, \"TAKE_SP_ALL\", \"\")\n</code></pre></p> <p>-- 'BET ANGEL 2' worksheet global command formula     <pre><code>=IF(TimeTillJump2 &lt; TakeSP, \"TAKE_SP_ALL\", \"\")\n</code></pre></p> <p>-- 'BET ANGEL 3' worksheet global command formula     <pre><code>=IF(TimeTillJump3 &lt; TakeSP, \"TAKE_SP_ALL\", \"\")\n</code></pre></p> <p>Excel functions</p> <ul> <li>IF function: IF(if this is true, do this, else do this)</li> <li>AND function: AND(this is true, and so is this, and so is this) - returns true or false</li> <li>COUNT function: returns number of cells in the range you pass in tha contain a number</li> <li>RANK function: returns the rank of a number in a list of numbers, with the smallest number returning the highest rank.</li> <li>Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. </li> </ul>"},{"location":"automation/betAngelMarketFavouriteAutomation/#-preparing-the-spreadsheet","title":"- Preparing the spreadsheet","text":"<p>You need to copy/paste these three formulas into the relevant cell on each green row - We copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight. </p> <ul> <li>Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner).</li> </ul> <p>``` excel tab=\"Multi line\" =IF(     AND(         (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) &lt; Fav+1,         TimeTillJump1 &lt; MaxTime,         TimeTillJump1 &gt; MinTime,         Overrounds1&lt;UserOverrounds,         ISBLANK(InPlay1)),     \"BACK\", \"\") <pre><code>``` excel tab=\"Single line\"\n=IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) &lt; Fav+1,TimeTillJump1 &lt; MaxTime,TimeTillJump1 &gt; MinTime,Overrounds1&lt;UserOverrounds,ISBLANK(InPlay1)),\"BACK\",\"\")\n</code></pre></p> <p></p> <ul> <li>Odds: as we said we're putting the bet up initially at odds of 1000, so this is a simple one. </li> </ul> <p>Note:</p> <p>The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all.</p> <p><code>=IF(B9=\"\",\"\",\"1000\")</code></p> <p></p> <ul> <li>Stake: it's completely up to you what staking approach you want to take. We're keeping it simple and using flat staking here, so will just place $10 on each runner. This goes in column N (N9 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. </li> </ul> <p><code>=IF(B9=\"\",\"\",\"10\")</code></p> <p></p> <ul> <li>Global Command: this is what triggers the open bets to convert to BSP, and only goes in one cell, L6. As soon as the countdown timer reaches less than 60 seconds this will fire. As mentioned above, remember to update the formula for each worksheet.</li> </ul> <p><code>=IF(TimeTillJump1 &lt; TakeSP, \"TAKE_SP_ALL\", \"\")</code></p> <p></p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#-you-know-the-drill","title":"- You know the drill","text":"<p>The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Bet Angel Pro.</p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#-video-walk-through","title":"- Video walk through","text":"<p>We've put together a litte video walk through to help make this process easier. </p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#-selecting-markets","title":"- Selecting markets","text":"<p>We used the markets menu in the Guardian tool to navigate to the tracks we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets.</p> <p>If you wanted to include all horse or greyhound races for a day you could use the 'quick picks' tab to do this more efficiently. </p> <p>Once you've chosen the races you're interested in click the 'add' button and you'll see them appear in the main body of the screen. </p> <p>Make sure you sort the races by start time, so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. </p> <p>You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up).</p> <p></p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#-linking-the-spreadsheet","title":"- Linking the spreadsheet","text":"<p>Open the 'Excel' tab in Guardian, then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. </p> <p></p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#and-youre-set","title":"And you're set!","text":"<p>Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off.</p> <p>Note:</p> <p>You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.</p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#bet-angel-features","title":"Bet Angel features","text":"<p>Here are some Bet Angel features that you'll need to consider.</p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#-multiple-betsclearing-status-cells","title":"- Multiple bets/clearing status cells","text":"<p>The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as my check, as bet trigger re-evaluated before columns T:AE had updated. </p> <p>As we want to use the same sheet for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status and global status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel. If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs. </p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#-turning-off-bet-confirmation","title":"- Turning off bet confirmation","text":"<p>Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings.</p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#-editing-the-spreadsheet","title":"- Editing the spreadsheet","text":"<p>The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits. </p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#areas-for-improvement","title":"Areas for improvement","text":"<p>There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. </p> <p>If the market changes significantly in those last few minutes and a third selection shortens in past the two we've placed bets on you could end up with bets on more than the intended two runner. This is something you could check for in your bet rule if you wanted to ensure you were only backing a set number of runners. </p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#what-next","title":"What next?","text":"<p>We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.</p>"},{"location":"automation/betAngelMarketFavouriteAutomation/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/betAngelRatingsAutomation/","title":"Bet Angel Pro: Ratings automation","text":""},{"location":"automation/betAngelRatingsAutomation/#automating-a-thoroughbred-ratings-strategy-across-multiple-markets-using-bet-angel-pro","title":"Automating a thoroughbred ratings strategy across multiple markets using Bet Angel Pro","text":"<p>Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to some of our other Betfair automation tutorials, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Bet Angel. We've also added some functionality to the spreadsheet to allow you to automate three simultaneous markets in the event that a market is delayed and race times for multiple markets overlap. </p> <p>Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. </p>"},{"location":"automation/betAngelRatingsAutomation/#-the-plan","title":"- The plan","text":"<p>Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here, incorporating the thoroughbred racing ratings into the automation. </p> <p>We'll step through how we went about getting Bet Angel Pro to place bets using the Betfair's Data Scientists' thoroughbred ratings model. Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. </p> <p></p> <p>Resources</p> <ul> <li>Ratings: Betfair's Data Scientists' thoroughbred ratings model</li> <li>Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings </li> <li>Tool: Bet Angel Pro</li> <li>Assigning multiple markets to your Excel worksheets in Bet Angel so you don't miss a race: Betfair automating simultaneous markets tutorial</li> </ul>"},{"location":"automation/betAngelRatingsAutomation/#-set-up","title":"- Set up","text":"<p>Make sure you've downloaded and installed Bet Angel Pro, and signed in.</p> <p>Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. </p> <p></p>"},{"location":"automation/betAngelRatingsAutomation/#-downloading-formatting-ratings","title":"- Downloading &amp; formatting ratings","text":"<p>Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. </p> <p></p> <p>Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. </p> <p></p> <p>Copy the ratings data over to the customised Bet Angel template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated.</p> <p></p>"},{"location":"automation/betAngelRatingsAutomation/#-writing-your-rules","title":"- Writing your rules","text":"<p>As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. </p> <p>We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. </p> <p>This is how we used Excel to implement our set of rules. </p>"},{"location":"automation/betAngelRatingsAutomation/#-trigger-to-place-bet","title":"- Trigger to place bet","text":"<p>In short, we want to back or lay runners when:</p> <ul> <li>The available to back price is greater than the rating for that runner, then we will back the runner</li> <li>The available to back price is less than the rating for that runner, then we will lay the runner</li> <li>Back market percentage is less than a certain value that we choose</li> <li>The scheduled event start time is less than a certain number of seconds that we choose</li> <li>The event isn't in play </li> </ul>"},{"location":"automation/betAngelRatingsAutomation/#-using-cell-references-to-simplify-formulas","title":"- Using cell references to simplify formulas","text":"<p>Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement.</p> <p>Cell names used in this tutorial</p> <ul> <li> <p>Ratings refers to the entire Column I in the 'RATINGS' worksheet</p> </li> <li> <p>SelectionID refers to the entire column G in the 'RATINGS' worksheet</p> </li> <li> <p>Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. </p> </li> <li> <p>UserOverround refers to cell H4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. </p> </li> <li> <p>UserTimeTillJump refers to cell H3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market</p> </li> <li> <p>BACKLAY refers to cell H5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners</p> </li> </ul> <p>This is our trigger for the 'BET ANGEL' worksheet:</p> <p>``` excel tab=\"Multi line\" =IF(     AND(         OR(              AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(A9,SelectionID,0))))),              AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),         Overrounds1&lt;UserOverround,         TimeTillJump1&lt;UserTimeTillJump,         ISBLANK(InPlay1)),         BACKLAY,     \"\" )</p> <pre><code>``` excel tab=\"Single line\"\n=IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(A9,SelectionID,0))))),AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),Overrounds1&lt;UserOverround,TimeTillJump1&lt;UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\")\n</code></pre> <p>Stepping through each step:</p> <ul> <li>Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell I5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings.  Instead of matching the runner name from the market to that which is defined in our ratings, we're using their selection ID instead. As the Selection ID is unique to each runner, it will help us eliminate any potential errors or issues. Because the ratings from the Betfair data scientists includes Selection ID, there's no additional work or mucking around to get this implemented. </li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(A9,SelectionID,0))))),\n             AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),\n        Overrounds1&lt;UserOverround,\n        TimeTillJump1&lt;UserTimeTillJump,\n        ISBLANK(InPlay1)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found here.</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(A9,SelectionID,0))))),\n             AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),\n        Overrounds1&lt;UserOverround,\n        TimeTillJump1&lt;UserTimeTillJump,\n        ISBLANK(InPlay1)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell F3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. </li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(A9,SelectionID,0))))),\n             AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),\n        Overrounds1&lt;UserOverround,\n        TimeTillJump1&lt;UserTimeTillJump,\n        ISBLANK(InPlay1)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <p>Calculating the time until the jump for multiple markets at the same time</p> <p>One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E.</p> <p>In the 'BET ANGEL' worksheet, the formulas will be written <code>TimeTillJump1&lt;UserTimeTillJump,</code> while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written <code>TimeTillJump2&lt;UserTimeTillJump,</code> and <code>TimeTillJump3&lt;UserTimeTillJump,</code> respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market.  </p> <ul> <li>Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. </li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(A9,SelectionID,0))))),\n             AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),\n        Overrounds1&lt;UserOverround,\n        TimeTillJump1&lt;UserTimeTillJump,\n        ISBLANK(InPlay1)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed.</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(A9,SelectionID,0))))),\n             AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),\n        Overrounds1&lt;UserOverround,\n        TimeTillJump1&lt;UserTimeTillJump,\n        ISBLANK(InPlay1)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <p>updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets</p> <p>You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market.  </p> <ul> <li>Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(A9,SelectionID,0))))),\n             AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),\n        Overrounds1&lt;UserOverround,\n        TimeTillJump1&lt;UserTimeTillJump,\n        ISBLANK(InPlay1)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(A9,SelectionID,0))))),\n             AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),\n        Overrounds1&lt;UserOverround,\n        TimeTillJump1&lt;UserTimeTillJump,\n        ISBLANK(InPlay1)),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <p>Excel functions</p> <ul> <li>IF function: IF(if this is true, do this, else do this)</li> <li>AND function: AND(this is true, and so is this, and so is this) - returns true or false</li> <li>And Or function: checks that the statement meets more than one condition. If this OR that, then do the following. </li> <li>Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. </li> </ul>"},{"location":"automation/betAngelRatingsAutomation/#-preparing-the-spreadsheet","title":"- Preparing the spreadsheet","text":"<p>You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight. </p> <ul> <li>Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner).</li> </ul> <p>``` excel tab=\"Multi line\" =IF(     AND(         OR(              AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(A9,SelectionID,0))))),              AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),         Overrounds1&lt;UserOverround,         TimeTillJump1&lt;UserTimeTillJump,         ISBLANK(InPlay1)),         BACKLAY,     \"\" ) <pre><code>``` excel tab=\"Single line\"\n=IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 &gt; (INDEX(Ratings,MATCH(A9,SelectionID,0))))),AND(BACKLAY = \"LAY\", (G9 &lt; (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),Overrounds1&lt;UserOverround,TimeTillJump1&lt;UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\")\n</code></pre></p> <p></p> <ul> <li>Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks'. For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. </li> </ul> <p>Note:</p> <p>The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. </p> <p><code>=IF(B9=\"\",\"\",G9)</code></p> <p></p> <ul> <li>Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. </li> </ul> <p><code>=IF(B9=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(G9-1),stake*(H9/(H9-1))-stake))</code></p> <p></p>"},{"location":"automation/betAngelRatingsAutomation/#-connecting-to-bet-angel","title":"- Connecting to Bet Angel","text":""},{"location":"automation/betAngelRatingsAutomation/#video-walk-through","title":"Video walk through","text":"<p>We've put together a litte video walk through to help make this process easier. </p>"},{"location":"automation/betAngelRatingsAutomation/#-selecting-markets","title":"- Selecting markets","text":"<p>We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets.</p> <p>Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. </p> <p>Make sure you sort the races by start time, so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. </p> <p>You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets.</p> <p></p> <p>The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature. </p>"},{"location":"automation/betAngelRatingsAutomation/#-linking-the-spreadsheet","title":"- Linking the spreadsheet","text":"<p>Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. </p> <p></p>"},{"location":"automation/betAngelRatingsAutomation/#and-youre-set","title":"And you're set!","text":"<p>Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off.</p> <p>Note:</p> <p>You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.</p>"},{"location":"automation/betAngelRatingsAutomation/#bet-angel-features","title":"Bet Angel features","text":"<p>Here are some Bet Angel features that you'll need to consider.</p>"},{"location":"automation/betAngelRatingsAutomation/#-multiple-betsclearing-status-cells","title":"- Multiple bets/clearing status cells","text":"<p>The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. </p> <p>As we want to use each worksheet over and over again for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel. If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs. </p>"},{"location":"automation/betAngelRatingsAutomation/#-turning-off-bet-confirmation","title":"- Turning off bet confirmation","text":"<p>Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. Bet Angel allows you to save different settings configurations as profiles. Depending what you are wanting to use Bet Angel for each time you open it up, you can select the appropriate setting profile to suit your needs without having to go through and change them every time. </p> <p></p>"},{"location":"automation/betAngelRatingsAutomation/#-editing-the-spreadsheet","title":"- Editing the spreadsheet","text":"<p>The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits. </p>"},{"location":"automation/betAngelRatingsAutomation/#areas-for-improvement","title":"Areas for improvement","text":"<p>There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. </p>"},{"location":"automation/betAngelRatingsAutomation/#what-next","title":"What next?","text":"<p>We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.</p>"},{"location":"automation/betAngelRatingsAutomation/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/betAngelSimultaneousMarkets/","title":"Bet Angel Pro: Automating simultaneous markets","text":""},{"location":"automation/betAngelSimultaneousMarkets/#dont-miss-out-on-a-market-with-simultaneous-automation","title":"Dont miss out on a market with simultaneous automation","text":"<p>If you have a concern of missing markets due to delays or unforeseen circumstances, Bet Angel is able to work off multiple worksheets for different meetings, all from the same workbook.  For example, if we are wanting to automate markets at 1.45pm at Swan Hill, 1.55pm at Townsville and 2.02pm at Menangle, but the Swan Hill market is delayed, we don't want to miss out on the Townsville or Menangle markets. </p> <p>Click Here to download the spreadsheet that we have edited which will allow you to bet on multiple markets</p> <p>To set this up, we need to make sure that there are enough instances of the \u2018Bet Angel\u2019 worksheet to cover markets in the event of a delay or on track issue. In this context, three worksheets should be enough to cover the days markets if some of them are delayed for whatever reason. </p> <p>We've created a special Excel file with macros that will allow up to three markets to be linked at the same time. These macros automatically clear the contents of each sheet when they detect a market change and clear any errors that may occur. If you want to have more worksheets to be linked to different markets, you will need to update the macros accordingly so that they all work independently from one another. </p> <p>Simultaneous markets spreadsheet</p> <p>Please note that the above edited excel workbook does not include any automated strategies. You will need to add this in yourself or take a look at our automated market ratings tutorial</p> <p></p> <p>Once this is done, save the file and close out of it completely (Bet Angel will open it back up for us automatically when we\u2019re ready to start our automation).  In Bet Angel, follow the usual process of clicking the \u2018Guardian\u2019 Icon, select your markets and the usual \u2018Browse for file\u2019 button to locate your Excel file. </p> <p>The only thing you will need to do differently for Bet Angel is to simply allocate a specific worksheet to a particular market that you are betting on. As each market concludes, the assigned worksheet will then reset and then re-assign itself to the next market in the guaradian list. </p> <p></p> <p>When you\u2019re ready for automation to take over, click the \u2018Connect\u2019 check box and then it will do its thing.</p>"},{"location":"automation/betAngelSimultaneousMarkets/#and-youre-set","title":"And you're set!","text":"<p>Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off.</p> <p>Note:</p> <p>You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.</p>"},{"location":"automation/betAngelSimultaneousMarkets/#what-next","title":"What next?","text":"<p>We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au</p>"},{"location":"automation/betAngelSimultaneousMarkets/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/betAngelTippingAutomation/","title":"Bet Angel Pro: Tipping automation","text":""},{"location":"automation/betAngelTippingAutomation/#automating-a-non-ratings-based-tipping-strategy-using-bet-angel-pro","title":"Automating a (non-ratings based) tipping strategy using Bet Angel Pro","text":"<p>We all love getting some good racing tips, but who has time to sit and place bets all day? Wouldn't it be easier if you could take those tips and get a program to automatically place the bets on your behalf? </p> <p>This is what we're going to explore here - we'll be using Bet Angel Pro to place bets automatically based on a set of tips. This is our first-time using Bet Angel for this approach and we are very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions. </p>"},{"location":"automation/betAngelTippingAutomation/#-the-plan","title":"- The plan","text":"<p>We have a set of tips that we've taken from our DataScientists' Racing Prediction Model, but this approach should work for any set of tips you may have. Our goal is to create an automated process which will let us choose our tips for the day, then walk away and the program do the leg work. </p> <p>Here we'll step through how we went about getting Bet Angel Pro to place bets on the favourite runner identified by Betfair's Data Scientists. There are no ratings associated with these tips, so we're happy to take Betfair's Starting Price instead of a price for these bets. </p> <p></p> <p>If you want to follow along and try this approach yourself you'll need to download Bet Angel Pro and sign up for either a subscription or at least a test period. They have a 14 day free trial that's valuable for establishing whether this tool will do what you want it to for your specific strategy. </p> <p>Resources</p> <ul> <li>Tips: Betfair Data Scientists' Racing Prediction Model</li> <li>Rules: here's the spreadsheet, we used to automate our tips but you may need to tweak it a bit to suit your own tips. </li> <li>Tool: Bet Angel Pro</li> </ul>"},{"location":"automation/betAngelTippingAutomation/#-set-up","title":"- Set up","text":"<p>First up we need to make sure we've downloaded and installed Bet Angel Pro, and signed in.</p> <p>Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. </p> <p></p>"},{"location":"automation/betAngelTippingAutomation/#-writing-your-rules","title":"-  Writing your rules","text":"<p>As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. </p> <p>We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our tips and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. </p> <p>This is how we used Excel to implement our set of rules. </p>"},{"location":"automation/betAngelTippingAutomation/#-trigger-to-place-bet","title":"- Trigger to place bet","text":"<p>In short, we want to back or lay runners when:</p> <ul> <li>The runners name has been specified in our tipping list</li> <li>Back market percentage is less than a certain value that we choose</li> <li>The scheduled event start time is less than a certain number of seconds that we choose</li> <li>The event isn't in play </li> </ul>"},{"location":"automation/betAngelTippingAutomation/#-using-cell-references-to-simplify-formulas","title":"- Using cell references to simplify formulas","text":"<p>Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement.</p> <p>Cell names used in this tutorial</p> <ul> <li> <p>RunnerName refers to the entire column A in the 'TIP' worksheet</p> </li> <li> <p>Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. </p> </li> <li> <p>UserOverround refers to cell H5 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. </p> </li> <li> <p>UserTimeTillJump refers to cell H2 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market</p> </li> <li> <p>BackStake refers to cell H3 in the 'SETTINGS' worksheet and like the name suggests, will be the stake for any back bets that are triggered</p> </li> <li> <p>LayStake refers to cell H4 in the 'SETTINGS' worksheet and will be the stake for any lay bets that are triggered</p> </li> <li> <p>BetType is the entire B column in the 'TIP' worksheet. Depending on your tip for each runner, you can choose whether you want a back or lay bet to be triggered for that runner</p> </li> </ul> <p>This is our trigger for the 'BET ANGEL' worksheet:</p> <p>``` excel tab=\"Multi line\" =IF(    AND(       COUNTIF(RunnerName,B9)&gt;0,       TimeTillJump1&lt;UserTimeTillJump,       Overrounds1&lt;UserOverround,       ISBLANK(InPlay1)),       INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),     \"\" ) <pre><code>``` excel tab=\"Single line\"\n=IF(AND(COUNTIF(RunnerName,B9)&gt;0,TimeTillJump1&lt;UserTimeTillJump,Overrounds1&lt;UserOverround,ISBLANK(InPlay1)),INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\"\")\n</code></pre></p> <p>Stepping through each step:</p> <ul> <li>Checking whether the runner is to have a bet placed: Here the trigger is checking the list of runners in the 'TIPS' worksheet so it can decide whether we want a bet to be placed or not. If the name does appear in our list, then it returns a TRUE flag and continues with the next trigger condition. If the name is not in the list, then no bet will be placed. </li> </ul> <pre><code>=IF(\n   AND(\n      COUNTIF(RunnerName,B9)&gt;0,\n      TimeTillJump1&lt;UserTimeTillJump,\n      Overrounds1&lt;UserOverround,\n      ISBLANK(InPlay1)),\n      INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\n    \"\"\n)\n</code></pre> <ul> <li>Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell H2 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E9 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. </li> </ul> <pre><code>=IF(\n   AND(\n      COUNTIF(RunnerName,B9)&gt;0,\n      TimeTillJump1&lt;UserTimeTillJump,\n      Overrounds1&lt;UserOverround,\n      ISBLANK(InPlay1)),\n      INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\n    \"\"\n)\n</code></pre> <p>Calculating the time until the jump for multiple markets at the same time</p> <p>One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E.</p> <p>In the 'BET ANGEL' worksheet, the formulas will be written <code>TimeTillJump1&lt;UserTimeTillJump,</code> while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written <code>TimeTillJump2&lt;UserTimeTillJump,</code> and <code>TimeTillJump3&lt;UserTimeTillJump,</code> respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market.  </p> <ul> <li>Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found here.</li> </ul> <pre><code>=IF(\n   AND(\n      COUNTIF(RunnerName,B9)&gt;0,\n      TimeTillJump1&lt;UserTimeTillJump,\n      Overrounds1&lt;UserOverround,\n      ISBLANK(InPlay1)),\n      INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\n    \"\"\n)\n</code></pre> <ul> <li>Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. </li> </ul> <pre><code>=IF(\n   AND(\n      COUNTIF(RunnerName,B9)&gt;0,\n      TimeTillJump1&lt;UserTimeTillJump,\n      Overrounds1&lt;UserOverround,\n      ISBLANK(InPlay1)),\n      INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\n    \"\"\n)\n</code></pre> <ul> <li>Result: if the statement above is true, check whether BACK or LAY has been selected in column B of the 'TIPS' worksheet for that runner. Whatever has been specified, trigger that bet type. </li> </ul> <pre><code>=IF(\n   AND(\n      COUNTIF(RunnerName,B9)&gt;0,\n      TimeTillJump1&lt;UserTimeTillJump,\n      Overrounds1&lt;UserOverround,\n      ISBLANK(InPlay1)),\n      INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\n    \"\"\n)\n</code></pre> <p>updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets</p> <p>You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market.  </p> <ul> <li>Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2</li> </ul> <pre><code>=IF(\n   AND(\n      COUNTIF(RunnerName,B9)&gt;0,\n      TimeTillJump2&lt;UserTimeTillJump,\n      Overrounds2&lt;UserOverround,\n      ISBLANK(InPlay2)),\n      INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\n    \"\"\n)\n</code></pre> <ul> <li>Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3</li> </ul> <pre><code>=IF(\n   AND(\n      COUNTIF(RunnerName,B9)&gt;0,\n      TimeTillJump3&lt;UserTimeTillJump,\n      Overrounds3&lt;UserOverround,\n      ISBLANK(InPlay3)),\n      INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\n    \"\"\n)\n</code></pre> <p>Excel functions</p> <ul> <li>IF function: IF(if this is true, do this, else do this)</li> <li>AND function: AND(this is true, and so is this, and so is this) - returns true or false</li> <li>[COUNTIF function:]</li> <li>Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. </li> </ul>"},{"location":"automation/betAngelTippingAutomation/#-preparing-the-spreadsheet","title":"- Preparing the spreadsheet","text":"<p>You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight. </p> <ul> <li>Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner).</li> </ul> <p>``` excel tab=\"Multi line\" =IF(    AND(       COUNTIF(RunnerName,B9)&gt;0,       TimeTillJump3&lt;UserTimeTillJump,       Overrounds3&lt;UserOverround,       ISBLANK(InPlay3)),       INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),     \"\" ) <pre><code>``` excel tab=\"Single line\"\n=IF(AND(COUNTIF(RunnerName,B9)&gt;0,TimeTillJump3&lt;UserTimeTillJump,Overrounds3&lt;UserOverround,ISBLANK(InPlay3)),INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\"\")\n</code></pre></p> <p></p> <ul> <li>Odds: Because we will be taking the BSP, we want to ensure that the initial bets that we place are not matched so that the \"TAKE_SP_ALL\" command can trigger for the global command. To do this, it checks the bet type for that particular runner. If it is Backing, then place odds at 1000 and if its going to be a lay bet, then set odds at 1.01</li> </ul> <p>Note:</p> <p>The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR which we're also using, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. </p> <p><code>=IFERROR(IF(B9=\"\",\"\",IF(INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2)=\"LAY\",1.01,1000)),\"\")</code></p> <p></p> <ul> <li>Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are using seperate variables for a back bet and lay bet. These variables can be easily changed from the 'SETTINGS' tab. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. </li> </ul> <p><code>=IFERROR(IF(B9=\"\",\"\",IF(INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2)=\"BACK\", BackStake,LayStake)),\"\")</code></p> <p></p>"},{"location":"automation/betAngelTippingAutomation/#-connecting-to-bet-angel","title":"- Connecting to Bet Angel","text":""},{"location":"automation/betAngelTippingAutomation/#video-walk-through","title":"Video walk through","text":"<p>We've put together a litte video walk through to help make this process easier. </p>"},{"location":"automation/betAngelTippingAutomation/#-selecting-markets","title":"- Selecting markets","text":"<p>We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets.</p> <p>Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. </p> <p>Make sure you sort the races by start time, so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. </p> <p>You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets.</p> <p></p> <p>The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature. </p>"},{"location":"automation/betAngelTippingAutomation/#-linking-the-spreadsheet","title":"- Linking the spreadsheet","text":"<p>Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. </p> <p></p>"},{"location":"automation/betAngelTippingAutomation/#and-youre-set","title":"And you're set!","text":"<p>Once you've set your rules up and got comfortable using Bet Angel Pro it should only take  number of seconds to load the markets up and choose your selections for the day. </p> <p>Note:</p> <p>You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.</p>"},{"location":"automation/betAngelTippingAutomation/#what-next","title":"What next?","text":"<p>We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to us at automation@betfair.com.au </p>"},{"location":"automation/betAngelTippingAutomation/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/betAngeladvanced/","title":"Bet Angel: An Advanced guide","text":""},{"location":"automation/betAngeladvanced/#automation","title":"Automation","text":"<p>Bet Angel offers two forms of automation through their Guardian interface: </p> <ol> <li> <p>Triggered betting which operates by linking pre-defined commands together to form an automated strategy </p> </li> <li> <p>Use Excel to monitor multiple markets and execute bets or trades based upon triggers set within the Excel spreadsheet within one or more markets. </p> </li> </ol> <p>The triggered betting feature is a good way to implement simple automations quickly while using Excel gives you a greater level of control and sophistication as you have the ability to use formulas and even create your own macros using Visual Basic for Applications (VBA).  </p>"},{"location":"automation/betAngeladvanced/#triggered-betting","title":"Triggered Betting","text":"<p>To begin forming your automation in the triggered betting feature within Bet Angel, you will need to open the Guardian feature by clicking on the  icon on the main Bet Angel toolbar. </p> <p></p> <p>The Guardian window will open where you have a number of tabs to select from. First, we want to have a list of markets that we want to automate.  </p> <ul> <li> <p>From the \u2018Market Selection\u2019 tab select each market (use Ctrl to select multiple) you want </p> </li> <li> <p>Click the \u2018Add Bet Angel markets\u2019 button.  </p> </li> <li> <p>Keep in mind that the order that the markets are in the list, will be the order that your automation will go through.  </p> </li> </ul> <p>Tip: Click on the \u2018Start time\u2019 column to order your list based on race start time. Guardian will work through your list in order. </p> <p></p> <p>Within the \u2018Markets\u2019 tab you will see the \u2018Refresh interval\u2019 dropdown box. This instructs Bet Angel how often to refresh the markets in the list.  </p> <p>We recommend setting this value to be the lowest possible by: </p> <ul> <li>Navigate back to the Main Bet Angel home screen </li> <li>Click the \u2018Settings\u2019 tab  </li> <li>Select \u2018Edit Settings\u2019 </li> <li>Click the \u2018Communications\u2019 tab  </li> <li>Check \u2018Use Exchange Streaming\u2019 </li> <li>Click \u2018Save\u2019 then \u2018Close\u2019 </li> <li>Navigate to the Guardian screen and select \u201820ms\u2019 from the \u2018Refresh interval\u2019 dropdown box. </li> </ul> <p>For triggered betting automations: </p> <ul> <li>Click on the \u2018Automation\u2019 tab  </li> </ul> <p>You can create an automation file from scratch and apply it to your selected markets or import other people's Bet Angel automation files.  </p> <p>To get started: </p> <ul> <li>Highlight one of the markets in your list and  </li> <li>select \u2018Create a new rules file for the selected market\u2019. </li> </ul> <p></p> <p>The Automation rules editor window will appear where you can start defining all the settings needed for your automation. Rule settings are divided into 5 different tabs: </p> <ul> <li>\u2018General\u2019 \u2013 where the type of action is defined and when that action will be triggered in relation to the market </li> <li>\u2018Parameters\u2019 \u2013 Here you\u2019ll specify the finer details of your bets such as the odds, stake and global settings </li> <li>\u2018Conditions\u2019 \u2013 create the conditions that have to be met before the rule triggers such as the markets weight of money or volume </li> <li>\u2018Signals\u2019 \u2013 for more complex automations, you may choose to set your rule to give off a certain signal when it triggers which in turn will cause other rules to trigger when they\u2019re set to listen for that signal </li> <li>\u2018Stored Values\u2019 \u2013 which allows you to record things like a Back or Lay bet, last traded price or volume which then can be used as conditions for other rules.  </li> </ul> <p></p> <p>There\u2019s a lot of options here, so we recommend taking a look at online resources such as the Bet Angel user guide and Youtube tutorials to get a more in depth understanding of what each option does and how it may apply to your specific strategy.  </p> <p>Once you have finished creating your automation, remember to click the save icon and give it a name. </p> <p></p> <p>Now you can close the Automation Rules editor window and select your automation from the drop down box called \u2018Rules file name\u2019. </p> <p></p> <p>Once your automation rule file is selected, in the \u2018Rules File Usage\u2019 you can choose from: </p> <ol> <li>\u2018Apply rules to all markets\u2019, </li> <li>\u2018Apply rules to selected markets \u2018 </li> <li>\u2018Remove rules from selected markets\u2019.  </li> </ol> <p>When your automation file is linked to a market it will appear in the \u2018Automation Rules\u2019 column and will begin automating your strategy for you. </p> <p></p>"},{"location":"automation/betAngeladvanced/#custom-columns-advanced","title":"Custom columns - Advanced","text":"<p>Please see our Intermediate guide on custom columns here before starting this advanced tutorial. </p> <p>Custom columns can be very useful to tailor your betting experience when placing manual bets. They can also be used to trigger betting automations. </p> <p>To link an automation to a custom column, click the star icon on the \u2018One-click\u2019 betting screen which will bring up your custom column settings window.  </p> <p></p> <ul> <li>Select \u2018Start an Automation Servant\u2019 from the \u2018Action\u2019 drop down box (highlighted below) </li> <li>Then from the \u2018Rules File\u2019 drop down box select the name of your automation file.  </li> <li>Add steps here (same as intermediate tute) to add the column </li> </ul> <p>After you\u2019ve saved the settings for your custom column, whenever the custom column is clicked on, it will trigger your automation. Applications for this can be varied such as backing a specific runner while laying the rest of the field and much more. </p> <p></p>"},{"location":"automation/betAngeladvanced/#excel-automation","title":"Excel Automation","text":"<p>Using an excel spreadsheet is a great tool to implement complex strategies through Bet Angel. The main way you will go about setting up an automation is to control when a back, lay or take SP command is printed into specific cells which Bet Angel continuously checks.  </p> <p>If Bet Angel detects that a certain cell contains \u201cBACK\u201d, it will place a back bet. The same applies with lay bets as well. Formulas and macros can be used to control when these messages are sent to Bet Angel which can be tied into various market factors or conditions.  </p> <p>To begin setting up an Excel automation:  </p> <ul> <li>click on the  icon to open Guardian </li> <li>and then click the Excel tab.  </li> </ul> <p>From here, you can choose a specific Excel template that you have set up on your computer or import an automation that has been downloaded online using: </p> <ul> <li>\u2018Browse for file\u2019 button  </li> <li>and then clicking \u2018Open workbook\u2019. </li> </ul> <p></p> <p>By default, Bet Angel won't automatically start using the Excel sheet to start placing bets until you check the boxes next to:  </p> <ul> <li>\u2018Connect\u2019,  </li> <li>\u2018Auto-bind Bet Angel sheets\u2019 and </li> <li>\u2018Auto-clear bindings\u2019 options.   </li> <li>Then select your file name from the \u2018Excel Sheet\u2019 column next to the markets you want to connect </li> </ul> <p>Once this is done, Bet Angel will start populating your spreadsheet with market data and start listening for the betting commands. </p> <p></p> <p>If you try to edit the Excel sheet while it\u2019s connected with Bet Angel it will most likely cause an error. If you want to make changes to your Excel workbook, simply untick the \u2018Connect\u2019 option in Guardian, make your changes and re select connect.  </p> <p>As previously mentioned, Bet Angel will populate data into specific cells and will listen for betting commands from other specific cells. You will need to be mindful of this If you wish to start adding functionality to your excel workbook to make sure that you\u2019re not placing it in a cell/s that Bet Angel will use. Bet Angel will simply override whatever you have entered with its own data.  </p> <p>For more tutorials on using the Bet Angel Excel function, take a look through our Automation Hub where we have created a number of tutorials for different strategies.  </p>"},{"location":"automation/betAngeladvanced/#resources","title":"Resources","text":"<ul> <li> <p>Betfair's interview with Bet Angel's creator, Peter Webb</p> </li> <li> <p>Bet Angel User Guide</p> <ul> <li>Triggered Betting</li> <li>Custom Columns</li> <li>Excel Automation</li> </ul> </li> <li> <p>Youtube</p> <ul> <li>Triggered Betting</li> <li>Custom Column (Advanced)</li> </ul> </li> <li> <p>Automation Hub</p> <ul> <li>Ratings Automation tutorial</li> <li>Market favourite automation</li> <li>Tipping automation tutorial</li> <li>Simultaneous markets tutorial</li> <li>Kelly criterion staking tutorial</li> </ul> </li> </ul>"},{"location":"automation/betAngeladvanced/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/betAngelbeginners/","title":"Bet Angel: A beginner\u2019s guide","text":""},{"location":"automation/betAngelbeginners/#installing-bet-angel","title":"Installing Bet Angel","text":"<p>Bet Angel offers three products for use with Betfair. We\u2019ll be using Bet Angel Professional in these tutorials as it is the Ultimate Trading Toolkit containing all the advanced tools and features every trader needs. To start using and learning Bet Angel Professional, first you need to download and install the program from the Bet Angel website.  </p> <p>By downloading it from the Bet Angel website, you can ensure that you\u2019re installing the most up to date version of the software which can be obtained from here. Make sure that you save the downloaded .exe file somewhere that you can easily access. Once it has finished downloading, double click the file (which will be called BAP_1_54_1.exe or something similar) to begin the setup process. For more details on downloading and installing Bet Angel see their handy user guide here. </p> <p>Follow the prompts throughout the installation and once completed, open Bet Angel Professional.  </p>"},{"location":"automation/betAngelbeginners/#registering-bet-angel","title":"Registering Bet Angel","text":"<p>Step 1 After opening Bet Angel Pro you will see a login dialogue box. Click on the \u2018Register using a serial number\u2026\u2019 button (highlighted below).  </p> <p></p> <p> Step 2 \u2013 Free Trial. (Skip to Step 4 if you already have a Bet Angel license).  Bet Angel offer a free 14-day trial for first time users. Select the option \u201cI wish to register for a FREE trial\u2026\u201d as highlighted below. Then enter your \u201cBetfair User Name\u201d and \u201cPassword\u201d details, and click \u2018Register Account\u2019. </p> <p></p> <p> Step 3</p> <p>A new window will appear where you will be prompted to provide your first and last name along with your email address. From here you\u2019ll need to verify your email by clicking a link that will be sent to you in an automated email by Bet Angel. If you don\u2019t see an automatic email from Bet Angel, make sure you check your junk / spam folder.  </p> <p></p> <p> Step 4 \u2013 Purchased serial number (Skip to Step 5 if you have registered for a free trial) </p> <p>Select \u201cI have a serial number......\u201d. You will now see a registration box in which you must enter the serial number sent to you along with the Betfair account username and password (for account verification). Your account password will NOT be stored or sent to us. </p> <p>Once you have entered your details click on the \u201cRegister Account\u201d button. </p> <p></p> <p>If you have not already done so a window will appear asking you to first associate a\u202fverified e-mail\u202faddress with your Betfair Client ID. </p> <p>This is a one-time process and will make activating future subscriptions much quicker and easier. Otherwise immediate access will be granted, and a confirmation message will appear on your screen. </p> <p> Step 5</p> <p>Once you have submitted your register account request, log into Bet Angel by entering your \u201cBetfair User Name\u201d and \u201cPassword\u201d in the boxes highlighted below. You can select either \u201cLive mode\u201d or \u201cPractice mode\u201d at the log in screen.  </p> <p>We recommend starting in Practice mode as this will allow you to experiment with Bet Angel without placing any bets to the Betfair exchange which is great while learning the ropes.  </p> <p>Note that it can take up to 15 minutes before you have full access to Bet Angel, so be patient while the system sets you up in the background.  </p> <p>Ensure that you tick the \u201cI have read and fully understand the risk notice\u201d and the \u201cI accept the Betfair API Terms and conditions\u201d boxes and then click the \u201cLog in\u201d button. </p> <p></p>"},{"location":"automation/betAngelbeginners/#basic-setup","title":"Basic Setup","text":"<p>Once we\u2019re logged into Bet Angel, the first thing that we\u2019re going to want to do is to configure Bet Angel so that it behaves in a way that is best suited for what we\u2019re going to use it for. Each user will have their own configuration preferences and we strongly recommend getting acquainted with the various options available to take full advantage of your Bet Angel experience. To understand all of the options available in Bet Angel, we recommend taking a look at the Bet Angel user guide. </p> <p>To get started, simply click the \u2018Settings\u2019 tab from the top toolbar and then \u2018Edit settings\u2019 from the drop-down box.  </p> <p></p> <p>The Settings window will then appear and is organized into 10 sections;  </p> <ul> <li>Display: to alter the colour preferences for market views, font size and weight of money </li> <li>Staking: where you can set default stakes and liabilities which will be applied to every market </li> <li>Behavior: Enable / disable confirm bet warnings as well as other program specific warnings </li> <li>Green Up: Specific options that affect unmatched bets when closing / greening a trade </li> <li>Communications: Control the data that Bet Angel is pulling from the Betfair exchange API </li> <li>Ladder: Change the colour scheme, columns, charts and more for the ladder functionality </li> <li>Automation: Modify some basic automation controls for bookmaking, ditching, Back and Lay </li> <li>Charts: Customise how charts are displayed in Bet Angel such as colour </li> <li>Sound Alerts: change what sounds will be heard from Bet Angel such as market jump warnings </li> <li>Excel: Control additional information which is populated in a connected excel sheet such as saddle cloth numbers, stall number projected / actual SP and more.  </li> </ul> <p>The most common settings to edit straight away are: - Turning off \u201cConfirm Bets\u201d  - As it states, unticking this option will mean you don\u2019t need to confirm bets. This is one of the advantages of using a 3rd party tool \u2013 please be aware that this will make betting faster, so gamble responsibly. </p> <p></p> <ul> <li>To use the ladder functionality within Bet Angel, go to the Ladder tab and then  click \u2018Show the Ladder Settings editor\u2019 which will bring up another window with a lot more options to play with. A lot of users like to enable the \u201cShow last traded volume\u201d option by <ul> <li>Clicking on the \u201cGeneral\u201d tab and then check \u201cShow last traded volume\u201d then </li> <li>Clicking on the \u201cColumns\u201d tab and under \u2018Last traded price chart column\u2019 (near top of window) check the \u2018Show chart column\u2019 box. </li> </ul> </li> </ul> <p></p> <p></p> <p>Whatever settings you decide to use, you can save them as a settings profile and use different profiles for different types of betting. Just click \u2018save\u2019 from the Settings window and give your settings profile a name.  </p> <p></p> <p>You can easily switch between different settings profiles that you\u2019ve created by clicking the \u2018Settings\u2019 tab and then choosing the profile name from the dropdown box next to \u2018Load settings\u2019  </p> <p></p> <p>To help you through getting setup in Bet Angel, take a look at an introduction to BetAngel video created specifically to help you get up and running.  </p>"},{"location":"automation/betAngelbeginners/#market-selection","title":"Market Selection","text":"<p>To open a market so we can start placing bets, first we need to select the market that we want to look at. Click the \u2018File\u2019 tab from the main navigation bar and click \u2018Select Market\u2019. </p> <p></p> <p>This will open the \u201cMarket Selection\u201d window where you can browse all the different markets available on the Betfair exchange. Click the arrows to the left of the menu items to expand and see the specific markets available.  </p> <p>Once you\u2019ve found a market that you\u2019re interested in, </p> <ul> <li>click on it once then  </li> <li>click the \u2018switch to market\u2019 button.   </li> </ul> <p></p> <p>Another handy feature is to create a quick pick list for specific markets that you\u2019re interested in. For example, if you\u2019re only interested in win horse racing markets within Australia, you can click the \u201cSettings\u201d tab in the \u201cMarket Selection\u201d window and modify the options to be applicable to what you\u2019re interested in. For example here we have selected Australian Horse Race Win markets by: </p> <ul> <li>Checking the \u201cAUS\u201d box under \u201cHorse Races\u201d  </li> <li>Checking the \u201cWin Markets\u201d under \u201cHorse Races\u201d </li> <li>Checking the box next to \u201cDisplay event start time first\u201d</li> </ul> <p>Then only those markets will appear in the \u2018Quick Picks\u2019 tab. </p> <p></p> <p></p> <p>Once you have clicked the \u2018Switch to market\u2019 button for your chosen market, you can easily cycle through to the next scheduled market by pressing Ctrl+N (or Ctrl+P for the previous market) without the need to return back to the market selection window. </p>"},{"location":"automation/betAngelbeginners/#default-stakes-and-lay-to-a-maximum-liability","title":"Default stakes and lay to a maximum liability","text":"<p>If you\u2019re a person who wants to place bets quickly in order to secure the best price, then you\u2019ll also want to be able to set a default stake for your bets.  </p> <p>In a market, you\u2019ll see two columns named \u2018Back stake\u2019 and \u2018Lay stake\u2019. You can easily change the stake value for each runner to whatever you like by clicking on the number and entering your own value manually.  </p> <p></p> <p>You can also set a default stake that will apply to all runners within a market. Simply change the value at the top of the column (highlighted in yellow) and ensure the \u2018All\u2019 check box is selected. </p> <p></p> <p>This can also be done for the lay column in the exact same way.  </p> <p>But what if we wanted to set a maximum liability for our lay bets as opposed to a straight stake? This is where the staking method option comes into play.  </p> <p>Simply change the \u201cStaking Method\u201d drop down box (top of screen) to \u2018By Liability (Lay only)\u2019 and any values which are entered in the lay stake column becomes your maximum liability. Your lay stake is calculated behind the scenes to ensure that you don\u2019t accidentally empty your Betfair account should your lay bet lose.  </p> <p></p>"},{"location":"automation/betAngelbeginners/#one-click-betting","title":"One click betting","text":"<p>Previously, we went over the available settings in Bet Angel to switch off an option called \u2018Confirm bets?\u2019 under the \u2018Behavior\u2019 tab of the settings window. This option is usually enabled by default and will cause a pop-up window to appear asking for you to confirm whether you want a bet to be placed on the exchange.  </p> <p></p> <p>By switching this setting off, you can place bets onto the exchange using a single click meaning that you can react much faster to changes in a market. This will also affect other views within Bet Angel such as the ladder feature within Bet Angel.  </p> <p></p>"},{"location":"automation/betAngelbeginners/#refresh-settings","title":"Refresh Settings","text":"<p>Bet Angel gives you the ability to control how often data is refreshed from the Betfair exchange through a number of options in settings. We recommend making sure that the refresh rate for Bet Angel is set to as low as possible to ensure that you are seeing the most up to date data at any one time.  </p> <ul> <li>Click the \u2018Settings\u2019 tab and then  </li> <li>Select \u2018Edit Settings\u2019 and then </li> <li>Click the \u2018Communications\u2019 tab.  </li> <li>Check \u2018Use Exchange Streaming\u2019 </li> <li>Click \u2018Save\u2019 then \u2018Close\u2019. </li> </ul> <p></p> <p>On the Bet Angel menu bar, change the \u2018Refresh every\u2019 option to 20ms (note 20ms will only be available if you have checked \u2018Use Exchange Streaming\u2019 in the previous step). </p> <p></p>"},{"location":"automation/betAngelbeginners/#fill-or-kill","title":"Fill or Kill","text":"<p>Fill or kill is a useful tool to instruct Bet Angel to cancel (kill) a bet if it\u2019s not matched within a period of time you have specified. To set up fill or kill, simply follow the following steps:</p> <ul> <li>Ensure that the \u2018Use Global Settings when placing a bet\u2019 is enabled (see highlighted button below) </li> </ul> <p></p> <ul> <li> <p>Tick \u2018Place fill or Kill bets\u2019 in the header menu </p> </li> <li> <p>In the \u2018seconds delay\u2019 box specify the number of seconds you would like Bet Angel to wait until it cancels the bet should it not be matched. Bet Angel recommends to ensure that a minimum value of 0.5 (half a second) is always applied to the fill or kill.  </p> </li> </ul>"},{"location":"automation/betAngelbeginners/#resources","title":"Resources","text":"<ul> <li> <p>Betfair's interview with Bet Angel's creator, Peter Webb</p> </li> <li> <p>Bet Angel User Guide</p> <ul> <li>Installation</li> <li>Settings</li> <li>Market Selection</li> <li>One Click betting</li> <li>Refresh settings</li> <li>Fill or Kill</li> </ul> </li> <li> <p>Youtube</p> <ul> <li>Using the settings feature</li> <li>One click trading screen</li> <li>Getting Bet Angel to update ten times faster - Refresh settings</li> </ul> </li> <li> <p>Betfair Hub</p> </li> </ul>"},{"location":"automation/betAngelbeginners/#what-next","title":"What next?","text":"<p>Now that you've got the beginner level sorted, take the next step and have a look at our Intermediate guide to Bet Angel.</p>"},{"location":"automation/betAngelbeginners/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/betAngelintermediate/","title":"Bet Angel: An Intermediate guide","text":""},{"location":"automation/betAngelintermediate/#the-ladder-interface","title":"The Ladder interface","text":"<p>The Bet Angel Ladder interface allows you to gain a deeper view of how bets are being placed on a small number of runners as opposed to the high-level overview of a market that we get from the \u2018One-Click\u2019 view. This also allows you to get an idea of any potential trends and help you make a more informed decision about your own bets.</p> <p></p> <p>By default, the ladder interface will show you the first three runners in a market. A handy button to keep in mind is the \u2018123\u2019 button which will automatically adjust which runners you see and order them in ascending price order with the favourite placed in \u2018Ladder 1\u2019 position. </p> <p></p> <p>You can also easily specify the runners you would like to look at by simply selecting them from the drop-down box above each ladder. </p> <p></p> <p>Just like the \u2018One-click screen\u2019, you can click within the blue (back) or pink (lay) columns to place a bet at those specific odds, allowing you to not only get a more detailed view of what the market is doing but quickly place bets of your own.  </p>"},{"location":"automation/betAngelintermediate/#in-play-trader-view","title":"In-Play trader view","text":"<p>The in-play trader view is an alternative way to look and place bets on markets which are in-play. It gives an overview of the highest and lowest price points traded for runners and allows you to easily place bets onto the market.  </p> <p>To open the in-play trader view:  </p> <p>click the in-play trader icon  near the top right of the main Bet Angel screen.</p> <p></p> <p>Once you\u2019ve clicked the button, a new window will open up showing every runner in the race. </p> <p></p> <p>As you can see, there are a number of dots and lines which give us specific information relating to where bets are being placed for each runner. The blue dot indicates the last back price, the pink shows the last lay price and the yellow indicates the last traded price. The grey bar gives us the highest and lowest traded range of that particular runner.  </p> <p>You\u2019ll also notice that there are a number of options that you can use to customize your experience with the tool depending on what information is important to you.   </p> <ul> <li>At the top left of the window from left to right, you can hide the blue, pink and yellow dots to simplify your view</li> <li>change the window to reflect the win percentage scale or tick scale</li> <li>sort your runner list depending of a range of factors</li> <li>change the height of your runner rows</li> <li>adjust the labels for the dots to reflect different information</li> </ul> <p>You can also use the in-play trader view to place bets just like in the one-click screen or ladder view. You can customise what betting action to take when the left mouse button is clicked (place a back bet, a lay bet or even set that the left mouse button will back while the right button lays).  </p> <p>By default, the odds that bets are placed at and the runner you bet on in this view will be determined by the location of your mouse cursor.  </p> <p>It pays to be mindful of this when using this view, especially if you have confirm bets turned off as an accidental click on the in-play trader view may cause you to place a bet that you didn\u2019t want. This being said, the price that bets are placed at can be easily changed to a different factor such as the best available back price instead of the mouse cursor location.  </p>"},{"location":"automation/betAngelintermediate/#custom-columns","title":"Custom Columns","text":"<p>Custom columns are an extremely helpful way to speed up how you bet in Bet Angel markets. They allow you customize not only the information that you see in the one-click betting screen, but also lets you place specific bets faster into the market. This is particularly useful when placing bets in-play and speed is a considerable factor for your strategy.</p> <p>To get started with your own custom columns </p> <ul> <li>click the yellow star icon which will bring up the custom column's editor. </li> </ul> <p></p> <p>Once the editor window pops up: </p> <ul> <li>Click \u2018New\u2019 </li> <li>Then enter a Profile Name for your custom column and click \u2018OK\u2019 </li> <li>Choose from the configure options (see below) </li> <li>Then click \u2018Save Column\u2019 </li> </ul> <p>You will have a range of options to configure your columns: </p> <ul> <li> <p>Custom Column Profiles \u2013 Create multiple profiles for different betting strategies that you may implement. Each profile can have different columns that do different things and these profiles allow you to easily switch between different modes depending on what you\u2019re wanting to do in Bet Angel. </p> </li> <li> <p>General: </p> <ul> <li>Title \u2013 Give your column a name to make it easily identifiable (this will be the name of the column in the column chooser list and title in the column header on the Bet Angel screen)</li> <li>ToolTip \u2013 a custom info box that appears when the mouse is hovered over your custom column </li> <li>Action \u2013 Specify what happens when you click on the column, whether it places a back or lay bet, cancels a bet or triggers an automation to run </li> <li>Button colour \u2013 customise the colour of your column to make it easily distinguishable </li> <li>Display \u2013 Choose whether each box within your custom column is populated with the runners odds or text </li> </ul> </li> <li> <p>Odds \u2013 If you decide to have odds populated in your custom column, you can choose from: </p> <ul> <li>fixed odds,  </li> <li>ticks offset from the best back price </li> <li>ticks offset from the best lay price  </li> <li>a percentage offset from best lay price </li> </ul> </li> <li> <p>Stake \u2013 This allows you to program a pre-determined stake that can be triggered when you click in the column, essentially giving you your own bet placement shortcut buttons.  </p> </li> </ul> <p></p> <p>Once you have set up your custom column the way that you would like it to operate, the next step is to make it visible on your screen so you can use it.  </p> <ul> <li>Select your custom column name from the drop down box next to the yellow star icon </li> <li>Click on  icon</li> <li>Scroll through the list and choose your custom column</li> </ul> <p>Your custom column should now be added to the right of the existing columns (see image below) </p> <p></p> <p>If you haven\u2019t created a profile for your custom columns, click the settings button (see below), then choose columns where pop up window will appear. Find your custom column from the list and tick the box next to it. </p> <p></p>"},{"location":"automation/betAngelintermediate/#offset-bets","title":"Offset Bets","text":"<p>Offset bets are extremely easy to achieve in Bet Angel thanks to a simple to use tool at the top of the One-click screen. Here you can choose to use a number of different offset bet actions and specify the number of ticks or percentage that you want your offset bet to be placed at.  </p> <p></p> <p>In the above example, if I placed a back bet on the runner named \u201c1. Andrew Swagger\u201d, the back bet will be placed at $55 and then Bet Angel will automatically place a lay bet on the same runner 2 ticks lower.  </p>"},{"location":"automation/betAngelintermediate/#dutching","title":"Dutching","text":"<p>The dutching screen within Bet Angel is often overlooked by Bet Angel users but is extremely handy, especially if your preference is lower risk (and lower profit/loss). The dutching screen will only place back bets into a market (while the bookmaking screen will only place lay bets-see next section). Bet Angel makes dutching a lot easier by allowing you to select a number of runners within a market that you think that the runner may be amongst and place bets with a specific stake, achieve a target profit or set a minimum stake for Bet Angel to use.  </p> <p></p> <p>In the above example, we\u2019ve selected 5 runners, out of a market of 10, who we think could possibly win the race. Our aim in this scenario is to come away with a profit of $20 if one of our selected runners win. Bet Angel will do all the calculations for you to work out the odds and stake for each runner, but you do have the ability to override a specific runners target profit \u2013 especially handy if you're not completely confident that a particular runner will win or not but you hold more or less confidence than the rest of your selections.  </p> <p>For example, for 8. Nobodys Puppet, which the best available back price is 10.5, we may think that there is still a possibility for it to win, so we can override the target profit for that runner to be $0 which will mean that if it wins, we won't make any profit but we also won't lose our stake.  </p> <p>You\u2019ll notice that there are 3 check box columns: Back, Lay and Manual. These refer to the odds that you\u2019ll be using. If the back-check box is selected, then you\u2019ll be using the best available back odds and the same concept applies if you have the lay check box selected. Manual will allow you to set your own odds for bets to be placed onto the market.  </p> <p>Bet Angel will also display the total stake for all of the bets to be placed based on your settings as well as the potential profit (indicated in green) should one of your selections win and the potential loss should they lose (indicated in red).  </p> <p></p> <p>For a more in depth run-through of the Dutching functionality, check out the Bet Angel YouTube video here. </p>"},{"location":"automation/betAngelintermediate/#bookmaking","title":"Bookmaking","text":"<p>The bookmaking screen operates almost identically to the Dutching screen except that it will only place lay bets into the market. The aim of the game in this screen to create a book that ends up being over 100% so that we can secure a profit.  </p> <p></p> <p>To see an example of the bookmaking screen in action, take a look at the Bet Angel video on Youtube here. </p>"},{"location":"automation/betAngelintermediate/#charting","title":"Charting","text":"<p>One of the main benefits to using programs such as Bet Angel is the functionality that allows you to gain a more in depth insight into how a market is going, see trends and help you make smarter calls when it comes to placing your own bets. Charts within Bet Angel takes the information from the Betfair exchange and illustrates it in a way that allows you to interpret complex data in a more concise visual format. Advanced charting is a feature that is easily accessed from the One-click, Ladder and manual bet screens and can be accessed by clicking the graph icon next to a runners name. </p> <p></p> <p>From here, a chart window will appear with various graphs and options to change the type of graph and the data being displayed.  </p> <p>For more detail and information regarding Bet Angels advanced charts, check out their video tutorial here.  </p>"},{"location":"automation/betAngelintermediate/#resources","title":"Resources","text":"<ul> <li> <p>Betfair's interview with Bet Angel's creator, Peter Webb</p> </li> <li> <p>Bet Angel user guide</p> <ul> <li>Ladder Mode</li> <li>In-Play trader view</li> <li>Custom columns</li> <li>Offset bets</li> <li>Dutching</li> <li>Bookmaking</li> </ul> </li> <li> <p>Youtube</p> <ul> <li>Ladder Mode</li> <li>In-Play trader view</li> <li>Custom Columns</li> <li>Dutching</li> <li>Bookmaking</li> </ul> </li> </ul>"},{"location":"automation/betAngelintermediate/#what-next","title":"What next?","text":"<p>Now that you've got the intermediate level sorted, take the next step and have a look at our Advanced guide to Bet Angel.</p>"},{"location":"automation/betAngelintermediate/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/betJetOverview/","title":"Overview","text":"<p>BetJet Pro is a third-party betting software tool that uses the Betfair Exchange. Whilst not having capability for automated betting, it provides an easy to use interface for betting and trading compatible across both Mac and PC. </p> <p></p> <p>The interface is highly customisable and able to be tailored to anyone's specific needs with sophisticated hotkeys including the aptly named \"Get Me Out\" button which closes all open trades on a specific market. Switching between ladder view and grid view is easy, with the tool able to easily show where the weight of money is going to predict price movements. </p> <p>BetJet Pro comes with tools that can be set to activate instantly upon the price reaching a certain point, a previous bet being matched or partially matched or a combination of things. Another cool feature of BetJet Pro is that it utilises the last traded price to generate implied probabilities for an in-play race guage, as well as a host of other tools.</p> <p>BET JET PRO KEY FEATURES</p> <ul> <li>Real-Time API Status</li> <li>Native for Mac and PC</li> <li>Programmable Tick Offset tool with hedge, zero hedge configurable functionality, Stop Loss, Fill or Kill &amp; Stop Entry Tools</li> <li>HotKeys for Lay, Back, Hedge, &amp; \u201cGet Me Out\u201d</li> <li>Race Meter visual representation of prices in Grid</li> <li>Racing Metadata including Form, Jockey, Trainer and Barrier</li> <li>Five staking sets with five different stakes plus one user editable stake box</li> <li>One-Click betting and moving bets within the ladder view</li> <li>Zero-Hedge Bets</li> </ul> <p>To find out more and to sign up for a FREE 14 Day trial, head to the BetJet Pro Website</p>"},{"location":"automation/cymaticTraderRatingsAutomation/","title":"Cymatic Trader: Ratings automation","text":""},{"location":"automation/cymaticTraderRatingsAutomation/#automating-a-thoroughbred-ratings-strategy-using-cymatic-trader","title":"Automating a thoroughbred ratings strategy using Cymatic Trader","text":"<p>Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to the ratings tutorials for Bet Angel and Gruss, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Cymatic Trader. </p> <p>Cymatic Trader has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Cymatic Trader and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. </p>"},{"location":"automation/cymaticTraderRatingsAutomation/#-the-plan","title":"- The plan","text":"<p>We'll step through how we went about getting Cymatic Trader to place bets using the Betfair's Data Scientists' thoroughbred ratings model. Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. </p> <p></p> <p>Resources</p> <ul> <li>Ratings: Betfair's Data Scientists' thoroughbred ratings model</li> <li>Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings </li> <li>Tool: Cymatic Trader</li> </ul>"},{"location":"automation/cymaticTraderRatingsAutomation/#-set-up","title":"- Set up","text":"<p>Make sure you've downloaded and installed Cymatic Trader, and signed in. Once you open the program, you will see an Excel icon which is where we will link our spreadsheet to Cymatic Trader </p> <p></p>"},{"location":"automation/cymaticTraderRatingsAutomation/#-downloading-formatting-ratings","title":"- Downloading &amp; formatting ratings","text":"<p>Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. </p> <p></p> <p>Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. </p> <p></p> <p>Copy the ratings data over to the customised Cymatic Trader template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated. </p> <p></p>"},{"location":"automation/cymaticTraderRatingsAutomation/#-writing-your-rules","title":"- Writing your rules","text":"<p>As with any automated strategy, one of the most important steps is deciding what logical approach you want to take, and writing rules that suit. </p> <p>We're using a customised Cymatic Trader template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. </p> <p>This is how we used Excel to implement our set of rules. </p>"},{"location":"automation/cymaticTraderRatingsAutomation/#-trigger-to-place-bet","title":"- Trigger to place bet","text":"<p>In short, we want to back or lay runners when:</p> <ul> <li>The available to back price is greater than the rating for that runner, then we will back the runner</li> <li>The available to back price is less than the rating for that runner, then we will lay the runner</li> <li>Back market percentage is less than a certain value that we choose</li> <li>The scheduled event start time is less than a certain number of seconds that we choose</li> <li>The event isn't in play </li> </ul>"},{"location":"automation/cymaticTraderRatingsAutomation/#-using-cell-references-to-simplify-formulas","title":"- Using cell references to simplify formulas","text":"<p>Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement.</p> <p>Cell names used in this tutorial</p> <ul> <li> <p>Ratings refers to the entire Column I in the 'RATINGS' worksheet</p> </li> <li> <p>RunnerName refers to the entire column H in the 'RATINGS' worksheet</p> </li> <li> <p>Overrounds refers to cell BJ7 in the 'CYMATIC'worksheet, where the overrounds for the current market are calculated. </p> </li> <li> <p>UserOverround refers to cell G4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>TimeTillJump refers to cell D4 in the 'SETTINGS' worksheet</p> </li> <li> <p>UserTimeTillJump refers to cell G3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>InPlay refers to cell E4 in the 'CYMATIC' worksheet. Cymatic Trader will populate a 'FALSE' flag leading up to the jump</p> </li> <li> <p>BACKLAY refers to cell G5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners</p> </li> </ul> <p>This is our Excel formula trigger:</p> <p>``` excel tab=\"Multi line\" =IF(     AND(         OR(              AND(BACKLAY = \"BACK\", (H8 &gt; (INDEX(Ratings,MATCH(A8,RunnerName,0))))),              AND(BACKLAY = \"LAY\", (H8 &lt; (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),         Overrounds&lt;UserOverround,         TimeTillJump&lt;UserTimeTillJump,         InPlay=\"FALSE\"),         BACKLAY,     \"\" ) <pre><code>``` excel tab=\"Single line\"\n=IF(AND(OR(AND(BACKLAY = \"BACK\", (H8 &gt; (INDEX(Ratings,MATCH(A8,RunnerName,0))))),AND(BACKLAY = \"LAY\", (H8 &lt; (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),Overrounds&lt;UserOverround,TimeTillJump&lt;UserTimeTillJump,InPlay=\"FALSE\"),BACKLAY,\"\")\n</code></pre></p> <p>Stepping through each step:</p> <p>Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell G5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings.</p> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (H8 &gt; (INDEX(Ratings,MATCH(A8,RunnerName,0))))),\n             AND(BACKLAY = \"LAY\", (H8 &lt; (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),\n        Overrounds&lt;UserOverround,\n        TimeTillJump&lt;UserTimeTillJump,\n        InPlay=\"FALSE\"),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Back market percentage (Overrounds) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'RATINGS' worksheet. Additional information relating to over-rounds can be found here</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (H8 &gt; (INDEX(Ratings,MATCH(A8,RunnerName,0))))),\n             AND(BACKLAY = \"LAY\", (H8 &lt; (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),\n        Overrounds&lt;UserOverround,\n        TimeTillJump&lt;UserTimeTillJump,\n        InPlay=\"FALSE\"),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell G3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell D4 of the 'SETTINGS' worksheet (named 'TimeTillJump'), where we've already done the calculations for you. </li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (H8 &gt; (INDEX(Ratings,MATCH(A8,RunnerName,0))))),\n             AND(BACKLAY = \"LAY\", (H8 &lt; (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),\n        Overrounds&lt;UserOverround,\n        TimeTillJump&lt;UserTimeTillJump,\n        InPlay=\"FALSE\"),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is populated with the 'FALSE' flag, it's safe to place bets. </li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (H8 &gt; (INDEX(Ratings,MATCH(A8,RunnerName,0))))),\n             AND(BACKLAY = \"LAY\", (H8 &lt; (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),\n        Overrounds&lt;UserOverround,\n        TimeTillJump&lt;UserTimeTillJump,\n        InPlay=\"FALSE\"),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed.</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY = \"BACK\", (H8 &gt; (INDEX(Ratings,MATCH(A8,RunnerName,0))))),\n             AND(BACKLAY = \"LAY\", (H8 &lt; (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),\n        Overrounds&lt;UserOverround,\n        TimeTillJump&lt;UserTimeTillJump,\n        InPlay=\"FALSE\"),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <p>Excel functions</p> <ul> <li>IF function: IF(if this is true, do this, else do this)</li> <li>AND function: AND(this is true, and so is this, and so is this) - returns true or false</li> <li>AND OR function: checks that the statement meets more than one condition. If this OR that, then do the following. </li> <li>Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. </li> </ul>"},{"location":"automation/cymaticTraderRatingsAutomation/#-preparing-the-spreadsheet","title":"- Preparing the spreadsheet","text":"<p>You need to copy/paste the trigger formula into the relevant cells on each row in the 'Command' (BA) column. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight. </p> <ul> <li>Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column BA (BA8 for the first runner).</li> </ul> <p>``` excel tab=\"Multi line\" =IF(     AND(         OR(              AND(BACKLAY = \"BACK\", (H8 &gt; (INDEX(Ratings,MATCH(A8,RunnerName,0))))),              AND(BACKLAY = \"LAY\", (H8 &lt; (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),         Overrounds&lt;UserOverround,         TimeTillJump&lt;UserTimeTillJump,         InPlay=\"FALSE\"),         BACKLAY,     \"\" ) <pre><code>``` excel tab=\"Single line\"\n=IF(AND(OR(AND(BACKLAY = \"BACK\", (H8 &gt; (INDEX(Ratings,MATCH(A8,RunnerName,0))))),AND(BACKLAY = \"LAY\", (H8 &lt; (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),Overrounds&lt;UserOverround,TimeTillJump&lt;UserTimeTillJump,InPlay=\"FALSE\"),BACKLAY,\"\")\n</code></pre></p> <p></p> <ul> <li>Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks'. For simplicity's sake we're now just using the currently available back odds (cell I8 for the first runner). This goes in column BB (BB8 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this article. </li> </ul> <p>Note:</p> <p>The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are cells that are not populated with runners. A similar effect to IFERROR, if Cymatic Trader hasn't populated cell A8 with a runner name, then dont populate this cell at all.</p> <p><code>=IF(A8=0,\"\",I8)</code></p> <p></p> <ul> <li>Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. </li> </ul> <p><code>=IF(A8=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(I8-1), stake*(J8/(J8-1))-stake))</code></p> <p></p>"},{"location":"automation/cymaticTraderRatingsAutomation/#-selecting-markets","title":"- Selecting markets","text":"<p>We used the Navigator menu in Cymatic Trader to navigate to the tracks we had ratings for. If you wanted to include all horse or greyhound races for a day you could use the 'autopilot' tool to do this more efficiently.  Once you've chosen the races you're interested in tick the 'autopilot' button and Gruss will automatically cycle through each market for you. </p> <p></p>"},{"location":"automation/cymaticTraderRatingsAutomation/#-linking-the-spreadsheet","title":"- Linking the spreadsheet","text":"<p>Click the Excel icon in the main tool bar and then 'connect Excel' from the drop down menu. From here, you will be able to point Cymatic Trader in the direction of where your Excel sheet is located on your computer. Make sure 'Enable Trigger Commands' is selected and 'Clear status cells when selecting different market\" if you are automating a series of markets. </p> <p></p>"},{"location":"automation/cymaticTraderRatingsAutomation/#and-youre-set","title":"And you're set!","text":"<p>Once you've set your spreadsheet set up and you're comfortable using Cymatic Trader it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off.</p> <p>Note:</p> <p>you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.</p>"},{"location":"automation/cymaticTraderRatingsAutomation/#areas-for-improvement","title":"Areas for improvement","text":"<p>There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. </p>"},{"location":"automation/cymaticTraderRatingsAutomation/#what-next","title":"What next?","text":"<p>We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.</p>"},{"location":"automation/cymaticTraderRatingsAutomation/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/geeksToyRefreshSettings/","title":"Geeks Toy: Optimising refresh settings","text":""},{"location":"automation/geeksToyRefreshSettings/#refresh-settings","title":"Refresh settings","text":"<p>Geeks Toy can refresh up to 4 times faster than the Exchange! You do also can change some settings to make the full market depth refresh constantly or just have selections closest to the match point refresh constantly, while the rest of the market displays at a slower rate.</p> <p>When you load Geeks Toy:</p> <ul> <li>Right click and go to \u201cShow/Hide\u201d</li> <li>Then click on \u201cAPI Settings Manager\u201d</li> </ul> <p></p> <p>You then have six different options, allowing you to change various refresh rates inside the software. (It is worth noting that the Exchange updates every 1000ms, so setting refresh rates to 250ms makes them refresh 4 times faster than the  Exchange)</p> <ul> <li>Bets - the information relating to your bets for a market, both matched and unmatched</li> <li>Prices\u00a0- the information relating to the amounts waiting to back and lay. This varies depending on the setting used on the Ladder for Price Display. If <code>Standard</code> or <code>Hybrid</code> is selected it refers to the front three back and lay prices. If <code>Complete</code> is selected it refers to all of the prices, this requires a lot more information to be downloaded each call which is not suitable for slower internet connections and those on limited download allowances</li> <li>Complete Prices in Hybrid mode\u00a0- the amounts waiting to back and lay outside of the front three prices, i.e. full market depth, when <code>Hybrid</code> Price Display is selected on the Ladder. For people with slow internet connections and those on limited download allowances it should not be set low i.e. less than 1000ms</li> <li>Traded Volume\u00a0- the information relating to traded volumes</li> <li>External Bets\u00a0- how often to poll for bets made external to the application. The default is 10 seconds. This saves on bandwith and number of calls if you only use this application for betting.</li> <li>Account Funds\u00a0- the information relating to your balance. If a change is made to a bet, i.e. a bet is submitted, altered or matched the balance will be updated automatically</li> </ul> <p>You can change visibility of these settings within the ladder format.</p>"},{"location":"automation/geeksToyRefreshSettings/#changing-market-ladder-settings","title":"Changing market ladder settings","text":"<p>Once you are in a market ladder do the following:</p> <ul> <li>Right click the market header</li> <li>Select <code>Visual Options</code></li> <li>Select <code>Price Display</code></li> </ul> <p>You will see three options:</p> <ul> <li><code>Standard</code> is where you will only see the first 3 sets of odds and they will be updated as per your API settings. The rest of the market will not be visible.</li> <li><code>Hybrid</code> is where the entire market is visible but only the first 3 prices will be updated as per your API settings.</li> <li><code>Complete</code> is where the entire market is updated as per the API settings.</li> </ul> <p></p>"},{"location":"automation/geeksToyRefreshSettings/#selection-profit-or-hedged-market-profit","title":"Selection profit or hedged (market) profit","text":"<p>As Geeks Toy is predominantly used for trading markets, a great setting to use is the function to show you your Profit/Loss on a certain selection within the market, or your overall Profit/Loss on the entire market (also known as your hedged profit).</p> <p>Once you are in a market ladder do the following:</p> <ul> <li>Right click the market header</li> <li>Select <code>Visual Options</code></li> <li>Select <code>Profit/Loss</code></li> </ul> <p>You will see two options:</p> <ul> <li><code>Selection Profit</code> is the option you will can select if you would like to see Profit/Loss on each selection in the market individually.</li> <li><code>Hedged Profit</code> is the option you can select for the software to show you your overall profit/loss across the entire market cumulatively.</li> </ul> <p></p>"},{"location":"automation/geeksToyRefreshSettings/#resources","title":"Resources","text":"<ul> <li>Geeks Toy Youtube Channel</li> <li>Caan Berry Pro Trader Youtube Channel</li> <li>Betfair Interview With Cann Berry</li> </ul>"},{"location":"automation/geeksToyRefreshSettings/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/grussKellyStake/","title":"Gruss Betting Assistant: Kelly Criterion staking","text":""},{"location":"automation/grussKellyStake/#automating-with-kelly-staking-method-and-gruss-betting-assistant","title":"Automating with Kelly staking method and Gruss Betting Assistant","text":"<p>In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters.</p> <p>Gruss Betting Assistant has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. </p>"},{"location":"automation/grussKellyStake/#-the-plan","title":"- The plan","text":"<p>We'll be building on the Gruss Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model. For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here, as the concepts and underlying trigger based strategy here do build on what we covered previously. </p> <p>Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. </p> <p>If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read about Kelly staking. There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. </p> <p>Resources</p> <ul> <li>Ratings: Betfair's Data Scientists' thoroughbred ratings model</li> <li>Before you start: check out the Gruss Ratings tutorial </li> <li>Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy</li> <li>Understanding how the Kelly Criterion staking strategy works</li> <li>Tool: Gruss Betting Assistant</li> </ul>"},{"location":"automation/grussKellyStake/#-recapping-the-strategy-covered-in-the-gruss-ratings-automation-tutorial","title":"- Recapping the strategy covered in the Gruss ratings automation tutorial","text":"<p>We'll be using the same trigger strategy that's outlined in the Gruss Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub. Whilst the trigger will remain unchanged, we'll need to make small tweaks to the stake column of the 'MARKET' worksheet (column S) and we've added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Gruss ratings tutorial, we highly recommend that you do so as to understand how the bet placement trigger works. The tutorial can be found here.</p>"},{"location":"automation/grussKellyStake/#-set-up","title":"- Set up","text":"<p>Make sure you've downloaded and installed Gruss, and signed in.</p> <p></p>"},{"location":"automation/grussKellyStake/#-writing-your-rules","title":"- Writing your rules","text":"<p>We're using a customised version of the Gruss Ratings tutorial template to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. </p> <p>This is how we used Excel to implement our set of rules. </p>"},{"location":"automation/grussKellyStake/#-using-cell-references-to-simplify-formulas","title":"- Using cell references to simplify formulas","text":"<p>Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement.</p> <p>Cell names used in this tutorial</p> <ul> <li>Account Balance refers to cell I2 of the 'MARKET' worksheet</li> <li>UserStake refers to cell D4 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake</li> <li>StakeType refers to cell X1 of the \"SETTINGS' worksheet</li> <li>Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet</li> <li>HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet</li> </ul> <p>Calculating the Kelly stake</p> <p>As explained here, the formula to claculate the Kelly stake is:</p> <p><code>(BP-Q)/B</code> Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P).</p> <p>To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet.  </p> <p></p>"},{"location":"automation/grussKellyStake/#stepping-through-each-step","title":"Stepping through each step:","text":"<ul> <li>Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('MARKET' worksheet) and return the best available back odds from the G column</li> </ul> <p><pre><code>=IFERROR(INDEX(Market!$B$5:$M$50,MATCH(H2,Market!$A$5:$A$50,0),5),\"\")\n</code></pre> </p> <ul> <li>Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds)</li> </ul> <p><pre><code>=IFERROR(K2-1,\"\")\n</code></pre> </p> <ul> <li>Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability</li> </ul> <p><pre><code>=IFERROR((1/I2),\"\")\n</code></pre> </p> <ul> <li>Column N - Probability of loss % 1 divided by the probability of a win from column M</li> </ul> <p><pre><code>=IFERROR(1-M2,\"\")\n</code></pre> </p> <ul> <li>Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1)</li> </ul> <p><pre><code>=IFERROR(((L2*M2)-N2)/L2,\"\")\n</code></pre> </p> <ul> <li>Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2</li> </ul> <p><pre><code>=IFERROR(O2/2,\"\")\n</code></pre> </p> <ul> <li>Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Gruss populates in cell I2 of the 'MARKET' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0.</li> </ul> <p><pre><code>=IFERROR(IF(AccountBalance*P2&gt;0,AccountBalance*O2,\"0\"),\"\")\n</code></pre> </p> <ul> <li>Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. </li> </ul> <p><pre><code>=IFERROR(IF(AccountBalance*P2&gt;0,AccountBalance*P2,\"0\"),\"\")\n</code></pre> </p> <ul> <li>Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly.</li> </ul> <p></p> <p></p> <p>Excel functions</p> <ul> <li>IF statement: IF(if this is true, do this, else do this)</li> <li>IFERROR: If there is an error that occurs in the cell, display nothing</li> <li>AND statement: AND(this is true, and so is this, and so is this) - returns true or false</li> <li>Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. </li> </ul>"},{"location":"automation/grussKellyStake/#-preparing-the-spreadsheet","title":"- Preparing the spreadsheet","text":"<p>You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight. </p> <ul> <li>Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet.</li> </ul> <p><code>=IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType))</code></p> <p></p>"},{"location":"automation/grussKellyStake/#-you-know-the-drill","title":"- You know the drill","text":"<p>The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Gruss.</p>"},{"location":"automation/grussKellyStake/#-selecting-markets","title":"- Selecting markets","text":"<p>Gruss makes it really easy to select markets in bulk. You could go through an add each market individually, but it's much easier to just use the quick pick functionality to add all Australian racing win markets. </p> <p></p> <p>You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps.</p> <p></p>"},{"location":"automation/grussKellyStake/#-linking-the-spreadsheet","title":"- Linking the spreadsheet","text":"<p>This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. </p> <p></p> <p>It will autofill the workbook and sheet names. You'll then need to make sure you tick:</p> <ul> <li>Enable triggered betting</li> <li>Clear Bet refs on auto select market</li> <li>Quick pick reload triggers select first market</li> <li>Show balance which is required for this staking strategy to work</li> </ul> <p>Then click OK and the sheet with be linked with the program. </p> <p></p>"},{"location":"automation/grussKellyStake/#and-youre-set","title":"And you're set!","text":"<p>Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off.</p> <p>Note:</p> <p>you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.</p>"},{"location":"automation/grussKellyStake/#areas-for-improvement","title":"Areas for improvement","text":"<p>There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. </p> <p>For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this. </p>"},{"location":"automation/grussKellyStake/#what-next","title":"What next?","text":"<p>We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.</p>"},{"location":"automation/grussKellyStake/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/grussMarketFavouriteAutomation/","title":"Gruss Betting Assistant: Market favourite automation","text":""},{"location":"automation/grussMarketFavouriteAutomation/#automating-a-market-favourite-strategy-using-gruss-betting-assistant","title":"Automating a market favourite strategy using Gruss Betting Assistant","text":"<p>Here we explore how to implement an automated strategy to place Betfair Starting Price (BSP) bets on the top two runners in the market. This lets you choose your selections based on market sentiment close to the jump, and not worry about current market price by using BSP to place your bets. You could equally use effectively the same approach if you wanted to lay the favourite(s) instead of backing them.</p> <p>Building on our previous articles, we're using the spreadsheet functionality available in Gruss Betting Assistant to implement this strategy. If you haven't already we'd recommend going back and having a read of this article, as the concepts here do build on what we covered previously. As we've said before, there are so many different ways to use this part of Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions. </p>"},{"location":"automation/grussMarketFavouriteAutomation/#-the-plan","title":"- The plan","text":"<p>Given that we're simply choosing our selections based on the market we don't need any ratings for this strategy. The plan is to look at the market a couple of minutes before the scheduled jump and place BSP bets based on its formation. </p> <p>Our approach here, and how we've set up the accompanying spreadsheet, backs the top two runners in the market two minutes out from the scheduled start time using the Betfair Starting Price. </p> <p>Resources</p> <ul> <li>Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and approach</li> <li>Tool: Gruss Betting Assistant</li> </ul>"},{"location":"automation/grussMarketFavouriteAutomation/#-set-up","title":"- Set up","text":"<p>Make sure you've downloaded and installed Gruss, and signed in.</p> <p></p>"},{"location":"automation/grussMarketFavouriteAutomation/#-writing-your-rules","title":"- Writing your rules","text":"<p>As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. </p> <p>We're using a customised version of the default Gruss template Excel sheet to implement our strategy, so it can make betting decisions based on the favourites being shown in the market. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. </p> <p>This is how we used Excel to implement our set of rules. </p>"},{"location":"automation/grussMarketFavouriteAutomation/#-trigger-to-place-bet","title":"- Trigger to place bet","text":"<p>In short, we want to back runners when:</p> <ul> <li>the selection's available to back price (Column F) is either the lowest or second lowest in the market - the top two market favourites with the ability to easily change this</li> <li>the scheduled event start time is less and greater than what we specify</li> <li>Back market percentage is less than a certain value that we choose</li> <li>the event isn't in play </li> </ul>"},{"location":"automation/grussMarketFavouriteAutomation/#-using-cell-references-to-simplify-formulas","title":"- Using cell references to simplify formulas","text":"<p>Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement.</p> <p>Cell names used in this tutorial</p> <ul> <li> <p>Fav refers to cell C5 in the 'SETTINGS' worksheet</p> </li> <li> <p>Overrounds1, Overrounds2 and Overrounds3 refers to cell Y4 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets</p> </li> <li> <p>UserOverround refers to cell C4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell C9, C10 and C11 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market</p> </li> <li> <p>MinTime refers to cell C3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>MaxTime refers to cell E3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>InPlay1, InPlay2, InPlay3 refers to cell E2 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets respectively. Gruss will populate a status in these worksheet cells such as \"Not in Play\" for each market</p> </li> <li> <p>MarketStatus1, MarketStatus2, MarketStatus3 refers to cell F2 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets respectively. Gruss will populate a status in these worksheet cells such as \"Suspended\" for each market</p> </li> </ul> <p>This is our trigger on Excel formula:</p> <p>``` excel tab=\"Multi line\" =IF(     AND(         (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)         -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) &lt; Fav+1,         TimeTillJump1 &lt; MaxTime,         TimeTillJump1 &gt; MinTime,         Overrounds1&lt;UserOverrounds,         InPlay1=\"Not In Play\",         MarketStatus1&lt;&gt;\"Suspended\"),     \"BACK-SP\",     \"\") <pre><code>``` excel tab=\"Single line\"\n=IF(\n    AND(\n        (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)-RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) &lt; Fav+1,TimeTillJump1 &lt; MaxTime,TimeTillJump1 &gt; MinTime,Overrounds1&lt;UserOverrounds,InPlay1=\"Not In Play\",MarketStatus1&lt;&gt;\"Suspended\"),\"BACK-SP\",\"\")\n</code></pre></p> <p>Stepping through each step:</p> <ul> <li>Finding the top two selections in the market: check each runner to see if they're one of the two market favourites - We're doing this by going through the best available to back (column F) price for each runner, ranking them in order (which sorts them from highest to lowest - which is the opposite of what we want) then subtracting that rank number from the total number of selections available to inverse the order. Finally, we plus one to the resulting rank - if we didn't do this then you'd have a rank order that started at 0, not 1, and we thought that would just confuse matters!</li> </ul> <p>Once it's established what each selection's rank is, we then check if that rank is less than three, and if it is we know that the runner in question is one of the top two in the market, based on the current available to back prices.</p> <pre><code>=IF(\n    AND(\n        (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)\n        -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) &lt; Fav+1,\n        TimeTillJump1 &lt; MaxTime,\n        TimeTillJump1 &gt; MinTime,\n        Overrounds1&lt;UserOverrounds,\n        InPlay1=\"Not In Play\",\n        MarketStatus1&lt;&gt;\"Suspended\"),\n    \"BACK-SP\",\n    \"\")\n</code></pre> <ul> <li>TimeTillJump1 &lt; MaxTime and &gt; MinTime: check whether the seconds left on the countdown are smaller than what is defined in cell C3 and greater than cell E3 in the 'SETTINGS' worksheet (named 'MinTime' and 'MaxTime' respectively). This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. We'll keep it simple by referencing the value in cell C9 (named 'TimeTillJump1') in the 'SETTINGS' worksheet, where we've already done the calculations for you.</li> </ul> <pre><code>=IF(\n    AND(\n        (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)\n        -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) &lt; Fav+1,\n        TimeTillJump1 &lt; MaxTime,\n        TimeTillJump1 &gt; MinTime,\n        Overrounds1&lt;UserOverrounds,\n        InPlay1=\"Not In Play\",\n        MarketStatus1&lt;&gt;\"Suspended\"),\n    \"BACK-SP\",\n    \"\")\n</code></pre> <ul> <li>Overrounds1 &lt; UserOverrounds: checking whether the market overrounds are less than the specific value that is specified in cell C4 of the 'SETTINGS' worksheet</li> </ul> <pre><code>=IF(\n    AND(\n        (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)\n        -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) &lt; Fav+1,\n        TimeTillJump1 &lt; MaxTime,\n        TimeTillJump1 &gt; MinTime,\n        Overrounds1&lt;UserOverrounds,\n        InPlay1=\"Not In Play\",\n        MarketStatus1&lt;&gt;\"Suspended\"),\n    \"BACK-SP\",\n    \"\")\n</code></pre> <ul> <li>InPlay1: checking whether the event has gone in play, as this is purely a pre-play strategy, though you could certainly take a similar approach to in-play markets. InPlay refers to E2 in the 'MARKET' worksheet, if this cell displays \"Not In Play\" as a value, it's safe to place bets. </li> </ul> <pre><code>=IF(\n    AND(\n        (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)\n        -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) &lt; Fav+1,\n        TimeTillJump1 &lt; MaxTime,\n        TimeTillJump1 &gt; MinTime,\n        Overrounds1&lt;UserOverrounds,\n        InPlay1=\"Not In Play\",\n        MarketStatus1&lt;&gt;\"Suspended\"),\n    \"BACK-SP\",\n    \"\")\n</code></pre> <ul> <li>MarketStatus1: checks whether the event is suspended, by checking if Gruss has populated cell F2 in the \"MARKET\" worksheet with \"Suspended\". If the cell has any other value, it's safe to place bets. </li> </ul> <pre><code>=IF(\n    AND(\n        (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)\n        -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) &lt; Fav+1,\n        TimeTillJump1 &lt; MaxTime,\n        TimeTillJump1 &gt; MinTime,\n        Overrounds1&lt;UserOverrounds,\n        InPlay1=\"Not In Play\",\n        MarketStatus1&lt;&gt;\"Suspended\"),\n    \"BACK-SP\",\n    \"\")\n</code></pre> <ul> <li>Result: if the statement above is true, the formula returns 'BACK-SP', at which point the bet will trigger, taking BSP. If any of the previous conditions are not met, then no bet will be placed and the cell will remin blank. </li> </ul> <pre><code>=IF(\n    AND(\n        (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)\n        -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) &lt; Fav+1,\n        TimeTillJump1 &lt; MaxTime,\n        TimeTillJump1 &gt; MinTime,\n        Overrounds1&lt;UserOverrounds,\n        InPlay1=\"Not In Play\",\n        MarketStatus1&lt;&gt;\"Suspended\"),\n    \"BACK-SP\",\n    \"\")\n</code></pre> <p>updating the trigger for 'MARKET 2' and 'MARKET 3' worksheets</p> <p>You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market.  </p> <ul> <li>Trigger for 'MARKET 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2</li> </ul> <pre><code>=IF(\n    AND(\n        (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)\n        -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) &lt; Fav+1,\n        TimeTillJump2 &lt; MaxTime,\n        TimeTillJump2 &gt; MinTime,\n        Overrounds2&lt;UserOverrounds,\n        InPlay2=\"Not In Play\",\n        MarketStatus2&lt;&gt;\"Suspended\"),\n    \"BACK-SP\",\n    \"\")\n</code></pre> <ul> <li>Trigger for 'MARKET 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3</li> </ul> <pre><code>=IF(\n    AND(\n        (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)\n        -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) &lt; Fav+1,\n        TimeTillJump3 &lt; MaxTime,\n        TimeTillJump3 &gt; MinTime,\n        Overrounds3&lt;UserOverrounds,\n        InPlay3=\"Not In Play\",\n        MarketStatus3&lt;&gt;\"Suspended\"),\n    \"BACK-SP\",\n    \"\")\n</code></pre> <p>Excel functions</p> <ul> <li>IF function: IF(if this is true, do this, else do this)</li> <li>AND function: AND(this is true, and so is this, and so is this) - returns true or false</li> <li>COUNT function: returns number of cells in the range you pass in tha contain a number</li> <li>RANK function: returns the rank of a number in a list of numbers, with the smallest number returning the highest rank.</li> <li>Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. </li> </ul>"},{"location":"automation/grussMarketFavouriteAutomation/#-preparing-the-spreadsheet","title":"- Preparing the spreadsheet","text":"<p>You need to copy/paste this formula into the relevant cells for each of the runners. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight. </p> <ul> <li>Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column Q (Q5 for the first runner).</li> </ul> <p>``` excel tab=\"Multi line\" =IF(     AND(         (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)         -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) &lt; Fav+1,         TimeTillJump1 &lt; MaxTime,         TimeTillJump1 &gt; MinTime,         Overrounds1&lt;UserOverrounds,         InPlay1=\"Not In Play\",         MarketStatus1&lt;&gt;\"Suspended\"),     \"BACK-SP\",     \"\") <pre><code>``` excel tab=\"Single line\"\n=IF(AND((COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)-RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) &lt; Fav+1,TimeTillJump1 &lt; MaxTime,TimeTillJump1 &gt; MinTime,Overrounds1&lt;UserOverrounds,InPlay1=\"Not In Play\",MarketStatus1&lt;&gt;\"Suspended\"),\"BACK-SP\",\"\")\n</code></pre></p> <p></p> <ul> <li>Odds: as we said we're putting the bet up initially at odds of 1000, so this is a simple one. </li> </ul> <p>Note:</p> <p>The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column A. A similar effect to IFERROR, if Gruss hasn't populated cell A5 with a runner name, then dont populate this cell at all.</p> <p><code>=IF(A5=\"\",\"\",\"1000\")</code></p> <p></p> <ul> <li>Stake: it's completely up to you what staking approach you want to take. We're keeping it simple and using flat stake here, so will just place $10 on each runner. This goes in column S (S5 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. </li> </ul> <p><code>=IF(A5=\"\",\"\",stake)</code></p> <p></p>"},{"location":"automation/grussMarketFavouriteAutomation/#-you-know-the-drill","title":"- You know the drill","text":"<p>The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Gruss.</p>"},{"location":"automation/grussMarketFavouriteAutomation/#-selecting-markets","title":"- Selecting markets","text":"<p>Gruss makes it really easy to select markets in bulk. You could go through an add each market individually, but it's much easier to just use the quick pick functionality to add all Australian racing win markets. </p> <p></p> <p>You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps.</p> <p></p>"},{"location":"automation/grussMarketFavouriteAutomation/#-linking-the-spreadsheet","title":"- Linking the spreadsheet","text":"<p>This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. </p> <p></p> <p>It will autofill the workbook and sheet names. You'll then need to make sure you tick:</p> <ul> <li>Enable triggered betting</li> <li>Clear Bet refs on auto select market</li> <li>Quick pick reload triggers select first market</li> </ul> <p>Then click OK and the sheet with be linked with the program. </p> <p></p>"},{"location":"automation/grussMarketFavouriteAutomation/#and-youre-set","title":"And you're set!","text":"<p>Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off.</p> <p>Note:</p> <p>you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.</p>"},{"location":"automation/grussMarketFavouriteAutomation/#areas-for-improvement","title":"Areas for improvement","text":"<p>There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. </p> <p>For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this. </p>"},{"location":"automation/grussMarketFavouriteAutomation/#what-next","title":"What next?","text":"<p>We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.</p>"},{"location":"automation/grussMarketFavouriteAutomation/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/grussRatingsAutomation/","title":"Gruss Betting Assistant: Ratings automation","text":""},{"location":"automation/grussRatingsAutomation/#automating-a-greyhound-ratings-strategy-using-gruss","title":"Automating a greyhound ratings strategy using Gruss","text":"<p>Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to the ratings automation tutorial for Bet Angel, but here we'll be using the ratings for greyhounds, created by the data science team at Betfair and incorporate them into our automation in Gruss. </p> <p>Gruss Betting Assistant has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. </p>"},{"location":"automation/grussRatingsAutomation/#-the-plan","title":"- The plan","text":"<p>We're using the Greyhound Ratings Model put together by some of our Data Scientists. This model creates ratings for Victorian and Queensland greyhound races daily and is freely available on the Hub. It's pretty good at predicting winners, so we're going to place back bets on the dogs with shorter ratings where the market price is better than the model's rating. Gruss Betting Assistant's Excel triggered betting feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here. </p> <p>Here we'll step through how we went about getting Gruss to place bets using these ratings. Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program running and be able to walk away. Obviously, you can use your own ratings and change the rules according to what your strategy is.</p> <p></p> <p>Resources</p> <ul> <li>Ratings: Betfair Data Scientists' Greyhound Ratings Model</li> <li>Rules: here's the spreadsheet We set up with our rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings </li> <li>Tool: Gruss Betting Assistant</li> </ul>"},{"location":"automation/grussRatingsAutomation/#-set-up","title":"- Set up","text":"<p>Make sure you've downloaded and installed Gruss Betting Assistant, and signed in.</p> <p></p>"},{"location":"automation/grussRatingsAutomation/#-downloading-formatting-ratings","title":"- Downloading &amp; formatting ratings","text":"<p>Here we're using the Betfair's Data Scientists' greyhound ratings model for greyhound racing but alternatively you can follow the same process using the Betfair's Data Scientists' thoroughbred Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. </p> <p></p> <p>Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. </p> <p></p> <p>Copy the ratings data over to the customised Gruss template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated.</p> <p></p>"},{"location":"automation/grussRatingsAutomation/#-writing-your-rules","title":"- Writing your rules","text":"<p>As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. </p> <p>We're using a customised version of the default Gruss template Excel sheet to implement our strategy, so it can make betting decisions based on my ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. There are lots of posts on the Gruss Forum on the topic if you want to explore it more yourself.</p> <p>This is how we used Excel to implement our set of rules. </p>"},{"location":"automation/grussRatingsAutomation/#-trigger-to-place-bet","title":"- Trigger to place bet","text":"<p>In short, we want to back runners when:</p> <ul> <li>The available to back price is greater than the rating for that runner, then we will back the runner</li> <li>The available to back price is less than the rating for that runner, then we will lay the runner</li> <li>Back market percentage is less than a certain value that we choose</li> <li>The scheduled event start time is less than a certain number of seconds that we choose</li> <li>The event isn't in play </li> </ul>"},{"location":"automation/grussRatingsAutomation/#-using-cell-references-to-simplify-formulas","title":"- Using cell references to simplify formulas","text":"<p>Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement.</p> <p>Cell names used in this tutorial</p> <ul> <li> <p>Ratings refers to the entire Column J in the 'RATINGS' worksheet</p> </li> <li> <p>SelectionID refers to the entire column G in the 'RATINGS' worksheet</p> </li> <li> <p>Overround refers to cell Y in the 'MARKET'worksheet, where the overrounds for the current market are calculated. </p> </li> <li> <p>UserOverround refers to cell C5 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>TimeTillJump refers to cell C9 in the 'SETTINGS' worksheet</p> </li> <li> <p>UserTimeTillJump refers to cell C4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners</p> </li> <li> <p>InPlay refers to cell E2 in the 'MARKET' worksheet. Gruss will populate a 'Not In Play' status leading up to the jump</p> </li> <li> <p>BACKLAY refers to cell C6 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners</p> </li> <li> <p>MARKETSTATUS refers to F2 in the 'MARKET' worksheet</p> </li> </ul> <p>This is our trigger for the 'MARKET' worksheet</p> <p>``` excel tab=\"Multi line\" =IF(     AND(         OR(              AND(BACKLAY=\"BACK\",(F5&gt;(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),              AND(BACKLAY=\"LAY\",(F5&lt;(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),         Overround&lt;UserOverround,         TimeTillJump&lt;UserTimeTillJump,         InPlay=\"Not In Play\",         MarketStatus&lt;&gt;\"Suspended\"),         BACKLAY,     \"\" ) <pre><code>``` excel tab=\"Single line\"\n=IF(AND(OR(AND(BACKLAY=\"BACK\",(F5&gt;(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),AND(BACKLAY=\"LAY\",(F5&lt;(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),Overround&lt;UserOverround,TimeTillJump&lt;UserTimeTillJump,InPlay=\"Not In Play\",MarketStatus&lt;&gt;\"Suspended\"),BACKLAY,\"\")\n</code></pre></p> <p>Stepping through each step:</p> <ul> <li>Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell D6). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings.</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY=\"BACK\",(F5&gt;(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),\n             AND(BACKLAY=\"LAY\",(F5&lt;(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),\n        Overround&lt;UserOverround,\n        TimeTillJump&lt;UserTimeTillJump,\n        InPlay=\"Not In Play\",\n        MarketStatus&lt;&gt;\"Suspended\"),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Back market percentage (Overround) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'RATINGS' worksheet. Additional information relating to over-rounds can be found here.</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY=\"BACK\",(F5&gt;(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),\n             AND(BACKLAY=\"LAY\",(F5&lt;(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),\n        Overround&lt;UserOverround,\n        TimeTillJump&lt;UserTimeTillJump,\n        InPlay=\"Not In Play\",\n        MarketStatus&lt;&gt;\"Suspended\"),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell C4 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. </li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY=\"BACK\",(F5&gt;(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),\n             AND(BACKLAY=\"LAY\",(F5&lt;(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),\n        Overround&lt;UserOverround,\n        TimeTillJump&lt;UserTimeTillJump,\n        InPlay=\"Not In Play\",\n        MarketStatus&lt;&gt;\"Suspended\"),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If cell E2 in the 'MARKET' worksheet has a 'Not In Play' flag, it's safe to place bets. </li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY=\"BACK\",(F5&gt;(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),\n             AND(BACKLAY=\"LAY\",(F5&lt;(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),\n        Overround&lt;UserOverround,\n        TimeTillJump&lt;UserTimeTillJump,\n        InPlay=\"Not In Play\",\n        MarketStatus&lt;&gt;\"Suspended\"),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Market Status checking whether the event has been suspended - if there is any flag other than 'Suspended' in cell F2 of the 'MARKET' worksheet, it's safe to place bets.</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY=\"BACK\",(F5&gt;(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),\n             AND(BACKLAY=\"LAY\",(F5&lt;(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),\n        Overround&lt;UserOverround,\n        TimeTillJump&lt;UserTimeTillJump,\n        InPlay=\"Not In Play\",\n        MarketStatus&lt;&gt;\"Suspended\"),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <ul> <li>Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed.</li> </ul> <pre><code>=IF(\n    AND(\n        OR(\n             AND(BACKLAY=\"BACK\",(F5&gt;(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),\n             AND(BACKLAY=\"LAY\",(F5&lt;(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),\n        Overround&lt;UserOverround,\n        TimeTillJump&lt;UserTimeTillJump,\n        InPlay=\"Not In Play\",\n        MarketStatus&lt;&gt;\"Suspended\"),\n        BACKLAY,\n    \"\"\n)\n</code></pre> <p>Excel functions</p> <ul> <li>IF function: IF(if this is true, do this, else do this)</li> <li>AND function: AND(this is true, and so is this, and so is this) - returns true or false</li> <li>AND OR function: checks that the statement meets more than one condition. If this OR that, then do the following. </li> <li>Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.  </li> </ul>"},{"location":"automation/grussRatingsAutomation/#-preparing-the-spreadsheet","title":"- Preparing the spreadsheet","text":"<p>You need to copy/paste these three formulas into the relevant cell on each runner - we did a few extra rows than the number of runners in the markets we were looking at, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight. </p> <ul> <li>Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column Q (Q5 for the first runner) in the 'MARKET' worksheet.</li> </ul> <p>``` excel tab=\"Multi line\" =IF(     AND(         OR(              AND(BACKLAY=\"BACK\",(F5&gt;(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),              AND(BACKLAY=\"LAY\",(F5&lt;(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),         Overround&lt;UserOverround,         TimeTillJump&lt;UserTimeTillJump,         InPlay=\"Not In Play\",         MarketStatus&lt;&gt;\"Suspended\"),         BACKLAY,     \"\" ) <pre><code>``` excel tab=\"Single line\"\n=IF(AND(OR(AND(BACKLAY=\"BACK\",(F5&gt;(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),AND(BACKLAY=\"LAY\",(F5&lt;(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),Overround&lt;UserOverround,TimeTillJump&lt;UserTimeTillJump,InPlay=\"Not In Play\",MarketStatus&lt;&gt;\"Suspended\"),BACKLAY,\"\")\n</code></pre></p> <p></p> <ul> <li>Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks'. For simplicity's sake we're now just using the currently available back odds (cell F5 for the first runner). This goes in column R (R5 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. </li> </ul> <p><code>=IF(F5=0,\"\",F5)</code></p> <p></p> <p>Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. </p> <p><code>=IF(A5=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(F5-1),stake*(H5/(H5-1))-stake)</code></p> <p></p>"},{"location":"automation/grussRatingsAutomation/#-selecting-markets","title":"- Selecting markets","text":"<p>Gruss makes it really easy to select markets in bulk. You could go through an add each market you have in your ratings individually, but it's much easier to just use the quick Pick functionality to add all Australian racing win markets. This is safe, because bets will only fire when they link up with a runner in your 'RATINGS' sheet. </p> <p></p> <p>You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps.</p> <p></p>"},{"location":"automation/grussRatingsAutomation/#-linking-the-spreadsheet","title":"- Linking the spreadsheet","text":"<p>This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. </p> <p></p> <p>It will autofill the workbook and sheet names. You'll then need to make sure you tick:</p> <ul> <li>Enable triggered betting</li> <li>Clear Bet refs on auto select market</li> <li>Quick pick reload triggers select first market</li> </ul> <p>Then click OK and the sheet with be linked with the program. </p> <p></p>"},{"location":"automation/grussRatingsAutomation/#and-youre-set","title":"And you're set!","text":"<p>Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off.</p> <p>Note:</p> <p>you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.</p>"},{"location":"automation/grussRatingsAutomation/#areas-for-improvement","title":"Areas for improvement","text":"<p>There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. </p> <p>For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this. </p>"},{"location":"automation/grussRatingsAutomation/#what-next","title":"What next?","text":"<p>We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au </p>"},{"location":"automation/grussRatingsAutomation/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/grusslSimultaneousMarkets/","title":"Gruss: Automating simultaneous markets","text":""},{"location":"automation/grusslSimultaneousMarkets/#dont-miss-out-on-a-market-with-simultaneous-automation","title":"Don't miss out on a market with simultaneous automation","text":"<p>If you have a concern of missing markets due to delays or unforeseen circumstances at a market, Gruss is able to work off multiple worksheets for different meetings, all from the same workbook.  For example, we can have worksheet one from our spreadsheet work through the markets taking place in Flemington, while worksheet 2 will work through markets at Sandown, worksheet 3 works through markets at Bendigo etc. </p> <p>To Set this up, we need to do need to duplicate the main \u2018Market\u2019 worksheet and have enough instances of the sheet for each meeting. There are four meetings that I want to cover, so I have duplicated the worksheet four times and renamed them to help me keep track. </p> <p></p> <p>After these changes have been made, restart Gruss so that the changes go into effect. Once Gruss is back open, as before, click \u2018Market\u2019, \u2018Add to Quick Pick List\u2019, \u2018Horse Racing\u2019 (Or any event you are automating for), the country that you would like to bet and then \u2018All Win\u2019. </p> <p></p> <p>This should generate a bunch of tabs in Gruss, each tab containing all the markets for a specific meeting. Simply close the ones you don\u2019t want. </p> <p>Once you\u2019re ready, click the \u2018Excel\u2019 menu in Gruss, and \u2018Log Multiple sheets Quick link\u2019. Select your Excel Workbook, assign the worksheets, select \u2018Enable Triggered Betting\u2019, \u201cCLEAR trigger clears matched odds\u2019, \u2018Clear bet refs on auto select markets\u2019, \u2018Clear bet refs on manual select market\u2019 and finally \u2018Auto Select First Market\u2019. Your screen should look something like this:</p> <p></p> <p>When you\u2019re ready for automation to take over, click \u2018Start logging\u2019 at the bottom of the window. </p>"},{"location":"automation/grusslSimultaneousMarkets/#and-youre-set","title":"And you're set!","text":"<p>Once you've set your spreadsheet set up and you're comfortable using Gruss it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off.</p> <p>Note:</p> <p>you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.</p>"},{"location":"automation/grusslSimultaneousMarkets/#what-next","title":"What next?","text":"<p>We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.</p>"},{"location":"automation/grusslSimultaneousMarkets/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/overview/","title":"Existing tools","text":"<p>There are many applications that have been built by external developers using Betfair's Vendor offering that can be used in conjunction with Betfair which can help wagering from more one-click bets to automating your strategies.</p> <p>If you haven\u2019t before used a betting tool, below is a overview of some of the most popular tools available. </p> <p></p> <p>If you have any suggestions for new tutorials/improvements please reach out to automation@betfair.com.au - We'd love to hear your thoughts and feedback. </p>"},{"location":"automation/overview/#popular-tools","title":"Popular tools","text":"<p>Bet Angel Pro // Gruss // Market Feeder // Cymatic Trader </p>"},{"location":"automation/overview/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/BfBotManager/BfBotManager/","title":"BfBotManager","text":"<p>Bf Bot Manager is software designed to help you automate your betting and trading at Betfair betting exchange.</p> <p>Bf Bot Manager allows you to run unlimited number of strategies/bots at the same time. You can have one strategy betting on favourites, second one trading on horse races, third one betting on tennis matches and several strategies betting on football events or tipster tips.</p> <p>You can set software to automatically download or import your or tipster betting tips and bet on them just few seconds before event start time.</p> <p>Your settings can be exported to file allowing you to easily share strategy with your friends or to create backup.</p> <p>Software also supports manual bet placement and has ladder and grid controls for one click betting. Hedge (green/red up) functionality can be set to execute automatically or on single click.</p> <p>Bf Bot Manager supports simulation mode, many staking plans, loss recovery and much more. It even has football statistics for major leagues allowing you to automatically bet only on teams with specific previous results.</p> <p>You can also order custom functionality that we can develop for you and adjust settings to suit your own requirements.</p> <p>BF BOT MANAGER KEY FEATURES</p> <ul> <li>Custom Bots</li> <li>Dutching</li> <li>Automation</li> <li>Greening Tools</li> <li>Simulation Mode</li> <li>One-click Betting</li> </ul> <p> </p> <p>To find out more and to sign up for a FREE 5 Day trial, head to the BF Bot Manager Website</p>"},{"location":"automation/CymaticTrader/CymaticTrader/","title":"Overview","text":"<p>Taking trading to another level, this fully featured, super fast trading software has many unique features developed over a number of years.  Intuitive throughout, it was designed for professional traders but is equally welcomed by novices.</p> <p>It was the first program to actually reveal your estimated position in the exchange order book queue (PIQ), helping you gain an advantage when trading manually or with the built in robot.  PIQ is now an essential feature for many traders.</p> <p>You can open multiple markets simultaneously and perform one-click trading using customizable ladders or grids.  Greening, tick-offset, stop-loss and a range of other features are included, as you\u2019d expect in a great low-latency trading app.</p> <p>The Excel integration capability enables you to view real-time prices in Excel, trigger orders from Excel and even create your own fully automated trading robot!</p> <p>The integrated advanced charting is capable of displaying candle-stick, bar or line charts, in various time frames, plus a huge range of technical analysis indicators.  Zooming, panning and drawing magnetic trend lines are all made easy.</p> <p>Another unique feature is the \u2018API Monitor\u2019, it can instantly reveal errors or bottlenecks caused by the internet or the exchange \u2013 thus helping you avoid trading during bad periods that other traders may often be oblivious to.  It also calculates real time statistics such as the latency of various types of calls to the Betfair API for your current trading session.  Warning messages and statistical reports can even be automatically emailed to you by the software.</p> <p>The software is well suited to single or multiple monitor scenarios.  Downloading/filtering of betting history and account statements is also included.</p> <p>GRUSS KEY FEATURES</p> <ul> <li>Rapid keyboard betting short cuts</li> <li>Advanced charting</li> <li>Football Odds predictor</li> <li>Ladder interface</li> <li>Excel integration</li> <li>Practice mode</li> </ul> <p> </p> <p>To find out more and to sign up for a FREE 14 Day trial, head to the Cymatic Trader Website.</p>"},{"location":"automation/Elo/Elo/","title":"Elo","text":""},{"location":"automation/Elo/Elo/#-what-is-elo","title":"- What is Elo?","text":"<p>The Elo rating system was orginally created by Hungarian born physics professor Arpad Elo who was a chess master and competed within the United States Chess Federation (USCF) and developed the system as an alternative to other rating systems that were considered to be inaccurate. Rather than rating a players performance by their overall wins and losses like previous systems, the Elo rating system works by assigning each player or team an Elo rating. When a team or player beats another, the winning side gains a portion of the losing sides points. This difference between Elo ratings between competitors is then used to create a probability for a particular outcome.</p> <p>The Elo rating system has become so popular since its inception, it is widely used today behind the scenes for a range of applications such as dating websites to determince compatibility between matches and even video game tournaments to ensure players of a similar skill level are matched together.        </p>"},{"location":"automation/Elo/Elo/#-how-does-elo-work","title":"- How does Elo work?","text":"<p>For the purpose of this explanation, we'll be using Elo in the context of rating AFL teams. Each team begins with an Elo rating of 1500 and will gain or lose a portion of their Elo from / to the opposite team depending if they win or lose a game. For example, if team A has 1500 Elo and they beat team B - who also started with a 1500 Elo, then a certain portion of Elo (usually 30) is deducted from the loser and awarded to the winner. In this scenario, Team A will have an Elo rating of 1530 and Team B will be left with 1470. What makes Elo great for betting in sports is that it can be converted to a probability which in turn can be converted into odds used to place bets.  </p>"},{"location":"automation/Elo/Elo/#-the-algorithm","title":"- The Algorithm","text":"<p>Each team will have their own Elo rating which will be determined based on their historical performance. If a team has never played a game, then they will begin with an Elo rating of 1500. The following formula is used to calculate the probability of a win for each team: </p> <p></p> <p>A crucial consideration of any Elo rating system is the implementation of what is known as the K-factor. This determines the \"sensitivity\" how much of an impact a win or a loss has on a teams Elo rating. A good K-factor is generally around 32 but this can be adjusted to suit. </p> <p>Once the match has been concluded, each teams Elo is then updated to reflect any changes to their Elo rating, taking into account the full sequence of wins and loses up to the most recent match played. For this, the below formula is used:</p> <p></p> <p>Where elo_i (t+1) is the updated Elo, elo_i (t) was their Elo before the match, K is the K-factor mentioned before (which we determined to be 32), outcome is an indicator of the match outcome (1 if it was won by team A, 0 if a loss), and Pwin was the pre-match probability of winning for team A, as given by the previous formula.</p> <p>If two teams play their first match, both with Elo of 1500, Pwin is equal to 0.5 so, for a K-factor of 32, the winning team would gain 16 points and the losing team would lose 16 points. In their next match, the teams start with Elo ratings of 1,516 and 1,484 respectively, and by updating their rating for all matches played, their current Elo rating can be calculated.</p>"},{"location":"automation/Elo/Elo/#-taking-elo-to-the-next-level","title":"- Taking Elo to the next level","text":"<p>If you're interested in learning more about Elo and implementing a dynamic K-factor instead of a static one for greater accuracy, check out this article which applies Elo in the context of modelling tennis. Doing so can bring greater accuracy, especially when applied to the AFL season as the K-factor can be set to change depending on the importance of a particular match such as finals and early / late season games. </p> <p>Resources</p> <ul> <li>Learn more about the origins of Elo by visiting the Elo Wikipedia page</li> <li>Take a look at this Elo Tennis model article</li> <li>Here's another good resource to help gain a better understanding of how Elo works and is applied: The Math behind Elo</li> </ul>"},{"location":"automation/Elo/Elo/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred.  Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"automation/GeeksToy/GeeksToy/","title":"Overview","text":"<p>Have this cookie :)</p>"},{"location":"automation/Gruss/Gruss/","title":"Overview","text":"<p>Gruss Software offers one-click betting in the standard or ladder type interface and also the ability to link into Excel using pre-defined triggers to place bets. Betting Assistant deploys a wealth of functionality such as green-up, fill or kill, stop loss, dutch betting, coupon market view and more.</p> <p>Gruss Software is a constantly evolving solution where the developers listen to the users and their needs. Users can communicate directly with the developers, make suggestions and exchange ideas on the Betting Assistant Forum.</p> <p>GRUSS KEY FEATURES</p> <ul> <li>One click betting</li> <li>Dutching tool</li> <li>Ladder interface</li> <li>One click betting</li> <li>Advanced market navigation</li> <li>Excel integration</li> </ul> <p> </p> <p>To find out more and to sign up for a FREE 30 Day trial, head to the Gruss Software Website.</p>"},{"location":"automation/MarketFeeder/MarketFeeder/","title":"MarketFeeder","text":"<p>Auto-Green-Up, Auto-Dutch and implement triggered betting with MarketFeeder Pro. As well as the standard manual betting tools you know and love, the auto-trading functions allow for stress-free betting with the triggered betting feature.</p> <p>A trigger is an effective way to maximise your time and safe-guard you from any errors. A short set of instructions can be set to perform one of 50 actions, including backing, laying, cancelling bets, slowing or speeding up market refresh, greening up and spreading the loss, alerting, emailing and indeed being your second hand on Betfair.</p> <p>MARKETFEEDER PRO KEY FEATURES</p> <ul> <li>Monitoring Simultanious Markets</li> <li>0.3 Second Refresh Rate</li> <li>Automated Market Search tool</li> <li>Triggered Betting</li> <li>Excel Spreadsheet Interaction</li> <li>Automatic Greenup</li> <li>Automatic Dutching</li> <li>Ladder Interface for Scalping</li> </ul> <p> </p> <p>To find out more and to sign up for a FREE 1 month trial, head to the MarketFeeder Pro Website</p>"},{"location":"automation/betAngel/betAngel/","title":"Overview","text":"<p>Bet Angel is one of the longest established API Products available.</p> <p>Bet Angel has three separate products available for Betfair customers. Bet Angel Basic is free and useful for placing occasional bets but has limited trading and professional tools. Bet Angel Trader contains the essential trading tools and Bet Angel Professional is the ultimate trading tool kit providing all the advanced tools and features required.</p> <p>Bet Angel Betting Applications offer a complete suite of tools all of which can be used with live or via a fully featured, risk free, practice mode. In a fully customisable interface, you can Back &amp; Lay across multiple screens on a range of sports.</p> <p>The Guardian Advanced Automation feature allows you to determine your automation rules with ease. Enabling complex automated betting based on timing, price triggers, comparative odds conditions and more. Automatically Cash-Out when your desired profit level is achieved and assure yourself a level profit (greening) no matter the result.</p> <p>For those who wish to write their own automated betting or trading strategies, Bet Angel can link to Microsoft Excel offering automation opportunities that are limitless.</p> <p>BET ANGEL KEY FEATURES</p> <ul> <li>Practice Mode</li> <li>One Click Betting</li> <li>Ladder Interface</li> <li>Advanced Charting</li> <li>Dutching and Bookmaking tools</li> <li>Trigger based automation </li> <li>Excel Spreadsheet integration</li> <li>Cash-Out</li> </ul> <p> </p> <p>To find out more and to sign up for a FREE 14 Day trial, head to the Bet Angel Website</p>"},{"location":"contactUs/test/","title":"Meet the Team","text":""},{"location":"contactUs/test/#ben-douglas","title":"Ben Douglas","text":"<p>Discord Handle: BenD_BF</p> <p>My Role: Automation Manager - Working with customers to help them get their strategies automated  and supplying customers with resources to increase their edge</p> <p>Date Started at Betfair: Feb 2020</p> <p>My Edge: Ability to focus for hours on end, so I can record every bet in a spreadsheet and track whether I'm winning or losing</p>"},{"location":"contactUs/test/#mitch-motyka","title":"Mitch Motyka","text":"<p>Discord Handle: MitchM_BF</p> <p>My Role: Client Development Executive - Helping clients automate their strategies and models</p> <p>Date Started at Betfair: Oct 2022</p> <p>My Edge: Low tech doesn't always mean low success</p> <p>Best Winning Moment: 7-leg UK Goliath Soccer Multi win on my birthday</p>"},{"location":"contactUs/test/#email-contacts","title":"Email Contacts","text":"<p>Automation Queries: automation@betfair.com.au</p> <p>Datathon Queries: datathon@betfair.com.au</p> <p>API Queries: api@betfair.com.au</p> <p>Commercial Enquiries: bdp@betfair.com.au</p>"},{"location":"data/dataListing/","title":"Historic Data Listing","text":"<p>There are many different ways in which historical Betfair data can be sourced in order to find an edge on a given market. </p> <p>Some examples being:</p> <ul> <li>Betfair Historic Data Site (Betfair Australia &amp; New Zealand customers should reach out to us directly before purchasing any data from here)</li> <li>Racing BSP File Listing (also called 'PROMO' Files)</li> <li>Betfair Hub Results Page</li> </ul> <p>However, trying to find sports data or more detailed racing data can be challenging, especially if you don't have a programming background to process the historic JSON Files (although we have an excellent tutorial). That's why we've worked to bring you some additional sports and racing files to make backtesting a model that much easier.</p> <p>As this site is run by Betfair Australia and New Zealand, unfortunately we are only able to provide data on markets which take place within Australia and New Zealand (so no English Premier League unfortunately). If, after viewing the data, there's something else you'd like to see, please reach out to us at data@betfair.com.au.</p>"},{"location":"data/dataListing/#disclaimer","title":"Disclaimer","text":"<p>By downloading this data, you acknowledge and agree that:</p> <ul> <li>(a) Betfair does not make any representations, or give any warranties, as to the accuracy or completeness of the data provided; and</li> <li>(b) you use the data at your own risk, and Betfair will not be liable for any loss suffered in using the data.</li> </ul>"},{"location":"data/dataListing/#australia-new-zealand-racing","title":"Australia &amp; New Zealand Racing","text":""},{"location":"data/dataListing/#thoroughbred","title":"Thoroughbred","text":"<ul> <li>Aus &amp; NZ Thoroughbreds 2023-01</li> <li>Aus &amp; NZ Thoroughbreds 2023-02</li> <li>Aus &amp; NZ Thoroughbreds 2023-03</li> <li>Aus &amp; NZ Thoroughbreds 2023-04</li> <li>Aus &amp; NZ Thoroughbreds 2023-05</li> <li>Aus &amp; NZ Thoroughbreds 2023-06</li> <li>Aus &amp; NZ Thoroughbreds 2023-07</li> <li>Aus &amp; NZ Thoroughbreds 2023-08</li> <li>Aus &amp; NZ Thoroughbreds 2023-09</li> <li>Aus &amp; NZ Thoroughbreds 2023-10</li> <li>Aus &amp; NZ Thoroughbreds 2023-11</li> <li>Aus &amp; NZ Thoroughbreds 2023-12</li> <li>Aus &amp; NZ Thoroughbreds 2024-01</li> <li>Aus &amp; NZ Thoroughbreds 2024-02</li> <li>Aus &amp; NZ Thoroughbreds 2024-03</li> <li>Aus &amp; NZ Thoroughbreds 2024-04</li> <li>Aus &amp; NZ Thoroughbreds 2024-05</li> <li>Aus &amp; NZ Thoroughbreds 2024-06</li> <li>Aus &amp; NZ Thoroughbreds 2024-07</li> </ul>"},{"location":"data/dataListing/#thoroughbred-predictions-model","title":"Thoroughbred Predictions Model","text":"<ul> <li>Thoroughbred Model Results 2021</li> <li>Thoroughbred Model Results 2022</li> <li>Thoroughbred Model Results 2023</li> <li>Thoroughbred Model Results 2024 Jan - Jul</li> </ul>"},{"location":"data/dataListing/#top-5-model","title":"Top 5 Model","text":"<ul> <li>Top 5 Model Results 2019</li> <li>Top 5 Model Results 2020</li> <li>Top 5 Model Results 2021</li> <li>Top 5 Model Results 2022</li> <li>Top 5 Model Results 2023</li> <li>Top 5 Model Results 2024 Jan - Jul</li> </ul>"},{"location":"data/dataListing/#harness","title":"Harness","text":"<ul> <li>Aus &amp; NZ Harness 2023-01</li> <li>Aus &amp; NZ Harness 2023-02</li> <li>Aus &amp; NZ Harness 2023-03</li> <li>Aus &amp; NZ Harness 2023-04</li> <li>Aus &amp; NZ Harness 2023-05</li> <li>Aus &amp; NZ Harness 2023-06</li> <li>Aus &amp; NZ Harness 2023-07</li> <li>Aus &amp; NZ Harness 2023-08</li> <li>Aus &amp; NZ Harness 2023-09</li> <li>Aus &amp; NZ Harness 2023-10</li> <li>Aus &amp; NZ Harness 2023-11</li> <li>Aus &amp; NZ Harness 2023-12</li> <li>Aus &amp; NZ Harness 2024-01</li> <li>Aus &amp; NZ Harness 2024-02</li> <li>Aus &amp; NZ Harness 2024-03</li> <li>Aus &amp; NZ Harness 2024-04</li> <li>Aus &amp; NZ Harness 2024-05</li> <li>Aus &amp; NZ Harness 2024-06</li> <li>Aus &amp; NZ Harness 2024-07</li> </ul>"},{"location":"data/dataListing/#harness-predictions-model","title":"Harness Predictions Model","text":"<ul> <li>Harness Model Results 2023</li> <li>Harness Model Results 2024 Jan - Jul</li> </ul>"},{"location":"data/dataListing/#greyhounds","title":"Greyhounds","text":"<ul> <li>Aus &amp; NZ Greyhounds 2023-01</li> <li>Aus &amp; NZ Greyhounds 2023-02</li> <li>Aus &amp; NZ Greyhounds 2023-03</li> <li>Aus &amp; NZ Greyhounds 2023-04</li> <li>Aus &amp; NZ Greyhounds 2023-05</li> <li>Aus &amp; NZ Greyhounds 2023-06</li> <li>Aus &amp; NZ Greyhounds 2023-07</li> <li>Aus &amp; NZ Greyhounds 2023-08</li> <li>Aus &amp; NZ Greyhounds 2023-09</li> <li>Aus &amp; NZ Greyhounds 2023-10</li> <li>Aus &amp; NZ Greyhounds 2023-11</li> <li>Aus &amp; NZ Greyhounds 2023-12</li> <li>Aus &amp; NZ Greyhounds 2024-01</li> <li>Aus &amp; NZ Greyhounds 2024-02</li> <li>Aus &amp; NZ Greyhounds 2024-03</li> <li>Aus &amp; NZ Greyhounds 2024-04</li> <li>Aus &amp; NZ Greyhounds 2024-05</li> <li>Aus &amp; NZ Greyhounds 2024-06</li> <li>Aus &amp; NZ Greyhounds 2024-07</li> </ul>"},{"location":"data/dataListing/#greyhound-predictions-model","title":"Greyhound Predictions Model","text":"<ul> <li>Greyhound Model Results 2020</li> <li>Greyhound Model Results 2021</li> <li>Greyhound Model Results 2022</li> <li>Greyhound Model Results 2023</li> <li>Greyhound Model Results 2024 Jan - Jul</li> </ul>"},{"location":"data/dataListing/#betfair-hub-tipsters-racing-tips","title":"Betfair Hub Tipsters - Racing Tips","text":"<ul> <li>Betfair Hub Tipsters 2023-03</li> <li>Betfair Hub Tipsters 2023-04</li> <li>Betfair Hub Tipsters 2023-05</li> <li>Betfair Hub Tipsters 2023-06</li> <li>Betfair Hub Tipsters 2023-07</li> <li>Betfair Hub Tipsters 2023-08</li> <li>Betfair Hub Tipsters 2023-09</li> <li>Betfair Hub Tipsters 2023-10</li> <li>Betfair Hub Tipsters 2023-11</li> <li>Betfair Hub Tipsters 2023-12</li> <li>Betfair Hub Tipsters 2024-01</li> <li>Betfair Hub Tipsters 2024-02</li> <li>Betfair Hub Tipsters 2024-03</li> <li>Betfair Hub Tipsters 2024-04</li> <li>Betfair Hub Tipsters 2024-05</li> <li>Betfair Hub Tipsters 2024-06</li> <li>Betfair Hub Tipsters 2024-07</li> </ul>"},{"location":"data/dataListing/#australian-sport","title":"Australian Sport","text":""},{"location":"data/dataListing/#afl","title":"AFL","text":"<ul> <li>AFL 2021 All Markets</li> <li>AFL 2021 Match Odds Markets</li> <li>AFL 2022 All Markets</li> <li>AFL 2022 Match Odds Markets</li> <li>AFL 2023 All Markets</li> <li>AFL 2023 Match Odds Markets</li> <li>AFL 2024 All Markets</li> <li>AFL 2024 Match Odds Markets</li> </ul>"},{"location":"data/dataListing/#aflw","title":"AFLW","text":"<ul> <li>AFLW 2021 All Markets</li> <li>AFLW 2021 Match Odds</li> <li>AFLW 2022 (S6) All Markets</li> <li>AFLW 2022 (S6) Match Odds</li> <li>AFLW 2022 (S7) All Markets</li> <li>AFLW 2022 (S7) Match Odds</li> <li>AFLW 2023 All Markets</li> <li>AFLW 2023 Match Odds</li> </ul>"},{"location":"data/dataListing/#nrl","title":"NRL","text":"<ul> <li>NRL 2021 All Markets</li> <li>NRL 2021 Match Odds</li> <li>NRL 2022 All Markets</li> <li>NRL 2022 Match Odds</li> <li>NRL 2023 All Markets</li> <li>NRL 2023 Match Odds</li> <li>NRL 2024 All Markets</li> <li>NRL 2024 Match Odds</li> </ul>"},{"location":"data/dataListing/#a-league","title":"A-League","text":"<ul> <li>A-League 2020-2021 All Markets</li> <li>A-League 2020-2021 Match Odds Markets</li> <li>A-League 2021-2022 All Markets</li> <li>A-League 2021-2022 Match Odds Markets</li> <li>A-League 2022-2023 All Markets</li> <li>A-League 2022-2023 Match Odds Markets</li> <li>A-League 2023-2024 All Markets</li> <li>A-League 2023-2024 Match Odds Markets</li> </ul>"},{"location":"data/dataListing/#a-league-womens","title":"A-League Womens","text":"<ul> <li>A-League Womens 2020-2021 All Markets</li> <li>A-League Womens 2020-2021 Match Odds Markets</li> <li>A-League Womens 2021-2022 All Markets</li> <li>A-League Womens 2021-2022 Match Odds Markets</li> <li>A-League Womens 2022-2023 All Markets</li> <li>A-League Womens 2022-2023 Match Odds Markets</li> <li>A-League Womens 2023-2024 All Markets</li> <li>A-League Womens 2023-2024 Match Odds Markets</li> </ul>"},{"location":"data/dataListing/#bbl","title":"BBL","text":"<ul> <li>BBL 2020-2021 All Markets</li> <li>BBL 2020-2021 Match Odds Markets</li> <li>BBL 2021-2022 All Markets</li> <li>BBL 2021-2022 Match Odds Markets</li> <li>BBL 2022-2023 All Markets</li> <li>BBL 2022-2023 Match Odds Markets </li> <li>BBL 2023-2024 All Markets</li> <li>BBL 2023-2024 Match Odds Markets</li> </ul>"},{"location":"data/dataListing/#wbbl","title":"WBBL","text":"<ul> <li>WBBL 2020 All Markets</li> <li>WBBL 2020 Match Odds Markets</li> <li>WBBL 2021 All Markets</li> <li>WBBL 2021 Match Odds Markets</li> <li>WBBL 2022 All Markets</li> <li>WBBL 2022 Match Odds Markets</li> <li>WBBL 2023 All Markets</li> <li>WBBL 2023 Match Odds Markets</li> </ul>"},{"location":"data/dataListing/#nbl","title":"NBL","text":"<ul> <li>NBL 2021-2022 All Markets</li> <li>NBL 2021-2022 Match Odds Markets</li> <li>NBL 2022-2023 All Markets</li> <li>NBL 2022-2023 Match Odds Markets</li> <li>NBL 2023-2024 All Markets</li> <li>NBL 2023-2024 Match Odds Markets</li> </ul>"},{"location":"mentalGame/HighExpectations/","title":"8: The High Cost of High Expectations","text":"<p>High expectations are simultaneously a driver of success and a cause of failure. They\u2019re a double edged sword capable of pushing you to be better and better, fueling motivation, but at the cost of your confidence. Confidence that is needed to handle the intense ups and downs of punting. You want to be steady mentally, not riding a roller coaster of emotions. </p> <p>On the surface high expectations don\u2019t seem like much of a problem. Why wouldn\u2019t you want to aim for the highest peaks? Of course you do and I agree with you. I don\u2019t want you to think for one second that my intent is for you to lower your gaze and accept outcomes below what you want to obtain. I actually want to help you get there, but to do that you need to understand the difference between goals and expectations, as well as the high cost of high expectations.</p> <p>Imagine your goals are the peak of a mountain, and to reach them you have to climb up there. You can\u2019t take a helicopter to the top, you\u2019ve got to take every step. When you are making that climb with high expectations looming over you (which damages confidence and I\u2019ll explain more on that in a bit), it feels like you\u2019re climbing that mountain with no rope \u2013 climbers call it \u201cfree soloing,\u201d where one wrong move means you\u2019ll plummet to your demise. </p> <p>At any moment you could feel like a failure because you don\u2019t have the confidence to support yourself on that climb. Instead, when you are not \u201cfree soloing\u201d and you climb with anchors every 3 to 4 meters, if you slip, you might get a little hurt but you can easily brush it off and figure out how to get past the part where you fell. Metaphorically, confidence gives you those anchors and the ability to recover quickly.</p> <p>Over the last several years I\u2019ve noticed more and more clients coming to me with problems in their mental game, not knowing that high expectations were the cause. Since this problem can be hard to spot, here are a few common characteristics to help you see if it\u2019s an issue for you:</p> <ul> <li>Checking results every day and tweaking your system (both model and staking) after just one day of poor performance</li> <li>Taking a system offline very quickly if there are a few poor days of performance or if profit and loss dips below zero</li> <li>Keeping a system off when it dips below zero for a long time to \u2018work\u2019 on the system</li> <li>Spending an unjustifiable amount of time writing perfect code when a fast work around will do</li> <li>Putting intense pressure on yourself to achieve but you can\u2019t handle the pressure</li> <li>Feeling like nothing is good enough, even after winning</li> <li>Never applaud good results, or not for very long</li> <li>Having a hard time moving on, letting go, or getting over a mistake</li> <li>Becoming self-critical over the slightest missteps</li> <li>Constantly comparing yourself to others</li> </ul> <p>With these in mind let\u2019s look at two causes of the damage high expectations inflict on confidence. </p>"},{"location":"mentalGame/HighExpectations/#expectations-goals","title":"Expectations \u2260 Goals","text":"<p>Many people think expectations and goals are the same thing, but their difference isn\u2019t just a choice of words. There\u2019s an intent that goes with each that is widely different. </p> <p>Whether you realize it or not, an expectation implies a guarantee. So in the back of your mind you\u2019re guaranteeing that you will attain the results you want, which means that you believe you either have the required skills to get there or you\u2019re certain you\u2019ll develop them along the way. As a result, expectations don\u2019t care one bit about progress, they only care about getting the outcome that you seek. So when you underperform, even just by a bit, expectations unleash. No wonder you\u2019re anxious \u2013 you fear the wrath of a potential beating.  </p> <p>On the other hand, goals imply the road is uncertain and while the skills needed may be known, how you\u2019ll acquire them is not. Goals imply learning. Goals imply ups and downs. Goals imply facing unexpected problems and the potential for chaos, all of which needs to be solved for. </p> <p>How does all of this impact confidence? You derive no confidence from results that meet your expectations. Why would you get credit for just doing what you\u2019re supposed to do? Since your expectations are so high, most often you fall short and lose confidence, having underperformed relative to your expectation. Over time you develop a negative self-perception because in your mind all you\u2019ve done is underperform.</p> <p>This, my friends, is a rigged game. The way to make it fair is to convert your expectations into goals, building out a plan with the smaller goals that build on each other along the way. Doing so allows you to get credit for your progress, learning, problem-solving and effort. You can be proud of yourself for each climb up the mountain. Doing this doesn\u2019t undermine your motivation like many fear that it would. Instead it fuels it because your confidence climbs along with you. You\u2019re no longer fearful of outcomes, you launch yourself towards your goals knowing that you have the internal confidence to handle whatever punting throws back at you. Of course, I\u2019m not talking about taking on excessive risk, I\u2019m simply saying to not fear bad outcomes that slow your learning and limit your success.</p>"},{"location":"mentalGame/HighExpectations/#reshaping-your-perspective","title":"Reshaping Your Perspective","text":"<p>Now that you have a clear understanding of the problem, here are two things you can do about it. First, you need to change how you evaluate yourself day-to-day. Shift your focus towards the decisions and actions that you take each day and rate yourself on them. Do that first before looking at your daily monetary results. Thus, even if you lost money, if you performed well yourself, your emotions start to shift and you\u2019ll feel less bad. This is a skill like any other so don\u2019t expect your emotions to dramatically change over night. But if you keep up with it day-after-day, eventually you\u2019ll care much more about what you have control over in the short-term, rather than outcomes that are highly variable. </p> <p>Second, you need to correct your faulty perception of the past that was skewed by your high expectations. To help you complete this task, I\u2019ve created a worksheet that you can download. </p> <p>Follow these steps:</p> <ul> <li>Step 1: List out your accomplishments as a punter, and do your personal ones if punting is new to you, or if high expectations are a problem there too.</li> <li>Step 2: For each accomplishment, answer the three questions listed in the worksheet.</li> <li>Step 3: List the skills you\u2019ve acquired as a punter, or a professional in general (page 3 of the worksheet).</li> </ul> <p>Here\u2019s the trick, you can only do this work for up to 10 to 15 minutes at a time, 3 times per day. Reshaping your perspective is not something you can do all at once. To truly reshape it you must commit to a regular dose of this work. </p> <p>I like to think of doing this work like eating food. For years your high expectations have been starving you of food that you rightfully earned. You can\u2019t eat all of the food you need for a month in one sitting, you must have meals each day. Do the same with this process and you\u2019ll be amazed at how much stronger your confidence will become. And as your confidence stabilizes and strengthens, you will have the anchors you need to climb the mountain and reach your goals.</p> <p>Jared Tendler, MS is a mental game coach for world champion poker players, PGA Tour players, sports bettors and financial traders from 45 countries. He is the author of three highly acclaimed books, The Mental Game of Poker 1 &amp; 2 and his newest book The Mental Game of Trading. Find out more about Jared\u2019s work at: https://jaredtendler.com/ </p>"},{"location":"mentalGame/InchwormConcept/","title":"7: Maximize Your Learning with the Inchworm Concept","text":"<p>The importance of maximizing your learning should be obvious. The punter who learns faster has an advantage, much like an athlete who separates themselves by developing skills faster than their competitors.</p> <p>But do you ever step back to ask yourself: Is the way I\u2019m working the best way for me to improve? If you\u2019re like many of the clients that I\u2019ve worked with across multiple industries, punters included, the answer is no.</p> <p>That doesn\u2019t mean you\u2019re not committed to developing your expertise and improving your analysis, modeling, risk management, or ability to identify new strategies. It is obvious you do those things, and more, by the sheer fact that you\u2019re reading this article. But I suspect that many of you have fallen into a common trap, where you take your overall process for granted and miss the chance to be more effective. The good news is that you can do that quite easily \u2013 it just takes thinking about learning from a new perspective.</p> <p>The easiest way for you to maximize your learning is not by focusing on your strengths, it\u2019s by improving your weaknesses. By this I mean the weaknesses that you cannot outsource to someone else or a computer, like your ability to handle the emotional ups/downs. Weaknesses are often well-known since they\u2019re so costly, and yet, they\u2019re equally detested. It\u2019s much more fun and exciting to build on existing strengths than it is to dig around in crap. But doing that dirty work accelerates your progress, while avoiding it paralyzes your progress.</p> <p>To drive home this dynamic, there\u2019s an insect that perfectly represents what I\u2019m talking about. You heard me right, an insect. It\u2019s called an inchworm, and it is a wonderful illustration of how to maximize your learning. It also happens to be one the most commonly cited concepts from my books that readers say had a big impact on them.</p>"},{"location":"mentalGame/InchwormConcept/#the-inchworm-concept","title":"The Inchworm Concept","text":"<p>As you can see below, an inchworm moves in a distinct way. If you look closely enough, it looks like a moving bell curve.</p> <p> </p> <p>As it relates to your performance, a bell curve can show the natural variation that exists in your decision-making. Think about the quality of your punting decisions over the last 6 to 12 months. Let\u2019s say you were able to accurately rate the quality of all of these decisions on a scale of 1 to 100, where 1 represents your worst decisions and 100 are your best, and then plotted them on a graph. What you\u2019d see is a bell curve showing the variation in your performance from best to worst, and everything in between.</p> <p>This defines your current range of decision-making. All the knowledge and skills that you\u2019re currently learning exist within that range. This is your A-,B-,C-game that I referred to in part III. The quality of your decision-making is bound by your range. There\u2019s a limit to how bad your decisions can be, and there\u2019s a cap on how good your decisions can be as well. (Of course, I know you can improve the top end decisions when you\u2019re in the zone or innovating - more on that in part IX.)</p> <p>The concept of Inchworm comes in when you look at how improvement happens over time. A bell curve is a static snapshot of your skills over a defined period of time and shows how frequently you were performing well, at average, or poorly. Improvement is the forward movement of your bell curve over time\u2014something an actual inchworm illustrates perfectly in the way it moves.</p> <p>Consistent improvement happens by taking one step forward from the front of your bell curve, where your A-game becomes even better, followed by another step forward from the back, where your C-game becomes less terrible. Over time, improving both sides of your range moves your entire bell curve further to the right, and your old C-game no longer becomes possible.</p> <p>However, if you focus only on improving your frontend, as many mistakenly do, your range gets wider. As a result, a host of problems develop, such as emotional swings, learning plateaus, burnout, and poor focus and motivation. Contrary to what many believe, your backend doesn\u2019t automatically move forward because the frontend did.</p>"},{"location":"mentalGame/InchwormConcept/#key-lessons-from-the-inchworm-concept","title":"Key Lessons From The Inchworm Concept","text":"<p>The lessons from Inchworm can be profound when you truly embrace it. That starts by getting serious about fixing your weaknesses. The good news is this series is designed to help you fix the mental and emotional issues that are part of your C-game. They are the biggest reason that punters aren\u2019t able to progress. </p> <p>Use the Mental Hand History from part V to help you identify the \u201crocks\u201d that effectively are anchoring the back of your Inchworm. Also, take the time to complete an A to C-game Analysis from part III as it will help you to identify the areas of your game that you\u2019re battling to correct.</p> <p>The old adage that a team is only as strong as its weakest link also applies to individuals. Your C-game holds you back, and if you don\u2019t work on it consistently, the instability caused by that wide range between C-game and A-game will ripple through every aspect of your punting. </p> <p>The key to working on your weakness points is what I like to call \u201csucking less.\u201d When risk aversion, overanalyzing, turning systems on/off randomly, scaling up to chase losses, or any other mistakes that are part of your C-game appear, your job is not to eliminate them all in one go, just to make them slightly less bad. Accomplish that and you\u2019ve made progress. Progress that you can feel good about. Progress that gives you something to build on.</p> <p>I know that may sound pathetic, but if I sent you into the gym right now to bench press some weight, you wouldn\u2019t just throw up 200kg when you can only lift 50kg. No, you\u2019d put up 52kg and then progressively add weight. That\u2019s how you become stronger. Pile up too much weight and you\u2019re likely to hurt yourself. The same idea applies to making your mental game weaknesses less weak. Try to do too much and you risk a mental or emotional injury - like loss of confidence and motivation. </p> <p>This kind of progress also helps you in a slump or when you\u2019ve had a bad day. People often compare their A-game to their C-game, or their best to their worst, when really they need to compare their current A-game to their previous A-game, or their current C-game to their previous C-game. This kind of evaluation leads to greater understanding of progress, or lack thereof. When you\u2019re struggling, ask yourself if your C-game is less bad than it was 1, 3, 6 or 12 months ago. If the answer is yes, great! You\u2019re moving forward. If the answer is no, it means that you haven\u2019t yet figured out what\u2019s causing your C-game. Dig deeper, reevaluate and create a new strategy.</p> <p>Improving your backend is truly transformational and can be the catalyst to a big \u201caha moment\u201d or new insights. By moving your back end forward, you free up mental space that was previously consumed by the problems there. With those problems truly gone, you will automatically think differently \u2013 whether about yourself, punting or your model, depending on the insight. That kind of progress is particularly exciting because it comes from your own mind and doesn\u2019t rely on advice or perspective from anyone else.</p> <p>I know the idea of working on weaknesses has been talked about before. But Inchworm gives you a new perspective on how focusing on your weaknesses can propel you forward, and shows you that just sucking less can make all the difference.</p> <p>Jared Tendler, MS is a mental game coach for world champion poker players, PGA Tour players, sports bettors and financial traders from 45 countries. He is the author of three highly acclaimed books, The Mental Game of Poker 1 &amp; 2 and his newest book The Mental Game of Trading. Find out more about Jared\u2019s work at: https://jaredtendler.com/ </p>"},{"location":"mentalGame/TappingYourIntuition/","title":"9: Tapping Your Intuition","text":"<p>As I wrap up this series I thought it would be nice to leave you on a high note and talk about something more aspirational, rather than another problem to solve. </p> <p>Intuition, or your \u201cgut instinct\u201d, has the potential to be another component of your edge, allowing you to unlock your highest level of decision-making, innovation, and ability to make critical, and sometimes split-second, decisions that impact your bottom line. It allows you to cut through the noise and self-doubt to notice, for example, that something \u201cdoesn\u2019t look right\u201d with a prediction from your model.</p> <p>Intuition is a strong feeling, sense, or thought about the right decision, but you can\u2019t fully explain why that decision is right. This is the mental equivalent of a goalie in football making a split-second adjustment to save a shot, or a tennis player picking a lethal moment to hit a drop shot. </p> <p>Being able to act on your intuition means you know when it\u2019s more likely to be right or wrong. Having that knowledge is powerful and lucrative. But intuition is a topic that is widely misunderstood, which often leads punters to go against it, to their own detriment. Let\u2019s try to change that. But, first, I have a warning for newer punters. </p> <p>Intuition relies on a strong base of knowledge in punting to be accurate. It comes from years of experience and is not something that you just pull out of thin air. So as a new punter your intuitive sense is much less likely to be accurate. I strongly advise that you're cautious about using any innovation or decisions that rely on intuition until you\u2019ve gained more experience.</p>"},{"location":"mentalGame/TappingYourIntuition/#why-is-intuition-hard-to-trust","title":"Why Is Intuition Hard to Trust?","text":"<p>Many experienced punters who are able to generate reliable intuition have a hard time trusting it. Why is that? One common reason is that they don\u2019t know what the hell it is! Your goals, livelihood and confidence are on the line, so why would you trust something you don\u2019t understand?</p> <p>Also, emotions can masquerade as intuition and there have been times when you put trust where it doesn\u2019t belong. Emotions like fear, anger and overconfidence can make it appear like your rash and impulsive decisions are based on intuition, when they aren\u2019t. These emotions create a false version of intuition that feels right and justified, but really it is the mind\u2019s way of getting what it wants \u2013 not allowing a profitable model to run for fear of losing or letting a model that you know isn\u2019t profitable run in the hope that it will win.</p> <p>To trust your intuition means you\u2019re relying on a feeling or a sense of what\u2019s right. When fear, anger and overconfidence are mixed in, your sense of what\u2019s correct has been altered by these emotions. You get creative, feel justified in your rationale, and seek confirmation from like-minded peers who don\u2019t challenge your perspective. Your aggressiveness or second-guessing is not the byproduct of a sixth sense that allows you to spot opportunities that others can\u2019t see. The opposite is true\u2014you\u2019re flying blind and out of touch with what makes your model profitable.</p>"},{"location":"mentalGame/TappingYourIntuition/#generating-and-using-intuition","title":"Generating and Using Intuition","text":"<p>Understanding intuition is nice, but it\u2019s not the same as unlocking the potential that intuition can have in your punting. Imagine consistently and accurately using intuition. You\u2019d be open to new opportunities, able to quickly assess opportunities outside your normal scope, and could more easily adapt to an ever-changing competitive landscape.</p> <p>The key to accessing intuition is being in the zone. When you\u2019re in the zone, you\u2019re more likely to generate intuition that you can actually trust, which means your goal should be to get in the zone more often. </p> <p>Reaching the peak of your mental performance is something that typically happens randomly (or at least appears to be random), but it doesn\u2019t have to be that way. The zone is predictable once you understand the conditions and factors that produce it for you. This is why I created a worksheet called the Zone Profile, to help you organize all the important details that will help you get into the zone more often. You can download it here.</p> <p>The most important feature of the zone is energy. You simply cannot get there without the right amount of energy. This is true in a mentally demanding endeavor like punting, just as it is in a physically demanding sport such as basketball. If your level of energy is too high or too low, you can perform well, but you won\u2019t be able to reach the zone. The key to getting in the zone consistently is determining what the \u201cright\u201d amount of energy is for you. If you\u2019re unsure, try writing a description of what it\u2019s like when you\u2019re in the zone. Describe the quality of your focus, how you make decisions, and your perception of time\u2014does it speed up or slow down? Taken together, you\u2019ll start to form a profile of what the zone is like for you.</p> <p>Once you\u2019re clear on what the zone looks like for you, create a routine that incorporates the key factors that get you there. When you get into the zone more frequently as a result of your routine, a causal relationship grows between them. When this association becomes strong enough, you\u2019ll begin to anticipate getting in the zone just by starting your routine, even when your energy isn\u2019t ideal.</p> <p>Having a structured routine is important to get you so focused that it\u2019s like your mind is in a bubble where nothing can distract you. Here are some key elements of a routine:</p> <ul> <li>Reading your goals to get you motivated and focused</li> <li>When you\u2019re finished working, take notes about mistakes, thoughts on your model, work you want to do tomorrow, or anything else on your mind</li> <li>Take time away from technology, or any other input, to allow your mind to relax and digest the information you\u2019ve been focused on</li> </ul> <p>Taken together these steps will help you to be more focused when you work, and help your mind to remain clear day after day. Clarity of mind is a key element of getting in the zone and accessing intuition. When you\u2019ve got too much on your mind, you don\u2019t leave room for intuition.</p> <p>Another thing you can do is narrow the gap between your A-game and C-game. Going back to part VII where I talked about The Inchworm Concept, when your Inchworm is stretched out too wide, the mind has too much to think about and that prevents you from regularly getting into the zone and accessing intuition. When that gap is narrow, your mind is clearer because there are fewer things you have to consider and intuition comes more easily. Make sure to be regularly focused on improving your B and C-game. This is a nice balance to your focus on the zone, where getting there is your top priority, but when that\u2019s not possible, your job is to push your B and C-game higher.</p> <p>Getting into the zone and accessing Intuition is a peak of your mental capacity. It\u2019s an exciting and, likely, profitable place to reach. To get there takes work, but you\u2019ll get a sense of pride and accomplishment that can only come from reaching your potential. </p> <p>Jared Tendler, MS is a mental game coach for world champion poker players, PGA Tour players, sports bettors and financial traders from 45 countries. He is the author of three highly acclaimed books, The Mental Game of Poker 1 &amp; 2 and his newest book The Mental Game of Trading. Find out more about Jared\u2019s work at: https://jaredtendler.com/ </p>"},{"location":"mentalGame/correctingyouremotions/","title":"5: Correcting Your Emotions","text":"<p>Imagine you\u2019re walking down the street and experience some pain in your foot. You stop and sit down, wondering what the problem is. Is there something wrong with the shoe you\u2019re wearing? Maybe you\u2019ve tied them too tight, or worse, perhaps there\u2019s a problem with your muscles or ligaments? The bottom line is that you\u2019re curious about what\u2019s causing your pain. You don\u2019t ignore it, you examine it with interest because doing so is required to fix the problem and eliminate the pain.</p> <p>Now imagine if you approached your emotions with that same curiosity. Instead, many people, punters included, treat their emotions as the problem, and contort themselves to avoid, deny or rationalise their fear, anger or lack of confidence. What if you did  that with the pain in your foot? In that analogy you\u2019re treating the pain as the problem. But what if you\u2019ve just got a rock in your shoe!? The rock is the problem, not the pain. The pain is signaling a problem for you to examine. Guess what, your emotions operate that way as well.</p> <p>I hope your approach to emotions is beginning to change as you\u2019ve read through the first four articles in this series, especially part I and part II. In this article I\u2019m going to show you how to dig around to find the metaphorical rocks in the back of your mind. These are the flaws, biases, illusions, wishes, or hopes that affect your perception of punting, yourself, your model, etc. They are the real cause of the costly mistakes affecting you. </p> <p>When I talk about digging around in the back of your mind there is always someone who immediately assumes I want to get into their personal life and talk about their childhood issues with their parents. While there are people for whom that would be helpful, that\u2019s not the focus of my work. I want to improve your performance. That means looking practically at your mistakes, the emotions surrounding them, and identifying the cause. For example, you could:</p> <ul> <li>Believe that mistakes should never happen</li> <li>Expect to always be at your best</li> <li>Have an excessive need to be right</li> <li>Expect perfection</li> <li>Be unable to handle uncertainty</li> <li>Think that taking on risk is gambling</li> <li>Wish that you could win a lot of money</li> <li>Listen to too many outside opinions</li> </ul> <p>These are just a handful of the many potential \u201crocks\u201d in the back of your mind that trigger strong emotional reactions.</p>"},{"location":"mentalGame/correctingyouremotions/#find-the-cause-of-your-emotions","title":"Find the Cause of Your Emotions","text":"<p>To correct the emotions that lead to problems, you first need to find them. The tool that I developed to help clients do this is called the Mental Hand History. You can download a copy for yourself here. The name comes from my attempts to get poker players to work on their mental game in the same structured way that they would work on their technical game. </p> <p>The Mental Hand History helps you analyze information and identify the flaw. There are five steps:</p> <p>Step 1: Describe the problem in detail. Step 2: Explain why it makes sense that you have this problem, or why you think, feel, or react that way. Step 3: Explain why the logic in Step 2 is flawed. Step 4: Come up with a correction to that flawed logic. Step 5: Explain why that correction is correct.</p> <p>Here\u2019s an example of a Mental Hand History to give you an idea what this looks like:</p> <p>Step 1: Describe the problem in detail.</p> <p>Losing streaks bring on a fear of the systems failing. The fear is that I have put in all this hard work and it won't continue to be profitable. It is a fear of losing what I imagine is possible with the winnings. Particularly I fear the fact that I would need to continue working when I want working at a job to be a choice not a necessity. I can handle one or two losing days even if they are large. It is when a string of days occurs in a short period of time that the fear sets in.</p> <p>Step 2: Explain why it makes sense that you have this problem, or why you think, feel, or react that way.</p> <p>It is logical because for me, the pain of losses is felt 10 times more than the excitement of winning. A lot of people lose in gambling and I know it requires a lot of work to remain in the top few winning punters. It's understandable that I don't like losses as this brings on fear of failure, fear of loss and fear of humiliation.</p> <p>Step 3: Explain why the logic in Step 2 is flawed.</p> <p>The logic is flawed because I have a long-term record that is profitable, and I put in the work and analysis that is needed to keep the models improving and making more money. I have many models being bet and they do not all need to be profitable for me to continue winning, and it would be almost impossible for all my models to start losing simultaneously.</p> <p>Step 4: Come up with a correction to that flawed logic.</p> <p>After a number of losing days, I should evaluate the 90 day and 180 day trend to remind myself of all of the winning I have been doing in the previous 3 to 6 months. I should compare the long-term profit with my paying job income. My profits on punting can be much larger than most people\u2019s jobs.</p> <p>Step 5: Explain why that correction is correct.</p> <p>The markets will evolve and I have the knowledge and work ethic to make it work. Losses are part of the game and I have bounced back many times from drawdown positions to have a bank high. It\u2019s about outlasting the losing runs, improving the models so they don't lose as much during the down periods and pushing through to a new bank high when the winning runs happen. It's the long term not short term that wins.</p>"},{"location":"mentalGame/correctingyouremotions/#tips-for-finding-your-flaws","title":"Tips For Finding Your Flaws","text":"<p>Completing these steps can be difficult the first time you do it. Your overall approach to solving mental and emotional problems is changing, so that alone gives many people difficulty. Do the best you can by creating a draft of your answers, then put it down for a day or two and come back to it. This is most important for step 2; a step that gives people the biggest headache. \u201cWhy does it make sense that I have this problem?! What are you talking about, I\u2019m just being an idiot. This is illogical or irrational,\u201d Is a common response. But this isn\u2019t a matter of intelligence, and your emotions are never illogical \u2013 you just lack the rationale to explain what\u2019s going on.</p> <p>Secondly, you\u2019re also constrained by your knowledge of the mental game. So using resources like my books, or other articles, blogs and videos can help you to identify and break down the flaws in your mind. In fact, both The Mental Game of Trading and The Mental Game of Poker were designed to make it easier to complete the Mental Hand History \u2013 which shows you how much I value this tool!</p> <p>Lastly, try to generate new insight or understanding rather than just dumping knowledge you already have into each answer. Most often to truly correct your emotions, you need to uncover something new. </p> <p>Once you have a Mental Hand History that has deepened your understanding, you can use your answers to steps 2 through 5 to correct your emotional reactions in real time. Here\u2019s how: When you recognize that your emotions have been triggered, ideally before they\u2019ve become too intense, remind yourself of the knowledge reflected in your answers. From the example above, you might say: \u201cLosses are part of it and I\u2019ve bounced back before. It\u2019s the long-term that matters.\u201d</p> <p>Do this is a strong way and it will have an effect on your emotions. Why? Because you're getting straight to the heart of the issue. Then do it again and again and again, whenever your emotions are triggered, and over time your emotional reactions will become less intense and eventually nonexistent. </p> <p>Just like the rock stops causing you pain when you take it out of your shoe, your fear, anger or lack of confidence stop being triggered when you correct the flaw causing them. </p> <p>Jared Tendler, MS is a mental game coach for world champion poker players, PGA Tour players, sports bettors and financial traders from 45 countries. He is the author of three highly acclaimed books, The Mental Game of Poker 1 &amp; 2 and his newest book The Mental Game of Trading. Find out more about Jared\u2019s work at: https://jaredtendler.com/ </p>"},{"location":"mentalGame/handlingBadVariance/","title":"3: Handling Bad Variance","text":"<p>No one likes to lose, but when variance is the cause, losing takes on new meaning. As much as you might wish punting was more predictable, it\u2019s not. It\u2019s nothing like, for example, a game of chess.</p> <p>On chess the board, the pieces and the moves your opponent makes are all there right in front of your eyes. There are no unknown variables. The game is played face up and whoever is the better player that day will win. </p> <p>Punting, however, is full of unknowns and even the factors that are known have huge amounts of variability, with swings that can drive you mad. You spend an incredible amount of time building a system, putting in long hours, staying up late at night, getting up early to complete testing, revising and resetting before finally nailing something that is profitable. And what happens when you run into a downswing and the variance, or luck, and the game turns against you? </p> <p>If you\u2019re like the punters that I coach, bad variance will make you question whether you need to tweak your system, and you\u2019ll face a barrage of internal questions: Should I let it run? What could be going wrong? How much longer is this going to last? How much more losing can I take? These and other questions race around in your mind, and go unanswered because they\u2019re unanswerable questions. Yet they intensify your doubt and frustration, making an already difficult situation unbearable. Some punters will even turn off a winning system if the streak is bad enough. Others will decrease stakes, hoping that will be the answer to their losses. </p> <p>What makes variance so sinister, is that it hides in plain sight. Outside of the obvious scenarios where bad luck strikes, there\u2019s a myriad of scenarios where accurately judging the influence of variance is a fool's errand. Misperception thrives in situations like this, and for less experienced punters it can be their undoing. The punters who have an edge, and have been in this game long-enough learn to deal with the inherent volatility that comes with it. Of course they\u2019re always looking for new edges and ways to improve, but they also know when to do nothing and to keep firing, knowing that variance will eventually turn in their favour. </p> <p>The problem is that many punters never reach that point. There\u2019s a graveyard of those who aspired to profitability and were undone by their inability to handle variance. That fact drives   me to do what I do. Sure, some people are naturally more suited for punting, but those who love it and want to be successful should not fail simply because they don\u2019t have the required emotional stability from the start. Especially when that\u2019s a skill that can be learned and not a personality trait given to a fortunate few.</p> <p>To learn this skill here\u2019s a tool that has helped punters, poker players and traders alike to handle bad variance with poise and confidence: The A to C-game Analysis</p>"},{"location":"mentalGame/handlingBadVariance/#what-is-an-a-to-c-game-analysis","title":"What is an A to C-game Analysis?","text":"<p>Most punters by default use monetary results as the metric to determine how they\u2019re doing. Many do so not even realizing that it\u2019s wrong. Punters default into using results as a measure of their performance because that\u2019s common and easy. The problem is that in the short-term results can be wildly uncorrelated with your performance. There are times when you\u2019re losing money making the right decision and making money making the wrong decision. The A to C-game Analysis is the answer to breaking out of this cycle of misperception. </p> <p>Imagine being given a superpower that allowed punting to be like chess, where you can see where exactly variance affects your results. It\u2019s not a superpower sexy enough to make a movie about, but you can imagine how valuable it would be. </p> <p>While the A to C-game Analysis can\u2019t give you that superpower, it gets you closer than anything I\u2019ve seen. The basic idea of the tool is to become aware of the variability in your performance. Thus, when you can accurately account for that, you remove a key component to misperception\u2013your mental and emotional state. First let me tell you a bit about what it is and how to create it, and then I\u2019ll explain how to use it. </p> <p>Imagine if we rated the quality of all the decisions you make in and around punting over the past six months and we could categorize them as being A-game (best), B-game (average) or C-game (poor). The A to C-game Analysis identifies the traits or characteristics in your mental game and tactical skills that highlight each level of your performance. Here\u2019s an example to give you an idea of what it looks like:</p> <p></p> <p>You can see that each level has distinct differences. This is key and where the superpower of this tool comes from. Imagine that you\u2019re going through a downswing, but based on your performance ratings over the past several weeks, you\u2019ve been in your A-game. Your confident you have been performing well and the data shows that Since short-term results aren\u2019t your primary metric of success, the recent losses don\u2019t affect you. You\u2019ve been in control of what you can control and that allows your confidence to stay consistent. That doesn\u2019t mean your system doesn\u2019t need to be adjusted either, there could be something that needs to be updated or improved. But because you're in a good mental state, your ability to make those changes is more likely to be accurate, rather than desperate.   </p>"},{"location":"mentalGame/handlingBadVariance/#creating-and-using-an-a-to-c-game-analysis","title":"Creating and Using an A to C-game Analysis","text":"<p>So how do you actually complete the A to C-game analysis? The first step is to brainstorm three different levels of your game both mentally and tactically. You want to get an accurate picture of your performance, so focus on the last 3 to 6 months. Anything less than that and your A to C-game Analysis is likely to be skewed towards the negative or positive based on how you\u2019ve been doing recently. </p> <p>The easiest way to start out is by focusing on the parts of your performance that are the most obvious. For my clients that means their A-game and C-game. Focusing on the two extremes is easier, and then once you\u2019ve got that nailed down, you can fill in B-game. Here a few tips to help:</p> <ul> <li>Take a few minutes to think about your performance at the start and end of the day, then during the day make note of any new details you find.</li> <li>Think about the situations or factors most likely to cause you to be in your B or C-game. That will help you remember so you aren\u2019t making those mistakes again to find them out.</li> <li>Don\u2019t get hung up on distinguishing between what is mental and what is technical. Being able to clearly identify the differences between your best, worst and average is much more important.</li> <li>At first the process can be a bit messy and hard, that\u2019s common. Don\u2019t be discouraged, just keep at it. This is a skill like any other. </li> </ul> <p>The goal is to get your A to C-game Analysis to a point where you stop finding new things to add. That could take you a few days or a few weeks, or even longer if you have to go through different cycles to nail it down. Don\u2019t worry, even just thinking about your performance in these terms is helpful. </p> <p>Once it\u2019s solid, you now have a measuring stick that you can use to grade your performance daily. And you guessed it, doing this also helps you to improve how you handle good luck by focusing you more and more on your performance, and less on short-term results, regardless of whether they\u2019re positive or negative</p> <p>Handling bad variance is a game of: How well can you maintain an accurate view of your results. Don\u2019t let variance torment you and force your perspective on your system to become inaccurate. Do the work to create an A to C-game Analysis and it will keep you level headed. </p> <p>Jared Tendler, MS is a mental game coach for world champion poker players, PGA Tour players, sports bettors and financial traders from 45 countries. He is the author of three highly acclaimed books, The Mental Game of Poker 1 &amp; 2 and his newest book The Mental Game of Trading. Find out more about Jared\u2019s work at: https://jaredtendler.com/ </p>"},{"location":"mentalGame/intro/","title":"The Mental Game of Wagering Article Series","text":"<p>I know your time is limited. You have a lot of ideas that you want to test and models that you want to refine, and you\u2019re probably wondering what psychology and emotions even have to do with wagering or punting. But ask yourself this question: What is more likely to increase your POT by 1% \u2013 doing that testing/modeling work, or eliminating the mistakes that cost you money time and again and are driven by anger, fear, greed, or confidence problems I can help you correct?</p> <p>The most successful punters out there are already focused on their mental game. If you haven\u2019t yet thought about how much your psychology or emotions are costing you, then you are either at a distinct disadvantage or, at a minimum, are missing an opportunity to improve your edge. </p> <p>We are not talking about psychobabble or sitting in your feelings here. I am focused on driving performance and my proven Mental Game system uses practical, logical strategies that deliver results when you do the work. I\u2019ll use this article series to teach you the basics of the system and equip you to improve your mental game. Invest the time and you won\u2019t be sorry. </p> <p>For the last 17 years I\u2019ve been coaching world champion poker players, PGA Tour players, financial traders and punters from around the world. My job is simply to help them be more successful, more consistent and perform at their highest levels. I\u2019m able to do that not because I\u2019m an elite poker player, trader or punter, but because I am an expert in how mental game impacts performance. I\u2019ve also written three highly acclaimed books, The Mental Game of Poker 1 &amp; 2 and my newest book, The Mental Game of Trading. Find out more about my work at: https://jaredtendler.com/ </p> <p>Watch this space as Jared will be writing an entire series of articles related to related to the mental game of wagering, we recommend following the articles in sequential order: </p> <ul> <li>Part I - Role of emotions</li> <li>Part II - Map your emotions</li> <li>Part III - Handling Bad Variance</li> <li>Part IV - Win by Being Great at Losing</li> <li>Part V - Correcting Your Emotions</li> <li>Part VI - Responsible Gambling</li> <li>Part VII - Maximize Your Learning with the Inchworm Concept</li> <li>Part VIII - The High Cost of High Expectations</li> <li>Part IX - Tapping Your Intuition</li> </ul>"},{"location":"mentalGame/mapYourEmotions/","title":"2: Map Your Emotions","text":"<p>Wagering is intense. The nature of it makes it easy for emotions to come out and affect your decisions. Whether you\u2019re turning a model on to run live for the first time, dealing with a big win or loss, or spending a long time developing a model only to have your back-testing show poor results, emotions will come up. </p> <p>Even with all these opportunities in wagering for your emotions to get out of control, they don\u2019t have to cause poor decisions. In this series of articles, I\u2019ll be providing you with perspective and tools that can help you to deal with your emotions in the right way. To begin, I want you to do something I call \u201cmapping your emotions.\u201d</p> <p>This may sound like a strange idea, but it\u2019s a very practical step that has been instrumental for my clients over the years. Some were even able to make solid improvements right away after just this first step. But before I get into the specific instructions for how to map your emotions, I want to give you one more bit of context to emphasize why this is so important.</p>"},{"location":"mentalGame/mapYourEmotions/#yerkes-dodson-law","title":"Yerkes-Dodson Law","text":"<p>In the previous article I talked about how emotions can override your ability to think and make decisions, leading to errors that not only cost you money but also decrease your enjoyment from punting.</p> <p>The graphic below gives you a visual of what this looks like. It\u2019s called the Yerkes-Dodson Law (I also like to call it the Performance/Stress curve) and it's been around for over 100 years. </p> <p></p> <p>What you see here is the relationship between performance and stress, or performance and emotion.</p> <p>As the graphic depicts, your performance is equally bad when your emotions are either too high or too low. When your energy or emotions are really low, you feel tired or burned out, and you procrastinate, get easily distracted and bored. You're on the left side of this curve and your performance is poor because you don\u2019t have enough energy or emotion to fuel you to be at your best. In my experience, most discipline issues come from lacking energy and being on this side of the curve. To perform better, you need more energy to power your mind.</p> <p>On the other side of the curve, when emotions like anger, fear, greed, or confidence get too high, your mind progressively shuts down until you reach the point where your mind completely shuts off and you're in a blind rage, panic or euphoria.</p> <p>Mapping your emotions is how you begin to prevent that scenario from happening. It gives you the awareness to see the early warning signs that intense fear, anger or overconfidence is coming, while you still have enough mental functioning to take action.</p>"},{"location":"mentalGame/mapYourEmotions/#how-to-map-your-emotions","title":"How to Map Your Emotions","text":"<p>Funny enough, your emotional reactions are very much like patterns in the betting market\u2014with enough study and examination, you can turn something that happens again and again into an opportunity. That idea is obvious when it comes to developing your model, but you\u2019re probably wondering how to do it for your emotions.</p> <p>Many of you right now are blind to the patterns in your mental game, much like you were unable to spot patterns in the betting market when you first started punting. But the more you worked at it the better you became. The same is true with your emotions.</p> <p>Start by focusing on the times when you already know that your emotions are more intense than ideal or when you\u2019re making repetitive mistakes. At those times, examine and note what\u2019s going on with your emotions, thoughts, actions, or changes in your decision-making, and what\u2019s causing your emotions to be high. Here\u2019s an example of what to write down (templates can be downloaded below):</p> <p>Situation/mistake: Winning a lot, consistently for a few days/week.</p> <p>Thoughts: Counting all the money I have made and thinking about spending it. I just need to collect the money each day.  My mind focuses on staking and how can I get on more money as more money on each bet means more profits. </p> <p>Emotions: Extreme overconfidence</p> <p>Behaviors: Bragging to people about how much money I am making per month without explaining the losses I also take at times.</p> <p>Changes to your decision-making: Projecting the profits into the future at these unrealistic levels.</p> <p>Of course, you can also think back to previous situations or mistakes and write down as much as you can remember. It will take time, but eventually you will be able to recognize more and more of the signals and the payoff is huge. </p> <p>As this process gets easier, start looking for earlier signals. What happened before your emotions became too intense? If you tend to get too caught up in the day to remember to track your emotions, consider setting an alarm at some regular frequency that isn\u2019t too disruptive (i.e., every 15, 30, or 60 minutes) to take stock of where you are. </p> <p>Continue to capture this mental game data until you gather enough where you can begin to create a map that charts the intensification of your emotion using a scale of 1 to 10. Basically you\u2019re going to look at the data you\u2019ve captured and assign a ranking where 1 would characterize the lowest level of anger, fear, overconfidence or loss of confidence, and 10 would equate with the highest level of those emotions. </p> <p>You don\u2019t need to have details for all 10 levels, certainly not at the start. Most of my clients are able to develop a map distinguishing around 4 or 5 levels, as you\u2019ll see in this example of a map of Anger: </p> <p>1: Dismissive of the loss like it didn\u2019t happen.</p> <p>2: Impatient. Especially when people or things are not happening.  It's like I need small quick wins inside and outside of gambling to feel like I am in control of the situation. My caffeine intake rises to include another cup of coffee for the day. This can be accompanied with feeling tired all the time. Unable to sit still (more than usual).</p> <p>3:</p> <p>4: Become more inward focused and analytical, but still social. I\u2019m thinking about \u201cWhat have I done differently?\u201d \u201cWhat can I change?\u201d \u201cWhat am I not seeing?\u201d I eat more chocolate and ice cream and know that I shouldn\u2019t. I can see things which are not there as I try and avoid the pain of a loss.  Changes made during this level can either be enormously beneficial or a massive mistake. </p> <p>5:</p> <p>6: Inwardly focused and cut off from the world. Swearing at myself. Irritable and want to be left alone and don't want people around. Interruptions and people talking to me cause me to be anxious and I am inconsiderate of others at this point. My decisions are urgent. There is an urgency to data analysis. It\u2019s hard to see the larger picture and I start to look for other signs such as a new player in the market, or see patterns which don't exist.</p> <p>7:</p> <p>8: Disbelief from the losses. Blaming others - \u201cHow could this happen!?!\u201d \u201cThis is impossible!\u201d are common phrases as I\u2019m raging inside. My decisions are really bad, and based on the immediate need to stop the losses.</p> <p>9:</p> <p>10: Intense physical symptoms - like the anger is pulsing through my veins. I have to leave immediately and go for a walk to burn off the anger. I\u2019m blunt with other people and generally in a bad mood.</p> <p>I\u2019m sure some of you feel overwhelmed by what I\u2019m asking you to do. Don\u2019t be. Many of my clients and readers of my books have felt that same way, unnecessarily, until they got started.  Building knowledge for the emotional patterns in your mental game is a skill like any other and you do this all the time with the betting markets! The only difference is you\u2019ve never done it with your emotions. But so what? Put in a bit of time starting to collect data and wait until you see how much knowledge has been waiting to be organized. </p> <p>To help you, you can download both the data collection worksheet and maps for anger, fear, and confidence below. </p> <ul> <li>Data Collection Worksheet</li> <li>Anger Profile</li> <li>Confidence Profile</li> <li>Fear Profile</li> </ul> <p>Once you have a solid draft, you can use it throughout the day to recognize when your emotions are at risk of escalating. Take action then, while you still have the literal capacity in your brain to do so, and you\u2019ll have a chance at avoiding those costly mistakes. </p> <p>Some of you will be more successful with this task than others. For those who aren\u2019t, it\u2019s not a failing on your part. It\u2019s an indication that you need to build a more robust mental game strategy and the remaining articles in this series will help you to do just that.</p> <p>Jared Tendler, MS is a mental game coach for world champion poker players, PGA Tour players, sports bettors and financial traders from 45 countries. He is the author of three highly acclaimed books, The Mental Game of Poker 1 &amp; 2 and his newest book The Mental Game of Trading. Find out more about Jared\u2019s work at: https://jaredtendler.com/ </p>"},{"location":"mentalGame/responsibleGambling/","title":"6: Responsible Gambling","text":"<p>When I first started working with poker players in 2007, people would often ask me if I was trying to get them to stop gambling. I laughed and said, \u201cNo, I\u2019m trying to help them win more money!\u201d</p> <p>Actually, the poker players, traders, and punters I work with all have one thing in common: they\u2019re not gambling. Gambling in my mind is betting with a negative edge, where you will lose money long term. Consider playing a slot machine where the casino has a built in edge, let\u2019s say 10%. Regardless of how much money you\u2019re able to make on a given night, the house makes 10 cents from every dollar that you put into that machine.</p> <p>In developing your punting model, you aspire to be the casino \u2013 where you have the edge that makes money long term. And until you reach that point, you\u2019re betting on yourself. You\u2019re investing money like a business start-up would in the early stage of research and development, where they\u2019re, for example, building a product to eventually bring to the market with the hopes that it can hit their sales targets. Invest money now to make more money in the future. General society doesn\u2019t view the people who start those types of businesses as gamblers, although some of them actually are. There\u2019s a graveyard of bad ideas that should never have been funded. Yet inventors and business owners are praised for their risk taking, not treated as the gamblers that they are.</p> <p>Of course, even great ideas can be a gamble. Business books are lined with stories of well-intended failures, along with lessons on how to avoid that fate and when to quit. Punting, much like poker and trading, can easily verge into that direction and it\u2019s important that you know where that line is for you. If you\u2019re losing money in the pursuit of a profitable model, that\u2019s not gambling ... until it is. </p> <p>For some, gambling is just about fun, a form of paid entertainment. In that case, determine an amount of money that determines when your fun is over. But if gambling is a serious hobby or a professional aspiration, how do you know when enough is enough and it\u2019s time to quit? I recommend you figure that out now, before you\u2019ve gotten to that point.</p> <p>At the end of the day, if punting is affecting your life in a negative way, beyond just the financial cost, you need to take a serious look at whether it\u2019s right for you. For example, if it affects your daily mood, your exercise routine, eating habits or stops you from having fun with family and family. You may still decide punting is right for you, but you need to make changes to ensure that punting can fit into your life without impacting you so negatively.  </p> <p>If, on the other hand, you decide you\u2019ve reached the point where it\u2019s time to quit, you also need to recognize that in the moment it can be tough to follow through on that decision and actually quit. </p>"},{"location":"mentalGame/responsibleGambling/#why-quitting-is-hard","title":"Why Quitting is Hard","text":"<p>There are numerous reasons why quitting is hard, some more severe than others. Here are a few of the ones that I have found to be most common.</p> <p>When you invest a lot of time, effort and money into punting it can be hard to walk away because of a bias called the \u201csunk cost fallacy.\u201d This phenomenon happens when a person is so heavily influenced by the sheer amount of time, effort and money they invested that they\u2019re unable to change course even when quitting would be better for them. </p> <p>I get it, no one wants to walk away when you\u2019ve already gone so far and you feel like you must be getting close to a breakthrough. If you feel stuck like this, it\u2019s at least an indication that you\u2019re not learning anything new. Step away from punting for at least a week or two to get a fresh perspective. When you\u2019re so \u201cin it\u201d you can\u2019t see the forest for the trees, you can\u2019t see how it\u2019s really affecting you, nor can you make a well-reasoned decision. Instead you feel forced just to carry on, when that may not be in your best interest.</p> <p>Another reason quitting is hard is that we don\u2019t have a crystal ball when we take on a new job, start dating someone new, or pick up a hobby or activity like punting. As much as we may think we\u2019ve made the right decision, we can\u2019t know exactly what is going to happen \u2013 we\u2019re just making a bet. Many people go into these situations assuming that it\u2019s the right decision from the start and so quitting is viewed as a mistake \u2013 a mistake that can be avoided by not quitting! Maybe it was the right decision at the start, but goals and motivation change. If yours have changed, that\u2019s not a mistake, that\u2019s human nature.</p> <p>Perhaps you came into punting thinking it was a way to make easy money, or because you loved sports and this was a way to stay close to the ones you love and do something cool. As you got deeper into it, punting became more challenging than you thought and took away from the passion that you had for those sports. You could not have known these facts before you started. Some things you can\u2019t learn from the sidelines, you have to get in there to find out. Look at this as a good thing. You\u2019ve learned more about what you like and don\u2019t like and will be able to make better decisions going forward having had this experience. You\u2019re not walking away empty handed when you learn lessons like that.</p> <p>Lastly when you invest a lot of your time, money and energy into being successful, your confidence can also be on the line. Quitting can feel like an admission of failure, and for some that will reinforce a sense that you are a failure. When failure cuts deep like that, it\u2019s often associated with desperate attempts to keep it going to avoid feeling so bad. Emotions run hotter and more intense and you\u2019ll justify decisions that are actually evidence of gambling. Desperation is the most intense performance issue I encounter, and it keeps people fighting for success when the odds are stacked against them. </p> <p>At the end of the day, if it\u2019s time for you to quit, understanding why it\u2019s so hard to quit may help you to follow through. And, of course, if you need help quitting, there are a number of places to turn. The important thing is to take care of yourself.</p> <p>Jared Tendler, MS is a mental game coach for world champion poker players, PGA Tour players, sports bettors and financial traders from 45 countries. He is the author of three highly acclaimed books, The Mental Game of Poker 1 &amp; 2 and his newest book The Mental Game of Trading. Find out more about Jared\u2019s work at: https://jaredtendler.com/ </p>"},{"location":"mentalGame/roleOfEmotions/","title":"1: The Role of Emotions in Wagering","text":"<p>While making money may not be the only reason you\u2019ve decided to become a punter, money is like a scoreboard telling you whether you\u2019re winning or losing in this game. Obviously, you want to win more than you lose. Yet whether you're a novice punter or a seasoned pro, sometimes you find yourself losing money when you shouldn\u2019t and leaving potential profits on the table. The question is why.</p> <p>Many of you have already tightened your system or worked up a new model as a way to stop these unnecessary losses. Those are the correct first steps. But what if the bleeding continues because you find yourself making mistakes like these: </p> <ul> <li>Launching a model  that doesn\u2019t pass all the checks</li> <li>Increasing stake size to recover previous losses</li> <li>Stopping a model running after a negative downturn</li> <li>Increasing stake size on a profitable model too quickly</li> <li>Continuing to use a losing model despite knowing that something needs to change</li> <li>Justifying a long term losing model with short term positive performance</li> </ul> <p>If that\u2019s the case, the solution is going to come from the mental and emotional side of wagering\u2014your mental game. When you can\u2019t stop yourself from making mistakes like these, and others like them, your emotions are the reason. When your emotions become too intense, they have the power to force you to make mistakes. </p> <p>This is critical so it bears repeating: Emotions are the reason  you can\u2019t stop these mistakes from happening. Fix your sights firmly on correcting fear, anger, overconfidence or a lack of confidence, because they are at the heart of what\u2019s costing you. </p>"},{"location":"mentalGame/roleOfEmotions/#the-first-rule-of-emotions","title":"The First Rule of Emotions","text":"<p>Just like any other game, if you\u2019re going to win, you need to know the rules. Otherwise, you have no chance of reliably correcting your mistakes. While I\u2019ve just explained that emotions are driving your mistakes, emotion is a subject that is widely misunderstood, not just within wagering. And there is one rule that you must understand: When the emotional system becomes overactive, it has the power to shut down higher brain functions.</p> <p>This is why you make those mistakes. Your emotions are running too hot and they override the part of the brain responsible for thinking and processing information correctly. Both positive and negative emotions can cause this breakdown in mental functioning. That\u2019s why you increase stakes or launch a model too quickly\u2014excitement or overconfidence prevent you from making a well-reasoned decision. Having confidence is important to performance, but too much of it can lead you to overlook some factors or overestimate the accuracy of your opinion.</p> <p>When your emotions are too intense, the following can also happen:</p> <ul> <li>Your mind moves so fast that you miss key pieces of data</li> <li>You know the right answer, but can\u2019t access it</li> <li>You\u2019re aware that what you\u2019re doing is wrong, but you can\u2019t stop yourself</li> <li>You overweight the importance of some factors and fail to consider relevant ones.</li> <li>When emotions are at their highest level\u2014shock, euphoria, blind rage\u2014your mind goes completely blank and you stop thinking altogether.</li> </ul> <p>When emotions are overactive, the loss of higher brain functions is something that no one can control. Why? Because emotions have the power to reduce and even shut down your decision-making process entirely. This basic mechanism in the brain is something that no one can control. It\u2019s hardwired. Many of you know it as the fight or flight mechanism. Once your emotional system becomes overactive, you lose access to knowledge that is critical to avoiding these mistakes.</p>"},{"location":"mentalGame/roleOfEmotions/#emotions-arent-all-bad","title":"Emotions Aren\u2019t All Bad","text":"<p>Now that you understand how powerful the emotional system is, it\u2019s tempting to write emotion off as bad, especially since they surround all the mistakes you want to avoid. </p> <p>Although emotions seem like the problem, you actually need it to fuel your performance, and you need them to be at your absolute best. In fact, even emotions that tend to be viewed as negative, like anger, can drive you to the highest level. Basketball legend Michael Jordan famously used anger as motivation to perform and that seems to have worked out pretty well for him!</p> <p>In the same way, experienced punters use their emotions as a guide, and their intuition or gut will show up as excitement, for example, when they sense a massive opportunity, or fear when something is off with the betting lines, or with their algo and they need to update their system.</p> <p>The distinction is that in order to have emotions aid your decision making like this they need to be clear, and free of flaws, biases, illusions, wishes, or personal needs that alter your perception about what\u2019s correct. This article series, and my books, are dedicated to helping you achieve this outcome.</p>"},{"location":"mentalGame/roleOfEmotions/#emotional-control-is-not-the-answer","title":"Emotional Control is Not the Answer","text":"<p>Even though you can\u2019t control an overactive emotional system, that doesn\u2019t mean you have no options. This is why the mental game matters\u2014it provides you with an understanding of how your brain works so you can properly devise a strategy. </p> <p>In this series of articles I\u2019ll help you with that strategy. But what I will tell you today is that simply striving to control your emotions is not the answer.</p> <p>Many punters believe that they should always be able to control their emotions. Some even expect that it should be easy to do that. But here\u2019s the thing, the part of the brain responsible for emotional control is called the prefrontal cortex. And when your emotional system becomes overactive it takes resources away from the prefrontal cortex, and lowers your ability to control your emotions. </p> <p>Which means that in order for you to have a chance at controlling your emotions, you must recognize the early signs of your emotions rising, while you still have the potential to stop the rise of emotion! The next article is available now, where I\u2019ll show you how to look for and identify those signs.</p> <p>Part II - Map Your Emotions</p> <p>Jared Tendler, MS is a mental game coach for world champion poker players, PGA Tour players, sports bettors and financial traders from 45 countries. He is the author of three highly acclaimed books, The Mental Game of Poker 1 &amp; 2 and his newest book The Mental Game of Trading. Find out more about Jared\u2019s work at: https://jaredtendler.com/ </p>"},{"location":"mentalGame/winbyBeingGreatatLosing/","title":"4: Win By Being Great at Losing","text":"<p>You don\u2019t start punting with losing money in mind, but soon you realize it\u2019s an inherent part of the game, like an expense on your business. No business has a 100% profit margin, so the question isn\u2019t will you lose. Of course you will. The question is whether you can you minimize your losses, and not fall into the trap of making unnecessary mistakes that can end up causing you to lose more money. </p> <p>Losing is going to happen, but for many punters it costs them more money than it has to \u2013 by chasing losses, making drastic changes to a successful strategy, or turning off their strategy entirely. Whether it\u2019s fear, anger, loss of confidence, laziness, or any of the other ways punters can handle losing poorly, if you want to be successful long term, you need to get better at this important skill.</p> <p>To be a great punter you need to be a great loser. That may sound strange, so hear me out. I don\u2019t want you to lose, nor am I suggesting that you feel good about losing money. But losing is part of punting and you need to develop the skill to handle losses the right way, like the greats in any profession or sport. The best of the best excel at handling losses. Michael Jordan actually has a fantastic commercial highlighting how much losing is connected to his success.</p> <p>It\u2019s ok to have trouble handling losing. Many punters do. To build the skill, you first need to understand why losing is hard for you. Then you can train yourself to see losing from a new perspective. </p>"},{"location":"mentalGame/winbyBeingGreatatLosing/#when-losing-causes-anger","title":"When Losing Causes Anger","text":"<p>Getting angry when you lose seems justified. Most people, punters included, have trouble losing without getting angry. While that may be common, don't let it justify allowing your anger to get the better of you. That\u2019s the real problem isn\u2019t it \u2013 not that you get angry, but that your anger leads to mistakes that cost you money.</p> <p>So why do you hate losing? Is it the feeling? Losing often feels terrible, can go on for a while, and even affects other aspects of your life, making you feel less successful in other areas too. Is it that you\u2019re overly competitive? Being fiercely competitive is a trait shared by many successful punters, but is a problem when your drive to win isn\u2019t matched with an ability to take a loss. Is it the money? Making money is ultimately how you\u2019re measured and it makes sense why you\u2019d hate losing it. You also may be worried about losing money you need for your life, which is why it's critical that you\u2019re only wagering money you can afford to lose.</p> <p>Regardless, if you truly hate to lose, you certainly don\u2019t want to be the cause of it!</p>"},{"location":"mentalGame/winbyBeingGreatatLosing/#when-losing-causes-fear","title":"When Losing Causes Fear","text":"<p>The threat of losing is always there and that makes some of you uneasy to even leave your strategy running. You could be doing quite well and yet, in the background, you\u2019re on edge, nervous and thinking too much. Not a recipe for good decision making. You may size down or turn your strategy off randomly, and then throw caution out the window once you\u2019ve stayed down or out for too long.</p> <p>So why does losing affect you so much? Much like anger, it could be the feeling or the utility of the money. But it also could be that losing feels like a mistake, like there\u2019s something wrong that you need to fix. Or losing could unleash a wave of self-doubt and criticism that feels like you\u2019re beating yourself up. Perhaps losing feels like failure \u2013 as absurd as that may sound, you\u2019ll be happy to know that a lot of people think losing means they\u2019ve failed.</p> <p>Lastly, you might fear more losses are coming and start thinking about the worst case scenario. This is a flaw I like to call \u201cshitty psychic,\u201d because you\u2019re not thinking about the future in a practical way, you're thinking that what you imagine could happen is an actual prediction of what\u2019s going to happen.</p> <p>Look closely for the reasons behind your fear. Correcting those misperceptions are key to handling losses. </p>"},{"location":"mentalGame/winbyBeingGreatatLosing/#when-losing-causes-a-loss-of-confidence","title":"When Losing Causes a Loss of Confidence","text":"<p>Losing confidence when you\u2019re losing money makes some sense. Your strategy is supposed to make money and when it\u2019s not, it\u2019s easy to lose confidence in it. But that can also lead to a loss in confidence in yourself, after all it is your strategy and if it\u2019s losing, that reflects poorly on you. </p> <p>Like anger and fear, confidence is an emotion. Not everyone thinks of it that way, but it\u2019s critical that you do. Emotions can change on a dime, especially with a change in perspective. Imagine knowing that your losses were entirely due to variance. That knowledge would instantly give you your confidence back. Thus, the A to C-game Analysis I mentioned in the previous article can help you here. It gives you a way to measure your performance or decision-making in a non-monetary way. That\u2019s especially important in the short-term to ensure your confidence is reflective of accurate information about your performance, not just recent results.</p> <p>Ideally a drop in confidence would be an indication that your strategy does need adjusting, and the A to C-game Analysis can help make that happen.</p>"},{"location":"mentalGame/winbyBeingGreatatLosing/#when-losing-causes-laziness","title":"When Losing Causes Laziness","text":"<p>If losing doesn\u2019t create any of the aforementioned emotions, it could take out your discipline.   Laziness, boredom, distractions, avoidance or procrastination are the most common culprits when losses mount. Do you fall victim to this silent killer of productivity? If so, correcting it may be more simple than you think.</p> <p>Discipline and a good work ethic are keys to sustaining a winning strategy, or even creating one. The first step to becoming more disciplined in the face of losing is to acknowledge that your success is entirely on you. You\u2019re the only one responsible for your goals, decisions, time, and work ethic. If you rely on external things like winning and losing, make excuses, or hope someone else or some event will drive your success, you\u2019ll never reach the level of discipline you aspire to.</p> <p>Push yourself to be more disciplined at these times and it will make you more disciplined in general. The process is simple: In the moments when you have the option to succumb to the weaknesses in your discipline, push yourself to do better. Building discipline is like building muscle, push hard and then give yourself a break.</p> <p>At the end of the day losing will never be as much fun as winning\u2026but, many of my clients have learned to have fun challenging themselves to handle losing better, and in doing so have a lot of fun losing less money than they did before.</p> <p>Jared Tendler, MS is a mental game coach for world champion poker players, PGA Tour players, sports bettors and financial traders from 45 countries. He is the author of three highly acclaimed books, The Mental Game of Poker 1 &amp; 2 and his newest book The Mental Game of Trading. Find out more about Jared\u2019s work at: https://jaredtendler.com/ </p>"},{"location":"modelling/AFLDisposals2024Datathon/","title":"Betfair\u2019s 2024 AFL Player Disposals Datathon","text":""},{"location":"modelling/AFLDisposals2024Datathon/#registration","title":"Registration","text":"<p>Registrations are closed</p>"},{"location":"modelling/AFLDisposals2024Datathon/#the-competition","title":"The Competition","text":"<p>Do you think you have what it takes to predict how many disposals a player will have in an AFL match?</p> <p>Betfair is giving you the chance to show off your data modelling skills by building a predictive model for the rounds 17 - 20 of the 2024 AFL Season.</p> <p>With $5,000 in prizes up for grabs, we\u2019re challenging you to use your modelling skills to your advantage, be that by building your first predictive sports model, improving on an existing design or have a go at something different by adapting your data skills from other fields.</p> <p>This year's AFL in-season datathon, over all 4 rounds in July, presents the perfect opportunity for keen sports and data enthusiasts to get involved. Put your skills to the test against the masses to compete for both prizes and ultimate glory!</p> <p>Submissions close at at 5:00pm AEST on Fridays and 12:00pm AEST on Saturday &amp; Sunday on each match day in rounds 17 - 20 (a total of 12 submissions)</p> <p>Weekly leaderboard updates will be published here throughout the competition so be sure to check back to follow your model\u2019s competition ranking.</p> <p>Please direct all questions and submissions to datathon@betfair.com.au</p>"},{"location":"modelling/AFLDisposals2024Datathon/#the-specifics","title":"The Specifics","text":"<p>The Terms and Conditions for the 2024 AFL Player Disposals Datathon can be viewed here</p>"},{"location":"modelling/AFLDisposals2024Datathon/#prizes","title":"Prizes","text":"<p>$5,000 in prizes are up for grabs! See the list below for the prizes each placing at the end of the competition will receive:</p> Place Prize 1 $2,500.00 2 $1,000.00 3 $500.00 4 $250.00 5 $250.00 6 $100.00 7 $100.00 8 $100.00 9 $100.00 10 $100.00 Total Prize Pool $5,000.00 <p>Prize winners will be announced, and all prizes will be distributed at the conclusion of the tournament</p>"},{"location":"modelling/AFLDisposals2024Datathon/#competition-rules","title":"Competition Rules","text":"<p>The 2024 AFL Player Disposals Datathon will see entrants provided with a bespoke set of historic AFL data for all matches from the AFL 2016 season until the present day which will be updated on a weekly basis during the competition. The goal from there is to use the provided data (or any other data) to build your own model to predict the disposal count of every player named in the squad of 26 for each team.</p> <p>How you go about building the model is entirely up to you \u2013 do you want to build an Elo model, a regression-based model, a Machine Learning algorithm or something entirely different? Get as innovative with it as you want!</p> <p>Your set of predictions will be due by 5:00pm AEST on Fridays and 12:00pm AEST on Saturday &amp; Sunday on the day of each nominated match \u2013 all submissions must be emailed to datathon@betfair.com.au</p> <p>See the Terms and Conditions for full competition rules.</p>"},{"location":"modelling/AFLDisposals2024Datathon/#historic-disposals-data","title":"Historic Disposals Data","text":"<p>All entrants received a bulk historic data file upon registration. Data updates will be posted here throughout the competition.</p> <ul> <li>Round 13 - Round 15 Data Update</li> <li>Round 16 Data Update</li> <li>Round 17 Data Update</li> <li>Round 18 Data Update</li> <li>Round 19 Data Update</li> </ul>"},{"location":"modelling/AFLDisposals2024Datathon/#submission-file-template","title":"Submission File Template","text":"<p>All submission file templates will be loaded here by 10:00am AEST on the Friday before the commencement of the round.</p> <ul> <li>Example Submission File</li> <li>Week 1 Submission File</li> <li>Week 2 Submission File</li> <li>Week 3 Submission File</li> <li>Week 4 Submission File</li> </ul>"},{"location":"modelling/AFLDisposals2024Datathon/#submissions-judging","title":"Submissions &amp; Judging","text":"<p>The Datathon will be judged using the root-mean-square-error (RMSE) method. The winner will be the Competition Entrant with the lowest average RMSE per player over all matches that take place between Round 17 and Round 20; 36 in total.</p> <p>One submission file template will be provided to Competition Entrants along with the data set upon registration. Submissions must follow the template set out in the submission file template provided and must be submitted in a csv format, not .numbers or .xlsx.</p> <p>For each player named in the squad of 26 for each team in that round, each entrant is required to predict the total number of disposals of each player in the match. This prediction will then be used against the player's actual disposal count to calculate the model\u2019s RMSE. Only players who actually take the field will be considered for scoring purposes. This means that players named as emergencies or as substitutes that do not take the field will be excluded from the scoring. </p> <p>Please name submission file using the following formatting:</p> <ul> <li>AFL_Disposals_Datathon_2024_Submission_Form_{Model_Name}_202407{day}.csv;</li> </ul> <p>To submit your model entry, please send it through to datathon@betfair.com.au</p>"},{"location":"modelling/AFLDisposals2024Datathon/#leaderboard-final","title":"Leaderboard - Final","text":"modelName RMSE PrizeRank Plugger 4.9736 1 crowbar 4.9987 2 PassThePill 4.9995 3 Gigi 5.0732 4 GoTheBloods 5.0788 5 willingly 5.1449 6 Nightingale 5.1561 7 TheBrew 5.2048 8 CaptainsChoice 5.2225 9 BAAALLLL 5.2334 10 RandomShrubbery 5.3179 11 randint(5,30) 5.3557 12 cyggy 5.3655 13 watpTV 5.445 14 blahboy 5.4664 15 Pudds 5.6644 16 DisposalDoctor 5.8644 17"},{"location":"modelling/AFLDisposals2024Datathon/#faqs","title":"FAQs","text":"<p>Will I receive a confirmation email once I submit my entry?</p> <p>No</p> <p>If I notice an error with my submission, can I resubmit?</p> <p>Yes, only the last entry received before the submission deadline will be considered for scoring</p> <p>What happens if there is a very late squad change where a player not named in the squad of 26 takes the field?</p> <p>The player will be removed from the competition scoring for that match</p> <p>What happens if a player is named as substitute OR is injured in the match OR is substituted out of the match after taking the field?</p> <p>The prediction for that player will stand</p> <p>What happens if a player is named as an emergency or as a substitute and does not take the field at any point?</p> <p>The player will be removed from the competition scoring for that match</p>"},{"location":"modelling/AFLPlayerDisposalsTutorial/","title":"AFL Player Disposals Tutorial","text":"<p>There are many possible ways to bet on an AFL match. Whilst Handicaps, Total Points and Match Odds have long been the traditional ways to bet into AFL markets, 'Player Proposition' bets have become the next big thing in AFL wagering. Traditionally, Same Game Multis will have options to pick players to have at least XX disposals, however 'Player Disposal Line' markets have quickly shot-up to be the next biggest market on the Betfair Exchange with regards to AFL. </p> <p>A player disposal line will be set at XX.5 disposals which, in theory, has a 50% chance of being over or under the true disposal prediction. The punter then needs to decide when they think the line is right or not, and take a position on either side. This tutorial here will outline how we can use data freely available online to generate predictions for player disposals.</p> <p>AFL Data is made available from the R package fitzRoy which requires installation of R and use of the Python R-emulator 'rpy2'. (Direct R code can also be used.) This package pulls data from four separate sites which all have similar data with only a few columns differing between each, however due to the differing ways these sources display team and player names, matching between them can be painful. For the purposes of this tutorial we will use the 'fryzigg' function in fitzRoy which pulls data from Squiggle, a renowned site for AFL modellers.</p>"},{"location":"modelling/AFLPlayerDisposalsTutorial/#requirements","title":"Requirements","text":"<ul> <li>A code editor with Jupyter Notebook functionality (e.g. VS Code)</li> <li>Python and R installations</li> </ul>"},{"location":"modelling/AFLPlayerDisposalsTutorial/#downloading-historic-data","title":"Downloading Historic Data","text":"Downloading data using rpy2 and fitzRoy<pre><code>import os\nimport rpy2.situation\nimport rpy2.robjects as robjects\nfrom rpy2.robjects.packages import importr\nimport pandas as pd\n\n# Set the R_HOME environment variable to the path of your R installation\n# NOTE: You must have your R installation saved to your system PATH\nos.environ['R_HOME'] = 'C:/Users/username/AppData/Local/Programs/R/R-43~1.0'\n\nprint(os.environ['R_HOME'])\n\n# Load the necessary R packages \n# These must be installed to your R installation first\nfitzRoy = importr('fitzRoy')\ndplyr = importr('dplyr')\n\n\nseasons = []\n\nfor i in range(2012,2024,1):\n    seasons.append(i)\n\nprint(seasons)\n\napi_queries = [\n                #'footywire',\n               'fryzigg',\n               #'afl',\n               #'afltables'\n               ]\n\nfor api in api_queries:\n\n    # Initialize an empty dataframe for storing the data\n    robjects.r('this_season &lt;- data.frame()')\n\n    # Loop through each season and fetch the data\n    for season in seasons:\n\n        query = 'fetch_player_stats_'+api\n        data = getattr(fitzRoy, query)(season=season, round_number=robjects.NULL)\n        robjects.globalenv['data'] = data\n        robjects.r('this_season &lt;- dplyr::bind_rows(this_season, data)')\n\n    # Retrieve the combined dataframe from R\n    this_season = robjects.r('this_season')\n\n    # Extract column names\n    column_names = list(this_season.colnames)\n\n    # Convert the R dataframe to a pandas dataframe\n    this_season_df = pd.DataFrame(robjects.conversion.rpy2py(this_season))\n\n    # Transpose the dataframe\n    this_season_df = this_season_df.T\n\n    # Set the correct column headers\n    this_season_df.columns = column_names\n\n    # Reset the index\n    this_season_df.reset_index(drop=True, inplace=True)\n\n    # Inspect the dataframe to ensure it's correctly oriented and headers are set\n    print(this_season_df.head())\n\n    # Save the dataframe to a CSV file\n    this_season_df.to_csv(api+'.csv', index=False)\n</code></pre> <p>This is the R code to do the same function for fryzigg</p> <pre><code>library(fitzRoy)\nlibrary(dplyr)\n\nseasons &lt;- 2012:2024\nthis_season &lt;- NULL\n\nfor (season in seasons) {\n\n  data &lt;- fitzRoy::fetch_player_stats_fryzigg(season = season)\n\n  this_season &lt;- dplyr::bind_rows(this_season, data)\n}\nwrite.csv(this_season,'fryzigg.csv')\n</code></pre>"},{"location":"modelling/AFLPlayerDisposalsTutorial/#processing-the-data","title":"Processing the data","text":"<p>Here we will load our csv file from the fryzigg function into a pandas dataframe for processing. </p> Loading our historical data<pre><code>import pandas as pd\nimport warnings\nfrom tqdm import tqdm\nfrom datetime import datetime\n\nwarnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nafl_data = pd.read_csv('fryzigg.csv',low_memory=False)\n\nafl_data = afl_data[[\n                                    'venue_name',\n                                    'match_id',\n                                    'match_home_team',\n                                    'match_away_team',\n                                    'match_date',\n                                    'match_round',\n                                    'match_home_team_score',\n                                    'match_away_team_score',\n                                    'match_margin',\n                                    'match_winner',\n                                    'match_weather_temp_c',\n                                    'match_weather_type',\n                                    'player_id',\n                                    'player_first_name',\n                                    'player_last_name',\n                                    'player_team',\n                                    'kicks',\n                                    'marks',\n                                    'handballs',\n                                    'disposals',\n                                    'effective_disposals',\n                                    'disposal_efficiency_percentage',\n                                    'goals',\n                                    'behinds',\n                                    'hitouts',\n                                    'tackles',\n                                    'rebounds',\n                                    'inside_fifties',\n                                    'clearances',\n                                    'clangers',\n                                    'free_kicks_for',\n                                    'free_kicks_against',\n                                    'contested_possessions',\n                                    'uncontested_possessions',\n                                    'contested_marks',\n                                    'marks_inside_fifty',\n                                    'one_percenters',\n                                    'bounces',\n                                    'goal_assists',\n                                    'time_on_ground_percentage',\n                                    'afl_fantasy_score',\n                                    'centre_clearances',\n                                    'stoppage_clearances',\n                                    'score_involvements',\n                                    'metres_gained',\n                                    'turnovers',\n                                    'intercepts',\n                                    'tackles_inside_fifty',\n                                    'contest_def_losses',\n                                    'contest_def_one_on_ones',\n                                    'contest_off_one_on_ones',\n                                    'contest_off_wins',\n                                    'def_half_pressure_acts',\n                                    'effective_kicks',\n                                    'f50_ground_ball_gets',\n                                    'ground_ball_gets',\n                                    'hitouts_to_advantage',\n                                    'intercept_marks',\n                                    'marks_on_lead',\n                                    'pressure_acts',\n                                    'rating_points',\n                                    'ruck_contests',\n                                    'score_launches',\n                                    'shots_at_goal',\n                                    'spoils'\n                                    ]]\n\n# This creates an unedited copy of the dataframe that will be used for calculating player level data\nplayer_data = afl_data.copy()                                \n</code></pre> <p>All the data here is split out by player, however, it is clear to anyone that watches AFL, a player's disposal count very much depends on the performance of the whole team. A defender will get a higher number of disposals if the team concedes a lot of forward 50 entries and a lower number if they don't. Here we will apply some functions to group this data by team, both for and against, and then concatenate it with the players individual data before we generate our features ready for training</p> Processing the data<pre><code>afl_data.rename(columns={'venue_name':'match_venue'}, inplace=True)\n\n# List of columns to calculate the sum for\ncolumns_to_sum = ['kicks', 'marks', 'handballs', 'disposals', 'effective_disposals', 'hitouts', 'tackles', 'rebounds', 'inside_fifties', 'clearances', 'clangers', 'free_kicks_for', 'free_kicks_against', 'contested_possessions', 'uncontested_possessions', 'contested_marks', 'marks_inside_fifty', 'one_percenters', 'bounces', 'goal_assists', 'centre_clearances', 'stoppage_clearances', 'score_involvements', 'metres_gained', 'turnovers', 'intercepts', 'tackles_inside_fifty', 'contest_def_losses', 'contest_def_one_on_ones', 'contest_off_one_on_ones', 'contest_off_wins', 'def_half_pressure_acts', 'effective_kicks', 'f50_ground_ball_gets', 'ground_ball_gets', 'hitouts_to_advantage', 'intercept_marks', 'marks_on_lead', 'pressure_acts', 'score_launches', 'shots_at_goal', 'spoils']\n\n# Calculate sum for each column separately\nsum_by_column = {}\nfor column in columns_to_sum:\n    sum_by_column[column] = afl_data.groupby(['match_id', 'player_team'])[column].sum()\n\n# Convert the dictionary to DataFrame\nsum_df = pd.DataFrame(sum_by_column)\n\nsum_df = sum_df.add_prefix('team_')\n\nteam_data = afl_data[[\n                                    'match_venue',\n                                    'match_id',\n                                    'player_team',\n                                    'match_date',\n                                    'match_round',\n                                    'match_winner',\n                                    'match_home_team_score',\n                                    'match_away_team_score',\n                                    'match_margin',\n                                    'match_weather_temp_c',\n                                    'match_weather_type',\n                                    'match_home_team',\n                                    'match_away_team'\n                                    ]]\nteam_data = team_data.drop_duplicates()\nteam_data = pd.merge(team_data,sum_df,how='left',on=['match_id','player_team'])\n\ndef home_away(row):\n    if row['match_away_team'] == row['player_team']:\n        return 'AWAY'\n    else:\n        return 'HOME'\n\nteam_data['home_away'] = team_data.apply(home_away, axis=1)\nteam_data.drop(columns=['player_team'],inplace=True)\n\nhome_team_data_score_data = team_data[team_data['home_away'] == 'HOME']\n\n# Add suffix '_against' to column names that do not begin with 'match_'\nfor col in home_team_data_score_data.columns:\n    if not col.startswith('match_'):\n        home_team_data_score_data.rename(columns={col: col + '_for'}, inplace=True)\n\nhome_team_data_concede_data = team_data[team_data['home_away'] == 'AWAY']\nhome_team_data_concede_data.drop(columns=['match_venue',\n                                    'match_date',\n                                    'match_round',\n                                    'match_winner',\n                                    'match_home_team_score',\n                                    'match_away_team_score',\n                                    'match_margin',\n                                    'match_weather_temp_c',\n                                    'match_weather_type',\n                                    'home_away'],inplace=True)\n\n\n# Add suffix '_against' to column names that do not begin with 'match_'\nfor col in home_team_data_concede_data.columns:\n    if not col.startswith('match_'):\n        home_team_data_concede_data.rename(columns={col: col + '_against'}, inplace=True)\n\nhome_team_data = pd.merge(home_team_data_score_data,home_team_data_concede_data,how='left',on=['match_id','match_home_team','match_away_team'])\nhome_team_data.rename(columns={'match_home_team_score':'team_points_for',\n                               'match_away_team_score':'team_points_against',\n                               'match_home_team':'match_team',\n                               'match_away_team':'match_opponent'}, inplace= True)\n\naway_team_data_score_data = team_data[team_data['home_away'] == 'AWAY']\n\n# Add suffix '_against' to column names that do not begin with 'match_'\nfor col in away_team_data_score_data.columns:\n    if not col.startswith('match_'):\n        away_team_data_score_data.rename(columns={col: col + '_for'}, inplace=True)\n\naway_team_data_concede_data = team_data[team_data['home_away'] == 'HOME']\naway_team_data_concede_data.drop(columns=['match_venue',\n                                    'match_date',\n                                    'match_round',\n                                    'match_winner',\n                                    'match_home_team_score',\n                                    'match_away_team_score',\n                                    'match_margin',\n                                    'match_weather_temp_c',\n                                    'match_weather_type',\n                                    'home_away'],inplace=True)\n\n# Add suffix '_against' to column names that do not begin with 'match_'\nfor col in away_team_data_concede_data.columns:\n    if not col.startswith('match_'):\n        away_team_data_concede_data.rename(columns={col: col + '_against'}, inplace=True)\n\naway_team_data = pd.merge(away_team_data_score_data,away_team_data_concede_data,how='left',on=['match_id','match_home_team','match_away_team'])\naway_team_data.rename(columns={'match_home_team_score':'team_points_against',\n                               'match_away_team_score':'team_points_for',\n                               'match_home_team':'match_opponent',\n                               'match_away_team':'match_team'}, inplace= True)\n\nafl_data = pd.concat([home_team_data,away_team_data])\nafl_data = afl_data[afl_data['team_spoils_for'] &gt; 0]\nafl_data['team_margin'] = afl_data['team_points_for'] - afl_data['team_points_against']\n\nstat_names = set('_'.join(col.split('_')[1:-1]) for col in afl_data.columns if col.startswith('team_') and (col.endswith('_for') or col.endswith('_against')))\n\n# Calculate the difference and create new columns\nfor stat in stat_names:\n    for_or_against = ['for', 'against']\n    for col_suffix in for_or_against:\n        col_name = f'team_{stat}_{col_suffix}'\n        if col_name in afl_data.columns:\n            for_or_against_value = afl_data[f'team_{stat}_{col_suffix}']\n            against_col_name = f'team_{stat}_against' if col_suffix == 'for' else f'team_{stat}_for'\n            against_value = afl_data[against_col_name]\n            diff_col_name = f'team_{stat}_diff'\n            afl_data[diff_col_name] = for_or_against_value - against_value\n\nafl_data.to_csv('afl_data.csv',index=False)\n</code></pre>"},{"location":"modelling/AFLPlayerDisposalsTutorial/#home-ground-advantage","title":"Home Ground Advantage","text":"<p>The next section here will be very prescriptive in how we define home ground advantage and neutral grounds. There are instances where a team will play another team at a venue that they both share as home ground and so true home ground advantage is lost (i.e. Richmond v Collingwood at the MCG), and so it may make sense for the purposes of the model to define both of these teams as being home teams (in terms of crowd, travel and ground dimensions). Additionally, we will define a function that calls out as neutral grounds for which neither team is a traditional home team for the venue(e.g. Geelong v Western Bulldogs at the Adelaide Oval).</p> Defining True Home Ground Advantage and Neutral Venues<pre><code># Provided dictionary of teams and their venues\nteams_venues = {\n    'Adelaide': ['Football Park', 'Adelaide Oval'],\n    'Port Adelaide': ['Football Park', 'Adelaide Oval'],\n    'Brisbane Lions': ['Metricon Stadium', 'Gabba'],\n    'Gold Coast': ['Metricon Stadium', 'Gabba','TIO Stadium'],\n    'Greater Western Sydney': ['ANZ Stadium', 'GIANTS Stadium', 'UNSW Canberra Oval', 'SCG'],\n    'Sydney': ['ANZ Stadium', 'GIANTS Stadium', 'SCG'],\n    'West Coast': ['Optus Stadium', 'Subiaco'],\n    'Fremantle': ['Optus Stadium', 'Subiaco'],\n    'Geelong': ['GMHBA Stadium', 'MCG', 'Marvel Stadium'],\n    'Carlton': ['MCG', 'Marvel Stadium'],\n    'Collingwood': ['MCG', 'Marvel Stadium'],\n    'Essendon': ['MCG', 'Marvel Stadium'],\n    'Hawthorn': ['MCG', 'Marvel Stadium'],\n    'Melbourne': ['MCG', 'Marvel Stadium'],\n    'North Melbourne': ['MCG', 'Marvel Stadium'],\n    'Richmond': ['MCG', 'Marvel Stadium'],\n    'St Kilda': ['MCG', 'Marvel Stadium'],\n    'Western Bulldogs': ['MCG', 'Marvel Stadium']\n}\n\n# Update 'home_away_for' column based on conditions\nfor index, row in afl_data.iterrows():\n    if row['home_away_for'] == 'AWAY' and row['match_venue'] in teams_venues.get(row['match_team'], []):\n        afl_data.at[index, 'home_away_for'] = 'HOME'\n\n# Provided dictionary of venues and their associated teams\nvenues_teams = {\n    'University of Tasmania Stadium': ['Hawthorn'],\n    'UNSW Canberra Oval': ['Greater Western Sydney'],\n    'GMHBA Stadium': ['Geelong'],\n    'Blundstone Arena': ['North Melbourne'],\n    'SCG': ['Greater Western Sydney', 'Sydney'],\n    'Gabba': ['Brisbane Lions', 'Gold Coast'],\n    'ANZ Stadium': ['Greater Western Sydney', 'Sydney'],\n    'MCG': ['Carlton', 'Collingwood', 'Essendon', 'Geelong', 'Hawthorn', 'Melbourne', 'North Melbourne', 'Richmond', 'St Kilda', 'Western Bulldogs'],\n    'Marvel Stadium': ['Carlton', 'Collingwood', 'Essendon', 'Geelong', 'Hawthorn', 'Melbourne', 'North Melbourne', 'Richmond', 'St Kilda', 'Western Bulldogs'],\n    'Metricon Stadium': ['Brisbane Lions', 'Gold Coast'],\n    'Subiaco': ['West Coast', 'Fremantle'],\n    'Optus Stadium': ['West Coast', 'Fremantle'],\n    'Football Park': ['Adelaide', 'Port Adelaide'],\n    'Adelaide Oval': ['Adelaide', 'Port Adelaide'],\n    'GIANTS Stadium': ['Greater Western Sydney'],\n    'Mars Stadium': ['Western Bulldogs'],\n    'TIO Stadium' : ['Gold Coast']\n}\n\n# Update 'home_away_for' column based on conditions\nfor index, row in afl_data.iterrows():\n    if row['match_team'] not in venues_teams.get(row['match_venue'], []) and row['match_opponent'] not in venues_teams.get(row['match_venue'], []):\n        afl_data.at[index, 'home_away_for'] = 'NEUTRAL'\n</code></pre>"},{"location":"modelling/AFLPlayerDisposalsTutorial/#creating-rolling-team-and-player-windows","title":"Creating rolling team and player windows","text":"<p>Let's create rolling windows for our team stats based on the last 10 matches and player stats for the last 5 matches. We'll then combine all this data ready for our algorithm</p> rolling windows<pre><code># Sort the DataFrame by 'match_team' alphabetically and 'match_id' ascending\nafl_data_sorted = afl_data.sort_values(by=['match_team', 'match_id'])\nrolling_team_columns = []\n\n# Identify columns that start with 'team'\nteam_columns = [col for col in afl_data_sorted.columns if col.startswith('team')]\n\n# Calculate rolling average for the last ten match_ids for each match_team, excluding the current match_id\ndef rolling_average_excluding_current(group):\n    for col in team_columns:\n        group[f'{col}_rolling_avg'] = group[col].shift(1).rolling(window=10, min_periods=10).mean()\n        group[f'{col}_rolling_var'] = group[col].shift(1).rolling(window=10, min_periods=10).var()\n        group[f'{col}_rolling_std'] = group[col].shift(1).rolling(window=10, min_periods=10).std()\n        group[f'{col}_rolling_median'] = group[col].shift(1).rolling(window=10, min_periods=10).median()\n    return group\n\n# Apply the rolling average function to each group of 'match_team'\nafl_data_rolling_avg = afl_data_sorted.groupby('match_team').apply(rolling_average_excluding_current)\n\nteam_rolling_columns_avg = [col for col in afl_data_rolling_avg.columns if 'rolling' in col]\n\n\nplayer_data = player_data[[\n    'match_id',\n    'player_id',\n    'player_first_name',\n    'player_last_name',\n    'player_position',\n    'guernsey_number',\n    'player_team'\n] + columns_to_sum]\n\nplayer_data_sorted = player_data.sort_values(by=['player_id', 'match_id'])\n\n# Calculate rolling statistics for the last five match_ids for each player, excluding the current match_id\ndef rolling_player_excluding_current(group):\n    for col in columns_to_sum:\n        group[f'player_{col}_rolling_avg'] = group[col].shift(1).rolling(window=5, min_periods=1).mean()\n        group[f'player_{col}_rolling_var'] = group[col].shift(1).rolling(window=5, min_periods=1).var()\n        group[f'player_{col}_rolling_std'] = group[col].shift(1).rolling(window=5, min_periods=1).std()\n        group[f'player_{col}_rolling_median'] = group[col].shift(1).rolling(window=5, min_periods=1).median()\n    return group\n\n# Get total number of player_id groups\ntotal_groups = len(player_data_sorted['player_id'].unique())\n\n# Apply rolling statistics function to each group of 'player_id' with tqdm progress bar\ntqdm.pandas(desc=\"Processing player_ids\", total=total_groups)\nplayer_data_rolling = player_data_sorted.groupby('player_id').progress_apply(rolling_player_excluding_current)\n\nplayer_rolling_columns_avg = [col for col in player_data_rolling.columns if 'rolling' in col]\n\nplayer_data_rolling = player_data_rolling.reset_index(drop=True)\nafl_data_rolling_avg = afl_data_rolling_avg.reset_index(drop=True)\ndataset = pd.merge(player_data_rolling,afl_data_rolling_avg,how='left',left_on=['match_id','player_team'],right_on=['match_id','match_team'])\n\ndataset = dataset[[\n    'match_id',\n    'match_date',\n    'match_round',\n    'match_team',\n    'match_opponent',\n    'match_venue',\n    'home_away_status',\n    'player_id',\n    'player_first_name',\n    'player_last_name',\n    'player_position',\n    'guernsey_number',\n    'disposals'\n\n] + team_rolling_columns_avg + player_rolling_columns_avg]\n\n# Discard 2012 data so that each team has a full rolling window of 10 matches available\ndataset = dataset[dataset['match_date'] &gt;= '2013-01-01']\n# Fill any missing data with 0\ndataset = dataset.fillna(0)\n\ndataset['match_date'] = pd.to_datetime(dataset['match_date'], format='%d/%m/%Y')\n\ntoday_date = datetime.today().date()\nnew_data = dataset[dataset['match_date'].dt.date &gt;= today_date]\nprint(new_data.head())\n\ndataset.to_csv('dataset.csv',index=False)\n</code></pre> <pre><code>   match_id match_date match_round match_team   match_opponent  \\\n0     13965 2012-03-31           1   Essendon  North Melbourne   \n1     13970 2012-04-07           2   Essendon    Port Adelaide   \n3     13988 2012-04-21           4   Essendon          Carlton   \n4     13996 2012-04-25           5   Essendon      Collingwood   \n5     14006 2012-05-05           6   Essendon   Brisbane Lions   \n\n      match_venue home_away_status  player_id player_first_name  \\\n0  Marvel Stadium             HOME      10398            Dustin   \n1  Marvel Stadium             HOME      10398            Dustin   \n3             MCG             HOME      10398            Dustin   \n4             MCG             HOME      10398            Dustin   \n5  Marvel Stadium             HOME      10398            Dustin   \n\n  player_last_name  ... player_score_launches_rolling_std  \\\n0         Fletcher  ...                          0.000000   \n1         Fletcher  ...                          0.000000   \n3         Fletcher  ...                          2.645751   \n4         Fletcher  ...                          2.380476   \n5         Fletcher  ...                          2.073644   \n\n   player_score_launches_rolling_median  player_shots_at_goal_rolling_avg  \\\n0                                   0.0                              0.00   \n1                                   0.0                              0.00   \n3                                   4.0                              0.00   \n4                                   2.5                              0.25   \n5                                   2.0                              0.20   \n\n   player_shots_at_goal_rolling_var  player_shots_at_goal_rolling_std  \\\n0                              0.00                          0.000000   \n1                              0.00                          0.000000   \n3                              0.00                          0.000000   \n4                              0.25                          0.500000   \n5                              0.20                          0.447214   \n\n   player_shots_at_goal_rolling_median  player_spoils_rolling_avg  \\\n0                                  0.0                       0.00   \n1                                  0.0                       6.00   \n3                                  0.0                       7.00   \n4                                  0.0                       5.25   \n5                                  0.0                       5.20   \n\n   player_spoils_rolling_var  player_spoils_rolling_std  \\\n0                   0.000000                   0.000000   \n1                   0.000000                   0.000000   \n3                   1.000000                   1.000000   \n4                  12.916667                   3.593976   \n5                   9.700000                   3.114482   \n\n   player_spoils_rolling_median  \n0                           0.0  \n1                           6.0  \n3                           7.0  \n4                           6.5  \n5                           6.0  \n\n[5 rows x 701 columns]\n</code></pre>"},{"location":"modelling/AFLPlayerDisposalsTutorial/#model-training","title":"Model Training","text":"<p>Let's now apply our LGBM model to this pre-processed data to create our pickle file</p> Model Training<pre><code>import pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport pickle\nfrom sklearn.model_selection import GridSearchCV\n\nfinal_dataset = dataset.copy()\ncategorical_columns = ['match_opponent', 'match_venue', 'home_away_status', 'player_position']\nfeature_columns = team_rolling_columns_avg + player_rolling_columns_avg + categorical_columns\n\n# Common models parameters\nverbose = 0\nlearning_rate = 0.1\nn_estimators = 500\n\ndef train_test_split(final_dataset, end_date):\n    '''\n    This function splits the dataset into a training set and a test set for the purposes of model training.\n    This is to enable testing of the trained model on an unseen test set to establish statistical metrics regarding its accuracy.\n    '''\n    final_dataset['match_date'] = pd.to_datetime(final_dataset['match_date'], format='%Y-%m-%d').dt.tz_localize(None)\n    # Split the data into train and test data\n    train_data = final_dataset[final_dataset['match_date'] &lt; end_date - relativedelta(years=2)].reset_index(drop=True)\n    test_data = final_dataset[(final_dataset['match_date'] &gt;= end_date - relativedelta(years=2)) &amp; (final_dataset['match_date'] &lt; end_date)].reset_index(drop=True)\n\n    return test_data, train_data\n\ntest_data, train_data = train_test_split(final_dataset, datetime.today())\n\ndef generate_xy(test_data, train_data, feature_cols):\n    '''\n    This function separates the target column 'disposals' from the actual features of the dataset and also separates the training features from the race metadata which is not being used for the training (e.g. raceId)\n    '''\n    train_x, train_y = train_data[feature_cols], train_data['disposals']\n    test_x, test_y = test_data[feature_cols], test_data['disposals']\n\n    return train_x, train_y, test_x, test_y\n\ntrain_x, train_y, test_x, test_y = generate_xy(test_data, train_data, feature_columns)\n\n# Convert categorical columns to 'category' type\nfor col in categorical_columns:\n    train_x[col] = train_x[col].astype('category')\n    test_x[col] = test_x[col].astype('category')\n\n# Define parameter grid for LGBMRegressor\nparam_grid = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'n_estimators': [100, 200, 500],\n    'num_leaves': [31, 80, 127],\n    'max_depth': [-1, 10, 20, 30],\n    'subsample': [0.7, 0.8, 0.9, 1.0]\n}\n\ndef LGBM_GridSearch(train_x, train_y, categorical_feature_indices):\n    # Initialize LGBMRegressor\n    lgbm = LGBMRegressor(force_col_wise=True, verbose=-1)\n\n    # Initialize GridSearchCV\n    grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n\n    # Fit GridSearchCV\n    grid_search.fit(train_x, train_y, categorical_feature=categorical_feature_indices)\n\n    # Print results\n    print(\"GridSearchCV Results:\")\n    print(\"Best parameters found:\", grid_search.best_params_)\n    print(\"Best negative mean squared error found:\", grid_search.best_score_)\n\n    lgbm_best_params = grid_search.best_params_\n\n    return lgbm_best_params\n\ncategorical_feature_indices = [train_x.columns.get_loc(col) for col in categorical_columns]\nlgbm_best_params = LGBM_GridSearch(train_x, train_y, categorical_feature_indices)\n\ndef apply_best_GridSearch_params(lgbm_best_params, train_x, train_y):\n    '''\n    This function takes the previously defined best parameters, trains the model using those parameters and then outputs the pickle file.\n    '''\n    # Define the model with best parameters\n    best_model = LGBMRegressor(force_col_wise=True, verbose=-1, n_estimators=lgbm_best_params['n_estimators'], learning_rate=lgbm_best_params['learning_rate'], num_leaves=lgbm_best_params['num_leaves'])\n\n    # Train the model on best parameters\n    best_model.fit(train_x, train_y, categorical_feature=categorical_feature_indices)\n\n    # Dump the pickle for the best model\n    with open('best_lgbm_model.pickle', 'wb') as f:\n        pickle.dump(best_model, f)\n\n    return best_model\n\nbest_model = apply_best_GridSearch_params(lgbm_best_params, train_x, train_y)\n\nBACKTESTING_COLUMNS = ['match_id',\n                       'match_date',\n                       'match_round',\n                       'match_team',\n                       'match_opponent',\n                       'match_venue',\n                       'home_away_status',\n                       'player_id',\n                       'player_first_name',\n                       'player_last_name',\n                       'guernsey_number',\n                       'disposals']\n\ndef best_model_predictions(best_model, test_data, test_x, output_file='lgbm_gridSearch_predictions.csv'):\n    '''\n    This function uses the best model to make predictions on the test data, adds the predictions as a column, and exports the predictions to a CSV file.\n    '''\n    # Predict using the best model\n    test_data['disposals_prediction'] = best_model.predict(test_x)\n\n    # Keep only required columns\n    export_columns = BACKTESTING_COLUMNS + ['disposals_prediction']\n    result_data = test_data[export_columns]\n\n    # Export DataFrame to CSV\n    result_data.to_csv(output_file, index=False)\n\n    return result_data\n\n# Example usage: Export predictions with disposals prediction as a column\ntest_data = best_model_predictions(best_model, test_data, test_x)\n</code></pre> <pre><code>GridSearchCV Results:\nBest parameters found: {'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 200, 'num_leaves': 31, 'subsample': 0.7}\nBest negative mean squared error found: -25.38659078300869\n</code></pre>"},{"location":"modelling/AFLPlayerDisposalsTutorial/#graphing-feature-importance","title":"Graphing feature importance","text":"Creating a feature importance graphic<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_feature_importance(model, feature_columns, top_n=30, output_file='feature_importance.jpg'):\n    '''\n    This function plots the feature importance of the trained model.\n    '''\n    # Get feature importances\n    feature_importances = model.feature_importances_\n\n    # Create a DataFrame for better visualization\n    importance_df = pd.DataFrame({'Feature': feature_columns, 'Importance': feature_importances})\n    importance_df = importance_df.sort_values(by='Importance', ascending=False).head(top_n)\n\n    # Plot the feature importances\n    plt.figure(figsize=(12, 10))\n    sns.barplot(x='Importance', y='Feature', data=importance_df)\n    plt.title('Top 30 Feature Importances')\n    plt.tight_layout()\n\n    # Save the plot as a .jpg file\n    plt.savefig(output_file, format='jpg')\n    plt.show()\n\n# Plot feature importance for the best model and save as .jpg\nplot_feature_importance(best_model, feature_columns)\n</code></pre>"},{"location":"modelling/AFLPlayerDisposalsTutorial/#creating-new-ratings","title":"Creating new ratings","text":"<p>Let's fetch upcoming lineups first</p> <p><pre><code>for (season in seasons) {\n  # Remember to change the round as required\n  data &lt;- fitzRoy::fetch_lineup(season = season, round_number = 17)\n\n  this_season &lt;- dplyr::bind_rows(this_season, data)\n}\nwrite.csv(this_season,'lineup.csv')\n</code></pre> Now let's add them to our original dataset - there will be a lot of missing data but we'll handle this.</p> adding new team data<pre><code>import pandas as pd\nfrom datetime import datetime, timedelta\n\n# Load the CSV file\nfryzigg = pd.read_csv('fryzigg.csv',low_memory=False)\n\n# Convert 'match_date' to datetime format if it's not already\nfryzigg['match_date'] = pd.to_datetime(fryzigg['match_date'],dayfirst=True)\n\n# Filter for match_date within the last 2 years\ntwo_years_ago = datetime.now() - timedelta(days=2*365)\nfiltered_fryzigg = fryzigg[fryzigg['match_date'] &gt;= two_years_ago]\n\n# Create the player_ids DataFrame with unique combinations\nplayer_ids = filtered_fryzigg[['player_id', 'player_first_name', 'player_last_name', 'player_team', 'guernsey_number']].drop_duplicates()\n\n# Load the CSV file\nlineup = pd.read_csv('lineup.csv')\n\n# Provided dictionary for team name replacements\nteams_dict = {\n    'Adelaide Crows': 'Adelaide',\n    'Brisbane Lions': 'Brisbane Lions',\n    'Carlton': 'Carlton',\n    'Collingwood': 'Collingwood',\n    'Essendon': 'Essendon',\n    'Fremantle': 'Fremantle',\n    'Geelong Cats': 'Geelong',\n    'Gold Coast SUNS': 'Gold Coast',\n    'GWS GIANTS': 'Greater Western Sydney',\n    'Hawthorn': 'Hawthorn',\n    'Melbourne': 'Melbourne',\n    'North Melbourne': 'North Melbourne',\n    'Port Adelaide': 'Port Adelaide',\n    'Richmond': 'Richmond',\n    'St Kilda': 'St Kilda',\n    'Sydney Swans': 'Sydney',\n    'West Coast Eagles': 'West Coast',\n    'Western Bulldogs': 'Western Bulldogs'\n}\n\n# Replace the values in the teamName column\nlineup['teamName'] = lineup['teamName'].replace(teams_dict)\n\n# Extract only the date part from utcStartTime column and format it\nlineup['utcStartTime'] = lineup['utcStartTime'].str.split('T').str[0]\nlineup['utcStartTime'] = pd.to_datetime(lineup['utcStartTime']).dt.strftime('%Y-%m-%d %h:%m%s')\n\n# Create 'away_teams' DataFrame\naway_teams = lineup[lineup['teamType'] == 'away'][['providerId', 'teamName']].drop_duplicates()\naway_teams.rename(columns={'teamName': 'match_away_team'}, inplace=True)\n\n# Generate new 'match_id' values for away_teams starting from max_match_id + 1\nmax_match_id = fryzigg['match_id'].max()\nnum_rows = len(away_teams)\nnew_match_ids = list(range(max_match_id + 1, max_match_id + 1 + num_rows))\n\n# Add 'match_id' column to away_teams with the new values\naway_teams['match_id'] = new_match_ids\n\n# Create 'home_teams' DataFrame\nhome_teams = lineup[lineup['teamType'] == 'home'][['providerId', 'teamName']].drop_duplicates()\nhome_teams.rename(columns={'teamName': 'match_home_team'}, inplace=True)\n\n# Merge 'away_teams' and 'home_teams' back to 'lineup'\nlineup = pd.merge(lineup, away_teams, on='providerId', how='left')\nlineup = pd.merge(lineup, home_teams, on='providerId', how='left')\n\n# Keep only the specified columns\ncolumns_to_keep = [\n    'utcStartTime',\n    'round.roundNumber',\n    'venue.name',\n    'teamName',\n    'position',\n    'player.playerJumperNumber',\n    'player.playerName.givenName',\n    'player.playerName.surname',\n    'match_home_team',\n    'match_away_team',\n    'match_id'\n]\nlineup = lineup[columns_to_keep]\n\n# Rename the columns using a dictionary\ncolumn_rename_dict = {\n    'utcStartTime': 'match_date',\n    'round.roundNumber': 'match_round',\n    'venue.name': 'venue_name',\n    'teamName': 'player_team',\n    'position': 'player_position',\n    'player.playerJumperNumber': 'guernsey_number',\n    'player.playerName.givenName': 'player_first_name',\n    'player.playerName.surname': 'player_last_name'\n}\nlineup.rename(columns=column_rename_dict, inplace=True)\n\n\n# Merge player_ids with lineup on all columns except 'player_id'\nlineup_with_playerId = pd.merge(lineup, player_ids, on=['player_first_name', 'player_last_name', 'player_team', 'guernsey_number'], how='left')\n\nmax_player_id = fryzigg['player_id'].max()\n# Generate new player_id values starting from max_player_id + 1 for NaN values in merged_df\nnext_player_id = max_player_id + 1\nlineup_with_playerId['player_id'] = lineup_with_playerId['player_id'].fillna(lineup_with_playerId.index.to_series().apply(lambda x: next_player_id + x))\nlineup_with_playerId['player_id'] = lineup_with_playerId['player_id'].astype('int64')\n\n# Concatenate fryzigg and lineup DataFrames\nfryzigg_with_lineup = pd.concat([fryzigg, lineup_with_playerId], axis=0, ignore_index=True)\nfryzigg_with_lineup.fillna(0, inplace=True)\n\n# Display or use the concatenated DataFrame as needed\nprint(fryzigg_with_lineup)\n\n# Important to save this as a new file to ensure that the file with actual stats is not corrupted.\nfryzigg_with_lineup.to_csv('fryzigg_with_lineup.csv',index=False)\n</code></pre> <p>The next step would be to reload our file fryzigg_with_lineup.csv and apply all of the pre-processing steps up until the point we trained our model. We will then load our pickle file and generate new ratings</p> Generate New Predictions<pre><code>import pickle\n\ntoday_date = datetime.today().date() - timedelta(days=7)\nnew_data = dataset[dataset['match_date'].dt.date &gt;= today_date]\n\ncategorical_columns = ['match_opponent', 'match_venue', 'home_away_status', 'player_position']\nfeature_columns = team_rolling_columns_avg + player_rolling_columns_avg + categorical_columns\n\n# Load the pre-trained model from the pickle file\nwith open('best_lgbm_model.pickle', 'rb') as f:\n    best_model = pickle.load(f)\n\n# Pre-process new_data to ensure it matches the training data format\n# Assuming 'new_data' is your new DataFrame and 'feature_columns' is defined as before\nnew_data_processed = new_data.copy()\n\n# If you have categorical features, ensure they are of type 'category'\nfor col in categorical_columns:\n    new_data_processed[col] = new_data_processed[col].astype('category')\n\n# Extract features for prediction\nnew_data_features = new_data_processed[feature_columns]\n\n# Generate predictions\nnew_data_processed['disposals_prediction'] = best_model.predict(new_data_features)\n\n# Select the required columns for the output\noutput_columns = [\n    'match_date',\n    'match_round',\n    'match_venue',\n    'player_id',\n    'guernsey_number',\n    'match_team',\n    'player_position',\n    'player_first_name',\n    'player_last_name',\n    'disposals_prediction'\n]\n\n# Ensure the output DataFrame has the required columns\noutput_data = new_data_processed[output_columns]\n\n# Export the predictions to a CSV file\noutput_data.to_csv('new_data_predictions.csv', index=False)\n\nprint(\"Predictions have been saved to 'new_data_predictions.csv'.\")\n</code></pre> <p>Congratulations! You've generated predictions for the upcoming matches! You can now use this output to bet into player disposal markets. For historical pricing data for these markets, visit the page here</p>"},{"location":"modelling/AFLPlayerDisposalsTutorial/#final-step","title":"Final Step","text":"<p>Remember to update your historical data with actual results once they become available (this usually occurs by Tuesday of the following week)</p>"},{"location":"modelling/AFLPlayerDisposalsTutorial/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"modelling/AFLmodellingPython/","title":"AFL Modelling Walkthrough","text":""},{"location":"modelling/AFLmodellingPython/#01-data-cleaning","title":"01. Data Cleaning","text":"<p>These tutorials will walk you through how to construct your own basic AFL model, using publicly available data. The output will be odds for each team to win, which will be shown on The Hub.</p> <p>In this notebook we will walk you through the basics of cleaning this dataset and how we have done it. If you want to get straight to feature creation or modelling, feel free to jump ahead!</p> <pre><code># Import libraries\nimport pandas as pd\nimport numpy as np\nimport re\npd.set_option('display.max_columns', None)\n</code></pre> <p>We will first explore the DataFrames, and then create functions to wrangle them and clean them into more consistent sets of data.</p> <pre><code># Read/clean each DataFrame\nmatch_results = pd.read_csv(\"data/afl_match_results.csv\")\nodds = pd.read_csv(\"data/afl_odds.csv\")\nplayer_stats = pd.read_csv(\"data/afl_player_stats.csv\")\n</code></pre> <pre><code>odds.tail(3)\n</code></pre> trunc event_name path selection_name odds 4179 2018-09-01 Match Odds VFL/Richmond Reserves v Williamstown Williamstown 2.3878 4180 2018-09-01 Match Odds WAFL/South Fremantle v West Perth South Fremantle 1.5024 4181 2018-09-01 Match Odds WAFL/South Fremantle v West Perth West Perth 2.7382 <pre><code>match_results.tail(3)\n</code></pre> Game Date Round Home.Team Home.Goals Home.Behinds Home.Points Away.Team Away.Goals Away.Behinds Away.Points Venue Margin Season Round.Type Round.Number 15395 15396 2018-08-26 R23 Brisbane Lions 11 6 72 West Coast 14 14 98 Gabba -26 2018 Regular 23 15396 15397 2018-08-26 R23 Melbourne 15 12 102 GWS 8 9 57 M.C.G. 45 2018 Regular 23 15397 15398 2018-08-26 R23 St Kilda 14 10 94 North Melbourne 17 15 117 Docklands -23 2018 Regular 23 <pre><code>player_stats.tail(3)\n</code></pre> AF B BO CCL CG CL CM CP D DE Date ED FA FF G GA HB HO I50 ITC K M MG MI5 Match_id One.Percenters Opposition Player R50 Round SC SCL SI Season Status T T5 TO TOG Team UP Venue 89317 38 1 0 0.0 0 0 1 2 9 55.6 25/08/2018 5 0 0 0 0 3 0 0 2.0 6 3 132.0 2 9711 0 Fremantle Christopher Mayne 1 Round 23 35 0.0 2.0 2018 Away 1 0.0 1.0 57 Collingwood 7 Optus Stadium 89318 38 0 0 0.0 3 0 0 3 9 55.6 25/08/2018 5 0 1 0 0 3 0 0 4.0 6 3 172.0 0 9711 2 Fremantle Nathan Murphy 5 Round 23 29 0.0 0.0 2018 Away 1 0.0 3.0 70 Collingwood 6 Optus Stadium 89319 56 1 0 0.0 1 0 0 3 8 62.5 25/08/2018 5 0 0 2 0 2 0 0 2.0 6 3 180.0 3 9711 2 Fremantle Jaidyn Stephenson 0 Round 23 56 0.0 4.0 2018 Away 3 1.0 2.0 87 Collingwood 5 Optus Stadium <p>Have a look at the structure of the DataFrames. Notice that for the odds DataFrame, each game is split between two rows, whilst for the match_results each game is on one row. We will have to get around this by splitting the games up onto two rows, as this will allow our feature transformation functions to be applied more easily later on. For the player_stats DataFrame we will aggregate these stats into each game on separate rows.</p> <p>First, we will write functions to make the odds data look a bit nicer, with only a team column, a date column and a 'home_game' column which takes the values 0 or 1 depending on if it was a home game for that team. To do this we will use the regex module to extract the team names from the path column, as well as the to_datetime function from pandas. We will also replace all the inconsistent team names with consistent team names.</p> <pre><code>def get_cleaned_odds(df=None):\n    # If a df hasn't been specified as a parameter, read the odds df\n    if df is None:\n        df = pd.read_csv(\"data/afl_odds.csv\")\n\n    # Get a dictionary of team names we want to change and their new values\n    team_name_mapping = {\n    'Adelaide Crows': 'Adelaide',\n    'Brisbane Lions': 'Brisbane',\n    'Carlton Blues': 'Carlton',\n    'Collingwood Magpies': 'Collingwood',\n    'Essendon Bombers': 'Essendon',\n    'Fremantle Dockers': 'Fremantle',\n    'GWS Giants': 'GWS',\n    'Geelong Cats': 'Geelong',\n    'Gold Coast Suns': 'Gold Coast',\n    'Greater Western Sydney': 'GWS',\n    'Greater Western Sydney Giants': 'GWS',\n    'Hawthorn Hawks': 'Hawthorn',\n    'Melbourne Demons': 'Melbourne', \n    'North Melbourne Kangaroos': 'North Melbourne',\n    'Port Adelaide Magpies': 'Port Adelaide',\n    'Port Adelaide Power': 'Port Adelaide', \n    'P Adelaide': 'Port Adelaide',\n    'Richmond Tigers': 'Richmond',\n    'St Kilda Saints': 'St Kilda', \n    'Sydney Swans': 'Sydney',\n    'West Coast Eagles': 'West Coast',\n    'Wetsern Bulldogs': 'Western Bulldogs',\n    'Western Bullbogs': 'Western Bulldogs'\n    }\n\n    # Add columns\n    df = (df.assign(date=lambda df: pd.to_datetime(df.trunc), # Create a datetime column\n                    home_team=lambda df: df.path.str.extract('(([\\w\\s]+) v ([\\w\\s]+))', expand=True)[1].str.strip(),\n                    away_team=lambda df: df.path.str.extract('(([\\w\\s]+) v ([\\w\\s]+))', expand=True)[2].str.strip())\n            .drop(columns=['path', 'trunc', 'event_name']) # Drop irrelevant columns\n            .rename(columns={'selection_name': 'team'}) # Rename columns\n            .replace(team_name_mapping)\n            .sort_values(by='date')\n            .reset_index(drop=True)\n            .assign(home_game=lambda df: df.apply(lambda row: 1 if row.home_team == row.team else 0, axis='columns'))\n            .drop(columns=['home_team', 'away_team']))\n    return df\n</code></pre> <pre><code># Apply the wrangling and cleaning function\nodds = get_cleaned_odds(odds)\nodds.tail()\n</code></pre> team odds date home_game 4177 South Fremantle 1.5024 2018-09-01 1 4178 Port Melbourne 2.8000 2018-09-01 0 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 <p>We now have a DataFrame that looks nice and easy to join with our other DataFrames. Now let's lean up the match_details DataFrame.</p> <pre><code># Define a function which cleans the match results df, and separates each teams' stats onto individual rows\ndef get_cleaned_match_results(df=None):\n    # If a df hasn't been specified as a parameter, read the match_results df\n    if df is None:\n        df = pd.read_csv(\"data/afl_match_results.csv\")\n\n    # Create column lists to loop through - these are the columns we want in home and away dfs\n    home_columns = ['Game', 'Date', 'Round.Number', 'Home.Team', 'Home.Goals', 'Home.Behinds', 'Home.Points', 'Margin', 'Venue', 'Away.Team', 'Away.Goals', 'Away.Behinds', 'Away.Points']\n    away_columns = ['Game', 'Date', 'Round.Number', 'Away.Team', 'Away.Goals', 'Away.Behinds', 'Away.Points', 'Margin', 'Venue', 'Home.Team', 'Home.Goals', 'Home.Behinds', 'Home.Points']\n\n    mapping = ['game', 'date', 'round', 'team', 'goals', 'behinds', 'points', 'margin', 'venue', 'opponent', 'opponent_goals', 'opponent_behinds', 'opponent_points']\n\n    team_name_mapping = {\n    'Brisbane Lions': 'Brisbane',\n    'Footscray': 'Western Bulldogs'\n    }\n\n    # Create a df with only home games\n    df_home = (df[home_columns]\n                .rename(columns={old_col: new_col for old_col, new_col in zip(home_columns, mapping)})\n                .assign(home_game=1))\n\n    # Create a df with only away games\n    df_away = (df[away_columns]\n                .rename(columns={old_col: new_col for old_col, new_col in zip(away_columns, mapping)})\n                .assign(home_game=0,\n                        margin=lambda df: df.margin * -1))\n\n    # Append these dfs together\n    new_df = (df_home.append(df_away)\n                     .sort_values(by='game') # Sort by game ID\n                     .reset_index(drop=True) # Reset index\n                     .assign(date=lambda df: pd.to_datetime(df.date)) # Create a datetime column\n                     .replace(team_name_mapping)) # Rename team names to be consistent with other dfs\n    return new_df\n</code></pre> <pre><code>match_results = get_cleaned_match_results(match_results)\nmatch_results.head()\n</code></pre> game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 3 2 1897-05-08 1 St Kilda 2 4 16 -25 Victoria Park Collingwood 5 11 41 0 4 3 1897-05-08 1 Geelong 3 6 24 -23 Corio Oval Essendon 7 5 47 1 <p>Now we have both the odds DataFrame and match_results DataFrame ready for feature creation! Finally, we will aggregate the player_stats DataFrame stats for each game rather than individual player stats. For this DataFrame we have regular stats, such as disposals, marks etc. and Advanced Stats, such as Tackles Inside 50 and Metres Gained. However these advanced stats are only available from 2015, so we will not be using them in this tutorial - as there isn't enough data from 2015 to train our models.</p> <p>Let's now aggregate the player_stats DataFrame.</p> <pre><code>def get_cleaned_aggregate_player_stats(df=None):\n    # If a df hasn't been specified as a parameter, read the player_stats df\n    if df is None:\n        df = pd.read_csv(\"data/afl_player_stats.csv\")\n\n    agg_stats = (df.rename(columns={ # Rename columns to lowercase\n                    'Season': 'season',\n                    'Round': 'round',\n                    'Team': 'team',\n                    'Opposition': 'opponent',\n                    'Date': 'date'\n                    })\n                   .groupby(by=['date', 'season', 'team', 'opponent'], as_index=False) # Groupby to aggregate the stats for each game\n                   .sum()\n                   .drop(columns=['DE', 'TOG', 'Match_id']) # Drop columns\n                   .assign(date=lambda df: pd.to_datetime(df.date, format=\"%d/%m/%Y\")) # Create a datetime object\n                   .sort_values(by='date')\n                   .reset_index(drop=True))\n    return agg_stats\n</code></pre> <pre><code>agg_stats = get_cleaned_aggregate_player_stats(player_stats)\n</code></pre> <pre><code>agg_stats.tail()\n</code></pre> date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3621 2018-08-26 2018 Brisbane West Coast 1652 5 0 14.0 49 37 8 132 394 302 20 18 11 9 167 48 49 59.0 227 104 5571.0 6 48 39 1645 23.0 86.0 62 13.0 69.0 256 3622 2018-08-26 2018 West Coast Brisbane 1548 11 5 13.0 49 42 9 141 360 262 18 20 14 8 137 39 56 70.0 223 95 5809.0 12 39 34 1655 29.0 94.0 55 6.0 59.0 217 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 <p>We now have a three fully prepared DataFrames which are almost ready to be analysed and for a model to be built on! Let's have a look at how they look and then merge them together into our final DataFrame.</p> <pre><code>odds.tail(3)\n</code></pre> team odds date home_game 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 <pre><code>match_results.tail(3)\n</code></pre> game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 30793 15397 2018-08-26 23 Melbourne 15 12 102 45 M.C.G. GWS 8 9 57 1 30794 15398 2018-08-26 23 St Kilda 14 10 94 -23 Docklands North Melbourne 17 15 117 1 30795 15398 2018-08-26 23 North Melbourne 17 15 117 23 Docklands St Kilda 14 10 94 0 <pre><code>agg_stats.tail(3)\n</code></pre> date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 <pre><code>merged_df = (odds[odds.team.isin(agg_stats.team.unique())]\n                .pipe(pd.merge, match_results, on=['date', 'team', 'home_game'])\n                .pipe(pd.merge, agg_stats, on=['date', 'team', 'opponent'])\n                .sort_values(by=['game']))\n</code></pre> <pre><code>merged_df.tail(3)\n</code></pre> team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3199 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3195 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3200 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 <p>Great! We now have a clean looking datset with each row representing one team in a game. Let's now eliminate the outliers from a dataset. We know that Essendon had a doping scandal which resulted in their entire team being banned for a year in 2016, so let's remove all of their 2016 games. To do this we will filter based on the team and season, and then invert this with ~.</p> <pre><code># Define a function which eliminates outliers\ndef outlier_eliminator(df):\n    # Eliminate Essendon 2016 games\n    essendon_filter_criteria = ~(((df['team'] == 'Essendon') &amp; (df['season'] == 2016)) | ((df['opponent'] == 'Essendon') &amp; (df['season'] == 2016)))\n    df = df[essendon_filter_criteria].reset_index(drop=True)\n\n    return df\n</code></pre> <pre><code>afl_data = outlier_eliminator(merged_df)\n</code></pre> <pre><code>afl_data.tail(3)\n</code></pre> team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 <p>Finally, let's mark all of the columns that we are going to use in feature creation with the string 'f_' at the start of their column name so that we can easily filter for these columns.</p> <pre><code>non_feature_cols = ['team', 'date', 'home_game', 'game', 'round', 'venue', 'opponent', 'season']\nafl_data = afl_data.rename(columns={col: 'f_' + col for col in afl_data if col not in non_feature_cols})\n</code></pre> <pre><code>afl_data.tail(3)\n</code></pre> team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 <p>Our data is now fully ready to be explored and for features to be created.</p>"},{"location":"modelling/AFLmodellingPython/#02-feature-creation","title":"02. Feature Creation","text":"<p>These tutorials will walk you through how to construct your own basic AFL model. The output will be odds for each team to win, which will be shown on The Hub.</p> <p>In this notebook we will walk you through creating features from our dataset, which was cleaned in the first tutorial. Feature engineering is an integral part of the Data Science process. Creative and smart features can be the difference between an average performing model and a model profitable which beats the market odds.</p>"},{"location":"modelling/AFLmodellingPython/#grabbing-our-dataset","title":"Grabbing Our Dataset","text":"<p>First, we will import our required modules, as well as the prepare_afl_data function which we created in our afl_data_cleaning script. This essentially cleans all the data for us so that we're ready to explore the data and make some features.</p> <pre><code># Import modules\nfrom afl_data_cleaning_v2 import *\nimport afl_data_cleaning_v2\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\n</code></pre> <pre><code># Use the prepare_afl_data function to prepare the data for us; this function condenses what we walked through in the previous tutorial\nafl_data = prepare_afl_data()\n</code></pre> <pre><code>afl_data.tail(3)\n</code></pre> team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269"},{"location":"modelling/AFLmodellingPython/#creating-a-feature-dataframe","title":"Creating A Feature DataFrame","text":"<p>Let's create a feature DataFrame and merge all of our features into this DataFrame as we go.</p> <pre><code>features = afl_data[['date', 'game', 'team', 'opponent', 'venue', 'home_game']].copy()\n</code></pre>"},{"location":"modelling/AFLmodellingPython/#what-each-column-refers-to","title":"What Each Column Refers To","text":"<p>Below is a DataFrame which outlines what each column refers to.</p> <pre><code>column_abbreviations = pd.read_csv(\"data/afl_data_columns_mapping.csv\")\ncolumn_abbreviations\n</code></pre> Feature Abbreviated Feature 0 GA Goal Assists 1 CP Contested Possessions 2 UP Uncontested Possessions 3 ED Effective Disposals 4 CM Contested Marks 5 MI5 Marks Inside 50 6 One.Percenters One Percenters 7 BO Bounces 8 K Kicks 9 HB Handballs 10 D Disposals 11 M Marks 12 G Goals 13 B Behinds 14 T Tackles 15 HO Hitouts 16 I50 Inside 50s 17 CL Clearances 18 CG Clangers 19 R50 Rebound 50s 20 FF Frees For 21 FA Frees Against 22 AF AFL Fantasy Points 23 SC Supercoach Points 24 CCL Centre Clearances 25 SCL Stoppage Clearances 26 SI Score Involvements 27 MG Metres Gained 28 TO Turnovers 29 ITC Intercepts 30 T5 Tackles Inside 50"},{"location":"modelling/AFLmodellingPython/#feature-creation","title":"Feature Creation","text":"<p>Now let's think about what features we can create. We have a enormous amount of stats to sift through. To start, let's create some simple features based on our domain knowledge of Aussie Rules.</p>"},{"location":"modelling/AFLmodellingPython/#creating-expontentially-weighted-rolling-averages-as-features","title":"Creating Expontentially Weighted Rolling Averages as Features","text":"<p>Next, we will create rolling averages of statistics such as Tackles, which we will use as features.</p> <p>It is fair to assume that a team's performance in a certain stat may have predictive power to the overall result. And in general, if a team consistently performs well in this stat, this may have predictive power to the result of their future games. We can't simply train a model on stats from the game which we are trying to predict (i.e. data that we don't have before the game begins), as this will leak the result. We need to train our model on past data. One way of doing this is to train our model on average stats over a certain amount of games. If a team is averaging high in this stat, this may give insight into if they are a strong team. Similarly, if the team is averaging poorly in this stat (relative to the team they are playing), this may have predictive power and give rise to a predicted loss.</p> <p>To do this we will create a function which calculates the rolling averages, known as create_exp_weighted_avgs, which takes our cleaned DataFrame as an input, as well as the alpha which, when higher, weights recent performances more than old performances. To read more about expontentially weighted moving averages, please read the documentation here.</p> <p>First, we will grab all the columns which we want to create EMAs for, and then use our function to create the average for that column. We will create a new DataFrame and add these columns to this new DataFrame.</p> <pre><code># Define a function which returns a DataFrame with the expontential moving average for each numeric stat\ndef create_exp_weighted_avgs(df, span):\n    # Create a copy of the df with only the game id and the team - we will add cols to this df\n    ema_features = df[['game', 'team']].copy()\n\n    feature_names = [col for col in df.columns if col.startswith('f_')] # Get a list of columns we will iterate over\n\n    for feature_name in feature_names:\n        feature_ema = (df.groupby('team')[feature_name]\n                         .transform(lambda row: (row.ewm(span=span)\n                                                    .mean()\n                                                    .shift(1))))\n        ema_features[feature_name] = feature_ema\n\n    return ema_features\n</code></pre> <pre><code>features_rolling_averages = create_exp_weighted_avgs(afl_data, span=10)\n</code></pre> <pre><code>features_rolling_averages.tail()\n</code></pre> game team f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3152 15396 West Coast 2.094236 12.809630 10.047145 86.904928 8.888770 11.435452 9.403444 78.016158 3193.612782 16.472115 11.958482 23.379562 100.095244 68.252001 27.688669 284.463270 719.884644 525.878017 36.762440 44.867118 25.618202 17.522871 270.478779 88.139376 105.698031 148.005305 449.405865 201.198907 11581.929999 20.048124 95.018480 74.180967 3314.157893 44.872398 177.894442 126.985101 20.565549 138.876613 438.848376 3153 15397 GWS 1.805565 13.100372 13.179329 91.781563 18.527618 10.371198 11.026754 73.253945 3165.127358 19.875913 12.947209 25.114002 105.856671 80.609640 23.374884 303.160047 741.439198 534.520295 42.597317 38.160889 26.208715 18.688880 300.188301 81.540693 106.989070 143.032506 441.250897 173.050118 12091.630837 21.106142 103.077097 80.201059 3419.245919 55.495610 219.879895 138.202470 25.313148 135.966798 438.466439 3154 15397 Melbourne 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.787800 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 3155 15398 North Melbourne 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.541130 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3156 15398 St Kilda 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.498760 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 <p>As you can see our function worked perfectly! Now we have a full DataFrame of exponentially weighted moving averages. Note that as these rolling averages have been shifted by 1 to ensure no data leakage, the first round of the data will have all NA values. We can drop these later.</p> <p>Let's add these averages to our features DataFrame</p> <pre><code>features = pd.merge(features, features_rolling_averages, on=['game', 'team'])\n</code></pre>"},{"location":"modelling/AFLmodellingPython/#creating-a-form-between-the-teams-feature","title":"Creating a 'Form Between the Teams' Feature","text":"<p>It is well known in Aussie Rules that often some teams perform better against certain teams than others. If we isolate our features to pure stats based on previous games not between the teams playing, or elo ratings, we won't account for any relationships between certain teams. An example is the Kennett Curse, where Geelong won 11 consecutive games against Hawthorn, despite being similarly matched teams. Let's create a feature which calculates how many games a team has won against their opposition over a given window of games.</p> <p>To do this, we will need to use historical data that dates back well before our current DataFrame starts at. Otherwise we will be using a lot of our games to calculate form, meaning we will have to drop these rows before feeding it into an algorithm. So let's use our prepare_match_results function which we defined in the afl_data_cleaning tutorial to grab a clean DataFrame of all match results since 1897. We can then calculate the form and join this to our current DataFrame.</p> <pre><code>match_results = afl_data_cleaning_v2.get_cleaned_match_results()\n</code></pre> <pre><code>match_results.head(3)\n</code></pre> game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 <pre><code>form_btwn_teams = match_results[['game', 'team', 'opponent', 'margin']].copy()\n\nform_btwn_teams['f_form_margin_btwn_teams'] = (match_results.groupby(['team', 'opponent'])['margin']\n                                                          .transform(lambda row: row.rolling(5).mean().shift())\n                                                          .fillna(0))\n\nform_btwn_teams['f_form_past_5_btwn_teams'] = \\\n(match_results.assign(win=lambda df: df.apply(lambda row: 1 if row.margin &gt; 0 else 0, axis='columns'))\n              .groupby(['team', 'opponent'])['win']\n              .transform(lambda row: row.rolling(5).mean().shift() * 5)\n              .fillna(0))\n</code></pre> <pre><code>form_btwn_teams.tail(3)\n</code></pre> game team opponent margin f_form_margin_btwn_teams f_form_past_5_btwn_teams 30793 15397 Melbourne GWS 45 -23.2 2.0 30794 15398 St Kilda North Melbourne -23 -3.2 2.0 30795 15398 North Melbourne St Kilda 23 3.2 3.0 <pre><code># Merge to our features df\nfeatures = pd.merge(features, form_btwn_teams.drop(columns=['margin']), on=['game', 'team', 'opponent'])\n</code></pre>"},{"location":"modelling/AFLmodellingPython/#creating-efficiency-features","title":"Creating Efficiency Features","text":""},{"location":"modelling/AFLmodellingPython/#disposal-efficiency","title":"Disposal Efficiency","text":"<p>Disposal efficiency is pivotal in Aussie Rules football. If you are dispose of the ball effectively you are much more likely to score and much less likely to concede goals than if you dispose of it ineffectively.</p> <p>Let's create a disposal efficiency feature by dividing Effective Disposals by Disposals.</p>"},{"location":"modelling/AFLmodellingPython/#inside-50rebound-50-efficiency","title":"Inside 50/Rebound 50 Efficiency","text":"<p>Similarly, one could hypothesise that teams who keep the footy in their Inside 50 regularly will be more likely to score, whilst teams who are effective at getting the ball out of their Defensive 50 will be less likely to concede. Let's use this logic to create Inside 50 Efficiency and Rebound 50 Efficiency features.</p> <p>The formula used will be: <pre><code>Inside 50 Efficiency = R50_Opponents / I50 (lower is better).\nRebound 50 Efficiency = R50 / I50_Opponents (higher is better).\n</code></pre></p> <p>Using these formulas, I50 Efficiency = R50 Efficiency_Opponent. So we will just need to create the formulas for I50 efficiency. To create these features we will need the opposition's Inside 50s/Rebound 50s. So we will split out data into two DataFrames, create a new DataFrame by joining these two DataFrames on the Game, calculate our efficiency features, then join our features with our main features DataFrame.</p> <pre><code># Get each match on single rows\nsingle_row_df = (afl_data[['game', 'team', 'f_I50', 'f_R50', 'f_D', 'f_ED', 'home_game', ]]\n                    .query('home_game == 1')\n                    .rename(columns={'team': 'home_team', 'f_I50': 'f_I50_home', 'f_R50': 'f_R50_home', 'f_D': 'f_D_home', 'f_ED': 'f_ED_home'})\n                    .drop(columns='home_game')\n                    .pipe(pd.merge, afl_data[['game', 'team', 'f_I50', 'f_R50', 'f_D', 'f_ED', 'home_game']]\n                                    .query('home_game == 0')\n                                    .rename(columns={'team': 'away_team', 'f_I50': 'f_I50_away', 'f_R50': 'f_R50_away', 'f_D': 'f_D_away', 'f_ED': 'f_ED_away'})\n                                    .drop(columns='home_game'), on='game'))\n</code></pre> <pre><code>single_row_df.head()\n</code></pre> game home_team f_I50_home f_R50_home f_D_home f_ED_home away_team f_I50_away f_R50_away f_D_away f_ED_away 0 13764 Carlton 69 21 373 268 Richmond 37 50 316 226 1 13765 Geelong 54 40 428 310 St Kilda 52 45 334 246 2 13766 Collingwood 70 38 398 289 Port Adelaide 50 44 331 232 3 13767 Adelaide 59 38 366 264 Hawthorn 54 38 372 264 4 13768 Brisbane 50 39 343 227 Fremantle 57 30 351 250 <pre><code>single_row_df = single_row_df.assign(f_I50_efficiency_home=lambda df: df.f_R50_away / df.f_I50_home,\n                                    f_I50_efficiency_away=lambda df: df.f_R50_home / df.f_I50_away)\n\nfeature_efficiency_cols = ['f_I50_efficiency_home', 'f_I50_efficiency_away']\n\n# Now let's create an Expontentially Weighted Moving Average for these features - we will need to reshape our DataFrame to do this\nefficiency_features_multi_row = (single_row_df[['game', 'home_team'] + feature_efficiency_cols]\n                                    .rename(columns={\n                                        'home_team': 'team',\n                                        'f_I50_efficiency_home': 'f_I50_efficiency',\n                                        'f_I50_efficiency_away': 'f_I50_efficiency_opponent',\n                                    })\n                                    .append((single_row_df[['game', 'away_team'] + feature_efficiency_cols]\n                                                 .rename(columns={\n                                                     'away_team': 'team',\n                                                     'f_I50_efficiency_home': 'f_I50_efficiency_opponent',\n                                                     'f_I50_efficiency_away': 'f_I50_efficiency',\n                                                 })), sort=True)\n                                    .sort_values(by='game')\n                                    .reset_index(drop=True))\n\nefficiency_features = efficiency_features_multi_row[['game', 'team']].copy()\nfeature_efficiency_cols = ['f_I50_efficiency', 'f_I50_efficiency_opponent']\n\nfor feature in feature_efficiency_cols:\n    efficiency_features[feature] = (efficiency_features_multi_row.groupby('team')[feature]\n                                        .transform(lambda row: row.ewm(span=10).mean().shift(1)))\n\n# Get feature efficiency df back onto single rows\nefficiency_features = pd.merge(efficiency_features, afl_data[['game', 'team', 'home_game']], on=['game', 'team'])\nefficiency_features_single_row = (efficiency_features.query('home_game == 1')\n                                    .rename(columns={\n                                        'team': 'home_team', \n                                        'f_I50_efficiency': 'f_I50_efficiency_home',\n                                        'f_I50_efficiency_opponent': 'f_R50_efficiency_home'})\n                                    .drop(columns='home_game')\n                                    .pipe(pd.merge, (efficiency_features.query('home_game == 0')\n                                                        .rename(columns={\n                                                            'team': 'away_team',\n                                                            'f_I50_efficiency': 'f_I50_efficiency_away',\n                                                            'f_I50_efficiency_opponent': 'f_R50_efficiency_away'})\n                                                        .drop(columns='home_game')), on='game'))\n</code></pre> <pre><code>efficiency_features_single_row.tail(5)\n</code></pre> game home_team f_I50_efficiency_home f_R50_efficiency_home away_team f_I50_efficiency_away f_R50_efficiency_away 1580 15394 Carlton 0.730668 0.675002 Adelaide 0.691614 0.677128 1581 15395 Sydney 0.699994 0.778280 Hawthorn 0.699158 0.673409 1582 15396 Brisbane 0.683604 0.691730 West Coast 0.696822 0.709605 1583 15397 Melbourne 0.667240 0.692632 GWS 0.684525 0.753783 1584 15398 St Kilda 0.730843 0.635819 North Melbourne 0.697018 0.654991 <p>We will merge these features back to our features df later, when the features data frame is on a single row as well.</p>"},{"location":"modelling/AFLmodellingPython/#creating-an-elo-feature","title":"Creating an Elo Feature","text":"<p>Another feature which we could create is an Elo feature. If you don't know what Elo is, go ahead and read an article on it here. We have also written a guide on using elo to model the 2021 Euro &amp; Copa America tournaments here.</p> <p>Essentially, Elo ratings increase if you win. The amount the rating increases is based on how strong the opponent is relative to the team who won. Weak teams get more points for beating stronger teams than they do for beating weaker teams, and vice versa for losses (teams lose points for losses).</p> <p>Mathematically, Elo ratings can also assign a probability for winning or losing based on the two Elo Ratings of the teams playing.</p> <p>So let's get into it. We will first define a function which calculates the elo for each team and applies these elos to our DataFrame.</p> <pre><code># Define a function which finds the elo for each team in each game and returns a dictionary with the game ID as a key and the\n# elos as the key's value, in a list. It also outputs the probabilities and a dictionary of the final elos for each team\ndef elo_applier(df, k_factor):\n    # Initialise a dictionary with default elos for each team\n    elo_dict = {team: 1500 for team in df['team'].unique()}\n    elos, elo_probs = {}, {}\n\n    # Get a home and away dataframe so that we can get the teams on the same row\n    home_df = df.loc[df.home_game == 1, ['team', 'game', 'f_margin', 'home_game']].rename(columns={'team': 'home_team'})\n    away_df = df.loc[df.home_game == 0, ['team', 'game']].rename(columns={'team': 'away_team'})\n\n    df = (pd.merge(home_df, away_df, on='game')\n            .sort_values(by='game')\n            .drop_duplicates(subset='game', keep='first')\n            .reset_index(drop=True))\n\n    # Loop over the rows in the DataFrame\n    for index, row in df.iterrows():\n        # Get the Game ID\n        game_id = row['game']\n\n        # Get the margin\n        margin = row['f_margin']\n\n        # If the game already has the elos for the home and away team in the elos dictionary, go to the next game\n        if game_id in elos.keys():\n            continue\n\n        # Get the team and opposition\n        home_team = row['home_team']\n        away_team = row['away_team']\n\n        # Get the team and opposition elo score\n        home_team_elo = elo_dict[home_team]\n        away_team_elo = elo_dict[away_team]\n\n        # Calculated the probability of winning for the team and opposition\n        prob_win_home = 1 / (1 + 10**((away_team_elo - home_team_elo) / 400))\n        prob_win_away = 1 - prob_win_home\n\n        # Add the elos and probabilities our elos dictionary and elo_probs dictionary based on the Game ID\n        elos[game_id] = [home_team_elo, away_team_elo]\n        elo_probs[game_id] = [prob_win_home, prob_win_away]\n\n        # Calculate the new elos of each team\n        if margin &gt; 0: # Home team wins; update both teams' elo\n            new_home_team_elo = home_team_elo + k_factor*(1 - prob_win_home)\n            new_away_team_elo = away_team_elo + k_factor*(0 - prob_win_away)\n        elif margin &lt; 0: # Away team wins; update both teams' elo\n            new_home_team_elo = home_team_elo + k_factor*(0 - prob_win_home)\n            new_away_team_elo = away_team_elo + k_factor*(1 - prob_win_away)\n        elif margin == 0: # Drawn game' update both teams' elo\n            new_home_team_elo = home_team_elo + k_factor*(0.5 - prob_win_home)\n            new_away_team_elo = away_team_elo + k_factor*(0.5 - prob_win_away)\n\n        # Update elos in elo dictionary\n        elo_dict[home_team] = new_home_team_elo\n        elo_dict[away_team] = new_away_team_elo\n\n    return elos, elo_probs, elo_dict\n</code></pre> <pre><code># Use the elo applier function to get the elos and elo probabilities for each game - we will map these later\nelos, probs, elo_dict = elo_applier(afl_data, 30)\n</code></pre> <p>Great! now we have both rolling averages for stats as a feature, and the elo of the teams! Let's have a quick look at the current elo standings with a k-factor of 30, out of curiosity.</p> <pre><code>for team in sorted(elo_dict, key=elo_dict.get)[::-1]:\n    print(team, elo_dict[team])\n\n    Richmond 1695.2241513840117\n    Sydney 1645.548990879842\n    Hawthorn 1632.5266709780622\n    West Coast 1625.871701773721\n    Geelong 1625.423154644809\n    GWS 1597.4158602131877\n    Adelaide 1591.1704934545442\n    Collingwood 1560.370309216614\n    Melbourne 1558.5666572771509\n    Essendon 1529.0198398117086\n    Port Adelaide 1524.8882517820093\n    North Melbourne 1465.5637511922569\n    Western Bulldogs 1452.2110697845148\n    Fremantle 1393.142087030804\n    St Kilda 1360.9120149937303\n    Brisbane 1276.2923772139352\n    Gold Coast 1239.174528704772\n    Carlton 1226.6780896643265\n</code></pre> <p>This looks extremely similar to the currently AFL ladder, so this is a good sign for elo being an effective predictor of winning.</p>"},{"location":"modelling/AFLmodellingPython/#merging-our-features-into-one-features-dataframe","title":"Merging Our Features Into One Features DataFrame","text":"<p>Now we need to reshape our features df so that we have all of the statistics for both teams in a game on a single row. We can then merge our elo and efficiency features to this df.</p> <pre><code># Look at our current features df\nfeatures.tail(3)\n</code></pre> date game team opponent venue home_game f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP f_form_margin_btwn_teams f_form_past_5_btwn_teams 3156 2018-08-26 15397 Melbourne GWS M.C.G. 1 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.78780 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 -23.2 2.0 3157 2018-08-26 15398 North Melbourne St Kilda Docklands 0 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.54113 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3.2 3.0 3158 2018-08-26 15398 St Kilda North Melbourne Docklands 1 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.49876 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 -3.2 2.0 <pre><code>one_line_cols = ['game', 'team', 'home_game'] + [col for col in features if col.startswith('f_')]\n\n# Get all features onto individual rows for each match\nfeatures_one_line = (features.loc[features.home_game == 1, one_line_cols]\n                     .rename(columns={'team': 'home_team'})\n                     .drop(columns='home_game')\n                     .pipe(pd.merge, (features.loc[features.home_game == 0, one_line_cols]\n                                              .drop(columns='home_game')\n                                              .rename(columns={'team': 'away_team'})\n                                              .rename(columns={col: col+'_away' for col in features.columns if col.startswith('f_')})), on='game')\n                    .drop(columns=['f_form_margin_btwn_teams_away', 'f_form_past_5_btwn_teams_away']))\n\n# Add our created features - elo, efficiency etc.\nfeatures_one_line = (features_one_line.assign(f_elo_home=lambda df: df.game.map(elos).apply(lambda x: x[0]),\n                                            f_elo_away=lambda df: df.game.map(elos).apply(lambda x: x[1]))\n                                      .pipe(pd.merge, efficiency_features_single_row, on=['game', 'home_team', 'away_team'])\n                                      .pipe(pd.merge, afl_data.loc[afl_data.home_game == 1, ['game', 'date', 'round', 'venue']], on=['game'])\n                                      .dropna()\n                                      .reset_index(drop=True)\n                                      .assign(season=lambda df: df.date.apply(lambda row: row.year)))\n\nordered_cols = [col for col in features_one_line if col[:2] != 'f_'] + [col for col in features_one_line if col.startswith('f_')]\n\nfeature_df = features_one_line[ordered_cols]\n</code></pre> <p>Finally, let's reduce the dimensionality of the features df by subtracting the home features from the away features. This will reduce the huge amount of columns we have and make our data more manageable. To do this, we will need a list of columns which we are subtracting from each other. We will then loop over each of these columns to create our new differential columns. </p> <p>We will then add in the implied probability from the odds of the home and away team, as our current odds feature is simply an exponential moving average over the past n games.</p> <pre><code># Create differential df - this df is the home features - the away features\ndiff_cols = [col for col in feature_df.columns if col + '_away' in feature_df.columns and col != 'f_odds' and col.startswith('f_')]\nnon_diff_cols = [col for col in feature_df.columns if col not in diff_cols and col[:-5] not in diff_cols]\n\ndiff_df = feature_df[non_diff_cols].copy()\n\nfor col in diff_cols:\n    diff_df[col+'_diff'] = feature_df[col] - feature_df[col+'_away']\n\n# Add current odds in to diff_df\nodds = get_cleaned_odds()\nhome_odds = (odds[odds.home_game == 1]\n             .assign(f_current_odds_prob=lambda df: 1 / df.odds)\n             .rename(columns={'team': 'home_team'})\n             .drop(columns=['home_game', 'odds']))\n\naway_odds = (odds[odds.home_game == 0]\n             .assign(f_current_odds_prob_away=lambda df: 1 / df.odds)\n             .rename(columns={'team': 'away_team'})\n             .drop(columns=['home_game', 'odds']))\n\ndiff_df = (diff_df.pipe(pd.merge, home_odds, on=['date', 'home_team'])\n              .pipe(pd.merge, away_odds, on=['date', 'away_team']))\n</code></pre> <pre><code>diff_df.tail()\n</code></pre> game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1626 15394 Carlton Adelaide 2018-08-25 23 Docklands 2018 6.467328 -26.2 1.0 2.066016 1230.072138 1587.776445 0.730668 0.675002 0.691614 0.677128 -3.498547 -5.527193 -26.518474 -34.473769 1.289715 0.217006 7.955295 -341.342677 -9.317269 3.088569 -2.600593 15.192839 -12.518345 -4.136673 -41.855717 -72.258378 -51.998775 9.499447 8.670917 -6.973088 -4.740623 -26.964945 -13.147675 -23.928700 -28.940883 -45.293433 -15.183406 -1900.784014 -0.362402 -1.314627 4.116133 -294.813511 -9.917793 -34.724925 -5.462844 -9.367141 -19.623785 -38.188082 0.187709 0.816860 1627 15395 Sydney Hawthorn 2018-08-25 23 S.C.G. 2018 2.128611 1.0 2.0 1.777290 1662.568452 1615.507209 0.699994 0.778280 0.699158 0.673409 -1.756730 -0.874690 -11.415069 -15.575319 0.014390 4.073909 4.160250 -174.005092 -0.942357 -4.078635 -4.192916 7.814496 -2.225780 6.215760 15.042979 -34.894261 -50.615255 4.214158 0.683548 -3.535594 -3.168608 -12.068691 -30.493980 -9.867332 2.588103 -22.825570 -5.604199 253.086090 -2.697132 -22.612327 25.340623 -90.812188 1.967104 -31.047879 0.007606 -6.880120 11.415593 -49.957313 0.440180 0.561924 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566"},{"location":"modelling/AFLmodellingPython/#wrapping-it-up","title":"Wrapping it Up","text":"<p>We now have a fairly decent amount of features. Some other features which could be added include whether the game is in a major Capital city outisde of Mebourne (i.e. Sydney, Adelaide or Peth), how many 'Elite' players are playing (which could be judged by average SuperCoach scores over 110, for example), as well as your own metrics for attacking and defending.</p> <p>Note that all of our features have columns starting with 'f_' so in the section, we will grab this feature dataframe and use these features to sport predicting the matches.</p>"},{"location":"modelling/AFLmodellingPython/#03-modelling","title":"03. Modelling","text":"<p>These tutorials will walk you through how to construct your own basic AFL model, using publically available data. The output will be odds for each team to win, which will be shown on The Hub.</p> <p>In this notebook we will walk you through modelling our AFL data to create predictions. We will train a variety of quick and easy models to get a feel of what works and what doesn't. We will then tune our hyperparameters so that we are ready to make week by week predictions.</p>"},{"location":"modelling/AFLmodellingPython/#grabbing-our-dataset_1","title":"Grabbing Our Dataset","text":"<p>First, we will import our required modules, as well as the prepare_afl_features function which we created in our afl_feature_creation script. This essentially creates some basic features for us so that we can get started on the modelling component.</p> <pre><code># Import libraries\nfrom afl_data_cleaning_v2 import *\nimport datetime\nimport pandas as pd\nimport numpy as np\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n# from xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.feature_selection import RFECV\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn import feature_selection\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nimport warnings\nwarnings.filterwarnings('ignore')\nimport afl_feature_creation_v2\nimport afl_data_cleaning_v2\n</code></pre> <pre><code># Grab our feature DataFrame which we created in the previous tutorial\nfeature_df = afl_feature_creation_v2.prepare_afl_features()\nafl_data = afl_data_cleaning_v2.prepare_afl_data()\n</code></pre> <pre><code>feature_df.tail(3)\n</code></pre> game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566 <pre><code># Get the result and merge to the feature_df\n\nmatch_results = (pd.read_csv(\"data/afl_match_results.csv\")\n                    .rename(columns={'Game': 'game'})\n                    .assign(result=lambda df: df.apply(lambda row: 1 if row['Home.Points'] &gt; row['Away.Points'] else 0, axis=1)))\n\n# Merge result column to feature_df\nfeature_df = pd.merge(feature_df, match_results[['game', 'result']], on='game')\n</code></pre>"},{"location":"modelling/AFLmodellingPython/#creating-a-training-and-testing-set","title":"Creating a Training and Testing Set","text":"<p>So that we don't train our data on the data that we will later test our model on, we will create separate train and test sets. For this exercise we will use the 2018 season to test how our model performs, whilst the rest of the data can be used to train the model.</p> <pre><code># Create our test and train sets from our afl DataFrame; drop the columns which leak the result, duplicates, and the advanced\n# stats which don't have data until 2015\n\nfeature_columns = [col for col in feature_df if col.startswith('f_')]\n\n# Create our test set\ntest_x = feature_df.loc[feature_df.season == 2018, ['game'] + feature_columns]\ntest_y = feature_df.loc[feature_df.season == 2018, 'result']\n\n# Create our train set\nX = feature_df.loc[feature_df.season != 2018, ['game'] + feature_columns]\ny = feature_df.loc[feature_df.season != 2018, 'result']\n\n# Scale features\nscaler = StandardScaler()\nX[feature_columns] = scaler.fit_transform(X[feature_columns])\ntest_x[feature_columns] = scaler.transform(test_x[feature_columns])\n</code></pre>"},{"location":"modelling/AFLmodellingPython/#using-cross-validation-to-find-the-best-algorithms","title":"Using Cross Validation to Find The Best Algorithms","text":"<p>Now that we have our training set, we can run through a list of popular classifiers to determine which classifier is best for modelling our data. To do this we will create a function which uses Kfold cross-validation to find the 'best' algorithms, based on how accurate the algorithms' predictions are.</p> <p>This function will take in a list of classifiers, which we will define below, as well as the training set and it's outcome, and output a DataFrame with the mean and std of the accuracy of each algorithm. Let's jump into it!</p> <pre><code># Create a list of standard classifiers\nclassifiers = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n\n    #GLM\n    linear_model.LogisticRegressionCV(),\n\n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n\n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n\n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n#     XGBClassifier()    \n]\n\n# Define a functiom which finds the best algorithms for our modelling task\ndef find_best_algorithms(classifier_list, X, y):\n    # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling\n    # Cross validate model with Kfold stratified cross validation\n    kfold = StratifiedKFold(n_splits=5)\n\n    # Grab the cross validation scores for each algorithm\n    cv_results = [cross_val_score(classifier, X, y, scoring = \"neg_log_loss\", cv = kfold) for classifier in classifier_list]\n    cv_means = [cv_result.mean() * -1 for cv_result in cv_results]\n    cv_std = [cv_result.std() for cv_result in cv_results]\n    algorithm_names = [alg.__class__.__name__ for alg in classifiers]\n\n    # Create a DataFrame of all the CV results\n    cv_results = pd.DataFrame({\n        \"Mean Log Loss\": cv_means,\n        \"Log Loss Std\": cv_std,\n        \"Algorithm\": algorithm_names\n    })\n\n    return cv_results.sort_values(by='Mean Log Loss').reset_index(drop=True)\n</code></pre> <pre><code>best_algos = find_best_algorithms(classifiers, X, y)\nbest_algos\n</code></pre> Mean Log Loss Log Loss Std Algorithm 0 0.539131 3.640578e-02 LogisticRegressionCV 1 0.551241 5.775685e-02 LinearDiscriminantAnalysis 2 0.630994 8.257481e-02 GradientBoostingClassifier 3 0.670041 9.205780e-03 AdaBoostClassifier 4 0.693147 2.360121e-08 GaussianProcessClassifier 5 0.712537 2.770864e-02 SVC 6 0.712896 2.440755e-02 NuSVC 7 0.836191 2.094224e-01 ExtraTreesClassifier 8 0.874307 1.558144e-01 RandomForestClassifier 9 1.288174 3.953037e-01 BaggingClassifier 10 1.884019 4.769589e-01 QuadraticDiscriminantAnalysis 11 2.652161 6.886897e-01 BernoulliNB 12 3.299651 6.427551e-01 GaussianNB <pre><code># Try a logistic regression model and see how it performs in terms of accuracy\nkfold = StratifiedKFold(n_splits=5)\ncv_scores = cross_val_score(linear_model.LogisticRegressionCV(), X, y, scoring='accuracy', cv=kfold)\ncv_scores.mean()\n    0.7452268937025035\n</code></pre>"},{"location":"modelling/AFLmodellingPython/#choosing-our-algorithms","title":"Choosing Our Algorithms","text":"<p>As we can see from above, there are some pretty poor algorithms for predicting the winner. On the other hand, whilst attaining an accuracy of 74.5% (at the time of writing) may seem like a decent result; we must first establish a baseline to judge our performance on. In this case, we will have two baselines; the proportion of games won by the home team and what the odds predict. If we can beat the odds we have created a very powerful model.</p> <p>Note that a baseline for the log loss can also be both the odds log loss and randomly guessing. Randomly guessing between two teams attains a log loss of log(2) = 0.69, so we have beaten this result.</p> <p>Once we establish our baseline, we will choose the top algorithms from above and tune their hyperparameters, as well as automatically selecting the best features to be used in our model.</p>"},{"location":"modelling/AFLmodellingPython/#defining-our-baseline","title":"Defining Our Baseline","text":"<p>As stated above, we must define our baseline so that we have a measure to beat. We will use the proportion of games won by the home team, as well as the proportion of favourites who won, based off the odds. To establish this baseline we will use our feature_df, as this has no dropped rows.</p> <pre><code># Find the percentage chance of winning at home in each season.\nafl_data = afl_data_cleaning_v2.prepare_afl_data()\nafl_data['home_win'] = afl_data.apply(lambda x: 1 if x['f_margin'] &gt; 0 else 0, axis=1)\nhome_games = afl_data[afl_data['home_game'] == 1]\nhome_games[[\"home_win\", 'season']].groupby(['season']).mean()\n</code></pre> season home_win 2011 0.561856 2012 0.563725 2013 0.561576 2014 0.574257 2015 0.539604 2016 0.606742 2017 0.604061 2018 0.540404 <pre><code># Find the proportion of favourites who have won\n\n# Define a function which finds if the odds correctly guessed the response\ndef find_odds_prediction(a_row):\n    if a_row['f_odds'] &lt;= a_row['f_odds_away'] and a_row['home_win'] == 1:\n        return 1\n    elif a_row['f_odds_away'] &lt; a_row['f_odds'] and a_row['home_win'] == 0:\n        return 1\n    else:\n        return 0\n\n# Define a function which splits our DataFrame so each game is on one row instead of two\ndef get_df_on_one_line(df):\n    cols_to_drop = ['date', 'home_game', 'opponent', \n       'f_opponent_behinds', 'f_opponent_goals', 'f_opponent_points', 'f_points',\n       'round', 'venue', 'season']\n\n    home_df = df[df['home_game'] == 1].rename(columns={'team': 'home_team'})\n    away_df = df[df['home_game'] == 0].rename(columns={'team': 'away_team'})\n    away_df = away_df.drop(columns=cols_to_drop)\n\n    # Rename away_df columns\n    away_df_renamed = away_df.rename(columns={col: col + '_away' for col in away_df.columns if col != 'game'})\n    merged_df = pd.merge(home_df, away_df_renamed, on='game')\n\n    merged_df['home_win'] = merged_df.f_margin.apply(lambda x: 1 if x &gt; 0 else 0)\n    return merged_df\n\nafl_data_one_line = get_df_on_one_line(afl_data)\nafl_data_one_line['odds_prediction'] = afl_data_one_line.apply(find_odds_prediction, axis=1)\nprint('The overall mean accuracy of choosing the favourite based on the odds is {}%'.format(\n    round(afl_data_one_line['odds_prediction'].mean() * 100, 2)))\nafl_data_one_line[[\"odds_prediction\", 'season']].groupby(['season']).mean()\n</code></pre> <pre><code>The overall mean accuracy of choosing the favourite based on the odds is 73.15%\n</code></pre> season odds_prediction 2011 0.784615 2012 0.774510 2013 0.748768 2014 0.727723 2015 0.727723 2016 0.713483 2017 0.659898 2018 0.712121 <pre><code>## Get a baseline log loss score from the odds\nafl_data_one_line['odds_home_prob'] = 1 / afl_data_one_line.f_odds\nafl_data_one_line['odds_away_prob'] = 1 / afl_data_one_line.f_odds_away\n</code></pre> <pre><code>metrics.log_loss(afl_data_one_line.home_win, afl_data_one_line[['odds_away_prob', 'odds_home_prob']])\n    0.5375306549682837\n</code></pre> <p>We can see that the odds are MUCH more accurate than just choosing the home team to win. We can also see that the mean accuracy of choosing the favourite is around 73%. That means that this is the score we need to beat. Similarly, the log loss of the odds is around 0.5385, whilst our model scores around 0.539 (at the time of writing), without hyperparamter optimisation. Let's choose only the algorithms with log losses below 0.67</p> <pre><code>chosen_algorithms = best_algos.loc[best_algos['Mean Log Loss'] &lt; 0.67, 'Algorithm'].tolist()\nchosen_algorithms\n    ['LogisticRegressionCV',\n     'LinearDiscriminantAnalysis',\n     'GradientBoostingClassifier']\n</code></pre>"},{"location":"modelling/AFLmodellingPython/#using-grid-search-to-tune-hyperparameters","title":"Using Grid Search To Tune Hyperparameters","text":"<p>Now that we have our best models, we can use Grid Search to optimise our hyperparameters. Grid search basically involves searching through a range of different algorithm hyperparameters, and choosing those which result in the best score from some metrics, which in our case is accuracy. Let's do this for the algorithms which have hyperparameters which can be tuned. Note that if you are running this on your own computer it may take up to 10 minutes.</p> <pre><code># Define a function which optimises the hyperparameters of our chosen algorithms\ndef optimise_hyperparameters(train_x, train_y, algorithms, parameters):\n    kfold = StratifiedKFold(n_splits=5)\n    best_estimators = []\n\n    for alg, params in zip(algorithms, parameters):\n        gs = GridSearchCV(alg, param_grid=params, cv=kfold, scoring='neg_log_loss', verbose=1)\n        gs.fit(train_x, train_y)\n        best_estimators.append(gs.best_estimator_)\n    return best_estimators\n\n# Define our parameters to run a grid search over\nlr_grid = {\n    \"C\": [0.0001, 0.001, 0.01, 0.05, 0.2, 0.5],\n    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]\n}\n\n# Add our algorithms and parameters to lists to be used in our function\nalg_list = [LogisticRegression()]\nparam_list = [lr_grid]\n</code></pre> <p><pre><code># Find the best estimators, then add our other estimators which don't need optimisation\nbest_estimators = optimise_hyperparameters(X, y, alg_list, param_list)\n</code></pre>     Fitting 5 folds for each of 18 candidates, totalling 90 fits</p> <pre><code>[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:    5.2s finished\n</code></pre> <pre><code>lr_best_params = best_estimators[0].get_params()\nlr_best_params\n    {'C': 0.01,\n     'class_weight': None,\n     'dual': False,\n     'fit_intercept': True,\n     'intercept_scaling': 1,\n     'max_iter': 100,\n     'multi_class': 'ovr',\n     'n_jobs': 1,\n     'penalty': 'l2',\n     'random_state': None,\n     'solver': 'newton-cg',\n     'tol': 0.0001,\n     'verbose': 0,\n     'warm_start': False}\n</code></pre> <pre><code>kfold = StratifiedKFold(n_splits=10)\ncv_scores = cross_val_score(linear_model.LogisticRegression(**lr_best_params), X, y, scoring='neg_log_loss', cv=kfold)\ncv_scores.mean()\n    -0.528741673153639\n</code></pre> <p>In the next iteration of this tutorial we will also optimise an XGB model and hopefully outperform our logistic regression model.</p>"},{"location":"modelling/AFLmodellingPython/#creating-predictions-for-the-2018-season","title":"Creating Predictions for the 2018 Season","text":"<p>Now that we have an optimised logistic regression model, let's see how it performs on predicting the 2018 season.</p> <pre><code>lr = LogisticRegression(**lr_best_params)\nlr.fit(X, y)\nfinal_predictions = lr.predict(test_x)\n\naccuracy = (final_predictions == test_y).mean() * 100\n\nprint(\"Our accuracy in predicting the 2018 season is: {:.2f}%\".format(accuracy))\n</code></pre> <pre><code>Our accuracy in predicting the 2018 season is: 67.68%\n</code></pre> <p>Now let's have a look at all the games which we incorrectly predicted.</p> <pre><code>game_ids = test_x[(final_predictions != test_y)].game\nafl_data_one_line.loc[afl_data_one_line.game.isin(game_ids), ['date', 'home_team', 'opponent', 'f_odds', 'f_odds_away', 'f_margin']]\n</code></pre> date home_team opponent f_odds f_odds_away f_margin 1386 2018-03-24 Gold Coast North Melbourne 2.0161 1.9784 16 1388 2018-03-25 Melbourne Geelong 1.7737 2.2755 -3 1391 2018-03-30 North Melbourne St Kilda 3.5769 1.3867 52 1392 2018-03-31 Carlton Gold Coast 1.5992 2.6620 -34 1396 2018-04-01 Western Bulldogs West Coast 1.8044 2.2445 -51 1397 2018-04-01 Sydney Port Adelaide 1.4949 3.0060 -23 1398 2018-04-02 Geelong Hawthorn 1.7597 2.3024 -1 1406 2018-04-08 Western Bulldogs Essendon 3.8560 1.3538 21 1408 2018-04-13 Adelaide Collingwood 1.2048 5.9197 -48 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1415 2018-04-15 Hawthorn Melbourne 2.2855 1.7772 67 1417 2018-04-20 Sydney Adelaide 1.2640 4.6929 -10 1420 2018-04-21 Port Adelaide Geelong 1.5053 2.9515 -34 1422 2018-04-22 North Melbourne Hawthorn 2.6170 1.6132 28 1423 2018-04-22 Brisbane Gold Coast 1.7464 2.3277 -5 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1427 2018-04-28 Geelong Sydney 1.5019 2.9833 -17 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 1439 2018-05-05 Sydney North Melbourne 1.2777 4.5690 -2 1444 2018-05-11 Hawthorn Sydney 1.6283 2.5818 -8 1445 2018-05-12 GWS West Coast 1.5425 2.8292 -25 1446 2018-05-12 Carlton Essendon 3.1742 1.4570 13 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1456 2018-05-19 Essendon Geelong 5.6530 1.2104 34 1460 2018-05-20 Brisbane Hawthorn 3.2891 1.4318 56 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1466 2018-05-26 GWS Essendon 1.4364 3.2652 -35 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 ... ... ... ... ... ... ... 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 1485 2018-06-11 Melbourne Collingwood 1.6034 2.6450 -42 1492 2018-06-21 West Coast Essendon 1.3694 3.6843 -28 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1499 2018-06-29 Western Bulldogs Geelong 6.2067 1.1889 2 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1504 2018-07-01 Melbourne St Kilda 1.1405 7.7934 -2 1505 2018-07-01 Essendon North Melbourne 2.0993 1.9022 17 1506 2018-07-01 Fremantle Brisbane 1.2914 4.3743 -55 1507 2018-07-05 Sydney Geelong 1.7807 2.2675 -12 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1516 2018-07-12 Adelaide Geelong 2.0517 1.9444 15 1518 2018-07-14 Hawthorn Brisbane 1.2281 5.4105 -33 1521 2018-07-14 GWS Richmond 2.7257 1.5765 2 1522 2018-07-15 Collingwood West Coast 1.5600 2.7815 -35 1523 2018-07-15 North Melbourne Sydney 1.9263 2.0647 -6 1524 2018-07-15 Fremantle Port Adelaide 5.9110 1.2047 9 1527 2018-07-21 Sydney Gold Coast 1.0342 27.8520 -24 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 1533 2018-07-22 Port Adelaide GWS 1.6480 2.5452 -22 1538 2018-07-28 Gold Coast Carlton 1.3933 3.5296 -35 1546 2018-08-04 Adelaide Port Adelaide 2.0950 1.9135 3 1548 2018-08-04 St Kilda Western Bulldogs 1.6120 2.6368 -35 1555 2018-08-11 Port Adelaide West Coast 1.4187 3.3505 -4 1558 2018-08-12 North Melbourne Western Bulldogs 1.3175 4.1239 -7 1559 2018-08-12 Melbourne Sydney 1.3627 3.7445 -9 1564 2018-08-18 GWS Sydney 1.8478 2.1672 -20 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 <p>Very interesting! Most of the games we got wrong were upsets. Let's have a look at the games we incorrectly predicted that weren't upsets.</p> <pre><code>(afl_data_one_line.loc[afl_data_one_line.game.isin(game_ids), ['date', 'home_team', 'opponent', 'f_odds', 'f_odds_away', 'f_margin']]\n    .assign(home_favourite=lambda df: df.apply(lambda row: 1 if row.f_odds &lt; row.f_odds_away else 0, axis=1))\n    .assign(upset=lambda df: df.apply(lambda row: 1 if row.home_favourite == 1 and row.f_margin &lt; 0 else \n                                      (1 if row.home_favourite == 0 and row.f_margin &gt; 0 else 0), axis=1))\n    .query('upset == 0'))\n</code></pre> date home_team opponent f_odds f_odds_away f_margin home_favourite upset 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1 0 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1 0 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 0 0 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 0 0 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 0 0 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1 0 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1 0 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 0 0 1479 2018-06-08 Port Adelaide Richmond 1.7422 2.3420 14 1 0 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 0 0 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1 0 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1 0 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 0 0 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1 0 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 0 0 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 0 0 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 0 0 <p>Let's now look at our model's log loss for the 2018 season compared to the odds.</p> <pre><code>predictions_probs = lr.predict_proba(test_x)\n</code></pre> <pre><code>metrics.log_loss(test_y, predictions_probs)\n    0.584824211055384\n</code></pre> <pre><code>test_x_unscaled = feature_df.loc[feature_df.season == 2018, ['game'] + feature_columns]\n\nmetrics.log_loss(test_y, test_x_unscaled[['f_current_odds_prob_away', 'f_current_odds_prob']])\n    0.5545776633924343\n</code></pre> <p>So whilst our model performs decently, it doesn't beat the odds in terms of log loss. That's okay, it's still a decent start. In future iterations we can implement other algorithms and create new features which may improve performance.</p>"},{"location":"modelling/AFLmodellingPython/#next-steps","title":"Next Steps","text":"<p>Now that we have a model up and running, the next steps are to implement the model on a week to week basis.</p>"},{"location":"modelling/AFLmodellingPython/#04-weekly-predictions","title":"04. Weekly Predictions","text":"<p>Now that we have explored different algorithms for modelling, we can implement our chosen model and predict this week's AFL games! All you need to do is run the afl_modelling script each Thursday or Friday to predict the following week's games.</p> <pre><code># Import Modules\nfrom afl_feature_creation_v2 import prepare_afl_features\nimport afl_data_cleaning_v2\nimport afl_feature_creation_v2\nimport afl_modelling_v2\nimport datetime\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', None)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre>"},{"location":"modelling/AFLmodellingPython/#creating-the-features-for-this-weekends-games","title":"Creating The Features For This Weekend's Games","text":"<p>To actually predict this weekend's games, we need to create the same features that we have created in the previous tutorials for the games that will be played this weekend. This includes all the rolling averages, efficiency features, elo features etc. So the majority of this tutorial will be using previously defined functions to create features for the following weekend's games.</p>"},{"location":"modelling/AFLmodellingPython/#create-next-weeks-dataframe","title":"Create Next Week's DataFrame","text":"<p>Let's first get our cleaned afl_data dataset, as well as the odds for next weekend and the 2018 fixture.</p> <pre><code># Grab the cleaned AFL dataset and the column order\nafl_data = afl_data_cleaning_v2.prepare_afl_data()\nordered_cols = afl_data.columns\n\n# Define a function which grabs the odds for each game for the following weekend\ndef get_next_week_odds(path):\n    # Get next week's odds\n    next_week_odds = pd.read_csv(path)\n    next_week_odds = next_week_odds.rename(columns={\"team_1\": \"home_team\", \n                                                \"team_2\": \"away_team\", \n                                                \"team_1_odds\": \"odds\", \n                                                \"team_2_odds\": \"odds_away\"\n                                               })\n    return next_week_odds\n\n# Import the fixture\n# Define a function which gets the fixture and cleans it up\ndef get_fixture(path):\n    # Get the afl fixture\n    fixture = pd.read_csv(path)\n\n    # Replace team names and reformat\n    fixture = fixture.replace({'Brisbane Lions': 'Brisbane', 'Footscray': 'Western Bulldogs'})\n    fixture['Date'] = pd.to_datetime(fixture['Date']).dt.date.astype(str)\n    fixture = fixture.rename(columns={\"Home.Team\": \"home_team\", \"Away.Team\": \"away_team\"})\n    return fixture\n\nnext_week_odds = get_next_week_odds(\"data/weekly_odds.csv\")\nfixture = get_fixture(\"data/afl_fixture_2018.csv\")\n</code></pre> <pre><code>fixture.tail()\n</code></pre> Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG <pre><code>next_week_odds\n</code></pre> home_team away_team odds odds_away 0 West Coast Collingwood 2.34 1.75 <p>Now that we have these DataFrames, we will define a function which combines the fixture and next week's odds to create a single DataFrame for the games over the next 7 days. To use this function we will need Game IDs for next week. So we will create another function which creates Game IDs by using the Game ID from the last game played and adding 1 to it.</p> <pre><code># Define a function which creates game IDs for this week's footy games\ndef create_next_weeks_game_ids(afl_data):\n    odds = get_next_week_odds(\"data/weekly_odds.csv\")\n\n    # Get last week's Game ID\n    last_afl_data_game = afl_data['game'].iloc[-1]\n\n    # Create Game IDs for next week\n    game_ids = [(i+1) + last_afl_data_game for i in range(odds.shape[0])]\n    return game_ids\n\n# Define a function which creates this week's footy game DataFrame\ndef get_next_week_df(afl_data):\n    # Get the fixture and the odds for next week's footy games\n    fixture = get_fixture(\"data/afl_fixture_2018.csv\")\n    next_week_odds = get_next_week_odds(\"data/weekly_odds.csv\")\n    next_week_odds['game'] = create_next_weeks_game_ids(afl_data)\n\n    # Get today's date and next week's date and create a DataFrame for next week's games\n#     todays_date = datetime.datetime.today().strftime('%Y-%m-%d')\n\n#     date_in_7_days = (datetime.datetime.today() + datetime.timedelta(days=7)).strftime('%Y-%m-%d')\n    todays_date = '2018-09-27'\n    date_in_7_days = '2018-10-04'\n    fixture = fixture[(fixture['Date'] &gt;= todays_date) &amp; (fixture['Date'] &lt; date_in_7_days)].drop(columns=['Season.Game'])\n    next_week_df = pd.merge(fixture, next_week_odds, on=['home_team', 'away_team'])\n\n    # Split the DataFrame onto two rows for each game\n    h_df = (next_week_df[['Date', 'game', 'home_team', 'away_team', 'odds', 'Season', 'Round', 'Venue']]\n               .rename(columns={'home_team': 'team', 'away_team': 'opponent'})\n               .assign(home_game=1))\n\n    a_df = (next_week_df[['Date', 'game', 'home_team', 'away_team', 'odds_away', 'Season', 'Round', 'Venue']]\n                .rename(columns={'odds_away': 'odds', 'home_team': 'opponent', 'away_team': 'team'})\n                .assign(home_game=0))\n\n    next_week = a_df.append(h_df).sort_values(by='game').rename(columns={\n        'Date': 'date',\n        'Season': 'season',\n        'Round': 'round',\n        'Venue': 'venue'\n    })\n    next_week['date'] = pd.to_datetime(next_week.date)\n    next_week['round'] = afl_data['round'].iloc[-1] + 1\n    return next_week\n</code></pre> <pre><code>next_week_df = get_next_week_df(afl_data)\ngame_ids_next_round = create_next_weeks_game_ids(afl_data)\nnext_week_df\n</code></pre> date round season venue game home_game odds opponent team 0 2018-09-29 27 2018 MCG 15407 0 1.75 West Coast Collingwood 0 2018-09-29 27 2018 MCG 15407 1 2.34 Collingwood West Coast <pre><code>fixture.tail()\n</code></pre> Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG"},{"location":"modelling/AFLmodellingPython/#create-each-feature","title":"Create Each Feature","text":"<p>Now let's append next week's DataFrame to our afl_data, match_results and odds DataFrames and then create all the features we used in the AFL Feature Creation Tutorial. We need to append the games and then feed them into our function so that we can create features for upcoming games.</p> <pre><code># Append next week's games to our afl_data DataFrame\nafl_data = afl_data.append(next_week_df).reset_index(drop=True)\n\n# Append next week's games to match results (we need to do this for our feature creation to run)\nmatch_results = afl_data_cleaning_v2.get_cleaned_match_results().append(next_week_df)\n\n# Append next week's games to odds\nodds = (afl_data_cleaning_v2.get_cleaned_odds().pipe(lambda df: df.append(next_week_df[df.columns]))\n       .reset_index(drop=True))\n</code></pre> <pre><code>features_df = afl_feature_creation_v2.prepare_afl_features(afl_data=afl_data, match_results=match_results, odds=odds)\n</code></pre> <pre><code>features_df.tail()\n</code></pre> game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_GA1_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_Unnamed: 0_diff f_behinds_diff f_goals_diff f_margin_diff f_opponent_behinds_diff f_opponent_goals_diff f_opponent_points_diff f_points_diff f_current_odds_prob f_current_odds_prob_away 1065 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.966936 -23.2 2.0 1.813998 1523.456734 1609.444874 0.653525 0.680168 0.704767 0.749812 140.535514 0.605144 -9.771981 5.892176 7.172376 6.614609 -1.365211 30.766262 21.998618 0.067228 -1.404730 -3.166732 6.933998 6.675576 0.000000 38.708158 24.587333 12.008987 10.482382 -16.709540 -15.415060 289.188486 6.350287 -2.263536 -20.966818 50.388632 0.723637 15.537783 22.912269 2.065039 10.215523 -6.689429 3259.163465 -0.136383 3.553795 16.563721 -2.353514 1.162696 4.622664 21.186385 0.661551 0.340379 1066 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.089084 -3.2 2.0 2.577161 1397.237139 1499.366007 0.725980 0.655749 0.723949 0.677174 51.799992 3.399035 6.067393 -2.189489 -10.475859 1.154766 -8.883840 -21.810962 33.058382 40.618410 2.286314 -0.345734 -3.778445 -2.182673 0.000000 19.816372 -21.562916 2.678384 -14.777698 13.242010 12.065594 -82.381996 -2.176564 2.335825 -4.952336 45.719406 3.344217 -2.095613 -3.929084 -3.182381 -12.832197 57.226776 -20221.371526 1.968709 -1.897958 -15.177001 1.067099 0.781811 5.757963 -9.419038 0.284269 0.717566 1067 15404 Collingwood GWS 2018-09-15 25 M.C.G. 2018 1.882301 12.6 3.0 2.018344 1546.000498 1590.806454 0.693185 0.706222 0.718446 0.727961 205.916671 -1.642954 -2.980828 -0.266023 8.547225 -3.751909 -0.664977 10.563513 48.175985 43.531908 -5.836979 5.388668 4.395675 2.555152 0.000000 51.588962 11.558254 4.276481 11.284445 -3.412977 -2.206815 -234.577304 2.637758 -10.537765 -11.127876 125.607377 -3.485896 3.532031 15.102292 -2.500685 8.187543 38.053445 12500.525732 -1.006173 2.520135 18.634835 -2.159882 -0.393386 -4.520198 14.114637 0.608495 0.393856 1068 15406 West Coast Melbourne 2018-09-22 26 Perth Stadium 2018 2.013572 21.2 3.0 1.884148 1577.888606 1542.095154 0.688877 0.708941 0.649180 0.698319 -118.135184 -3.005709 2.453190 -5.103869 -14.368949 -12.245458 2.771411 -45.364271 -60.210182 -24.049523 -2.791277 6.115918 -5.041030 -5.335746 0.000000 -78.816902 -18.784547 -13.957754 -5.527613 18.606721 25.366778 -910.988860 -5.515812 -9.483590 8.914093 -131.380758 -7.142529 -49.484957 -13.718798 -4.862994 -9.834616 -23.673638 -3178.282073 -1.785349 -2.569957 -20.008787 0.476202 0.387915 2.803694 -17.205093 0.543774 0.457875 1069 15407 West Coast Collingwood 2018-09-29 27 MCG 2018 1.981832 17.2 3.0 1.838864 1591.348723 1562.924273 0.679011 0.724125 0.711352 0.709346 159.522670 0.893421 -0.475725 3.391070 -5.088751 5.875388 5.352234 7.729063 -7.358202 -4.719968 6.113565 4.822252 2.871241 2.690270 3.636364 -64.238180 -0.631102 2.078832 6.005613 56.879978 34.373271 1016.491933 1.199751 2.454685 12.197047 219.666562 2.484363 0.379162 2.566991 0.639666 2.258377 -23.841529 -368920.360240 -0.646160 0.892051 3.040850 1.589568 0.012622 1.665299 4.706148 0.427350 0.571429"},{"location":"modelling/AFLmodellingPython/#create-predictions-for-the-upcoming-round","title":"Create Predictions For the Upcoming Round","text":"<p>Now that we have our features, we can use our model that we created in part 3 to predict the next round. First we need to filter our features_df into a training df and a df with next round's features/matches. Then we can use the model created in the last tutorial to create predictions. For simplicity, I have hardcoded the parameters we used in the last tutorial.</p> <pre><code># Get the train df by only taking the games IDs which aren't in the next week df\ntrain_df = features_df[~features_df.game.isin(next_week_df.game)]\n\n# Get the result and merge to the feature_df\nmatch_results = (pd.read_csv(\"data/afl_match_results.csv\")\n                    .rename(columns={'Game': 'game'})\n                    .assign(result=lambda df: df.apply(lambda row: 1 if row['Home.Points'] &gt; row['Away.Points'] else 0, axis=1)))\n\ntrain_df = pd.merge(train_df,  match_results[['game', 'result']], on='game')\n\ntrain_x = train_df.drop(columns=['result'])\ntrain_y = train_df.result\n\nnext_round_x = features_df[features_df.game.isin(next_week_df.game)]\n</code></pre> <pre><code># Fit out logistic regression model - note that our predictions come out in the order of [away_team_prob, home_team_prob]\n\nlr_best_params = {'C': 0.01,\n 'class_weight': None,\n 'dual': False,\n 'fit_intercept': True,\n 'intercept_scaling': 1,\n 'max_iter': 100,\n 'multi_class': 'ovr',\n 'n_jobs': 1,\n 'penalty': 'l2',\n 'random_state': None,\n 'solver': 'newton-cg',\n 'tol': 0.0001,\n 'verbose': 0,\n 'warm_start': False}\n\nfeature_cols = [col for col in train_df if col.startswith('f_')]\n\n# Scale features\nscaler = StandardScaler()\ntrain_x[feature_cols] = scaler.fit_transform(train_x[feature_cols])\nnext_round_x[feature_cols] = scaler.transform(next_round_x[feature_cols])\n\nlr = LogisticRegression(**lr_best_params)\nlr.fit(train_x[feature_cols], train_y)\nprediction_probs = lr.predict_proba(next_round_x[feature_cols])\n\nmodelled_home_odds = [1/i[1] for i in prediction_probs]\nmodelled_away_odds = [1/i[0] for i in prediction_probs]\n</code></pre> <pre><code># Create a predictions df\npreds_df = (next_round_x[['date', 'home_team', 'away_team', 'venue', 'game']].copy()\n               .assign(modelled_home_odds=modelled_home_odds,\n                      modelled_away_odds=modelled_away_odds)\n               .pipe(pd.merge, next_week_odds, on=['home_team', 'away_team'])\n               .pipe(pd.merge, features_df[['game', 'f_elo_home', 'f_elo_away']], on='game')\n               .drop(columns='game')\n           )\n</code></pre> <pre><code>preds_df\n</code></pre> date home_team away_team venue modelled_home_odds modelled_away_odds odds odds_away f_elo_home f_elo_away 0 2018-09-29 West Coast Collingwood MCG 2.326826 1.753679 2.34 1.75 1591.348723 1562.924273 <p>Alternatively, if you want to generate predictions using a script which uses all the above code, just run the following:</p> <pre><code>print(afl_modelling_v2.create_predictions())\n</code></pre> <pre><code>        date   home_team    away_team venue  modelled_home_odds  \\\n0 2018-09-29  West Coast  Collingwood   MCG            2.326826\n\n   modelled_away_odds  odds  odds_away   f_elo_home   f_elo_away  \n0            1.753679  2.34       1.75  1591.348723  1562.924273\n</code></pre>"},{"location":"modelling/AFLmodellingPython/#conclusion","title":"Conclusion","text":"<p>Congratulations! You have created AFL predictions for this week. If you are beginner to this, don't be overwhelmed. The process gets easier each time you do it. And it is super rewarding. In future iterations we will update this tutorial to predict actual odds, and then integrate this with Betfair's API so that you can create an automated betting strategy using Machine Learning to create your predictions!</p>"},{"location":"modelling/AFLmodellingPython/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"modelling/AusOpenPythonTutorial/","title":"Australian Open Datathon Python Tutorial","text":""},{"location":"modelling/AusOpenPythonTutorial/#overview","title":"Overview","text":""},{"location":"modelling/AusOpenPythonTutorial/#the-task","title":"The Task","text":"<p>This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide.</p> <p>The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here.</p> <p>How an outline of our methodology and thought process, read this article.</p>"},{"location":"modelling/AusOpenPythonTutorial/#intention","title":"Intention","text":"<p>This notebook will demonstrate how to:</p> <ul> <li>Process the raw data sets</li> <li>Produce simple features</li> <li>Run a predictive model on H2O</li> <li>Outputs the final predictions for the submissions</li> <li>Load the data and required packages</li> </ul> <pre><code>import numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\nimport h2o\nfrom h2o.automl import H2OAutoML\n\n\npd.options.display.max_columns = 999\n</code></pre> <pre><code># We are loading both the mens and womens match csvs\ndf_atp = pd.read_csv(\"data/ATP_matches.csv\")\ndf_wta = pd.read_csv(\"data/WTA_matches.csv\")\n</code></pre>"},{"location":"modelling/AusOpenPythonTutorial/#data-pre-processing","title":"Data pre-processing","text":"<p>Filter the matches to hard and indoor hard only due to the fact that Australian Open is on hard surface and we want the models to train specifically for hard surfaces matches</p> <p>Convert the columns in both datasets to the correct types. For example, we want to make sure the date columns are in the datetime format and numerical columns are either integer or floats. This will help reduce the memory in use and make the feature engineering process easier</p> <pre><code>### Include hard and indoor hard only\ndf_atp = df_atp.loc[df_atp.Court_Surface.isin(['Hard','Indoor Hard'])]\ndf_wta = df_wta.loc[df_wta.Court_Surface.isin(['Hard','Indoor Hard'])]\n\n### Exclude qualifying rounds\ndf_atp = df_atp.loc[df_atp.Round_Description != 'Qualifying']\ndf_wta = df_wta.loc[df_wta.Round_Description != 'Qualifying']\n\n# Store the shape of the data for reference check later\natp_shape = df_atp.shape\nwta_shape = df_wta.shape\n</code></pre> <pre><code>numeric_columns = ['Winner_Rank', 'Loser_Rank', 'Retirement_Ind',\n                   'Winner_Sets_Won', 'Winner_Games_Won', 'Winner_Aces',\n                   'Winner_DoubleFaults', 'Winner_FirstServes_Won',\n                   'Winner_FirstServes_In', 'Winner_SecondServes_Won',\n                   'Winner_SecondServes_In', 'Winner_BreakPoints_Won',\n                   'Winner_BreakPoints', 'Winner_ReturnPoints_Won',\n                   'Winner_ReturnPoints_Faced', 'Winner_TotalPoints_Won', 'Loser_Sets_Won',\n                   'Loser_Games_Won', 'Loser_Aces', 'Loser_DoubleFaults',\n                   'Loser_FirstServes_Won', 'Loser_FirstServes_In',\n                   'Loser_SecondServes_Won', 'Loser_SecondServes_In',\n                   'Loser_BreakPoints_Won', 'Loser_BreakPoints', 'Loser_ReturnPoints_Won',\n                   'Loser_ReturnPoints_Faced', 'Loser_TotalPoints_Won']\n\ntext_columns = ['Winner', 'Loser',  'Tournament', 'Court_Surface','Round_Description'] \n\ndate_columns = ['Tournament_Date']\n\n# we set the **erros** to coerce so any non-numerical values (text,special characters) will return an NA\ndf_atp[numeric_columns] = df_atp[numeric_columns].apply(pd.to_numeric,errors = 'coerce')\ndf_wta[numeric_columns] = df_wta[numeric_columns].apply(pd.to_numeric,errors = 'coerce')\n\n\ndf_atp[date_columns] = df_atp[date_columns].apply(pd.to_datetime) \ndf_wta[date_columns] = df_wta[date_columns].apply(pd.to_datetime)\n</code></pre>"},{"location":"modelling/AusOpenPythonTutorial/#feature-engineering","title":"Feature Engineering","text":"<p>The raw datasets are constructed in a way that each row will have the seperate stats for both the winner and loser of that match. However, we want to reshape the data so that each row we will only have one player randomly selected from the winner/loser columns and the features are the difference between opponents statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns.</p> <p>In addition, for the features, we will take the rolling average of the player's most recent 15 matches before the particular tournament starts. For example, if the match is the second round of the Australian Open 2018, the features will be the last 15 matches before the first round of Australian Open 2018. The reason of not including the stats in the first round is that we would not have known the player's stats in the first round for the final submissions</p> <p>A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, difference in ELO rating etc. The target variable will be whether or not Player A wins (1=Player A wins and 0=lose).</p> <p>The steps we take are:</p> <ul> <li>Convert the raw data frames into long format:</li> <li>Create some new features</li> <li> <p>Take the rolling average for each player and each match     Since we will be only training our models on US Open and Australian Open, we will only be creating features for those matches. However, the rolling average will take into account any hard surface matches before those tournaments</p> </li> <li> <p>Calculate the difference of averages for each match in the data frames</p> </li> </ul>"},{"location":"modelling/AusOpenPythonTutorial/#convert-the-raw-data-frames-into-long-format","title":"Convert the raw data frames into long format:","text":"<pre><code># Before we split the data frame into winner and loser, we want to create a feature that captures the total number of games the match takes.\n# We have to do it before the split or we will lose this information\ndf_atp['Total_Games'] = df_atp.Winner_Games_Won + df_atp.Loser_Games_Won\ndf_wta['Total_Games'] = df_wta.Winner_Games_Won + df_wta.Loser_Games_Won\n\n\n# Get the column names for the winner and loser stats\nwinner_cols = [col for col in df_atp.columns if col.startswith('Winner')]\nloser_cols = [col for col in df_atp.columns if col.startswith('Loser')]\n\n# create a winner data frame to store the winner stats and a loser data frame for the losers\n# In addition to the winner and loser columns, we are adding common columns as well (e.g. tournamnt dates)\ncommon_cols = ['Total_Games','Tournament','Tournament_Date', 'Court_Surface','Round_Description']\ndf_winner_atp = df_atp[winner_cols + common_cols]\ndf_loser_atp = df_atp[loser_cols + common_cols]\ndf_winner_wta = df_wta[winner_cols + common_cols]\ndf_loser_wta = df_wta[loser_cols + common_cols]\n\n# Create a new column to show whether the player has won or not. \ndf_winner_atp[\"won\"] = 1\ndf_loser_atp[\"won\"] = 0\n\ndf_winner_wta[\"won\"] = 1\ndf_loser_wta[\"won\"] = 0\n\n\n# Rename the columns for the winner and loser data frames so we can append them later on.\n# We will rename the Winner_ / Loser_ columns to Player_\n\nnew_column_names = [col.replace('Winner','Player') for col in winner_cols]\n\ndf_winner_atp.columns = new_column_names + common_cols + ['won']\n\n# They all should be the same\ndf_loser_atp.columns  = df_winner_atp.columns\ndf_winner_wta.columns  = df_winner_atp.columns\ndf_loser_wta.columns  = df_winner_atp.columns\n\n\n# append the winner and loser data frames \n\ndf_long_atp= df_winner_atp.append(df_loser_atp)\ndf_long_wta= df_winner_wta.append(df_loser_wta)\n</code></pre> <p>So now our data frames are in long format and should looks like this</p> <pre><code>df_long_atp.head()\n</code></pre> Player Player_Rank Player_Sets_Won Player_Games_Won Player_Aces Player_DoubleFaults Player_FirstServes_Won Player_FirstServes_In Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Total_Games Tournament Tournament_Date Court_Surface Round_Description won Edouard Roger-Vasselin 106.0 2.0 12 5.0 2.0 22 30 12 19 4.0 7.0 25.0 59.0 59 19 Chennai 2012-01-02 Hard First Round 1 Dudi Sela 83.0 2.0 12 2.0 0.0 14 17 11 16 6.0 14.0 36.0 58.0 61 13 Chennai 2012-01-02 Hard First Round 1 Go Soeda 120.0 2.0 19 6.0 1.0 48 64 19 39 5.0 11.0 42.0 105.0 109 33 Chennai 2012-01-02 Hard First Round 1 Yuki Bhambri 345.0 2.0 12 1.0 2.0 22 29 9 17 5.0 13.0 34.0 62.0 65 17 Chennai 2012-01-02 Hard First Round 1 Yuichi Sugita 235.0 2.0 12 3.0 1.0 37 51 11 27 3.0 7.0 22.0 54.0 70 19 Chennai 2012-01-02 Hard First Round 1"},{"location":"modelling/AusOpenPythonTutorial/#create-some-new-features","title":"Create some new features","text":"<p>Thinking about the dynamics of tennis, we know that players often will matches by \u201cbreaking\u201d the opponent\u2019s serve (i.e. winning a game when the opponent is serving). This is especially important in tennis. Let\u2019s create a feature called Player_BreakPoints_Per_Game, which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let\u2019s also create a feature called Player_Return_Win_Ratio which is the proportion of points won when returning.</p> <p>Similarly, \u201cholding\u201d serve is important (i.e. winning a game when you are serving). Let\u2019s create a feature called Player_Serve_Win_Ratio which is the proportion of points won when serving.</p> <p>Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let\u2019s create a feature called Player_Game_Win_Percentage which is the propotion of games that a player wins.</p> <p>So the four new features we will create are:</p> <ul> <li><code>Player_Serve_Win_Ratio</code></li> <li><code>Player_Return_Win_Ratio</code></li> <li><code>Player_BreakPoints_Per_Return_Game</code></li> <li><code>Player_Game_Win_Percentage</code></li> </ul> <pre><code># Here, we will define a function so we can apply it to both atp and wta data frames\ndef get_new_features(df):\n\n    # Input: \n#     df: data frame to get the data from\n\n     # Return: the df with the new features\n\n\n    # Point Win ratio when serving\n    df['Player_Serve_Win_Ratio'] = (df.Player_FirstServes_Won + df.Player_SecondServes_Won - df.Player_DoubleFaults) \\\n                                  /(df.Player_FirstServes_In + df.Player_SecondServes_In + df.Player_DoubleFaults)\n    # Point win ratio when returning\n    df['Player_Return_Win_Ratio'] = df.Player_ReturnPoints_Won / df.Player_ReturnPoints_Faced\n    # Breakpoints per receiving game\n    df['Player_BreakPoints_Per_Return_Game'] = df.Player_BreakPoints/df.Total_Games  \n    df['Player_Game_Win_Percentage'] = df.Player_Games_Won/df.Total_Games\n\n    return df\n\n\n# Apply the function we just created to the long data frames\ndf_long_atp = get_new_features(df_long_atp)\ndf_long_wta = get_new_features(df_long_wta)\n</code></pre> <pre><code># The long table should have exactly twice of the rows of the original data\nassert df_long_atp.shape[0] == atp_shape[0]*2\nassert df_long_wta.shape[0] == wta_shape[0]*2\n</code></pre>"},{"location":"modelling/AusOpenPythonTutorial/#take-the-rolling-average-for-each-player-and-each-match","title":"Take the rolling average for each player and each match","text":"<p>To train our models, we cannot simply use the player stats for that current match. In fact, we wont be able to use any stats from the same tournament. The logic behind this is that when we try to predict the results in 2019, we would not know the stats of any of the matches in the Australian Open 2019 tournament. As a result, we will use the players' past performance. Here, we will do a rolling average of the most recent 15 matches before the tournament.</p> <p>To do the above, we will follow the steps below:</p> <ol> <li>List all the tournament dates for US and Australian Opens</li> <li>Loop through the dates from point 1, for each date, we filter the data to only include matches before that date and take the most recent 15 games</li> <li>Take the average of those 15 games</li> </ol> <pre><code># the two tournaments we will be using for training and thus the feature generation\ntournaments = ['U.S. Open, New York','Australian Open, Melbourne']\n\n# Store the dates for the loops \ntournament_dates_atp = df_atp.loc[df_atp.Tournament.isin(tournaments)].groupby(['Tournament','Tournament_Date']) \\\n.size().reset_index()[['Tournament','Tournament_Date']]\n\ntournament_dates_wta = df_wta.loc[df_wta.Tournament.isin(tournaments)].groupby(['Tournament','Tournament_Date']) \\\n.size().reset_index()[['Tournament','Tournament_Date']]\n\n\n# We are adding one more date for the final prediction\ntournament_dates_atp.loc[-1] = ['Australian Open, Melbourne',pd.to_datetime('2019-01-15')]\ntournament_dates_wta.loc[-1] = ['Australian Open, Melbourne',pd.to_datetime('2019-01-15')]\n</code></pre> <p>Following are the dates for each tournament</p> <pre><code>tournament_dates_atp\n</code></pre> Tournament Tournament_Date Australian Open, Melbourne 2012-01-16 Australian Open, Melbourne 2013-01-1 Australian Open, Melbourne 2014-01-1 Australian Open, Melbourne 2015-01-1 Australian Open, Melbourne 2016-01-1 Australian Open, Melbourne 2017-01-1 Australian Open, Melbourne 2018-01-1 U.S. Open, New York 2012-08-2 U.S. Open, New York 2013-08-2 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 <pre><code>tournament_dates_wta\n</code></pre> Tournament Tournament_Date Australian Open, Melbourne 2014-01-13 Australian Open, Melbourne 2015-01-19 Australian Open, Melbourne 2016-01-18 Australian Open, Melbourne 2017-01-16 Australian Open, Melbourne 2018-01-15 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 <p>They look fine but it is interesting that for men's, we have two more years of data from 2012 to 2013</p> <pre><code># Let's define a function to calculate the rolling averages\ndef get_rolling_features (df, date_df=None,rolling_cols = None, last_cols= None):\n\n    # Input: \n#     df: data frame to get the data from\n#     date_df: data frame that has the start dates for each tournament\n#     rolling_cols: columns to get the rolling averages\n#     last_cols: columns to get the last value (most recent)\n\n     # Return: the df with the new features\n\n\n    # Sort the data by player and dates so the most recent matches are at the bottom\n    df = df.sort_values(['Player','Tournament_Date','Tournament'], ascending=True)\n\n    # For each tournament, get the rolling averages of that player's past matches before the tournament start date\n    for index, tournament_date in enumerate(date_df.Tournament_Date):\n        # create a temp df to store the interim results\n        df_temp = df.loc[df.Tournament_Date &lt; tournament_date]\n\n        # for ranks, we only take the last one. (comment this out if want to take avg of rank)\n        df_temp_last = df_temp.groupby('Player')[last_cols].last().reset_index()\n\n        # take the most recent 15 matches for the rolling average\n        df_temp = df_temp.groupby('Player')[rolling_cols].rolling(15, min_periods=1).mean().reset_index()\n        df_temp = df_temp.groupby('Player').tail(1) # take the last row of the above\n\n        df_temp= df_temp.merge(df_temp_last, on='Player', how='left')\n\n        if index ==0:\n            df_result = df_temp\n            df_result['tournament_date_index'] = tournament_date # so we know which tournament this feature is for\n        else:\n            df_temp['tournament_date_index'] = tournament_date\n            df_result = df_result.append(df_temp)\n\n    df_result.drop('level_1', axis=1,inplace=True)\n\n    return df_result\n</code></pre> <pre><code># columns we are applying the rolling averages on\nrolling_cols = ['Player_Serve_Win_Ratio', 'Player_Return_Win_Ratio',\n               'Player_BreakPoints_Per_Return_Game', 'Player_Game_Win_Percentage']\n\n# columns we are taking the most recent values on\n# For the player rank, we think we can just use the latest rank (before the tournament starts) \n# as it should refect the most recent performance of the player\nlast_cols = ['Player_Rank']\n\n\n# Apply the rolling average function to the long data frames (it will take a few mins to run)\ndf_rolling_atp = get_rolling_features (df_long_atp, tournament_dates_atp, rolling_cols, last_cols= last_cols )\ndf_rolling_wta = get_rolling_features (df_long_wta, tournament_dates_wta, rolling_cols, last_cols= last_cols)\n</code></pre> <pre><code>df_rolling_atp.head(2)\n</code></pre> Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Adrian Mannarino 0.623408 0.353397 0.257859 0.447246 87.0 2012-01-16 Albert Montanes 0.507246 0.195652 0.000000 0.294118 50.0 2012-01-16 <pre><code>df_rolling_wta.head(2)\n</code></pre> Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Agnieszka Radwanska 0.413333 0.475410 0.350000 0.350000 5.0 2014-01-13 Ajla Tomljanovic 0.468457 0.407319 0.242253 0.462634 75.0 2014-01-13"},{"location":"modelling/AusOpenPythonTutorial/#calculate-the-difference-of-averages-for-each-match-in-the-data-frames","title":"Calculate the difference of averages for each match in the data frames","text":"<p>In the original data frames, the first column is always the winner and followed by the loser. Same for the player stats. Thus, we cannot simply calculate the difference between winner and loser and create a target variable indicating player 1 will win or not because it will always be the winner in this case (target always = 1). As a result, we need to pick a player randomly so the player might or might not be the winner</p> <p>In addition, instead of using both the features for player 1 and 2, we will take the difference of averages between the randomised player 1 and 2. The main benefit is that it will reduce the number of features to half</p> <p>Steps:</p> <ol> <li>We will create a random number for each player which only return 0 or 1</li> <li>If it is zero, we will assign the winner to player 1 and loser to player 2</li> <li>We will join the features to the player 1 and 2. The join will be on the player names and the tournament date (tournament_index in the feature data frames)</li> <li>For players who do not have any history, we will fill the stats by zeros and rank by 999</li> </ol> <pre><code># Randomise the match_wide dataset so the first player is not always the winner\n\n# set a seed so the random number is reproducable\nnp.random.seed(2)\n\n# randomise a number 0/1 with 50% chance each\n# if 0 then take the winner, 1 then take loser\n\ndf_atp['random_number'] = np.random.randint(2, size=len(df_atp))\ndf_atp['randomised_player_1'] = np.where(df_atp['random_number']==0,df_atp['Winner'],df_atp['Loser'])\ndf_atp['randomised_player_2'] = np.where(df_atp['random_number']==0,df_atp['Loser'],df_atp['Winner'])\n\ndf_wta['random_number'] = np.random.randint(2, size=len(df_wta))\ndf_wta['randomised_player_1'] = np.where(df_wta['random_number']==0,df_wta['Winner'],df_wta['Loser'])\ndf_wta['randomised_player_2'] = np.where(df_wta['random_number']==0,df_wta['Loser'],df_wta['Winner'])\n\n# set the target (win/loss) based on the new randomise number\ndf_atp['player_1_win'] = np.where(df_atp['random_number']==0,1,0)\ndf_wta['player_1_win'] = np.where(df_wta['random_number']==0,1,0)\n\n\nprint ('After shuffling, the win rate for player 1 for the mens is {}%'.format(df_atp['player_1_win'].mean()*100))\nprint ('After shuffling, the win rate for player 1 for the womens is {}%'.format(df_wta['player_1_win'].mean()*100))\n</code></pre> <p>After shuffling, the win rate for player 1 for the mens is 49.64798919857267% After shuffling, the win rate for player 1 for the womens is 49.697671426733564% The win rates are close enough to 50%. So we are good to go</p> <pre><code># To get our data frames ready for model training, we will exclude other tournaments from the data now because we have gotten the rolling averages from them and \n# for training, we only need US and Australian Open matches\ndf_atp = df_atp.loc[df_atp.Tournament.isin(tournaments)]\ndf_wta = df_wta.loc[df_wta.Tournament.isin(tournaments)]\n\n# now we can remove other stats columns because we will be using the differences\ncols_to_keep = ['Winner','Loser','Tournament','Tournament_Date',\n                    'player_1_win','randomised_player_1',\n                    'randomised_player_2']\n\ndf_atp = df_atp[cols_to_keep]\ndf_wta = df_wta[cols_to_keep]\n\n\n# Here, we are joining the rolling average data frames to the individual matches. \n# We need to do it twice. One for player 1 and one for player 2\n\n# Get the rolling features for player 1\ndf_atp = df_atp.merge(df_rolling_atp, how='left',\n                      left_on = ['randomised_player_1','Tournament_Date'],\n                      right_on = ['Player','tournament_date_index'],\n                      validate ='m:1')\n\ndf_wta = df_wta.merge(df_rolling_wta, how='left',\n                      left_on = ['randomised_player_1','Tournament_Date'],\n                      right_on = ['Player','tournament_date_index'],\n                      validate ='m:1')\n\n# Get the rolling features for player 2\n# we will use '_p1' to denote player 1 and '_p2' for player 2\ndf_atp = df_atp.merge(df_rolling_atp, how='left',\n                      left_on = ['randomised_player_2','Tournament_Date'],\n                      right_on = ['Player','tournament_date_index'],\n                      validate ='m:1',\n                      suffixes=('_p1','_p2'))\n\ndf_wta = df_wta.merge(df_rolling_wta, how='left',\n                      left_on = ['randomised_player_2','Tournament_Date'],\n                      right_on = ['Player','tournament_date_index'],\n                      validate ='m:1',\n                      suffixes=('_p1','_p2'))\n</code></pre> <pre><code># How many players do not have previous match history\nprint('{} player_1s do Not have previous match history before the tournament'.format(df_atp.loc[df_atp.Player_p1.isna(),'randomised_player_1'].nunique()))\nprint('{} player_2s do Not have previous match history before the tournament'.format(df_atp.loc[df_atp.Player_p2.isna(),'randomised_player_2'].nunique()))\n</code></pre> <p>59 player_1s do Not have previous match history before the tournament 56 player_2s do Not have previous match history before the tournament</p> <pre><code># How many players do not have previous match history\nprint('{} player_1s do Not have previous match history before the tournament'.format(df_wta.loc[df_wta.Player_p1.isna(),'randomised_player_1'].nunique()))\nprint('{} player_2s do Not have previous match history before the tournament'.format(df_wta.loc[df_wta.Player_p2.isna(),'randomised_player_2'].nunique()))\n</code></pre> <p>41 player_1s do Not have previous match history before the tournament 37 player_2s do Not have previous match history before the tournament</p> <pre><code># Most of the missing are for the early years which makes sense as we dont have enough history for them\ndf_wta.loc[df_wta.Player_p1.isna(),'Tournament_Date'].value_counts()\n</code></pre> <pre><code>2014-01-13    29\n2014-08-25     7\n2015-08-31     5\n2015-01-19     3\n2017-08-28     3\n2018-01-15     3\n2018-08-27     3\nName: Tournament_Date, dtype: int64\n</code></pre> <pre><code>df_atp.loc[df_atp.Player_p1.isna(),'Tournament_Date'].value_counts()\n</code></pre> <pre><code>2012-01-16    29\n2012-08-27     9\n2014-01-13     5\n2013-08-26     5\n2016-01-18     5\n2013-01-14     4\n2014-08-25     3\n2018-01-15     3\n2017-08-28     3\n2018-08-27     2\n2016-08-29     2\n2015-01-19     1\nName: Tournament_Date, dtype: int64\n</code></pre> <p>Now we have gotten the rolling averages for both player 1 and 2. What we need to do next is to simply calculate their difference.</p> <p>To calculate the difference, we need to:</p> <ol> <li>Split the data frames into two new data frames: Player 1 and Player 2</li> <li>Take the difference between the two data frames</li> </ol> <pre><code>def get_player_difference(df, diff_cols = None):\n\n        # Input: \n#     df: data frame to get the data from\n#     diff_cols: columns we take the difference on. For example is diff_cols = win rate. This function will calculate the \n#                difference of the win rates between player 1 and player 2\n\n     # Return: the df with the new features\n\n    p1_cols = [i + '_p1' for i in diff_cols] # column names for player 1 stats\n    p2_cols = [i + '_p2' for i in diff_cols] # column names for player 2 stats\n\n\n    # For any missing values, we will fill them by zeros except the ranking where we will use 999\n    df['Player_Rank_p1'] = df['Player_Rank_p1'].fillna(999)\n    df[p1_cols] = df[p1_cols].fillna(0)\n\n    df['Player_Rank_p2'] = df['Player_Rank_p2'].fillna(999)\n    df[p2_cols] = df[p2_cols].fillna(0)\n\n\n    new_column_name = [i + '_diff' for i in diff_cols]\n\n    # Take the difference\n    df_p1 = df[p1_cols]\n    df_p2 = df[p2_cols]\n\n    df_p1.columns=new_column_name\n    df_p2.columns=new_column_name\n\n    df_diff = df_p1 - df_p2\n    df_diff.columns = new_column_name\n\n    # drop the p1 and p2 columns because We have the differences now\n    df.drop(p1_cols + p2_cols, axis=1, inplace=True)\n\n    # Concat the df_diff and raw_df\n    df = pd.concat([df, df_diff], axis=1)\n\n    return df,new_column_name\n</code></pre> <pre><code>diff_cols = ['Player_Serve_Win_Ratio',\n            'Player_Return_Win_Ratio',\n            'Player_BreakPoints_Per_Return_Game',\n            'Player_Game_Win_Percentage','Player_Rank']\n\n# Apply the function and get the difference between player 1 and 2\ndf_atp,_ = get_player_difference(df_atp,diff_cols=diff_cols)\ndf_wta,_ = get_player_difference(df_wta,diff_cols=diff_cols)\n\n# Make a copy of the data frames in case we need to come back to check the values\ndf_atp_final = df_atp.copy()\ndf_wta_final = df_wta.copy()\n</code></pre> <pre><code>df_atp_final.head()\n</code></pre> Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Juan Martin del Potro Adrian Mannarino Australian Open, Melbourne 2012-01-16 1 Juan Martin del Potro Adrian Mannarino Juan Martin del Potro 2012-01-16 Adrian Mannarino 2012-01-16 0.035030 -0.021271 -0.025975 0.103479 -76.0 Pere Riba Albert Montanes Australian Open, Melbourne 2012-01-16 1 Pere Riba Albert Montanes Pere Riba 2012-01-16 Albert Montanes 2012-01-16 -0.156369 0.008893 0.066667 -0.094118 39.0 Tomas Berdych Albert Ramos-Vinolas Australian Open, Melbourne 2012-01-16 0 Albert Ramos-Vinolas Tomas Berdych Albert Ramos-Vinolas 2012-01-16 NaN NaT 0.498027 0.380092 0.414815 0.394444 -934.0 Rafael Nadal Alex Kuznetsov Australian Open, Melbourne 2012-01-16 0 Alex Kuznetsov Rafael Nadal NaN NaT Rafael Nadal 2012-01-16 -0.670139 -0.423057 -0.445623 -0.574767 997.0 Roger Federer Alexander Kudryavtsev Australian Open, Melbourne 2012-01-16 0 Alexander Kudryavtsev Roger Federer NaN NaT Roger Federer 2012-01-16 -0.721415 -0.449516 -0.360255 -0.668090 996.0"},{"location":"modelling/AusOpenPythonTutorial/#modelling","title":"Modelling","text":"<p>We will trian two models here, one for mens and one for womens.</p> <p>For training, we will use all available data from the second year (too many missing values in the first year) up until 2017.</p> <p>For validation, we will test the model on the 2018 Australian Open data</p> <p>This setup allows us to 'mimic' the final prediction (using historical matches to predict 2019 results)</p> <pre><code>df_train_atp = df_atp_final.loc[(df_atp_final.Tournament_Date != '2018-01-15') # excluding Aus Open 2018, and\n                                &amp; (df_atp_final.Tournament_Date &gt; '2012-01-16')] # excluding first year\ndf_valid_atp = df_atp_final.loc[df_atp_final.Tournament_Date == '2018-01-15'] # Australian Open 2018 only\n\ndf_train_wta = df_wta_final.loc[(df_wta_final.Tournament_Date != '2018-01-15') # excluding Aus Open 2018, and\n                                &amp; (df_wta_final.Tournament_Date &gt; '2014-01-13')] # excluding first year\n</code></pre> <pre><code>df_train_atp.head()\n</code></pre> Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Daniel Brands Adrian Ungur U.S. Open, New York 2012-08-27 0 Adrian Ungur Daniel Brands NaN NaT Daniel Brands 2012-08-27 -0.535211 -0.300000 -0.043478 -0.434783 870.0 Richard Gasquet Albert Montanes U.S. Open, New York 2012-08-27 1 Richard Gasquet Albert Montanes Richard Gasquet 2012-08-27 Albert Montanes 2012-08-27 0.080003 0.077451 0.180847 0.131108 -37.0 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 1 Martin Klizan Alejandro Falla Martin Klizan 2012-08-27 Alejandro Falla 2012-08-27 0.077117 -0.044716 -0.087362 0.068180 -2.0 Andy Murray Alex Bogomolov Jr. U.S. Open, New York 2012-08-27 1 Andy Murray Alex Bogomolov Jr. Andy Murray 2012-08-27 Alex Bogomolov Jr. 2012-08-27 0.039641 0.031701 0.094722 0.059010 -69.0 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 1 Tommy Robredo Andreas Seppi Tommy Robredo 2012-08-27 Andreas Seppi 2012-08-27 -0.026814 0.006442 -0.009930 -0.067780 151.0 <pre><code># target variable\ntarget= 'player_1_win'\n\n# features being fed into the models\nfeats = ['Player_Serve_Win_Ratio_diff',\n         'Player_Return_Win_Ratio_diff',\n         'Player_BreakPoints_Per_Return_Game_diff',\n         'Player_Game_Win_Percentage_diff',\n         'Player_Rank_diff']\n\nprint(feats)\n</code></pre>"},{"location":"modelling/AusOpenPythonTutorial/#h2o-model-for-atp","title":"H2O model for ATP","text":"<pre><code>h2o.init()\n\n# Convert to an h2o frame\ndf_train_atp_h2o = h2o.H2OFrame(df_train_atp)\ndf_valid_atp_h2o = h2o.H2OFrame(df_valid_atp)\n\n\n# For binary classification, response should be a factor\ndf_train_atp_h2o[target] = df_train_atp_h2o[target].asfactor()\ndf_valid_atp_h2o[target] = df_valid_atp_h2o[target].asfactor()\n\n# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\naml_atp = h2o.automl.H2OAutoML(max_runtime_secs=300,\n                           max_models=100,\n                           stopping_metric='logloss',\n                           sort_metric='logloss',\n                           balance_classes=True,\n                           seed=183\n                          )\naml_atp.train(x=feats, y=target, training_frame=df_train_atp_h2o,validation_frame=df_valid_atp_h2o)\n\n# View the AutoML Leaderboard\nlb = aml_atp.leaderboard\nlb.head()\n</code></pre> model_id auc logloss mean_per_class_error rmse mse GBM_5_AutoML_20181221_094949 0.790281 0.554852 0.281363 0.431379 0.186088 GBM_grid_1_AutoML_20181221_094949_model_15 0.789329 0.556804 0.29856 0.431931 0.186564 GBM_grid_1_AutoML_20181221_094949_model_7 0.788013 0.557808 0.295899 0.432968 0.187461 StackedEnsemble_BestOfFamily_AutoML_20181221_094949 0.788131 0.558028 0.285321 0.432849 0.187358 GBM_grid_1_AutoML_20181221_094949_model_20 0.785633 0.561094 0.283932 0.43479 0.189043 StackedEnsemble_AllModels_AutoML_20181221_094949 0.784411 0.561587 0.293244 0.434667 0.188935 GBM_grid_1_AutoML_20181221_094949_model_25 0.785311 0.561783 0.291912 0.434888 0.189127 GBM_grid_1_AutoML_20181221_094949_model_17 0.774832 0.570883 0.295836 0.439375 0.193051 DeepLearning_1_AutoML_20181221_094949 0.779388 0.572823 0.311737 0.438479 0.192264 GBM_grid_1_AutoML_20181221_094949_model_14 0.7718 0.578867 0.285835 0.441373 0.19481 <pre><code>H2O model for WTA\n</code></pre> <pre><code># Convert to an h2o frame\ndf_train_wta_h2o = h2o.H2OFrame(df_train_wta)\ndf_valid_wta_h2o = h2o.H2OFrame(df_valid_wta)\n\n\n# For binary classification, response should be a factor\ndf_train_wta_h2o[target] = df_train_wta_h2o[target].asfactor()\ndf_valid_wta_h2o[target] = df_valid_wta_h2o[target].asfactor()\n\n# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\naml_wta = h2o.automl.H2OAutoML(max_runtime_secs=300,\n                           max_models=100,\n                           stopping_metric='logloss',\n                           sort_metric='logloss',\n                           balance_classes=True,\n                           seed=183\n                          )\naml_wta.train(x=feats, y=target, training_frame=df_train_wta_h2o,validation_frame=df_valid_wta_h2o)\n\n# View the AutoML Leaderboard\nlb = aml_wta.leaderboard\nlb.head()\n</code></pre> model_id auc logloss mean_per_class_error rmse mse StackedEnsemble_AllModels_AutoML_20181221_095400 0.726046 0.60827 0.321222 0.457117 0.208956 StackedEnsemble_BestOfFamily_AutoML_20181221_095400 0.724911 0.609329 0.337847 0.457659 0.209452 DeepLearning_grid_1_AutoML_20181221_095400_model_3 0.729152 0.612669 0.315971 0.45641 0.20831 GBM_grid_1_AutoML_20181221_095400_model_7 0.721204 0.615763 0.336848 0.460885 0.212415 GBM_5_AutoML_20181221_095400 0.719252 0.616535 0.319179 0.461055 0.212572 GBM_grid_1_AutoML_20181221_095400_model_15 0.715921 0.619263 0.318673 0.462215 0.213643 GLM_grid_1_AutoML_20181221_095400_model_1 0.726048 0.622989 0.366124 0.463099 0.214461 GBM_grid_1_AutoML_20181221_095400_model_17 0.709261 0.624902 0.34876 0.465628 0.216809 GBM_grid_1_AutoML_20181221_095400_model_18 0.70946 0.625704 0.393556 0.466147 0.217293 DeepLearning_grid_1_AutoML_20181221_095400_model_2 0.713419 0.628008 0.311334 0.463638 0.21496"},{"location":"modelling/AusOpenPythonTutorial/#use-the-models-to-predict-and-make-submissions","title":"Use the models to predict and make submissions","text":"<p>Now let's use the models we just created to make the submissions</p> <pre><code>df_predict_atp = pd.read_csv(\"data/men_dummy_submission_file.csv\")\ndf_predict_wta = pd.read_csv(\"data/women_dummy_submission_file.csv\", encoding='latin1') # for womens, there are some names need a different encoding\n</code></pre> <pre><code>df_predict_wta.head(2)\n</code></pre> player_1 player_2 player_1_win_probability Simona Halep Angelique Kerber 0.5 Simona Halep Caroline Wozniacki 0.5"},{"location":"modelling/AusOpenPythonTutorial/#get-the-features-for-the-predict-df","title":"Get the features for the predict df","text":"<p>We need to join the features to the 2019 players</p> <pre><code># Before we join the features by the names and the dates, we need to convert any non-english characters to english first\ntranslationTable = str.maketrans(\"\u00e9\u00e0\u00e8\u00f9\u00e2\u00ea\u00ee\u00f4\u00fb\u00e7\u00f1\u00e1\", \"eaeuaeioucna\")\n\n\ndf_predict_atp['player_1'] = df_predict_atp.player_1.apply(lambda x: x.translate(translationTable))\ndf_predict_atp['player_2'] = df_predict_atp.player_2.apply(lambda x: x.translate(translationTable))\ndf_predict_wta['player_1'] = df_predict_wta.player_1.apply(lambda x: x.translate(translationTable))\ndf_predict_wta['player_2'] = df_predict_wta.player_2.apply(lambda x: x.translate(translationTable))\n\n\n# Also we need to convert the names into lower cases \ndf_predict_atp['player_1'] =df_predict_atp['player_1'].str.lower() \ndf_predict_atp['player_2'] =df_predict_atp['player_2'].str.lower() \ndf_predict_wta['player_1'] =df_predict_wta['player_1'].str.lower() \ndf_predict_wta['player_2'] =df_predict_wta['player_2'].str.lower() \n\ndf_rolling_atp['Player'] = df_rolling_atp['Player'].str.lower() \ndf_rolling_wta['Player'] = df_rolling_wta['Player'].str.lower() \n\n\n# Lastly, some players have slightly difference names in the submission data and the match data. So we are editing them here manually\ndf_predict_atp.loc[df_predict_atp.player_1=='jaume munar','player_1'] = 'jaume antoni munar clar'\ndf_predict_atp.loc[df_predict_atp.player_2=='jaume munar','player_2'] = 'jaume antoni munar clar'\n\ndf_predict_wta.loc[df_predict_wta.player_1=='daria kasatkina','player_1'] = 'darya kasatkina'\ndf_predict_wta.loc[df_predict_wta.player_2=='daria kasatkina','player_2'] = 'darya kasatkina'\ndf_predict_wta.loc[df_predict_wta.player_1=='lesia tsurenko','player_1'] = 'lesya tsurenko'\ndf_predict_wta.loc[df_predict_wta.player_2=='lesia tsurenko','player_2'] = 'lesya tsurenko'\ndf_predict_wta.loc[df_predict_wta.player_1=='danielle collins','player_1'] = 'danielle rose collins'\ndf_predict_wta.loc[df_predict_wta.player_2=='danielle collins','player_2'] = 'danielle rose collins'\ndf_predict_wta.loc[df_predict_wta.player_1=='anna karolina schmiedlova','player_1'] = 'anna schmiedlova'\ndf_predict_wta.loc[df_predict_wta.player_2=='anna karolina schmiedlova','player_2'] = 'anna schmiedlova'\ndf_predict_wta.loc[df_predict_wta.player_1=='georgina garcia perez','player_1'] = 'georgina garcia-perez'\ndf_predict_wta.loc[df_predict_wta.player_2=='georgina garcia perez','player_2'] = 'georgina garcia-perez'\n</code></pre> <pre><code># create and tournament date column and set it to 2019 so we can join the lastest features\ndf_predict_atp['Tournament_Date'] = pd.to_datetime('2019-01-15')\ndf_predict_wta['Tournament_Date'] = pd.to_datetime('2019-01-15')\n\n# Get the rolling features for player 1\ndf_predict_atp = df_predict_atp.merge(df_rolling_atp, how='left',\n                     left_on = ['player_1','Tournament_Date'],\n                     right_on = ['Player','tournament_date_index'],validate ='m:1')\ndf_predict_wta = df_predict_wta.merge(df_rolling_wta, how='left',\n                     left_on = ['player_1','Tournament_Date'],\n                     right_on = ['Player','tournament_date_index'],validate ='m:1')\n\n\n# Get the rolling features for player 2\n# For duplicate columns, we will use '_p1' to denote player 1 and '_p2' for player 2\ndf_predict_atp = df_predict_atp.merge(df_rolling_atp, how='left',\n                     left_on = ['player_2','Tournament_Date'],\n                     right_on = ['Player','tournament_date_index'],validate ='m:1',suffixes=('_p1','_p2'))\ndf_predict_wta = df_predict_wta.merge(df_rolling_wta, how='left',\n                     left_on = ['player_2','Tournament_Date'],\n                     right_on = ['Player','tournament_date_index'],validate ='m:1',suffixes=('_p1','_p2'))\n</code></pre> <pre><code>df_predict_atp.head(2)\n</code></pre> player_1 player_2 player_1_win_probability Tournament_Date Player_p1 Player_Serve_Win_Ratio_p1 Player_Return_Win_Ratio_p1 Player_BreakPoints_Per_Return_Game_p1 Player_Game_Win_Percentage_p1 Player_Rank_p1 tournament_date_index_p1 Player_p2 Player_Serve_Win_Ratio_p2 Player_Return_Win_Ratio_p2 Player_BreakPoints_Per_Return_Game_p2 Player_Game_Win_Percentage_p2 Player_Rank_p2 tournament_date_index_p2 novak djokovic rafael nadal 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 rafael nadal 0.622425 0.401028 0.334270 0.570833 1.0 2019-01-15 novak djokovic roger federer 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 roger federer 0.620070 0.389781 0.269224 0.564244 3.0 2019-01-15 <pre><code># How many players do not have previous match history\nprint('{} player_1s do Not have previous match history before the tournament in the mens'.format(df_predict_atp.loc[df_predict_atp.Player_p1.isna(),'player_1'].nunique()))\nprint('{} player_2s do Not have previous match history before the tournament in the mens'.format(df_predict_atp.loc[df_predict_atp.Player_p2.isna(),'player_2'].nunique()))\n</code></pre> <p>3 player_1s do Not have previous match history before the tournament in the mens 3 player_2s do Not have previous match history before the tournament in the mens</p> <pre><code># How many players do not have previous match history\nprint('{} player_1s do Not have previous match history before the tournament in the womens'.format(df_predict_wta.loc[df_predict_wta.Player_p1.isna(),'player_1'].nunique()))\nprint('{} player_2s do Not have previous match history before the tournament in the womens'.format(df_predict_wta.loc[df_predict_wta.Player_p2.isna(),'player_2'].nunique()))\n</code></pre> <p>0 player_1s do Not have previous match history before the tournament in the womens 0 player_2s do Not have previous match history before the tournament in the womens</p> <pre><code>print(df_predict_atp.loc[df_predict_atp.Player_p1.isna(),'player_1'].unique().tolist())\n['christian garin', 'pedro sousa', 'hugo dellien']\n</code></pre> <pre><code>print(df_predict_wta.loc[df_predict_wta.Player_p1.isna(),'player_1'].unique().tolist())\n[]\n</code></pre> <p>We will do the differencing again for the prediction data frames exactly like what we did for training</p> <pre><code># Apply the function and get the difference between player 1 and 2\ndf_predict_atp,_ = get_player_difference(df_predict_atp,diff_cols=diff_cols)\ndf_predict_wta,_ = get_player_difference(df_predict_wta,diff_cols=diff_cols)\n</code></pre>"},{"location":"modelling/AusOpenPythonTutorial/#make-the-prediction","title":"Make the prediction","text":"<pre><code>df_predict_atp_h2o = h2o.H2OFrame(df_predict_atp[feats])\ndf_predict_wta_h2o = h2o.H2OFrame(df_predict_wta[feats])\n\natp_preds = aml_atp.predict(df_predict_atp_h2o)['p1'].as_data_frame()\nwta_preds = aml_wta.predict(df_predict_wta_h2o)['p1'].as_data_frame()\n\ndf_predict_atp['player_1_win_probability'] = atp_preds\ndf_predict_wta['player_1_win_probability'] = wta_preds\n</code></pre> <pre><code>atp_submission = df_predict_atp[['player_1','player_2','player_1_win_probability']]\nwta_submission = df_predict_wta[['player_1','player_2','player_1_win_probability']]\n</code></pre> <pre><code>atp_submission.head()\n</code></pre> player_1 player_2 player_1_win_probability novak djokovic rafael nadal 0.571588 novak djokovic roger federer 0.662511 novak djokovic juan martin del potro 0.544306 novak djokovic alexander zverev 0.709483 novak djokovic kevin anderson 0.687195 <pre><code>wta_submission.head()\n</code></pre> player_1 player_2 player_1_win_probability simona halep angelique kerber 0.455224 simona halep caroline wozniacki 0.546276 simona halep elina svitolina 0.408014 simona halep naomi osaka 0.285125 simona halep sloane stephens 0.576643 <p>Let's look at who has the highest win rate from our models</p> <pre><code>atp_submission.groupby('player_1')['player_1_win_probability'].mean() \\\n.reset_index().sort_values('player_1_win_probability',ascending=False).head()\n</code></pre> player_1 player_1_win_probability novak djokovic 0.846377 juan martin del potro 0.787337 karen khachanov 0.782963 rafael nadal 0.778707 roger federer 0.767337 <pre><code>wta_submission.groupby('player_1')['player_1_win_probability'].mean() \\\n.reset_index().sort_values('player_1_win_probability',ascending=False).head()\n</code></pre> player_1 player_1_win_probability madison keys 0.750580 naomi osaka 0.749195 caroline wozniacki 0.722409 kiki bertens 0.713904 aryna sabalenka 0.707368 <p>Now we can output the predictions as csvs</p> <pre><code>atp_submission.to_csv('submission/atp_submission_python.csv',index=False)\nwta_submission.to_csv('submission/wta_submission_pthon.csv',index=False)\n</code></pre> <pre><code>atp_submission.shape\n</code></pre> <p>(16256, 3)</p>"},{"location":"modelling/AusOpenPythonTutorial/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"modelling/AusOpenRTutorial/","title":"Australian Open Datathon R Tutorial","text":""},{"location":"modelling/AusOpenRTutorial/#overview","title":"Overview","text":""},{"location":"modelling/AusOpenRTutorial/#the-task","title":"The Task","text":"<p>This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide.</p> <p>The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here.</p> <p>How an outline of our methodology and thought process, read this article.</p>"},{"location":"modelling/AusOpenRTutorial/#exploring-the-data","title":"Exploring the Data","text":"<p>First we need to get an idea of what the data looks like. Let's read the men's data in and get an idea of what it looks like. Note that you will need to install all the packages listed below unless you already have them.</p> <p>Note that for this tutorial I will be using <code>dplyr</code>, if you are not familiar with the syntax I encourage you to read up on the basics.</p> <pre><code># Import libraries\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(RcppRoll)\nlibrary(tidyselect)\nlibrary(lubridate)\nlibrary(stringr)\nlibrary(zoo)\nlibrary(purrr)\nlibrary(h2o)\nlibrary(DT)\nmens = readr::read_csv('data/ATP_matches.csv', na = \".\") # NAs are indicated by .\nmens %&gt;%\n  datatable(rownames = FALSE, extensions = 'Scroller', \n            options = list(dom = \"t\", \n            scrollY = 450,\n            scroller = TRUE,\n            scrollX = 600,\n            fixedColumns = TRUE)) %&gt;%\n  formatRound(columns=pluck(., \"x\", \"data\") %&gt;% colnames(), digits=3)\n</code></pre> Winner Loser Tournament Tournament_Date Court_Surface Round_Description Winner_Rank Loser_Rank Retirement_Ind Winner_Sets_Won ... Loser_DoubleFaults Loser_FirstServes_Won Loser_FirstServes_In Loser_SecondServes_Won Loser_SecondServes_In Loser_BreakPoints_Won Loser_BreakPoints Loser_ReturnPoints_Won Loser_ReturnPoints_Faced Loser_TotalPoints_Won Edouard Roger-Vasselin Eric Prodon Chennai 2-Jan-12 Hard First Round 106 97 0 2 ... 3 21 33 13 26 1 3 15 49 49 Dudi Sela Fabio Fognini Chennai 2-Jan-12 Hard First Round 83 48 0 2 ... 4 17 32 5 26 0 1 8 33 30 Go Soeda Frederico Gil Chennai 2-Jan-12 Hard First Round 120 102 0 2 ... 2 45 70 18 35 2 4 36 103 99 Yuki Bhambri Karol Beck Chennai 2-Jan-12 Hard First Round 345 101 0 2 ... 1 15 33 13 29 2 3 15 46 43 Yuichi Sugita Olivier Rochus Chennai 2-Jan-12 Hard First Round 235 67 0 2 ... 0 19 32 13 22 1 7 30 78 62 Benoit Paire Pere Riba Chennai 2-Jan-12 Hard First Round 95 89 0 2 ... 5 13 20 12 32 0 1 9 44 34 Victor Hanescu Sam Querrey Chennai 2-Jan-12 Hard First Round 90 93 0 2 ... 8 29 41 7 24 1 3 17 57 53 Yen-Hsun Lu Thiemo de Bakker Chennai 2-Jan-12 Hard First Round 82 223 0 2 ... 0 20 32 10 24 1 1 19 57 49 Andreas Beck Vasek Pospisil Chennai 2-Jan-12 Hard First Round 98 119 0 2 ... 3 39 57 16 38 1 5 24 74 79 Ivan Dodig Vishnu Vardhan Chennai 2-Jan-12 Hard First Round 36 313 0 2 ... 5 41 59 13 27 2 8 34 101 88 David Goffin Xavier Malisse Chennai 2-Jan-12 Hard First Round 174 49 0 2 ... 1 31 43 19 34 1 4 27 85 77 David Goffin Andreas Beck Chennai 2-Jan-12 Hard Second Round 174 98 0 2 ... 0 43 71 14 27 2 8 27 82 84 Dudi Sela Benoit Paire Chennai 2-Jan-12 Hard Second Round 83 95 0 2 ... 5 40 58 21 46 1 7 26 87 87 Stan Wawrinka Edouard Roger-Vasselin Chennai 2-Jan-12 Hard Second Round 17 106 0 2 ... 0 43 70 16 34 4 6 28 82 87 Go Soeda Ivan Dodig Chennai 2-Jan-12 Hard Second Round 120 36 0 2 ... 2 31 41 11 28 1 4 23 73 65 Milos Raonic Victor Hanescu Chennai 2-Jan-12 Hard Second Round 31 90 0 2 ... 1 25 38 5 14 0 4 15 56 45 Yuichi Sugita Yen-Hsun Lu Chennai 2-Jan-12 Hard Second Round 235 82 0 2 ... 4 34 45 12 34 2 9 38 93 84 Janko Tipsarevic Yuki Bhambri Chennai 2-Jan-12 Hard Second Round 9 345 0 2 ... 2 12 22 9 17 0 1 8 41 29 Janko Tipsarevic David Goffin Chennai 2-Jan-12 Hard Quarter-finals 9 174 0 2 ... 5 34 51 19 40 1 2 18 67 71 Milos Raonic Dudi Sela Chennai 2-Jan-12 Hard Quarter-finals 31 83 0 2 ... 2 23 31 19 28 0 3 16 69 58 Go Soeda Stan Wawrinka Chennai 2-Jan-12 Hard Quarter-finals 120 17 0 2 ... 4 18 34 13 31 3 7 31 74 62 Nicolas Almagro Yuichi Sugita Chennai 2-Jan-12 Hard Quarter-finals 10 235 0 2 ... 1 36 65 30 40 3 12 45 123 111 Janko Tipsarevic Go Soeda Chennai 2-Jan-12 Hard Semi-finals 9 120 0 2 ... 1 21 33 10 28 1 1 10 44 41 Milos Raonic Nicolas Almagro Chennai 2-Jan-12 Hard Semi-finals 31 10 0 2 ... 0 31 45 8 15 0 3 12 54 51 Milos Raonic Janko Tipsarevic Chennai 2-Jan-12 Hard Finals 31 9 0 2 ... 2 59 83 34 55 0 4 25 113 118 Igor Andreev Adrian Mannarino Brisbane 2-Jan-12 Hard First Round 115 87 0 2 ... 3 24 35 13 25 1 3 21 70 58 Alexandr Dolgopolov Alejandro Falla Brisbane 2-Jan-12 Hard First Round 15 74 0 2 ... 3 16 33 12 25 3 7 33 75 61 Tatsuma Ito Benjamin Mitchell Brisbane 2-Jan-12 Hard First Round 122 227 0 2 ... 6 30 44 7 24 0 2 13 52 50 Kei Nishikori Cedrik-Marcel Stebe Brisbane 2-Jan-12 Hard First Round 25 81 0 2 ... 2 27 49 23 41 3 6 28 75 78 Denis Istomin Florian Mayer Brisbane 2-Jan-12 Hard First Round 73 23 1 1 ... 1 28 38 11 17 0 2 15 56 54 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Malek Jaziri Fernando Verdasco Paris 29-Oct-18 Indoor Hard Second Round 55 27 0 2 ... 6 46 60 16 35 3 13 39 104 101 Alexander Zverev Frances Tiafoe Paris 29-Oct-18 Indoor Hard Second Round 5 44 0 2 ... 4 26 40 16 36 2 10 27 72 69 Dominic Thiem Gilles Simon Paris 29-Oct-18 Indoor Hard Second Round 8 31 0 2 ... 1 13 26 12 25 2 2 23 59 48 Novak Djokovic Joao Sousa Paris 29-Oct-18 Indoor Hard Second Round 2 48 0 2 ... 2 25 35 6 22 1 10 27 74 58 Karen Khachanov Matthew Ebden Paris 29-Oct-18 Indoor Hard Second Round 18 39 1 2 ... 6 8 18 5 20 1 2 10 30 23 John Isner Mikhail Kukushkin Paris 29-Oct-18 Indoor Hard Second Round 9 54 0 2 ... 1 54 80 24 39 0 1 13 90 91 Kevin Anderson Nikoloz Basilashvili Paris 29-Oct-18 Indoor Hard Second Round 6 22 0 2 ... 7 43 54 30 49 0 3 26 106 99 Marin Cilic Philipp Kohlschreiber Paris 29-Oct-18 Indoor Hard Second Round 7 43 0 2 ... 1 19 34 12 20 1 1 17 55 48 Jack Sock Richard Gasquet Paris 29-Oct-18 Indoor Hard Second Round 23 28 0 2 ... 4 18 33 16 29 0 4 19 59 53 Grigor Dimitrov Roberto Bautista Agut Paris 29-Oct-18 Indoor Hard Second Round 10 25 0 2 ... 0 34 48 11 20 2 4 27 76 72 Damir Dzumhur Stefanos Tsitsipas Paris 29-Oct-18 Indoor Hard Second Round 52 16 0 2 ... 3 14 26 15 30 2 2 17 52 46 Dominic Thiem Borna Coric Paris 29-Oct-18 Indoor Hard Third Round 8 13 0 2 ... 1 39 57 16 38 2 2 27 88 82 Novak Djokovic Damir Dzumhur Paris 29-Oct-18 Indoor Hard Third Round 2 52 1 2 ... 4 15 28 7 18 0 0 8 28 30 Alexander Zverev Diego Schwartzman Paris 29-Oct-18 Indoor Hard Third Round 5 19 0 2 ... 2 22 37 12 24 0 4 18 58 52 Roger Federer Fabio Fognini Paris 29-Oct-18 Indoor Hard Third Round 3 14 0 2 ... 6 22 32 15 37 1 5 16 54 53 Marin Cilic Grigor Dimitrov Paris 29-Oct-18 Indoor Hard Third Round 7 10 0 2 ... 1 37 55 14 32 1 5 22 71 73 Karen Khachanov John Isner Paris 29-Oct-18 Indoor Hard Third Round 18 9 0 2 ... 4 67 80 19 38 0 0 17 100 103 Kei Nishikori Kevin Anderson Paris 29-Oct-18 Indoor Hard Third Round 11 6 0 2 ... 1 26 33 11 19 0 0 11 51 48 Jack Sock Malek Jaziri Paris 29-Oct-18 Indoor Hard Third Round 23 55 0 2 ... 6 13 21 10 24 0 0 9 41 32 Karen Khachanov Alexander Zverev Paris 29-Oct-18 Indoor Hard Quarter-finals 18 5 0 2 ... 7 26 47 4 21 1 3 10 36 40 Dominic Thiem Jack Sock Paris 29-Oct-18 Indoor Hard Quarter-finals 8 23 0 2 ... 5 44 59 19 37 2 10 34 97 97 Roger Federer Kei Nishikori Paris 29-Oct-18 Indoor Hard Quarter-finals 3 11 0 2 ... 0 21 37 16 26 0 1 12 56 49 Novak Djokovic Marin Cilic Paris 29-Oct-18 Indoor Hard Quarter-finals 2 7 0 2 ... 0 38 55 11 28 2 5 29 85 78 Karen Khachanov Dominic Thiem Paris 29-Oct-18 Indoor Hard Semi-finals 18 8 0 2 ... 0 19 29 8 26 1 3 15 47 42 Novak Djokovic Roger Federer Paris 29-Oct-18 Indoor Hard Semi-finals 2 3 0 2 ... 2 69 93 25 46 1 2 29 113 123 Karen Khachanov Novak Djokovic Paris 29-Oct-18 Indoor Hard Finals 18 2 0 2 ... 1 30 43 14 28 1 5 20 66 64 Jaume Antoni Munar Clar Frances Tiafoe Milan 5-Nov-18 Indoor Hard NA 76 40 0 3 ... 3 21 29 6 17 0 2 5 46 32 Frances Tiafoe Hubert Hurkacz Milan 5-Nov-18 Indoor Hard NA 40 85 0 3 ... 4 35 48 10 19 1 7 22 78 67 Hubert Hurkacz Jaume Antoni Munar Clar Milan 5-Nov-18 Indoor Hard NA 85 76 0 3 ... 1 43 63 15 35 3 9 29 80 87 Andrey Rublev Liam Caruana Milan 5-Nov-18 Indoor Hard NA 68 NA 0 3 ... 1 28 39 4 14 1 3 18 57 50 <p>As we can see, we have a <code>Winner</code> column, a <code>Loser</code> column, as well as other columns detailing the match details, and other columns which have the stats for that match. As we have a <code>Winner</code> column, if we use the current data structure to train a model we will leak the result. The model will simply learn that the actual winner comes from the <code>Winner</code> column, rather than learning from other features that we can create, such as <code>First Serve %</code>.</p> <p>To avoid this problem, let's reshape the data from wide to long, then shuffle the data. For this, we will define a function, <code>split_winner_loser_columns</code>, which splits the raw data frame into two data frames, appends them together, and then shuffles the data.</p> <p>Let's also remove all Grass and Clay matches from our data, as we will be modelling the Australian Open which is a hardcourt surface.</p> <p>Additionally, we will add a few columns, such as <code>Match_Id</code> and <code>Total_Games</code>. These will be useful later.</p> <pre><code>split_winner_loser_columns &lt;- function(df) {\n  # This function splits the raw data into two data frames and appends them together then shuffles them\n  # This output is a data frame with only one player's stats on each row (i.e. in long format)\n\n  # Grab a df with only the Winner's stats\n  winner = df %&gt;% \n    select(-contains(\"Loser\")) %&gt;% # Select only the Winner columns + extra game info columns as a df\n    rename_at( # Rename all columns containing \"Winner\" to \"Player\" \n      vars(contains(\"Winner\")),\n      ~str_replace(., \"Winner\", \"Player\")\n    ) %&gt;%\n    mutate(Winner = 1) # Create a target column\n\n  # Repeat the process with the loser's stats\n  loser = df %&gt;%\n    select(-contains(\"Winner\")) %&gt;%\n    rename_at(\n      vars(contains(\"Loser\")),\n      ~str_replace(., \"Loser\", \"Player\")\n    ) %&gt;%\n    mutate(Winner = 0)\n\n  set.seed(183) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut)\n\n  # Create a df that appends both the Winner and loser df together\n  combined_df = winner %&gt;% \n    rbind(loser) %&gt;% # Append the loser df to the Winner df\n    slice(sample(1:n())) %&gt;% # Randomise row order\n    arrange(Match_Id) %&gt;% # Arrange by Match_Id\n    return()\n}\n</code></pre> <pre><code># Read in men and womens data; randomise the data to avoid result leakage\nmens = readr::read_csv('data/ATP_matches.csv', na = \".\") %&gt;%\n  filter(Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\") %&gt;% # Filter to only use hardcourt games\n  mutate(Match_Id = row_number(), # Add a match ID column to be used as a key\n         Tournament_Date = dmy(Tournament_Date), # Change Tournament to datetime\n         Total_Games = Winner_Games_Won + Loser_Games_Won) %&gt;% # Add a total games played column\n  split_winner_loser_columns() # Change the data frame from wide to long\nmens %&gt;%\n  datatable(rownames = FALSE, extensions = 'Scroller', \n            options = list(dom = \"t\", \n            scrollY = 450,\n            scroller = TRUE,\n            scrollX = 600,\n            fixedColumns = TRUE)) %&gt;%\n  formatRound(columns=pluck(., \"x\", \"data\") %&gt;% colnames(), digits=3)\n</code></pre> Player Tournament Tournament_Date Court_Surface Round_Description Player_Rank Retirement_Ind Player_Sets_Won Player_Games_Won Player_Aces ... Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Match_Id Total_Games Winner Eric Prodon Chennai 2012-01-02 Hard First Round 97 0 0 7 2 ... 13 26 1 3 15 49 49 1 19 0 Edouard Roger-Vasselin Chennai 2012-01-02 Hard First Round 106 0 2 12 5 ... 12 19 4 7 25 59 59 1 19 1 Dudi Sela Chennai 2012-01-02 Hard First Round 83 0 2 12 2 ... 11 16 6 14 36 58 61 2 13 1 Fabio Fognini Chennai 2012-01-02 Hard First Round 48 0 0 1 1 ... 5 26 0 1 8 33 30 2 13 0 Frederico Gil Chennai 2012-01-02 Hard First Round 102 0 1 14 5 ... 18 35 2 4 36 103 99 3 33 0 Go Soeda Chennai 2012-01-02 Hard First Round 120 0 2 19 6 ... 19 39 5 11 42 105 109 3 33 1"},{"location":"modelling/AusOpenRTutorial/#feature-creation","title":"Feature Creation","text":"<p>Now that we have a fairly good understanding of what the data looks like, let's add some features. To do this we will define a function. Ideally we want to add features which will provide predictive power to our model. </p> <p>Thinking about the dynamics of tennis, we know that players often will matches by \"breaking\" the opponent's serve (i.e. winning a game when the opponent is serving). This is especially important in mens tennis. Let's create a feature called <code>F_Player_BreakPoints_Per_Game</code>, which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let's also create a feature called <code>F_Player_Return_Win_Ratio</code> which is the proportion of points won when returning.</p> <p>Similarly, \"holding\" serve is important (i.e. winning a game when you are serving). Let's create a feature called <code>F_Player_Serve_Win_Ratio</code> which is the proportion of points won when serving.</p> <p>Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let's create a feature called <code>F_Player_Game_Win_Percentage</code> which is the propotion of games that a player wins.</p> <pre><code>add_ratio_features &lt;- function(df) {\n  # This function adds ratio features to a long df\n  df %&gt;%\n    mutate(\n      # Point Win ratio when serving\n      F_Player_Serve_Win_Ratio = (Player_FirstServes_Won + Player_SecondServes_Won - Player_DoubleFaults) / \n        (Player_FirstServes_In + Player_SecondServes_In + Player_DoubleFaults), \n      # Point win ratio when returning\n      F_Player_Return_Win_Ratio = Player_ReturnPoints_Won / Player_ReturnPoints_Faced, \n      # Breakpoints per receiving game\n      F_Player_BreakPoints_Per_Game = Player_BreakPoints / Total_Games, \n      F_Player_Game_Win_Percentage = Player_Games_Won / Total_Games\n    ) %&gt;%\n    mutate_at(\n      vars(colnames(.), -contains(\"Rank\"), -Tournament_Date), # Replace all NAs with0 apart from Rank, Date\n      ~ifelse(is.na(.), 0, .)\n    ) %&gt;%\n    return()\n}\nmens = mens %&gt;%\n  add_ratio_features() # Add features\n</code></pre> <p>Now that we have added our features, we need to create rolling averages for them. We cannot simply use current match statistics, as they will leak the result to the model. Instead, we need to use past match statistics to predict future matches. Here we will use a rolling mean with a window of 15. If the player hasn't played 15 games, we will instead use a cumulative mean. We will also lag the result so as to not leak the result.</p> <p>This next chunk of code simply takes all the columns starting with F_ and calculates these means.</p> <pre><code>mens = mens %&gt;% \n  group_by(Player) %&gt;% # Group by player\n  mutate_at( # Create a rolling mean with window 15 for each player. \n    vars(starts_with(\"F_\")), # If the player hasn't played 15 games, use a cumulative mean\n    ~coalesce(rollmean(., k = 15, align = \"right\", fill = NA_real_), cummean(.)) %&gt;% lag()\n  ) %&gt;%\n  ungroup()\n</code></pre>"},{"location":"modelling/AusOpenRTutorial/#creating-a-training-feature-matrix","title":"Creating a Training Feature Matrix","text":"<p>In predictive modelling language - features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options</p> <p>We can train the model on every tennis match in the data set, or We can only train the model on Australian Open matches. Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure.</p> <p>Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on.</p> <p>We have decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface - hard court.</p> <p>However, we also need to train our model in the same way that will be used to predict the 2019 Australian Open. When predicting the 2nd round, we won't have data from the 1st round. So we will need to build our training feature matrix with this in mind. We should extract features for a player from past games at the start of the tournament and apply them to every matchup that that player plays.</p> <p>To do this, we will create a function, <code>extract_latest_features_for_tournament</code>, which maps over our feature data frame for the dates in the first round of a tournament and grabs features.</p> <p>First, we need the Australian Open and US Open results - let's grab these and then apply our function.</p> <pre><code># Get Australian Open and US Open Results\naus_us_open_results = \n  mens %&gt;%\n  filter((Tournament == \"Australian Open, Melbourne\" | Tournament == \"U.S. Open, New York\")\n         &amp; Round_Description != \"Qualifying\" &amp; Tournament_Date != \"2012-01-16\") %&gt;% # Filter out qualifiers\n  select(Match_Id, Player, Tournament, Tournament_Date, Round_Description, Winner)\n# Create a function which extracts features for each tournament\nextract_latest_features_for_tournament = function(df, dte) {\n\n  df %&gt;% # Filter for the 1st round\n    filter(Tournament_Date == dte, Round_Description == \"First Round\", Tournament_Date != \"2012-01-16\") %&gt;% \n    group_by(Player) %&gt;% # Group by player\n    select_at(\n      vars(Match_Id, starts_with(\"F_\"), Player_Rank) # Grab the players' features\n    ) %&gt;%\n    rename(F_Player_Rank = Player_Rank) %&gt;%\n    ungroup() %&gt;%\n    mutate(Feature_Date = dte) %&gt;%\n    select(Player, Feature_Date, everything())\n}\n# Create a feature matrix in long format\nfeature_matrix_long = \n  aus_us_open_results %&gt;%\n  distinct(Tournament_Date) %&gt;% # Pull all Tournament Dates\n  pull() %&gt;%\n  map_dfr(\n    ~extract_latest_features_for_tournament(mens, .) # Get the features\n  ) %&gt;%\n  filter(Feature_Date != \"2012-01-16\") %&gt;% # Filter out the first Aus Open\n  mutate_at( # Replace NAs with the mean\n    vars(starts_with(\"F_\")),\n    ~ifelse(is.na(.), mean(., na.rm = TRUE), .)\n  )\n</code></pre> <p>Now that we have a feature matrix in long format, we need to convert it to wide format so that the features are on the same row. To do this we will define a function <code>gather_df</code>, which converts the data frame from long to wide. Let's also join the results to the matrix and convert the <code>Winner</code> column to a factor. Finally, we will take the difference of player1 and player2's features, so as to reduce the dimensionality of the model.</p> <pre><code>gather_df &lt;- function(df) {\n  # This function puts the df back into its original format of each row containing stats for both players\n  df %&gt;%\n    arrange(Match_Id) %&gt;%\n    filter(row_number() %% 2 != 0) %&gt;% # Filter for every 2nd row, starting at the 1st index. e.g. 1, 3, 5\n    rename_at( # Rename columns to player_1\n      vars(contains(\"Player\")),\n      ~str_replace(., \"Player\", \"player_1\")\n    ) %&gt;%\n    inner_join(df %&gt;%\n                 filter(row_number() %% 2 == 0) %&gt;%\n                 rename_at(\n                   vars(contains(\"Player\")), # Rename columns to player_2\n                   ~str_replace(., \"Player\", \"player_2\")\n                 ) %&gt;%\n                 select(Match_Id, contains(\"Player\")),\n               by=c('Match_Id')\n    ) %&gt;%\n    select(Match_Id, player_1, player_2, Winner, everything()) %&gt;%\n    return()\n}\n# Joining results to features\nfeature_matrix_wide = aus_us_open_results %&gt;%\n  inner_join(feature_matrix_long %&gt;% \n               select(-Match_Id), \n             by = c(\"Player\", \"Tournament_Date\" = \"Feature_Date\")) %&gt;%\n  gather_df() %&gt;%\n  mutate(\n    F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio,\n    F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio,\n    F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage,\n    F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game,\n    F_Rank_Diff = (F_player_1_Rank - F_player_2_Rank),\n    Winner = as.factor(Winner)\n  ) %&gt;%\n  select(Match_Id, player_1, player_2, Tournament, Tournament_Date, Round_Description, Winner, contains(\"Diff\"))\ntrain = feature_matrix_wide\ntrain %&gt;%\n  datatable(rownames = FALSE, extensions = 'Scroller', \n            options = list(dom = \"t\", \n            scrollY = 450,\n            scroller = TRUE,\n            scrollX = 600,\n            fixedColumns = TRUE)) %&gt;%\n  formatRound(columns=pluck(., \"x\", \"data\") %&gt;% colnames(), digits=3)\n</code></pre> Match_Id player_1 player_2 Tournament Tournament_Date Round_Description Winner F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff 1139 Adrian Ungur Daniel Brands U.S. Open, New York 2012-08-27 First Round 0 0.03279412 -0.014757229 0.002877458 0.073938088 -13 1140 Albert Montanes Richard Gasquet U.S. Open, New York 2012-08-27 First Round 0 -0.08000322 -0.077451342 -0.131108056 -0.180846832 97 1141 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 First Round 1 0.07711693 -0.044715517 0.068179841 -0.087361962 1 1142 Alex Bogomolov Jr. Andy Murray U.S. Open, New York 2012-08-27 First Round 0 -0.03964074 -0.031700826 -0.059010072 -0.094721700 69 1143 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 First Round 1 -0.02681392 0.006442134 -0.067779660 -0.009930089 151 1144 Ryan Harrison Benjamin Becker U.S. Open, New York 2012-08-27 First Round 1 0.04251983 0.018604623 0.026486753 -0.003548973 -24"},{"location":"modelling/AusOpenRTutorial/#creating-the-feature-matrix-for-the-2019-australian-open","title":"Creating the Feature Matrix for the 2019 Australian Open","text":"<p>Now that we have our training set, <code>train</code>, we need to create a feature matrix to create predictions on. To do this, we need to generate features again. We could simply append a player list to our raw data frame, create a mock date and then use the <code>extract_latest_features_for_tournament</code> function that we used before.  Instead, we're going to create a lookup table for each unique player in the 2019 Australian Open. We will need to get their last 15 games and then find the mean for each feature so that our features are the same.</p> <p>Let's first explore what the dummy submission file looks like, then use it to get the unique players.</p> <pre><code>read_csv('data/men_dummy_submission_file.csv') %&gt;% glimpse()\n</code></pre> <p>As we can see, the dummy submission file contains every potential match up for the Open. This will be updated a few days before the Open starts with the actual players playing. Let's now create the lookup feature table.</p> <pre><code># Get a vector of unique players in this years' open using the dummy submission file\nunique_players = read_csv('data/men_dummy_submission_file.csv') %&gt;% pull(player_1) %&gt;% unique()\n# Get the last 15 games played for each unique player and find their features\nlookup_feature_table = read_csv('data/ATP_matches.csv', na = \".\") %&gt;%\n  filter(Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\") %&gt;%\n  mutate(Match_Id = row_number(), # Add a match ID column to be used as a key\n         Tournament_Date = dmy(Tournament_Date), # Change Tournament to datetime\n         Total_Games = Winner_Games_Won + Loser_Games_Won) %&gt;% # Add a total games played column\n  # clean_missing_data() %&gt;% # Clean missing data\n  split_winner_loser_columns() %&gt;% # Change the data frame from wide to long\n  add_ratio_features() %&gt;%\n  filter(Player %in% unique_players) %&gt;%\n  group_by(Player) %&gt;%\n  top_n(15, Match_Id) %&gt;%\n  summarise(\n    F_Player_Serve_Win_Ratio = mean(F_Player_Serve_Win_Ratio),\n    F_Player_Return_Win_Ratio = mean(F_Player_Return_Win_Ratio),\n    F_Player_BreakPoints_Per_Game = mean(F_Player_BreakPoints_Per_Game),\n    F_Player_Game_Win_Percentage = mean(F_Player_Game_Win_Percentage),\n    F_Player_Rank = last(Player_Rank)\n  )\n</code></pre> <p>Now let's create features for every single combination. To do this we'll join our <code>lookup_feature_table</code> to the <code>player_1</code> and <code>player_2</code> columns in the <code>dummy_submission_file</code>.</p> <pre><code># Create feature matrix for the Australian Open for all player 1s\nfeatures_player_1 = read_csv('data/men_dummy_submission_file.csv') %&gt;%\n  select(player_1) %&gt;%\n  inner_join(lookup_feature_table, by=c(\"player_1\" = \"Player\")) %&gt;%\n  rename(F_player_1_Serve_Win_Ratio = F_Player_Serve_Win_Ratio,\n         F_player_1_Return_Win_Ratio = F_Player_Return_Win_Ratio,\n         F_player_1_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game,\n         F_player_1_Game_Win_Percentage = F_Player_Game_Win_Percentage,\n         F_player_1_Rank = F_Player_Rank)\n# Create feature matrix for the Australian Open for all player 2s\nfeatures_player_2 = read_csv('data/men_dummy_submission_file.csv') %&gt;%\n  select(player_2) %&gt;%\n  inner_join(lookup_feature_table, by=c(\"player_2\" = \"Player\")) %&gt;%\n  rename(F_player_2_Serve_Win_Ratio = F_Player_Serve_Win_Ratio,\n         F_player_2_Return_Win_Ratio = F_Player_Return_Win_Ratio,\n         F_player_2_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game,\n         F_player_2_Game_Win_Percentage = F_Player_Game_Win_Percentage,\n         F_player_2_Rank = F_Player_Rank)\n# Join the two dfs together and subtract features to create Difference features\naus_open_2019_features = features_player_1 %&gt;% \n  bind_cols(features_player_2) %&gt;%\n  select(player_1, player_2, everything()) %&gt;%\n  mutate(\n    F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio,\n    F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio,\n    F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage,\n    F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game,\n    F_Rank_Diff = (F_player_1_Rank - F_player_2_Rank)\n  ) %&gt;%\n  select(player_1, player_2, contains(\"Diff\"))\naus_open_2019_features  %&gt;%\n  datatable(rownames = FALSE, extensions = 'Scroller', \n            options = list(dom = \"t\",\n                          scrollY = 450,\n                          scroller = TRUE,\n                          scrollX = 600,\n                          fixedColumns = TRUE)) %&gt;%\n  formatRound(columns=pluck(., \"x\", \"data\") %&gt;% colnames(), digits=3)\n</code></pre> player_1 player_2 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff Novak Djokovic Rafael Nadal 0.06347805 0.02503802 0.07002382 0.08951024 1 Novak Djokovic Roger Federer 0.06583364 0.03628491 0.07661295 0.15455628 -1 Novak Djokovic Juan Martin del Potro 0.01067079 0.03436023 0.06382353 0.11259979 -2 Novak Djokovic Alexander Zverev 0.11117863 0.03125651 0.11055585 0.08661036 -3 Novak Djokovic Kevin Anderson 0.02132375 0.10449337 0.11184503 0.23684083 -4 Novak Djokovic Marin Cilic 0.08410746 0.02434916 0.07653035 0.08355134 -5"},{"location":"modelling/AusOpenRTutorial/#generating-2019-australian-open-predictions","title":"Generating 2019 Australian Open Predictions","text":"<p>Now that we have our features, we can finally train our model and generate predictions for the 2019 Australian Open. Due to its simplicity, we will use h2o's Auto Machine Learning function <code>h2o.automl</code>. This will train a heap of different models and optimise the hyperparameters, as well as creating stacked ensembles automatically for us. We will use optimising by log loss.</p> <p>First, we must create h2o frames for our training and feature data frames. Then we will run <code>h2o.automl</code>. Note that we can set the <code>max_runtime_secs</code> parameter. As this is a notebook, I have set it for 30 seconds - but I suggest you give it 10 minutes to create the best model. We can then create our predictions and assign them back to our <code>aus_open_2019_features</code> data frame. Finally, we will group_by player and find the best player, on average.</p> <p><pre><code>## Setup H2O\nh2o.init(ip = \"localhost\",\n         port = 54321,\n         enable_assertions = TRUE,\n         nthreads = 2,\n         max_mem_size = \"24g\"\n\n)\n## Sending file to h2o\ntrain_h2o = feature_matrix_wide %&gt;%\n  select(contains(\"Diff\"), Winner) %&gt;%\n  as.h2o(destination_frame = \"train_h2o\")\naus_open_2019_features_h2o = aus_open_2019_features %&gt;%\n  select(contains(\"Diff\")) %&gt;%\n  as.h2o(destination_frame = \"aus_open_2019_features_h2o\")\n## Running Auto ML \nmens_model = h2o.automl(y = \"Winner\",\n                        training_frame = train_h2o,\n                        max_runtime_secs = 30,\n                        max_models = 100,\n                        stopping_metric = \"logloss\",\n                        sort_metric = \"logloss\",\n                        balance_classes = TRUE,\n                        seed = 183) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut)\n## Predictions on test frame\npredictions = h2o.predict(mens_model@leader, aus_open_2019_features_h2o) %&gt;%\n  as.data.frame()\naus_open_2019_features$prob_player_1 = predictions$p1\naus_open_2019_features$prob_player_2 = predictions$p0\nh2o.shutdown(prompt = FALSE)\n</code></pre> Now let's find the best player by taking the mean of the prediction probability by player. <pre><code>aus_open_2019_features %&gt;% \n  select(player_1, starts_with(\"F_\"), prob_player_1) %&gt;%\n  group_by(player_1) %&gt;%\n  summarise_all(mean) %&gt;%\n  arrange(desc(prob_player_1)) %&gt;%\n  datatable(rownames = FALSE, extensions = 'Scroller', \n            options = list(dom = \"t\",\n                          scrollY = 450,\n                          scroller = TRUE,\n                          scrollX = 600,\n                          fixedColumns = TRUE)) %&gt;%\n  formatRound(columns=pluck(., \"x\", \"data\") %&gt;% colnames(), digits=3)\n</code></pre></p> player_1 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff prob_player_1 Novak Djokovic 0.1109364627 0.076150615 0.1483970690 0.17144300 NA 0.8616486 Karen Khachanov 0.0960639298 0.061436164 0.1059967623 0.04544955 NA 0.8339594 Juan Martin del Potro 0.1003931993 0.042025222 0.0847985439 0.05943767 NA 0.8218308 Rafael Nadal 0.0480432305 0.051531252 0.0790179917 0.08181694 NA 0.8032543 Gilles Simon 0.0646937767 0.084843307 0.0901401318 0.08675350 NA 0.7985995 Roger Federer 0.0452014997 0.040992497 0.0725719954 0.01817046 NA 0.7962289 Kei Nishikori 0.0777155934 0.018720226 0.0800648870 0.02740276 NA 0.7843631 Marin Cilic 0.0285413602 0.053017465 0.0736687072 0.08883055 NA 0.7804876 Tomas Berdych 0.0471654691 0.047289449 0.0737401748 0.10584114 NA 0.7739211 Daniil Medvedev 0.0275430665 0.031121856 0.0721948279 0.01803757 NA 0.7543269 Stefanos Tsitsipas 0.0470382377 0.023825850 0.0577628626 0.02105227 NA 0.7511674 Dominic Thiem 0.0258904189 0.032481624 0.0483707080 0.05857158 NA 0.7451547 Alexander Zverev 0.0006199716 0.044811275 0.0380134371 0.08423392 NA 0.7374897 Kyle Edmund 0.0558006240 0.011963627 0.0478850676 0.05142186 NA 0.7304873 Pablo Carreno Busta 0.0321878318 0.029862068 0.0413674481 -0.00229784 NA 0.7302043 Borna Coric 0.0762084129 -0.010097922 0.0413621283 -0.01924267 NA 0.7268124 Kevin Anderson 0.0907358428 -0.027171681 0.0381421997 -0.06362578 NA 0.7260799 David Goffin -0.0034821911 0.037247336 0.0162572061 0.05603565 NA 0.7155908 Fernando Verdasco 0.0229261365 0.032884054 0.0521212576 0.04668854 NA 0.7120831 Roberto Bautista Agut 0.0047641170 0.049939608 0.0218975349 0.07331023 NA 0.7009891 Milos Raonic 0.0849726089 -0.028732182 0.0385944327 -0.08009382 NA 0.6986865 Fabio Fognini -0.0394792678 0.047935185 0.0226546894 0.06213496 NA 0.6982031 Hyeon Chung 0.0042489153 0.047722133 0.0158096386 0.04823304 NA 0.6958943 Jack Sock -0.0099659903 0.026454984 0.0186547428 0.02307214 NA 0.6757770 Diego Schwartzman -0.0317130675 0.032098381 0.0006215006 0.05621187 NA 0.6631067 John Millman 0.0016290285 0.042676556 0.0119857356 0.06228135 NA 0.6603912 Nikoloz Basilashvili -0.0099968609 0.005561102 0.0473876170 0.03661962 NA 0.6602628 John Isner 0.1346946527 -0.070556940 0.0161348609 -0.11425009 NA 0.6598097 Gael Monfils -0.0074254934 0.024286746 0.0295568649 0.04007519 NA 0.6449506 Richard Gasquet 0.0296009556 -0.011382437 0.0013138324 -0.03972967 NA 0.6442043 ... ... ... ... ... ... ... Laslo Djere -0.042300822 -0.0150684095 -0.064667709 -0.0349151578 NA 0.3606923 David Ferrer -0.036179509 0.0532782117 0.012751020 0.0914824480 NA 0.3488057 Bradley Klahn -0.001248083 -0.0444982448 -0.025987040 -0.1181295700 NA 0.3487806 Marcel Granollers -0.031011830 -0.0094056152 -0.049853664 0.0136841358 NA 0.3460035 Ricardas Berankis -0.022557215 -0.0103782963 -0.047937290 -0.0468488990 NA 0.3454980 Radu Albot -0.040829057 0.0076150564 -0.034891704 0.0443672533 NA 0.3420615 Jordan Thompson -0.068554906 0.0261969117 -0.044349181 0.0206636045 NA 0.3358572 Thomas Fabbiano -0.060583307 0.0275756029 -0.025883493 0.0709707306 NA 0.3319778 Roberto Carballes Baena -0.054016396 -0.0091521177 -0.019093050 0.0347187874 NA 0.3312105 Paolo Lorenzi -0.038613500 -0.0212206827 -0.052602703 0.0199474025 NA 0.3299791 Guido Andreozzi -0.038614385 -0.0133763922 0.029549861 0.0636745661 NA 0.3288762 Peter Polansky 0.007461636 -0.0163389196 -0.024034159 -0.0442144260 NA 0.3216756 Ernests Gulbis -0.062827089 -0.0134699552 -0.027633425 -0.0518663252 NA 0.3123511 Thiago Monteiro 0.001235931 -0.0288349103 -0.043831840 -0.0654744344 NA 0.3122069 Casper Ruud 0.016838968 -0.0178511679 0.015234507 0.0219131874 NA 0.3119321 Marco Trungelliti -0.022148774 -0.0005658242 0.048542554 0.1243537739 NA 0.3092636 Jiri Vesely -0.050204009 -0.0351868278 -0.042887646 -0.0160467165 NA 0.3089287 Guillermo Garcia-Lopez -0.090076100 -0.0108663630 -0.048712763 -0.0124446402 NA 0.3080898 Michael Mmoh -0.063802934 -0.0079053251 -0.011112236 -0.0332042032 NA 0.2822330 Jason Kubler -0.124758873 -0.0202756806 -0.013998570 0.1020895301 NA 0.2814246 Ruben Bemelmans -0.029036164 -0.0138846550 -0.032256254 -0.0363563402 NA 0.2772185 Bjorn Fratangelo -0.014149222 0.0033574304 -0.019931504 -0.0360199607 NA 0.2652527 Pablo Andujar -0.042869833 -0.0488261697 -0.070057834 -0.0164918910 NA 0.2647100 Christian Garin -0.046150875 0.0235799476 -0.006209664 0.0736304057 NA 0.2631607 Ivo Karlovic 0.071597162 -0.1093833837 0.001410787 -0.1237762218 NA 0.2500242 Juan Ignacio Londero -0.026454456 -0.0715665271 -0.016749898 -0.0363353678 NA 0.2351747 Ramkumar Ramanathan -0.005371622 -0.0606138479 -0.041631884 -0.0005573405 NA 0.2272977 Reilly Opelka 0.025704824 -0.0607219257 -0.015474944 -0.0720809006 NA 0.2262993 Carlos Berlocq -0.063580460 0.0074576369 -0.054277974 -0.0165235079 NA 0.2112275 Pedro Sousa -0.197333352 -0.0734557562 -0.161962722 -0.1023311674 NA 0.1502313"},{"location":"modelling/AusOpenRTutorial/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"modelling/Brownlow2023Datathon/","title":"Betfair\u2019s 2023 AFL Brownlow Medal Datathon","text":""},{"location":"modelling/Brownlow2023Datathon/#registrations","title":"Registrations","text":"<p>Register here</p> <p>Once you have registered, you will receive the dataset and submission template by email within 2 business days.</p> <p>Registrations close 4:45PM AEST Friday September 22nd 2023</p>"},{"location":"modelling/Brownlow2023Datathon/#the-competition","title":"The Competition","text":"<p>Do you think you have what it takes to predict how the umpires will vote on the Brownlow medal?</p> <p>Betfair is giving you the chance to show off your data modelling skills by building a predictive model for the 2023 Brownlow Medal count!</p> <p>With $5,000 in prizes up for grabs, we\u2019re challenging you to use your modelling skills to your advantage, be that by building your first predictive sports model, improving on an existing design or have a go at something different by adapting your data skills from other fields.</p> <p>Submissions close at 2:59pm AEST Monday September 25th 2023! Time to get modelling!</p> <p>Round by Round updates will be posted in the Quants Discord server with the final results being posted here.  For an invite to the discord server, please fill out the form here</p> <p>Please direct all questions and submissions to datathon@betfair.com.au.</p>"},{"location":"modelling/Brownlow2023Datathon/#previous-afl-modelling-meet-up","title":"Previous AFL Modelling Meet-Up","text":"<p>Check out our Online Meet-Up with Stats Insider on modelling the AFL here</p>"},{"location":"modelling/Brownlow2023Datathon/#the-specifics","title":"The specifics","text":"<p>The Terms and Conditions for the 2023 Brownlow Medal Datathon Competition can be viewed here</p>"},{"location":"modelling/Brownlow2023Datathon/#prizes","title":"Prizes","text":"<p>$5,000 in prizes are up for grabs! See the list below for the prizes each placing at the end of the competition will receive:</p> Place Prize 1 $2,500.00 2 $1,000.00 3 $500.00 4 $250.00 5 $250.00 6 $100.00 7 $100.00 8 $100.00 9 $100.00 10 $100.00 Total Prize Pool $5,000.00 <p>Prize winners will be announced at the end of the count and prizes will be distributed in the following days</p>"},{"location":"modelling/Brownlow2023Datathon/#final-results","title":"Final Results","text":"<p>Model|LogLoss|Rank| |----------------------------------------|---------|---------|-------------| Vertex|5.59971899693986|1| Kathys-Lucky-Guesses|5.65064148455871|2| ForTheTrees|5.78990422858799|3| belleepoque|5.79400002454156|4| ChromeBook|5.85635976392115|5| FootyProphet|5.90339607366938|6| LifesGood|5.96807894493434|7| BLT|5.97007252475371|8| MyLittlePony|6.04155148419435|9| Bwownwow-modew-UwU|6.0417051646312|10| RandomShrubbery|6.0833130481194|11| Cindy|6.20046166558851|12| Midfielders-Medal|6.22448844896409|13| NotYourModelV2|6.23632064877733|14| PabloEscobar|6.23860449303679|15| Snoopy|6.28912469990177|16| nzCharlieChooser|6.3362015352423|17| random-number-generator|6.35297828843829|18| NickyDwillwin|6.62981370403745|19| LongRange|6.6834374158962|20| Sportsmanifold-com|6.77706578552931|21| BrownHighLow|7.31510324865778|22| DCL|7.75006150745972|23| VinhoVerde|7.87199859723157|24| Corebridge-Analytics|8.52861655118625|25| WATP|9.70207349246611|26| DataWading|24.9278696539264|27| craigs-list|31.4775455046783|28| boylanc|33.2383077997184|29| ReverseBanana|44.9988536000238|30|</p>"},{"location":"modelling/Brownlow2023Datathon/#competition-rules","title":"Competition Rules","text":"<p>The 2023 Brownlow Medal Datathon will see entrants provided with a bespoke set of historic AFL data from 2012 until the end of Round 24 2023.</p> <p>The goal from there is to use the provided data (or any other data) to build your own model to predict the probability of a player receiving at least 1 Brownlow vote in every match for the 2023 season.</p> <p>With 207 matches and 23 players per side, this is a whopping 9522 predictions!</p> <p>How you go about building the model is entirely up to you \u2013 do you want to build an Elo model, a regression-based model, a Machine Learning algorithm or something entirely different? Get as innovative with it as you want!</p> <p>Your set of predictions will be due before the 2:59pm AEST Monday September 25th 2023. All submissions must be emailed to datathon@betfair.com.au</p> <p>See the Terms and Conditions for full competition rules. </p>"},{"location":"modelling/Brownlow2023Datathon/#submissions-judging","title":"Submissions &amp; Judging","text":"<p>The Datathon will be judged using the log-loss method where the log-loss score is calculated using the difference between the predicted probability and the actual outcome. The scoring calculation will involve finding the SUM of the log-loss across all 46 players in one single match and then finding the AVERAGE log-loss of each match. The entrant with the lowest log-loss will be victorious.</p> <p>One submission file template will be provided to Competition Entrants along with the data set upon registration. Submissions must follow the template set out in the submission file template provided and must be submitted in a csv format.</p> <p>For each Home and Away Match, each entrant is required to predict the probability of a player receiving at least 1 Brownlow vote. This prediction will then be used against the actual outcome to calculate the model\u2019s log-loss. The sum of the probabilities for every match must equal 3 \u2013 being the number of players in each match that will receive a Brownlow vote.</p> <p>Please name submission file using the following formatting:</p> <p>\u2022 Brownlow_Datathon_2023_Submission_Form_{Model_Name}.csv;</p> <p>To submit your model entry, please send it through to datathon@betfair.com.au</p>"},{"location":"modelling/Brownlow2023Datathon/#results","title":"Results","text":"<p>Round-by-Round updates will be posted in our Discord server. Final results will be posted here.</p>"},{"location":"modelling/Brownlow2023Datathon/#registrations_1","title":"Registrations","text":"<p>Register here</p> <p>Once you have registered, you will receive the dataset and submission template by email within 2 business days.</p>"},{"location":"modelling/EPLmlPython/","title":"EPL Machine Learning Walkthrough","text":""},{"location":"modelling/EPLmlPython/#01-data-acquisition-exploration","title":"01. Data Acquisition &amp; Exploration","text":"<p>Welcome to the first part of this Machine Learning Walkthrough. This tutorial will be made of four parts; how we actually acquired our data (programmatically), exploring the data to find potential features, building the model and using the model to make predictions.</p>"},{"location":"modelling/EPLmlPython/#data-acquisition","title":"Data Acquisition","text":"<p>We will be grabbing our data from football-data.co.uk, which has an enormous amount of soccer data dating back to the 90s. They also generously allow us to use it for free! However, the data is in separate CSVs based on the season. That means we would need to manually download 20 different files if we wanted the past 20 seasons. Rather than do this laborious and boring task, let's create a function which downloads the files for us, and appends them all into one big CSV.</p> <p>To do this, we will use BeautifulSoup, a Python library which helps to pull data from HTML and XML files. We will then define a function which collates all the data for us into one DataFrame.</p> <pre><code># Import Modules\n\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport datetime\npd.set_option('display.max_columns', 100)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom data_preparation_functions import *\n</code></pre> <pre><code>def grab_epl_data():\n    # Connect to football-data.co.uk\n    res = requests.get(\"http://www.football-data.co.uk/englandm.php\")\n\n    # Create a BeautifulSoup object\n    soup = BeautifulSoup(res.content, 'lxml')\n\n    # Find the tables with the links to the data in them.\n    table = soup.find_all('table', {'align': 'center', 'cellspacing': '0', 'width': '800'})[1]\n    body = table.find_all('td', {'valign': 'top'})[1]\n\n    # Grab the urls for the csv files\n    links = [link.get('href') for link in body.find_all('a')]\n    links_text = [link_text.text for link_text in body.find_all('a')]\n\n    data_urls = []\n\n    # Create a list of links\n    prefix = 'http://www.football-data.co.uk/'\n    for i, text in enumerate(links_text):\n        if text == 'Premier League':\n            data_urls.append(prefix + links[i])\n\n    # Get rid of last 11 uls as these don't include match stats and odds, and we\n    # only want from 2005 onwards\n    data_urls = data_urls[:-12]\n\n    df = pd.DataFrame()\n\n    # Iterate over the urls\n    for url in data_urls:\n        # Get the season and make it a column\n        season = url.split('/')[4]\n\n        print(f\"Getting data for season {season}\")\n\n        # Read the data from the url into a DataFrame\n        temp_df = pd.read_csv(url)\n        temp_df['season'] = season\n\n        # Create helpful columns like Day, Month, Year, Date etc. so that our data is clean\n        temp_df = (temp_df.dropna(axis='columns', thresh=temp_df.shape[0]-30)\n                          .assign(Day=lambda df: df.Date.str.split('/').str[0],\n                                  Month=lambda df: df.Date.str.split('/').str[1],\n                                  Year=lambda df: df.Date.str.split('/').str[2])\n                          .assign(Date=lambda df: df.Month + '/' + df.Day + '/' + df.Year)\n                          .assign(Date=lambda df: pd.to_datetime(df.Date))\n                          .dropna())\n\n        # Append the temp_df to the main df\n        df = df.append(temp_df, sort=True)\n\n    # Drop all NAs\n    df = df.dropna(axis=1).dropna().sort_values(by='Date')\n    print(\"Finished grabbing data.\")\n\n    return df\n</code></pre> <pre><code>df = grab_epl_data()\n# df.to_csv(\"data/epl_data.csv\", index=False)\n\n    Getting data for season 1819\n    Getting data for season 1718\n    Getting data for season 1617\n    Getting data for season 1516\n    Getting data for season 1415\n    Getting data for season 1314\n    Getting data for season 1213\n    Getting data for season 1112\n    Getting data for season 1011\n    Getting data for season 0910\n    Getting data for season 0809\n    Getting data for season 0708\n    Getting data for season 0607\n    Getting data for season 0506\n    Finished grabbing data.\n</code></pre> <p>Whenever we want to update our data (for example if we want the most recent Gameweek included), all we have to do is run that function and then save the data to a csv with the commented out line above.</p>"},{"location":"modelling/EPLmlPython/#data-exploration","title":"Data Exploration","text":"<p>Now that we have our data, let's explore it. Let's first look at home team win rates since 2005 to see if there is a consistent trend. To get an idea of what our data looks like, we'll look at the tail of the dataset first.</p> <pre><code>df.tail(3)\n</code></pre> AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv&lt;2.5 BbAv&gt;2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx&lt;2.5 BbMx&gt;2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season 28 3.0 11.0 0.0 9.0 3.0 2.0 Crystal Palace 3.00 3.25 2.60 2.95 3.1 2.55 42.0 20.0 -0.25 1.71 2.13 2.92 1.73 2.16 3.22 2.55 1.79 2.21 3.04 1.77 2.23 3.36 2.66 39.0 2018-08-26 26 E0 1.0 2.0 H 6.0 14.0 0.0 13.0 5.0 0.0 0.0 D 4.0 Watford 2.95 3.20 2.5 2.90 3.1 2.50 08 A Taylor 2.90 3.3 2.6 18 1819 27 5.0 8.0 0.0 15.0 3.0 1.0 Chelsea 1.66 4.00 5.75 1.67 3.8 5.25 42.0 22.0 1.00 1.92 1.88 1.67 2.18 1.71 3.90 5.25 2.01 1.95 1.71 2.28 1.76 4.17 5.75 40.0 2018-08-26 26 E0 2.0 1.0 A 4.0 16.0 0.0 6.0 2.0 0.0 0.0 D 3.0 Newcastle 1.70 3.75 5.0 1.67 3.8 5.25 08 P Tierney 1.67 4.0 5.5 18 1819 29 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.90 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.00 1.76 2.25 3.40 2.67 40.0 2018-08-27 27 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.6 2.75 3.2 2.55 08 C Pawson 2.90 3.3 2.6 18 1819 <pre><code># Create Home Win, Draw Win and Away Win columns\ndf = df.assign(homeWin=lambda df: df.apply(lambda row: 1 if row.FTHG &gt; row.FTAG else 0, axis='columns'),\n              draw=lambda df: df.apply(lambda row: 1 if row.FTHG == row.FTAG else 0, axis='columns'),\n              awayWin=lambda df: df.apply(lambda row: 1 if row.FTHG &lt; row.FTAG else 0, axis='columns'))\n</code></pre>"},{"location":"modelling/EPLmlPython/#home-ground-advantage","title":"Home Ground Advantage","text":"<pre><code>win_rates = \\\n(df.groupby('season')\n    .mean()\n    .loc[:, ['homeWin', 'draw', 'awayWin']])\n\nwin_rates\n</code></pre> homeWin draw awayWin season 0506 0.505263 0.202632 0.292105 0607 0.477573 0.258575 0.263852 0708 0.463158 0.263158 0.273684 0809 0.453826 0.255937 0.290237 0910 0.507895 0.252632 0.239474 1011 0.471053 0.292105 0.236842 1112 0.450000 0.244737 0.305263 1213 0.433862 0.285714 0.280423 1314 0.472973 0.208108 0.318919 1415 0.453826 0.245383 0.300792 1516 0.414248 0.282322 0.303430 1617 0.492105 0.221053 0.286842 1718 0.455263 0.260526 0.284211 1819 0.466667 0.200000 0.333333"},{"location":"modelling/EPLmlPython/#findings","title":"Findings","text":"<p>As we can see, winrates across home team wins, draws and away team wins are very consistent. It seems that the home team wins around 46-47% of the time, the draw happens about 25% of the time, and the away team wins about 27% of the time. Let's plot this DataFrame so that we can see the trend more easily.</p> <pre><code># Set the style\nplt.style.use('ggplot')\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nhome_line = ax.plot(win_rates.homeWin, label='Home Win Rate')\naway_line = ax.plot(win_rates.awayWin, label='Away Win Rate')\ndraw_line = ax.plot(win_rates.draw, label='Draw Win Rate')\nax.set_xlabel(\"season\")\nax.set_ylabel(\"Win Rate\")\nplt.title(\"Win Rates\", fontsize=16)\n\n# Add the legend locations\nhome_legend = plt.legend(handles=home_line, loc='upper right', bbox_to_anchor=(1, 1))\nax = plt.gca().add_artist(home_legend)\naway_legend = plt.legend(handles=away_line, loc='center right', bbox_to_anchor=(0.95, 0.4))\nax = plt.gca().add_artist(away_legend)\ndraw_legend = plt.legend(handles=draw_line, loc='center right', bbox_to_anchor=(0.95, 0.06))\n</code></pre> <p></p> <p>As we can see, the winrates are relatively stable each season, except for in 14/15 when the home win rate drops dramatically.</p> <p>Out of interest, let's also have a look at which team has the best home ground advantage. Let's define HGA as home win rate - away win rate. And then plot some of the big clubs' HGA against each other.</p> <pre><code>home_win_rates = \\\n(df.groupby(['HomeTeam'])\n    .homeWin\n    .mean())\n\naway_win_rates = \\\n(df.groupby(['AwayTeam'])\n    .awayWin\n    .mean())\n\nhga = (home_win_rates - away_win_rates).reset_index().rename(columns={0: 'HGA'}).sort_values(by='HGA', ascending=False)\n</code></pre> <pre><code>hga.head(10)\n</code></pre> HomeTeam HGA 15 Fulham 0.315573 7 Brighton 0.304762 20 Man City 0.244980 14 Everton 0.241935 30 Stoke 0.241131 10 Charlton 0.236842 0 Arsenal 0.236140 27 Reading 0.234962 33 Tottenham 0.220207 21 Man United 0.215620 <p>So the club with the best HGA is Fulham - interesting. This is most likely because Fulham have won 100% of home games in 2018 so far which is skewing the mean. Let's see how the HGA for some of the big clubs based compare over seasons.</p> <pre><code>big_clubs = ['Liverpool', 'Man City', 'Man United', 'Chelsea', 'Arsenal']\nhome_win_rates_5 = df[df.HomeTeam.isin(big_clubs)].groupby(['HomeTeam', 'season']).homeWin.mean()\naway_win_rates_5 = df[df.AwayTeam.isin(big_clubs)].groupby(['AwayTeam', 'season']).awayWin.mean()\n\nhga_top_5 = home_win_rates_5 - away_win_rates_5\n\nhga_top_5.unstack(level=0)\n</code></pre> HomeTeam Arsenal Chelsea Liverpool Man City Man United season 0506 0.421053 0.368421 0.263158 0.263158 0.052632 0607 0.263158 0.000000 0.421053 -0.052632 0.105263 0708 0.210526 -0.052632 0.157895 0.368421 0.368421 0809 0.105263 -0.157895 -0.052632 0.578947 0.210526 0910 0.368421 0.368421 0.421053 0.315789 0.263158 1011 0.157895 0.368421 0.368421 0.263158 0.684211 1112 0.157895 0.315789 -0.105263 0.421053 0.105263 1213 0.052632 0.105263 0.105263 0.248538 0.201754 1314 0.143275 0.251462 0.307018 0.362573 -0.026316 1415 0.131579 0.210526 0.105263 0.210526 0.421053 1516 0.210526 -0.105263 0.000000 0.263158 0.263158 1617 0.263158 0.210526 0.105263 -0.052632 -0.105263 1718 0.578947 0.052632 0.157895 0.000000 0.263158 1819 0.500000 0.000000 0.000000 0.500000 0.500000 <p>Now let's plot it.</p> <pre><code>sns.lineplot(x='season', y='HGA', hue='team', data=hga_top_5.reset_index().rename(columns={0: 'HGA', 'HomeTeam': 'team'}))\nplt.legend(loc='lower center', ncol=6, bbox_to_anchor=(0.45, -0.2))\nplt.title(\"HGA Among the top 5 clubs\", fontsize=14)\nplt.show()\n</code></pre> <p></p> <p>The results here seem to be quite erratic, although it seems that Arsenal consistently has a HGA above 0.</p> <p>Let's now look at the distributions of each of our columns. The odds columns are likely to be highly skewed, so we may have to account for this later.</p> <pre><code>for col in df.select_dtypes('number').columns:\n    sns.distplot(df[col])\n    plt.title(f\"Distribution for {col}\")\n    plt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"modelling/EPLmlPython/#exploring-referee-home-ground-bias","title":"Exploring Referee Home Ground Bias","text":"<p>What may be of interest is whether certain referees are correlated with the home team winning more often. Let's explore referee home ground bias for referees for the top 10 Referees based on games.</p> <p><pre><code>print('Overall Home Win Rate: {:.4}%'.format(df.homeWin.mean() * 100))\n\n# Get the top 10 refs based on games\ntop_10_refs = df.Referee.value_counts().head(10).index\n\ndf[df.Referee.isin(top_10_refs)].groupby('Referee').homeWin.mean().sort_values(ascending=False)\n</code></pre>     Overall Home Win Rate: 46.55%</p> <pre><code>Referee\nL Mason          0.510373\nC Foy            0.500000\nM Clattenburg    0.480000\nM Jones          0.475248\nP Dowd           0.469880\nM Atkinson       0.469565\nM Oliver         0.466019\nH Webb           0.456604\nA Marriner       0.455516\nM Dean           0.442049\nName: homeWin, dtype: float64\n</code></pre> <p>It seems that L Mason may be the most influenced by the home crowd. Whilst the overall home win rate is 46.5%, the home win rate when he is the Referee is 51%. However it should be noted that this doesn't mean that he causes the win through bias. It could just be that he referees the best clubs, so naturally their home win rate is high.</p>"},{"location":"modelling/EPLmlPython/#variable-correlation-with-margin","title":"Variable Correlation With Margin","text":"<p>Let's now explore different variables' relationships with margin. First, we'll create a margin column, then we will pick a few different variables to look at the correlations amongst each other, using a correlation heatmap.</p> <pre><code>df['margin'] = df['FTHG'] - df['FTAG']\n</code></pre> <pre><code>stat_cols = ['AC', 'AF', 'AR', 'AS', 'AST', 'AY', 'HC', 'HF', 'HR', 'HS', 'HST', 'HTR', 'HY', 'margin']\n\nstat_correlations = df[stat_cols].corr()\nstat_correlations['margin'].sort_values()\n\n    AST      -0.345703\n    AS       -0.298665\n    HY       -0.153806\n    HR       -0.129393\n    AC       -0.073204\n    HF       -0.067469\n    AF        0.005474\n    AY        0.013746\n    HC        0.067433\n    AR        0.103528\n    HS        0.275847\n    HST       0.367591\n    margin    1.000000\n    Name: margin, dtype: float64\n</code></pre> <p>Unsurprisingly, Home Shots on Target correlate the most with Margin, and Away Reds is also high. What is surprising is that Home Yellows has quite a strong negative correlation with margin - this may be because players will play more aggresively when they are losing to try and get the lead back, and hence receive more yellow cards.</p> <p>Let's now look at the heatmap between variables.</p> <pre><code>sns.heatmap(stat_correlations, annot=True, annot_kws={'size': 10})\n    &lt;matplotlib.axes._subplots.AxesSubplot at 0x220a4227048&gt;\n</code></pre> <p></p>"},{"location":"modelling/EPLmlPython/#analysing-features","title":"Analysing Features","text":"<p>What we are really interested in, is how our features (creating in the next tutorial), correlate with winning. We will skip ahead here and use a function to create our features for us, and then examine how the moving averages/different features correlate with winning.</p> <pre><code># Create a cleaned df of all of our data\npre_features_df = create_df('data/epl_data.csv')\n\n# Create our features\nfeatures = create_feature_df(pre_features_df)\n    Creating all games feature DataFrame\n\n    C:\\Users\\wardj\\Documents\\Betfair Public Github\\predictive-models\\epl\\data_preparation_functions.py:419: RuntimeWarning: invalid value encountered in double_scalars\n      .pipe(lambda df: (df.eloAgainst * df[goalsForOrAgainstCol]).sum() / df.eloAgainst.sum()))\n\n    Creating stats feature DataFrame\n    Creating odds feature DataFrame\n    Creating market values feature DataFrame\n    Filling NAs\n    Merging stats, odds and market values into one features DataFrame\n    Complete.\n</code></pre> <pre><code>features = (pre_features_df.assign(margin=lambda df: df.FTHG - df.FTAG)\n                           .loc[:, ['gameId', 'margin']]\n                           .pipe(pd.merge, features, on=['gameId']))\n</code></pre> <pre><code>features.corr().margin.sort_values(ascending=False)[:20]\n\n    margin                     1.000000\n    f_awayOdds                 0.413893\n    f_totalMktH%               0.330420\n    f_defMktH%                 0.325392\n    f_eloAgainstAway           0.317853\n    f_eloForHome               0.317853\n    f_midMktH%                 0.316080\n    f_attMktH%                 0.312262\n    f_sizeOfHandicapAway       0.301667\n    f_goalsForHome             0.298930\n    f_wtEloGoalsForHome        0.297157\n    f_shotsForHome             0.286239\n    f_cornersForHome           0.279917\n    f_gkMktH%                  0.274732\n    f_homeWinPc38Away          0.271326\n    f_homeWinPc38Home          0.271326\n    f_wtEloGoalsAgainstAway    0.269663\n    f_goalsAgainstAway         0.258418\n    f_cornersAgainstAway       0.257148\n    f_drawOdds                 0.256807\n    Name: margin, dtype: float64\n</code></pre> <p>As we can see away odds is most highly correlated to margin. This makes sense, as odds generally have most/all information included in the price. What is interesting is that elo seems to also be highly correlated, which is good news for our elo model that we made. Similarly, weighted goals and the the value of the defence relative to other teams ('defMktH%' etc.) is strongly correlated to margin.</p>"},{"location":"modelling/EPLmlPython/#02-data-preparation-feature-engineering","title":"02. Data Preparation &amp; Feature Engineering","text":"<p>Welcome to the second part of this Machine Learning Walkthrough. This tutorial will focus on data preparation and feature creation, before we dive into modelling in the next tutorial.</p> <p>Specifically, this tutorial will cover a few things:</p> <ol> <li>Data wrangling specifically for sport</li> <li>Feature creation - focussing on commonly used features in sports modelling, such as exponential moving averages</li> <li>Using functions to modularise the data preparation process</li> </ol>"},{"location":"modelling/EPLmlPython/#data-wrangling","title":"Data Wrangling","text":"<p>We will begin by utilising functions we have defined in our data_preparation_functions script to wrangle our data into a format that can be consumed by Machine Learning algorithms.</p> <p>A typical issue faced by aspect of modelling sport is the issue of Machine Learning algorithms requiring all features for the teams playing to be on the same row of a table, whereas when we actual calculate these features, we usually require the teams to be on separate rows as it makes it a lot easier to calculate typical features, such as expontentially weighted moving averages. We will explore this issue and show how we deal with issues like these.</p> <pre><code># Import libraries\nfrom data_preparation_functions import *\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', 100)\n</code></pre> <p>We have created some functions which prepare the data for you. For thoroughly commented explanation of how the functions work, read through the data_preparation_functions.py script along side this walkthrough.</p> <p>Essentially, each functions wrangles the data through a similar process. It first reads in the data from a csv file, then converts the columns to datatypes that we can work with, such as converting the Date column to a datetime data type. It then adds a Game ID column, so each game is easily identifiable and joined on. We then assign the DataFrame some other columns which may be useful, such as 'Year', 'Result' and 'homeWin'. Finally, we drop redundant column and return the DataFrame.</p> <p>Let us now create six different DataFrames, which we will use to create features. Later, we will join these features back into one main feature DataFrame.</p>"},{"location":"modelling/EPLmlPython/#create-6-distinct-dataframes","title":"Create 6 distinct DataFrames","text":"<pre><code># This table includes all of our data in one big DataFrame\ndf = create_df('data/epl_data.csv')\ndf.head(3)\n</code></pre> AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv&lt;2.5 BbAv&gt;2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx&lt;2.5 BbMx&gt;2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.0 2.38 8 A Wiley 2.75 3.25 2.4 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.0 2.10 8 M Riley 3.10 3.25 2.2 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.2 3.75 8 G Poll 1.80 3.30 4.5 2005 0506 3 0 1 away <pre><code># This includes only the typical soccer stats, like home corners, home shots on target etc.\nstats = create_stats_df('data/epl_data.csv')\nstats.head(3)\n</code></pre> gameId HomeTeam AwayTeam FTHG FTAG HTHG HTAG HS AS HST AST HF AF HC AC HY AY HR AR 0 1 West Ham Blackburn 3.0 1.0 0.0 1.0 13.0 11.0 5.0 5.0 11.0 14.0 2.0 6.0 0.0 1.0 0.0 1.0 1 2 Aston Villa Bolton 2.0 2.0 2.0 2.0 3.0 13.0 2.0 6.0 14.0 16.0 7.0 8.0 0.0 2.0 0.0 0.0 2 3 Everton Man United 0.0 2.0 0.0 1.0 10.0 12.0 5.0 5.0 15.0 14.0 8.0 6.0 3.0 1.0 0.0 0.0 <pre><code># This includes all of our betting related data, such as win/draw/lose odds, asian handicaps etc.\nbetting = create_betting_df('data/epl_data.csv')\nbetting.head(3)\n</code></pre> B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv&lt;2.5 BbAv&gt;2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx&lt;2.5 BbMx&gt;2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Day Div IWA IWD IWH LBA LBD LBH Month VCA VCD VCH Year homeWin awayWin result HomeTeam AwayTeam gameId 0 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 13 E0 2.7 3.0 2.3 2.75 3.0 2.38 8 2.75 3.25 2.4 2005 1 0 home West Ham Blackburn 1 1 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 13 E0 3.1 3.0 2.1 3.20 3.0 2.10 8 3.10 3.25 2.2 2005 0 0 draw Aston Villa Bolton 2 2 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 13 E0 1.8 3.1 3.8 1.83 3.2 3.75 8 1.80 3.30 4.5 2005 0 1 away Everton Man United 3 <pre><code># This includes all of the team information for each game.\nteam_info = create_team_info_df('data/epl_data.csv')\nteam_info.head(3)\n</code></pre> gameId Date season HomeTeam AwayTeam FTR HTR Referee 0 1 2005-08-13 0506 West Ham Blackburn H A A Wiley 1 2 2005-08-13 0506 Aston Villa Bolton D D M Riley 2 3 2005-08-13 0506 Everton Man United A A G Poll <pre><code># Whilst the other DataFrames date back to 2005, this DataFrame has data from 2001 to 2005.\nhistoric_games = create_historic_games_df('data/historic_games_pre2005.csv')\nhistoric_games.head(3)\n</code></pre> Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin 0 2001-08-18 Charlton Everton 1 2 -1 20012002 0 1 2001-08-18 Derby Blackburn 2 1 -1 20012002 1 2 2001-08-18 Leeds Southampton 2 0 -1 20012002 1 <pre><code># This is the historic_games DataFrame appended to the df DataFrame.\nall_games = create_all_games_df('data/epl_data.csv', 'data/historic_games_pre2005.csv')\nall_games.head(3)\n</code></pre> Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin awayWin homeWinPc5 homeWinPc38 awayWinPc5 awayWinPc38 gameIdHistoric 0 2001-08-18 Charlton Everton 1.0 2.0 -1 20012002 0 1 NaN NaN NaN NaN 1 1 2001-08-18 Derby Blackburn 2.0 1.0 -1 20012002 1 0 NaN NaN NaN NaN 2 2 2001-08-18 Leeds Southampton 2.0 0.0 -1 20012002 1 0 NaN NaN NaN NaN 3"},{"location":"modelling/EPLmlPython/#feature-creation","title":"Feature Creation","text":"<p>Now that we have all of our pre-prepared DataFrames, and we know that the data is clean, we can move onto feature creation. As is common practice with sports modelling, we are going to start by creating expontentially weighted moving averages (EMA) as features. To get a better understanding of how EMAs work, read here. </p> <p>In short, an EMA is like a simple moving average, except it weights recent instances more than older instances based on an alpha parameter. The documentation for the pandas (emw method)[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html] we will be using states that we can specify alpha in a number of ways. We will specify it in terms of span, where $\\alpha = 2 / (span+1), span \u2265 1 $.</p> <p>Let's first define a function which calculates the exponential moving average for each column in the stats DataFrame. We will then apply this function with other functions we have created, such as create_betting_features_ema, which creates moving averages of betting data.</p> <p>However, we must first change the structure of our data. Notice that currently each row has both the Home Team's data and the Away Team's data on a single row. This makes it difficult to calculate rolling averages, so we will restructure our DataFrames to ensure each row only contains single team's data. To do this, we will define a function, reate_multiline_df_stats.</p> <pre><code># Define a function which restructures our DataFrame\ndef create_multiline_df_stats(old_stats_df):\n    # Create a list of columns we want and their mappings to more interpretable names\n    home_stats_cols = ['HomeTeam', 'FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY',\n                       'HR', 'AR']\n\n    away_stats_cols = ['AwayTeam', 'FTAG', 'FTHG', 'HTAG', 'HTHG', 'AS', 'HS', 'AST', 'HST', 'AF', 'HF', 'AC', 'HC', 'AY', 'HY',\n                       'AR', 'HR']\n\n    stats_cols_mapping = ['team', 'goalsFor', 'goalsAgainst', 'halfTimeGoalsFor', 'halfTimeGoalsAgainst', 'shotsFor',\n                          'shotsAgainst', 'shotsOnTargetFor', 'shotsOnTargetAgainst', 'freesFor', 'freesAgainst', \n                          'cornersFor', 'cornersAgainst', 'yellowsFor', 'yellowsAgainst', 'redsFor', 'redsAgainst']\n\n    # Create a dictionary of the old column names to new column names\n    home_mapping = {old_col: new_col for old_col, new_col in zip(home_stats_cols, stats_cols_mapping)}\n    away_mapping = {old_col: new_col for old_col, new_col in zip(away_stats_cols, stats_cols_mapping)}\n\n    # Put each team onto an individual row\n    multi_line_stats = (old_stats_df[['gameId'] + home_stats_cols] # Filter for only the home team columns\n                    .rename(columns=home_mapping) # Rename the columns\n                    .assign(homeGame=1) # Assign homeGame=1 so that we can use a general function later\n                    .append((old_stats_df[['gameId'] + away_stats_cols]) # Append the away team columns\n                            .rename(columns=away_mapping) # Rename the away team columns\n                            .assign(homeGame=0), sort=True)\n                    .sort_values(by='gameId') # Sort the values\n                    .reset_index(drop=True))\n    return multi_line_stats\n</code></pre> <pre><code># Define a function which creates an EMA DataFrame from the stats DataFrame\ndef create_stats_features_ema(stats, span):\n    # Create a restructured DataFrames so that we can calculate EMA\n    multi_line_stats = create_multiline_df_stats(stats)\n\n    # Create a copy of the DataFrame\n    ema_features = multi_line_stats[['gameId', 'team', 'homeGame']].copy()\n\n    # Get the columns that we want to create EMA for\n    feature_names = multi_line_stats.drop(columns=['gameId', 'team', 'homeGame']).columns\n\n    # Loop over the features\n    for feature_name in feature_names:\n        feature_ema = (multi_line_stats.groupby('team')[feature_name] # Calculate the EMA\n                                                  .transform(lambda row: row.ewm(span=span, min_periods=2)\n                                                             .mean()\n                                                             .shift(1))) # Shift the data down 1 so we don't leak data\n        ema_features[feature_name] = feature_ema # Add the new feature to the DataFrame\n    return ema_features\n</code></pre> <pre><code># Apply the function\nstats_features = create_stats_features_ema(stats, span=5)\nstats_features.tail()\n</code></pre> gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9903 4952 Newcastle 1 4.301743 4.217300 11.789345 12.245066 0.797647 0.833658 0.644214 0.420832 2.323450e-10 3.333631e-01 11.335147 13.265955 3.211345 4.067990 1.848860 1.627140 9904 4953 Burnley 0 4.880132 5.165915 13.326703 8.800033 1.945502 0.667042 0.609440 0.529409 3.874405e-03 3.356120e-10 13.129631 10.642381 4.825874 3.970285 0.963527 0.847939 9905 4953 Fulham 1 4.550255 4.403060 10.188263 8.555589 2.531046 1.003553 0.860573 0.076949 1.002518e-04 8.670776e-03 17.463779 12.278877 8.334019 4.058213 0.980097 1.102974 9906 4954 Man United 1 3.832573 4.759683 11.640608 10.307946 1.397234 1.495032 1.034251 0.809280 6.683080e-05 1.320468e-05 8.963022 10.198642 3.216957 3.776900 1.040077 1.595650 9907 4954 Tottenham 0 3.042034 5.160211 8.991460 9.955635 1.332704 2.514789 0.573728 1.010491 4.522878e-08 1.354409e-05 12.543406 17.761004 3.757437 7.279845 1.478976 1.026601 <p>As we can see, we now have averages for each team. Let's create a quick table to see the top 10 teams' goalsFor average EMAs since 2005.</p> <pre><code>pd.DataFrame(stats_features.groupby('team')\n                           .goalsFor\n                           .mean()\n                           .sort_values(ascending=False)[:10])\n</code></pre> goalsFor team Man United 1.895026 Chelsea 1.888892 Arsenal 1.876770 Man City 1.835863 Liverpool 1.771125 Tottenham 1.655063 Leicester 1.425309 Blackpool 1.390936 Everton 1.387110 Southampton 1.288349"},{"location":"modelling/EPLmlPython/#optimising-alpha","title":"Optimising Alpha","text":"<p>It looks like Man United and Chelsea have been two of the best teams since 2005, based on goalsFor. Now that we have our stats features, we may be tempted to move on. However, we have arbitrarily chosen a span of 5. How do we know that this is the best value? We don't. Let's try and optimise this value. </p> <p>To do this, we will use a simple Logistic Regression model to create probabilistic predictions based on the stats features we created before. We will iterate a range of span values, from say, 3 to 15, and choose the value which produces a model with the lowest log loss, based on cross validation.</p> <p>To do this, we need to restructure our DataFrame back to how it was before.</p> <pre><code>def restructure_stats_features(stats_features):\n    non_features = ['homeGame', 'team', 'gameId']\n\n    stats_features_restructured = (stats_features.query('homeGame == 1')\n                                    .rename(columns={col: 'f_' + col + 'Home' for col in stats_features.columns if col not in non_features})\n                                    .rename(columns={'team': 'HomeTeam'})\n                                    .pipe(pd.merge, (stats_features.query('homeGame == 0')\n                                                        .rename(columns={'team': 'AwayTeam'})\n                                                        .rename(columns={col: 'f_' + col + 'Away' for col in stats_features.columns \n                                                                         if col not in non_features})), on=['gameId'])\n                                    .pipe(pd.merge, df[['gameId', 'result']], on='gameId')\n                                    .dropna())\n    return stats_features_restructured\n\nrestructure_stats_features(stats_features).head()\n</code></pre> gameId HomeTeam homeGame_x f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome AwayTeam homeGame_y f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway result 20 21 Birmingham 1 4.8 7.8 12.0 9.4 1.2 0.6 0.6 0.6 0.0 0.0 11.4 8.2 6.4 2.8 1.0 2.6 Middlesbrough 0 3.0 5.6 14.0 12.8 1.2 0.0 0.0 0.0 0.0 0.4 17.2 8.8 7.6 2.6 3.0 1.4 away 21 22 Portsmouth 1 2.6 4.6 21.8 16.6 2.0 0.6 1.0 0.0 0.0 0.0 8.0 10.4 3.6 4.0 3.2 1.8 Aston Villa 0 9.8 7.0 14.2 18.2 1.4 0.8 0.8 0.8 0.0 0.0 16.0 3.0 9.6 2.6 2.0 0.6 draw 22 23 Sunderland 1 5.0 5.0 11.6 18.0 1.8 0.4 1.0 0.4 0.4 0.6 14.6 6.0 5.2 3.2 1.2 2.6 Man City 0 7.8 3.6 8.6 12.4 0.6 1.2 0.6 0.6 0.0 0.0 10.6 11.4 2.4 6.8 3.0 1.4 away 23 24 Arsenal 1 3.0 7.4 17.0 18.6 0.6 0.8 0.0 0.0 0.4 0.0 6.2 11.4 4.0 6.6 1.6 1.8 Fulham 0 7.2 3.0 20.8 13.2 1.2 0.6 0.6 0.0 0.0 0.0 12.4 10.8 7.0 5.2 2.0 1.6 home 24 25 Blackburn 1 1.4 7.2 12.8 21.2 1.8 1.6 0.0 1.0 0.0 0.4 10.0 14.0 4.4 7.4 1.2 1.6 Tottenham 0 6.4 3.8 11.2 18.8 0.0 2.0 0.0 0.4 0.0 0.0 11.6 15.2 4.6 7.2 0.6 2.6 draw <p>Now let's write a function that optimises our span based on log loss of the output of a Logistic Regression model.</p> <pre><code>def optimise_alpha(features):\n    le = LabelEncoder()\n    y = le.fit_transform(features.result) # Encode the result from away, draw, home win to 0, 1, 2\n    X = features[[col for col in features.columns if col.startswith('f_')]] # Only get the features - these all start with f_\n    lr = LogisticRegression()\n\n    kfold = StratifiedKFold(n_splits=5)\n    ave_cv_score = cross_val_score(lr, X, y, scoring='neg_log_loss', cv=kfold).mean()\n    return ave_cv_score\n</code></pre> <pre><code>best_score = np.float('inf')\nbest_span = 0\ncv_scores = []\n\n# Iterate over a range of spans\nfor span in range(1, 120, 3):\n    stats_features = create_stats_features_ema(stats, span=span)\n    restructured_stats_features = restructure_stats_features(stats_features)\n    cv_score = optimise_alpha(restructured_stats_features)\n    cv_scores.append(cv_score)\n\n    if cv_score * -1 &lt; best_score:\n        best_score = cv_score * -1\n        best_span = span\n</code></pre> <pre><code>plt.style.use('ggplot')\nplt.plot(list(range(1, 120, 3)), (pd.Series(cv_scores)*-1)) # Plot our results\n\nplt.title(\"Optimising alpha\")\nplt.xlabel(\"Span\")\nplt.ylabel(\"Log Loss\")\nplt.show()\n\nprint(\"Our lowest log loss ({:2f}) occurred at a span of {}\".format(best_score, best_span))\n</code></pre> <p></p> <pre><code>Our lowest log loss (0.980835) occurred at a span of 55\n</code></pre> <p>The above method is just an example of how you can optimise hyparameters. Obviously this example has many limitations, such as attempting to optimise each statistic with the same alpha. However, for the rest of these tutorial series we will use this span value.</p> <p>Now let's create the rest of our features. For thorough explanations and the actual code behind some of the functions used, please refer to the data_preparation_functions.py script.</p>"},{"location":"modelling/EPLmlPython/#creating-our-features-dataframe","title":"Creating our Features DataFrame","text":"<p>We will utilise pre-made functions to create all of our features in just a few lines of code.</p> <p>As part of this process we will create features which include margin weighted elo, an exponential average for asian handicap data, and odds as features.</p> <p>Our Elo function is essentially the same as the one we created in the AFL tutorial; if you would like to know more about Elo models please read this article.</p> <p>Note that the cell below may take a few minutes to run.</p> <p><pre><code># Create feature DataFrames\nfeatures_all_games = create_all_games_features(all_games)\n</code></pre>     C:\\Users\\wardj\\Documents\\Betfair Public Github\\predictive-models\\epl\\data_preparation_functions.py:419: RuntimeWarning: invalid value encountered in double_scalars       .pipe(lambda df: (df.eloAgainst * df[goalsForOrAgainstCol]).sum() / df.eloAgainst.sum()))</p> <p>The features_all_games df includes elo for each team, as well as their win percentage at home and away over the past 5 and 38 games. For more information on how it was calculated, read through the data_preparation_functions script.</p> <pre><code>features_all_games.head(3)\n</code></pre> Date awayWin awayWinPc38 awayWinPc5 eloAgainst eloFor gameId gameIdHistoric goalsAgainst goalsFor homeGame homeWin homeWinPc38 homeWinPc5 season team wtEloGoalsFor wtEloGoalsAgainst 0 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 2.0 1.0 1 0 NaN NaN 20012002 Charlton NaN NaN 1 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 1.0 2.0 0 0 NaN NaN 20012002 Everton NaN NaN 2 2001-08-18 0 NaN NaN 1500.0 1500.0 -1 2 1.0 2.0 1 1 NaN NaN 20012002 Derby NaN NaN <p>The features_stats df includes all the expontential weighted averages for each stat in the stats df.</p> <pre><code># Create feature stats df\nfeatures_stats = create_stats_features_ema(stats, span=best_span)\nfeatures_stats.tail(3)\n</code></pre> gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9905 4953 Fulham 1 6.006967 5.045733 10.228997 9.965651 2.147069 1.093550 0.630485 0.364246 0.032937 0.043696 16.510067 11.718122 7.184386 4.645762 1.310424 1.389716 9906 4954 Man United 1 4.463018 5.461075 11.605712 10.870367 0.843222 1.586308 0.427065 0.730650 0.042588 0.027488 10.865754 13.003121 3.562675 4.626450 1.740735 1.712785 9907 4954 Tottenham 0 3.868619 6.362901 10.784145 10.140388 0.954928 2.100166 0.439129 0.799968 0.024351 0.026211 9.947515 16.460598 3.370010 6.136120 1.925005 1.364268 <p>The features_odds df includes a moving average of some of the odds data.</p> <pre><code># Create feature_odds df\nfeatures_odds = create_betting_features_ema(betting, span=10)\nfeatures_odds.tail(3)\n</code></pre> gameId team avAsianHandicapOddsAgainst avAsianHandicapOddsFor avgreaterthan2.5 avlessthan2.5 sizeOfHandicap 9905 4953 Fulham 1.884552 1.985978 1.756776 2.128261 0.502253 9906 4954 Man United 1.871586 2.031787 1.900655 1.963478 -0.942445 9907 4954 Tottenham 1.947833 1.919607 1.629089 2.383593 -1.235630 <p>The features market values has market values and the % of total market for each position. These values are in millions.</p> <pre><code># Create feature market values df\nfeatures_market_values = create_market_values_features(df) # This creates a df with one game per row\nfeatures_market_values.head(3)\n</code></pre> gameId Year HomeTeam AwayTeam defMktValH attMktValH gkMktValH totalMktValH midMktValH defMktValA attMktValA gkMktValA totalMktValA midMktValA attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% 0 1 2005 West Ham Blackburn 16.90 18.50 6.40 46.40 4.60 27.25 13.00 3.25 70.70 27.20 2.252911 1.583126 0.588168 3.477861 2.486940 4.010007 4.524247 2.297469 1.913986 2.916354 1 2 2005 Aston Villa Bolton 27.63 31.85 7.60 105.83 38.75 9.60 24.55 8.50 72.40 29.75 3.878659 2.989673 4.954673 3.803910 4.065926 1.412700 5.372543 6.008766 4.365456 2.986478 2 3 2005 Everton Man United 44.35 31.38 8.55 109.78 25.50 82.63 114.60 9.25 288.48 82.00 3.821423 13.955867 3.260494 10.484727 6.526378 12.159517 6.044111 6.538951 4.528392 11.899714 <pre><code>all_games_cols = ['Date', 'gameId', 'team', 'season', 'homeGame', 'homeWinPc38', 'homeWinPc5', 'awayWinPc38', 'awayWinPc5', 'eloFor', 'eloAgainst', 'wtEloGoalsFor', 'wtEloGoalsAgainst']\n\n# Join the features together\nfeatures_multi_line = (features_all_games[all_games_cols]\n                                         .pipe(pd.merge, features_stats.drop(columns='homeGame'), on=['gameId', 'team'])\n                                         .pipe(pd.merge, features_odds, on=['gameId', 'team']))\n</code></pre> <pre><code># Put each instance on an individual row\nfeatures_with_na = put_features_on_one_line(features_multi_line)\n\nmarket_val_feature_names = ['attMktH%', 'attMktA%', 'midMktH%', 'midMktA%', 'defMktH%', 'defMktA%', 'gkMktH%', 'gkMktA%', 'totalMktH%', 'totalMktA%']\n\n# Merge our team values dataframe to features and result from df\nfeatures_with_na = (features_with_na.pipe(pd.merge, (features_market_values[market_val_feature_names + ['gameId']])\n                                                      .rename({col: 'f_' + col for col in market_val_feature_names}), on='gameId')\n                            .pipe(pd.merge, df[['HomeTeam', 'AwayTeam', 'gameId', 'result', 'B365A', 'B365D', 'B365H']], on=['HomeTeam', 'AwayTeam', 'gameId']))\n\n# Drop NAs from calculating the rolling averages - don't drop Win Pc 38 and Win Pc 5 columns\nfeatures = features_with_na.dropna(subset=features_with_na.drop(columns=[col for col in features_with_na.columns if 'WinPc' in col]).columns)\n\n# Fill NAs for the Win Pc columns\nfeatures = features.fillna(features.mean())\n</code></pre> <pre><code>features.head(3)\n</code></pre> Date gameId HomeTeam season homeGame f_homeWinPc38Home f_homeWinPc5Home f_awayWinPc38Home f_awayWinPc5Home f_eloForHome f_eloAgainstHome f_wtEloGoalsForHome f_wtEloGoalsAgainstHome f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome f_avAsianHandicapOddsAgainstHome f_avAsianHandicapOddsForHome f_avgreaterthan2.5Home f_avlessthan2.5Home f_sizeOfHandicapHome AwayTeam f_homeWinPc38Away f_homeWinPc5Away f_awayWinPc38Away f_awayWinPc5Away f_eloForAway f_eloAgainstAway f_wtEloGoalsForAway f_wtEloGoalsAgainstAway f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway f_avAsianHandicapOddsAgainstAway f_avAsianHandicapOddsForAway f_avgreaterthan2.5Away f_avlessthan2.5Away f_sizeOfHandicapAway attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% result B365A B365D B365H 20 2005-08-23 21 Birmingham 0506 1 0.394737 0.4 0.263158 0.2 1478.687038 1492.866048 1.061763 1.260223 4.981818 7.527273 12.000000 9.945455 1.018182 0.509091 0.509091 0.509091 0.000000 0.000000 11.945455 8.018182 6.490909 2.981818 1.000000 2.509091 1.9090 1.9455 2.0510 1.6735 -0.1375 Middlesbrough 0.394737 0.4 0.263158 0.2 1492.866048 1478.687038 1.12994 1.279873 2.545455 5.509091 13.545455 13.436364 1.018182 0.000000 0.000000 0.000000 0.0 0.490909 17.018182 8.072727 7.509091 2.509091 3.0 1.490909 1.9395 1.9095 2.0035 1.7155 0.3875 5.132983 5.260851 3.341048 4.289788 3.502318 4.168935 2.332815 3.216457 3.934396 4.522205 away 2.75 3.2 2.50 21 2005-08-23 22 Portsmouth 0506 1 0.447368 0.4 0.263158 0.4 1405.968416 1489.229314 1.147101 1.503051 2.509091 4.963636 21.981818 16.054545 2.000000 0.509091 1.000000 0.000000 0.000000 0.000000 8.454545 10.490909 3.963636 4.454545 3.018182 1.527273 1.8965 1.9690 2.0040 1.7005 0.2500 Aston Villa 0.447368 0.4 0.263158 0.4 1489.229314 1405.968416 1.17516 1.263229 9.527273 7.000000 14.472727 17.563636 1.490909 0.981818 0.981818 0.981818 0.0 0.000000 15.545455 3.000000 9.054545 2.509091 2.0 0.509091 1.8565 1.9770 1.8505 1.8485 0.7125 3.738614 3.878659 4.494368 4.954673 2.884262 4.065926 3.746642 5.372543 3.743410 4.365456 draw 2.75 3.2 2.50 22 2005-08-23 23 Sunderland 0506 1 0.236842 0.0 0.236842 0.4 1277.888970 1552.291880 0.650176 1.543716 5.000000 5.000000 12.418182 17.545455 1.981818 0.490909 1.000000 0.490909 0.490909 0.509091 14.509091 6.909091 5.018182 3.927273 1.018182 2.509091 1.8520 1.9915 1.8535 1.8500 0.7125 Man City 0.236842 0.0 0.236842 0.4 1552.291880 1277.888970 1.28875 1.287367 7.527273 3.509091 8.963636 12.490909 0.509091 1.018182 0.509091 0.509091 0.0 0.000000 10.963636 11.945455 2.490909 6.981818 3.0 1.490909 1.8150 2.0395 2.0060 1.7095 -0.2000 0.706318 3.750792 1.476812 1.070209 2.634096 4.455890 0.777605 4.913050 1.499427 3.151477 away 2.50 3.2 2.75 <p>We now have a features DataFrame ready, with all the feature columns beginning with the \"f_\". In the next section, we will walk through the modelling process to try and find the best type of model to use.</p>"},{"location":"modelling/EPLmlPython/#03-model-building-hyperparameter-tuning","title":"03. Model Building &amp; Hyperparameter Tuning","text":"<p>Welcome to the third part of this Machine Learning Walkthrough. This tutorial will focus on the model building process, including how to tune hyperparameters. In the next tutorial, we will create weekly predictions based on the model we have created here.</p> <p>Specifically, this tutorial will cover a few things:</p> <ol> <li>Choosing which Machine Learning algorithm to use from a variety of choices</li> <li>Hyperparameter Tuning</li> <li>Overfitting/Underfitting</li> </ol>"},{"location":"modelling/EPLmlPython/#choosing-an-algorithm","title":"Choosing an Algorithm","text":"<p>The best way to decide on specific algorithm to use, is to try them all! To do this, we will define a function which we first used in our AFL Predictions tutorial. This will iterate over a number of algorithms and give us a good indication of which algorithms are suited for this dataset and exercise.</p> <p>Let's first use grab the features we created in the last tutorial. This may take a minute or two to run.</p> <pre><code>## Import libraries\nfrom data_preparation_functions import *\nimport pandas as pd\nimport numpy as np\nimport matplotlib as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn import linear_model, tree, discriminant_analysis, naive_bayes, ensemble, gaussian_process\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.metrics import log_loss, confusion_matrix\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 100)\n</code></pre> <pre><code>features = create_feature_df()\n    Creating all games feature DataFrame\n    Creating stats feature DataFrame\n    Creating odds feature DataFrame\n    Creating market values feature DataFrame\n    Filling NAs\n    Merging stats, odds and market values into one features DataFrame\n    Complete.\n</code></pre> <p>To start our modelling process, we need to make a training set, a test set and a holdout set. As we are using cross validation, we will make our training set all of the seasons up until 2017/18, and we will use the 2017/18 season as the test set.</p> <pre><code>feature_list = [col for col in features.columns if col.startswith(\"f_\")]\nbetting_features = []\n\nle = LabelEncoder() # Initiate a label encoder to transform the labels 'away', 'draw', 'home' to 0, 1, 2\n\n# Grab all seasons except for 17/18 to use CV with\nall_x = features.loc[features.season != '1718', ['gameId'] + feature_list]\nall_y = features.loc[features.season != '1718', 'result']\nall_y = le.fit_transform(all_y)\n\n# Create our training vector as the seasons except 16/17 and 17/18\ntrain_x = features.loc[~features.season.isin(['1617', '1718']), ['gameId'] + feature_list]\ntrain_y = le.transform(features.loc[~features.season.isin(['1617', '1718']), 'result'])\n\n# Create our holdout vectors as the 16/17 season\nholdout_x = features.loc[features.season == '1617', ['gameId'] + feature_list]\nholdout_y = le.transform(features.loc[features.season == '1617', 'result'])\n\n# Create our test vectors as the 17/18 season\ntest_x = features.loc[features.season == '1718', ['gameId'] + feature_list]\ntest_y = le.transform(features.loc[features.season == '1718', 'result'])\n</code></pre> <pre><code># Create a list of standard classifiers\nclassifiers = [\n\n    #GLM\n    linear_model.LogisticRegressionCV(),\n\n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n\n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n\n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n#     xgb.XGBClassifier()    \n]\n</code></pre> <pre><code>def find_best_algorithms(classifier_list, X, y):\n    # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling\n    # Cross validate model with Kfold stratified cross validation\n    kfold = StratifiedKFold(n_splits=5)\n\n    # Grab the cross validation scores for each algorithm\n    cv_results = [cross_val_score(classifier, X, y, scoring = \"neg_log_loss\", cv = kfold) for classifier in classifier_list]\n    cv_means = [cv_result.mean() * -1 for cv_result in cv_results]\n    cv_std = [cv_result.std() for cv_result in cv_results]\n    algorithm_names = [alg.__class__.__name__ for alg in classifiers]\n\n    # Create a DataFrame of all the CV results\n    cv_results = pd.DataFrame({\n        \"Mean Log Loss\": cv_means,\n        \"Log Loss Std\": cv_std,\n        \"Algorithm\": algorithm_names\n    }).sort_values(by='Mean Log Loss')\n    return cv_results\n</code></pre> <pre><code>algorithm_results = find_best_algorithms(classifiers, all_x, all_y)\n</code></pre> <pre><code>algorithm_results\n</code></pre> Mean Log Loss Log Loss Std Algorithm 0 0.966540 0.020347 LogisticRegressionCV 3 0.986679 0.015601 LinearDiscriminantAnalysis 1 1.015197 0.017466 BernoulliNB 10 1.098612 0.000000 GaussianProcessClassifier 5 1.101281 0.044383 AdaBoostClassifier 8 1.137778 0.153391 GradientBoostingClassifier 7 2.093981 0.284831 ExtraTreesClassifier 9 2.095088 0.130367 RandomForestClassifier 6 2.120571 0.503132 BaggingClassifier 4 4.065796 1.370119 QuadraticDiscriminantAnalysis 2 5.284171 0.826991 GaussianNB <p>We can see that LogisticRegression seems to perform the best out of all the algorithms, and some algorithms have a very high log loss. This is most likely due to overfitting. It would definitely be useful to condense our features down to reduce the dimensionality of the dataset.</p>"},{"location":"modelling/EPLmlPython/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>For now, however, we will use logistic regression. Let's first try and tune a logistic regression model with cross validation. To do this, we will use grid search. Grid search essentially tries out each combination of values and finds the model with the lowest error metric, which in our case is log loss. 'C' in logistic regression determines the amount of regularization. Lower values increase regularization.</p> <pre><code># Define our parameters to run a grid search over\nlr_grid = {\n    \"C\": [0.0001, 0.01, 0.05, 0.2, 1],\n    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]\n}\n\nkfold = StratifiedKFold(n_splits=5)\n\ngs = GridSearchCV(LogisticRegression(), param_grid=lr_grid, cv=kfold, scoring='neg_log_loss')\ngs.fit(all_x, all_y)\nprint(\"Best log loss: {}\".format(gs.best_score_ *-1))\nbest_lr_params = gs.best_params_\n\n  Best log loss: 0.9669551970849734\n</code></pre>"},{"location":"modelling/EPLmlPython/#defining-a-baseline","title":"Defining a Baseline","text":"<p>We should also define a baseline, as we don't really know if our log loss is good or bad. Randomly assigning a 1/3 chance to each selection yields a log loss of log3 = 1.09. However, what we are really interested in, is how our model performs relative to the odds. So let's find the log loss of the odds.</p> <pre><code># Finding the log loss of the odds\nlog_loss(all_y, 1 / all_x[['f_awayOdds', 'f_drawOdds', 'f_homeOdds']])\n\n  0.9590114943474463\n</code></pre> <p>This is good news: our algorithm almost beats the bookies in terms of log loss. It would be great if we could beat this result.</p>"},{"location":"modelling/EPLmlPython/#analysing-the-errors-made","title":"Analysing the Errors Made","text":"<p>Now that we have a logistic regression model tuned, let's see what type of errors it made. To do this we will look at the confusion matrix produced when we predict our holdout set.</p> <pre><code>lr = LogisticRegression(**best_lr_params) # Instantiate the model\nlr.fit(train_x, train_y) # Fit our model\nlr_predict = lr.predict(holdout_x) # Predict the holdout values\n</code></pre> <pre><code># Create a confusion matrix\nc_matrix = (pd.DataFrame(confusion_matrix(holdout_y, lr_predict), columns=le.classes_, index=le.classes_)\n .rename_axis('Actual')\n .rename_axis('Predicted', axis='columns'))\n\nc_matrix\n</code></pre> Predicted away draw home Actual away 77 0 32 draw 26 3 55 home 33 7 147 <p>As we can see, when we predicted 'away' as the result, we correctly predicted 79 / 109 results, a hit rate of 70.6%. However, when we look at our draw hit rate, we only predicted 6 / 84 correctly, meaning we only had a hit rate of around 8.3%. For a more in depth analysis of our predictions, please skip to the Analysing Predictions &amp; Staking Strategies section of the tutorial.</p> <p>Before we move on, however, let's use our model to predict the 17/18 season and compare how we went with the odds.</p> <p><pre><code># Get test predictions\n\ntest_lr = LogisticRegression(**best_lr_params)\ntest_lr.fit(all_x, all_y)\ntest_predictions_probs = lr.predict_proba(test_x)\ntest_predictions = lr.predict(test_x)\n\ntest_ll = log_loss(test_y, test_predictions_probs)\ntest_accuracy = (test_predictions == test_y).mean()\n\nprint(\"Our predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.2f}\".format(test_ll, test_accuracy))\n</code></pre>     Our predictions for the 2017/18 season have a log loss of: 0.95767 and an accuracy of: 0.56</p> <p><pre><code># Get accuracy and log loss based on the odds\nodds_ll = log_loss(test_y, 1 / test_x[['f_awayOdds', 'f_drawOdds', 'f_homeOdds']])\n\nodds_predictions = test_x[['f_awayOdds', 'f_drawOdds', 'f_homeOdds']].apply(lambda row: row.idxmin()[2:6], axis=1).values\nodds_accuracy = (odds_predictions == le.inverse_transform(test_y)).mean()\n\nprint(\"Odds predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.3f}\".format(odds_ll, odds_accuracy))\n</code></pre>     Odds predictions for the 2017/18 season have a log loss of: 0.94635 and an accuracy of: 0.545</p>"},{"location":"modelling/EPLmlPython/#results","title":"Results","text":"<p>There we have it! The odds predicted 54.5% of EPL games correctly in the 2017/18 season, whilst our model predicted 54% correctly. This is a decent result for the first iteration of our model. In future iterations, we could wait a certain number of matches each season and calculate EMAs for on those first n games. This may help the issue of players switching clubs and teams becoming relatively stronger/weaker compared to previous seasons.</p>"},{"location":"modelling/EPLmlPython/#04-weekly-predictions","title":"04. Weekly Predictions","text":"<p>Welcome to the final part of this Machine Learning Walkthrough. This tutorial will be a walk through of creating weekly EPL predictions from the basic logistic regression model we built in the previous tutorial. We will then analyse our predictions and create staking strategies in the next tutorial.</p> <p>Specifically, this tutorial will cover a few things:</p> <ol> <li>Obtaining Weekly Odds / Game Info Using Betfair's API</li> <li>Data Wrangling This Week's Game Info Into Our Feature Set</li> </ol>"},{"location":"modelling/EPLmlPython/#obtaining-weekly-odds-game-info-using-betfairs-api","title":"Obtaining Weekly Odds / Game Info Using Betfair's API","text":"<p>The first thing we need to do to create weekly predictions is get both the games being played this week, as well as match odds from Betfair to be used as features.</p> <p>To make this process easier, I have created a csv file with the fixture for the 2018/19 season. Let's load that now.</p> <pre><code>## Import libraries\nimport pandas as pd\nfrom weekly_prediction_functions import *\nfrom data_preparation_functions import *\nfrom sklearn.metrics import log_loss, confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 100)\n</code></pre> <pre><code>fixture = (pd.read_csv('data/fixture.csv')\n              .assign(Date=lambda df: pd.to_datetime(df.Date)))\n</code></pre> <pre><code>fixture.head()\n</code></pre> Date Time (AEST) HomeTeam AwayTeam Venue TV Year round season 0 2018-08-11 5:00 AM Man United Leicester Old Trafford, Manchester Optus, Fox Sports (delay) 2018 1 1819 1 2018-08-11 9:30 PM Newcastle Tottenham St.James\u2019 Park, Newcastle Optus, SBS 2018 1 1819 2 2018-08-12 12:00 AM Bournemouth Cardiff Vitality Stadium, Bournemouth Optus 2018 1 1819 3 2018-08-12 12:00 AM Fulham Crystal Palace Craven Cottage, London Optus 2018 1 1819 4 2018-08-12 12:00 AM Huddersfield Chelsea John Smith\u2019s Stadium, Huddersfield Optus, Fox Sports (delay) 2018 1 1819 <p>Now we are going to connect to the API and retrieve game level information for the next week. To do this, we will use an R script. If you are not familiar with R, don't worry, it is relatively simple to read through. For this, we will run the script weekly_game_info_puller.R. Go ahead and run that script now.</p> <p>Note that for this step, you will require a Betfair API App Key. If you don't have one, visit this page and follow the instructions.</p> <p>I will upload an updated weekly file, so you can follow along regardless of if you have an App Key or not. Let's load that file in now.</p> <pre><code>game_info = create_game_info_df(\"data/weekly_game_info.csv\")\n</code></pre> <pre><code>game_info.head(3)\n</code></pre> AwayTeam HomeTeam awaySelectionId drawSelectionId homeSelectionId draw marketId marketStartTime totalMatched eventId eventName homeOdds drawOdds awayOdds competitionId Date localMarketStartTime 0 Arsenal Cardiff 1096 58805 79343 The Draw 1.146897152 2018-09-02 12:30:00 30123.595116 28852020 Cardiff v Arsenal 7.00 4.3 1.62 10932509 2018-09-02 Sun September  2, 10:30PM 1 Bournemouth Chelsea 1141 58805 55190 The Draw 1.146875421 2018-09-01 14:00:00 30821.329656 28851426 Chelsea v Bournemouth 1.32 6.8 12.00 10932509 2018-09-01 Sun September  2, 12:00AM 2 Fulham Brighton 56764 58805 18567 The Draw 1.146875746 2018-09-01 14:00:00 16594.833096 28851429 Brighton v Fulham 2.36 3.5 3.50 10932509 2018-09-01 Sun September  2, 12:00AM <p>Finally, we will use the API to grab the weekly odds. This R script is also provided, but I have also included the weekly odds csv for convenience.</p> <pre><code>odds = (pd.read_csv('data/weekly_epl_odds.csv')\n           .replace({\n                'Man Utd': 'Man United',\n                'C Palace': 'Crystal Palace'}))\n</code></pre> <pre><code>odds.head(3)\n</code></pre> HomeTeam AwayTeam f_homeOdds f_drawOdds f_awayOdds 0 Leicester Liverpool 7.80 5.1 1.48 1 Brighton Fulham 2.36 3.5 3.50 2 Everton Huddersfield 1.54 4.4 8.20"},{"location":"modelling/EPLmlPython/#data-wrangling-this-weeks-game-info-into-our-feature-set","title":"Data Wrangling This Week's Game Info Into Our Feature Set","text":"<p>Now we have the arduous task of wrangling all of this info into a feature set that we can use to predict this week's games. Luckily our functions we created earlier should work if we just append the non-features to our main dataframe.</p> <pre><code>df = create_df('data/epl_data.csv')\n</code></pre> <pre><code>df.head()\n</code></pre> AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv&lt;2.5 BbAv&gt;2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx&lt;2.5 BbMx&gt;2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.50 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.90 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.00 2.38 8 A Wiley 2.75 3.25 2.40 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.30 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.40 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.00 2.10 8 M Riley 3.10 3.25 2.20 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.00 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.80 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.20 3.75 8 G Poll 1.80 3.30 4.50 2005 0506 3 0 1 away 3 6.0 13.0 0.0 7.0 4.0 2.0 Birmingham 2.87 3.25 2.37 2.80 3.20 2.30 56.0 21.0 0.00 1.69 2.04 2.87 2.05 1.81 3.16 2.31 1.77 2.24 3.05 2.11 1.85 3.30 2.60 36.0 2005-08-13 13 E0 0.0 0.0 D 6.0 12.0 0.0 15.0 7.0 0.0 0.0 D 1.0 Fulham 2.9 3.0 2.2 2.88 3.00 2.25 8 R Styles 2.80 3.25 2.35 2005 0506 4 0 0 draw 4 6.0 11.0 0.0 13.0 3.0 3.0 West Brom 5.00 3.40 1.72 4.80 3.45 1.65 55.0 23.0 -0.75 1.77 1.94 4.79 1.76 2.10 3.38 1.69 1.90 2.10 5.60 1.83 2.19 3.63 1.80 36.0 2005-08-13 13 E0 0.0 0.0 D 3.0 13.0 0.0 15.0 8.0 0.0 0.0 D 2.0 Man City 4.2 3.2 1.7 4.50 3.25 1.67 8 C Foy 5.00 3.25 1.75 2005 0506 5 0 0 draw <p>Now we need to specify which game week we would like to predict. We will then filter the fixture for this game week and append this info to the main DataFrame</p> <pre><code>round_to_predict = int(input(\"Which game week would you like to predict? Please input next week's Game Week\\n\"))\n</code></pre> <pre><code>Which game week would you like to predict? Please input next week's Game Week\n4\n</code></pre> <pre><code>future_predictions = (fixture.loc[fixture['round'] == round_to_predict, ['Date', 'HomeTeam', 'AwayTeam', 'season']]\n                             .pipe(pd.merge, odds, on=['HomeTeam', 'AwayTeam'])\n                             .rename(columns={\n                                 'f_homeOdds': 'B365H',\n                                 'f_awayOdds': 'B365A',\n                                 'f_drawOdds': 'B365D'})\n                             .assign(season=lambda df: df.season.astype(str)))\n</code></pre> <pre><code>df_including_future_games = (pd.read_csv('data/epl_data.csv', dtype={'season': str})\n                .assign(Date=lambda df: pd.to_datetime(df.Date))\n                .pipe(lambda df: df.dropna(thresh=len(df) - 2, axis=1))  # Drop cols with NAs\n                .dropna(axis=0)  # Drop rows with NAs\n                .sort_values('Date')\n                .append(future_predictions, sort=True)\n                .reset_index(drop=True)\n                .assign(gameId=lambda df: list(df.index + 1),\n                            Year=lambda df: df.Date.apply(lambda row: row.year),\n                            homeWin=lambda df: df.apply(lambda row: 1 if row.FTHG &gt; row.FTAG else 0, axis=1),\n                            awayWin=lambda df: df.apply(lambda row: 1 if row.FTAG &gt; row.FTHG else 0, axis=1),\n                            result=lambda df: df.apply(lambda row: 'home' if row.FTHG &gt; row.FTAG else ('draw' if row.FTHG == row.FTAG else 'away'), axis=1)))\n</code></pre> <pre><code>df_including_future_games.tail(12)\n</code></pre> AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv&lt;2.5 BbAv&gt;2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx&lt;2.5 BbMx&gt;2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 4952 4.0 8.0 0.0 12.0 2.0 1.0 Burnley 4.33 3.40 2.00 4.0 3.3 2.00 39.0 20.0 -0.25 1.65 2.22 4.14 2.22 1.69 3.36 1.98 1.72 2.31 4.5 2.32 1.74 3.57 2.04 36.0 2018-08-26 26.0 E0 2.0 4.0 H 6.0 11.0 0.0 25.0 12.0 2.0 3.0 H 2.0 Fulham 4.10 3.35 1.97 3.90 3.2 2.00 8.0 D Coote 4.33 3.4 2.0 2018 1819 4953 1 0 home 4953 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.9 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.0 1.76 2.25 3.40 2.67 40.0 2018-08-27 27.0 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.60 2.75 3.2 2.55 8.0 C Pawson 2.90 3.3 2.6 2018 1819 4954 0 1 away 4954 NaN NaN NaN NaN NaN NaN Liverpool 1.48 5.10 7.80 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-01 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Leicester NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4955 0 0 away 4955 NaN NaN NaN NaN NaN NaN Fulham 3.50 3.50 2.36 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Brighton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4956 0 0 away 4956 NaN NaN NaN NaN NaN NaN Man United 1.70 3.90 6.60 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Burnley NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4957 0 0 away 4957 NaN NaN NaN NaN NaN NaN Bournemouth 12.00 6.80 1.32 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Chelsea NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4958 0 0 away 4958 NaN NaN NaN NaN NaN NaN Southampton 4.50 3.55 2.04 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Crystal Palace NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4959 0 0 away 4959 NaN NaN NaN NaN NaN NaN Huddersfield 8.20 4.40 1.54 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Everton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4960 0 0 away 4960 NaN NaN NaN NaN NaN NaN Wolves 2.98 3.50 2.62 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN West Ham NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4961 0 0 away 4961 NaN NaN NaN NaN NaN NaN Newcastle 32.00 12.50 1.12 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Man City NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4962 0 0 away 4962 NaN NaN NaN NaN NaN NaN Arsenal 1.62 4.30 7.00 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Cardiff NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4963 0 0 away 4963 NaN NaN NaN NaN NaN NaN Tottenham 1.68 4.30 5.90 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-03 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Watford NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4964 0 0 away <p>As we can see, what we have done is appended the Game information to our main DataFrame. The rest of the info is left as NAs, but this will be filled when we created our rolling average features. This is a 'hacky' type of way to complete this task, but works well as we can use the same functions that we created in the previous tutorials on this DataFrame. We now need to add the odds from our odds DataFrame, then we can just run our create features functions as usual.</p>"},{"location":"modelling/EPLmlPython/#predicting-next-gameweeks-results","title":"Predicting Next Gameweek's Results","text":"<p>Now that we have our feature DataFrame, all we need to do is split the feature DataFrame up into a training set and next week's games, then use the model we tuned in the last tutorial to create predictions!</p> <pre><code>features = create_feature_df(df=df_including_future_games)\n\n    Creating all games feature DataFrame\n    Creating stats feature DataFrame\n    Creating odds feature DataFrame\n    Creating market values feature DataFrame\n    Filling NAs\n    Merging stats, odds and market values into one features DataFrame\n    Complete.\n</code></pre> <pre><code># Create a feature DataFrame for this week's games.\nproduction_df = pd.merge(future_predictions, features, on=['Date', 'HomeTeam', 'AwayTeam', 'season'])\n</code></pre> <pre><code># Create a training DataFrame\ntraining_df = features[~features.gameId.isin(production_df.gameId)]\n</code></pre> <pre><code>feature_names = [col for col in training_df if col.startswith('f_')]\n\nle = LabelEncoder()\ntrain_y = le.fit_transform(training_df.result)\ntrain_x = training_df[feature_names]\n</code></pre> <pre><code>lr = LogisticRegression(C=0.01, solver='liblinear')\nlr.fit(train_x, train_y)\npredicted_probs = lr.predict_proba(production_df[feature_names])\npredicted_odds = 1 / predicted_probs\n</code></pre> <pre><code># Assign the modelled odds to our predictions df\npredictions_df = (production_df.loc[:, ['Date', 'HomeTeam', 'AwayTeam', 'B365H', 'B365D', 'B365A']]\n                               .assign(homeModelledOdds=[i[2] for i in predicted_odds],\n                                      drawModelledOdds=[i[1] for i in predicted_odds],\n                                      awayModelledOdds=[i[0] for i in predicted_odds])\n                               .rename(columns={\n                                   'B365H': 'BetfairHomeOdds',\n                                   'B365D': 'BetfairDrawOdds',\n                                   'B365A': 'BetfairAwayOdds'}))\n</code></pre> <pre><code>predictions_df\n</code></pre> Date HomeTeam AwayTeam BetfairHomeOdds BetfairDrawOdds BetfairAwayOdds homeModelledOdds drawModelledOdds awayModelledOdds 0 2018-09-01 Leicester Liverpool 7.80 5.10 1.48 5.747661 5.249857 1.573478 1 2018-09-02 Brighton Fulham 2.36 3.50 3.50 2.183193 3.803120 3.584057 2 2018-09-02 Burnley Man United 6.60 3.90 1.70 5.282620 4.497194 1.699700 3 2018-09-02 Chelsea Bournemouth 1.32 6.80 12.00 1.308366 6.079068 14.047070 4 2018-09-02 Crystal Palace Southampton 2.04 3.55 4.50 2.202871 4.213695 3.239122 5 2018-09-02 Everton Huddersfield 1.54 4.40 8.20 1.641222 3.759249 8.020055 6 2018-09-02 West Ham Wolves 2.62 3.50 2.98 1.999816 4.000456 4.000279 7 2018-09-02 Man City Newcastle 1.12 12.50 32.00 1.043103 29.427939 136.231983 8 2018-09-02 Cardiff Arsenal 7.00 4.30 1.62 6.256929 4.893445 1.572767 9 2018-09-03 Watford Tottenham 5.90 4.30 1.68 5.643663 4.338926 1.688224 <p>Above are the predictions for this Gameweek's matches. </p>"},{"location":"modelling/EPLmlPython/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"modelling/SpringRacingDatathon/","title":"Betfair\u2019s 2023 Spring Racing Data Modelling Competition","text":""},{"location":"modelling/SpringRacingDatathon/#registrations","title":"Registrations","text":"<p>Registrations have now closed</p>"},{"location":"modelling/SpringRacingDatathon/#the-competition","title":"The Competition","text":"<p>Do you think you have what it takes to model Victorian thoroughbred racing this Spring? Betfair is giving you the chance to show off your data modelling skills with a prize pool of $50,000! </p> <p>The 2023 Spring Racing Data Modelling Competition will see Competition Entrants provided with a bespoke set of Punting Form historic thoroughbred data. </p> <p>The goal from there is to use the provided data (or any other data) to build your own model, to predict the winning probability of each selection, on nominated race days and meetings (see below) starting on Saturday 28 October 2023 and concluding on Saturday 18 November 2023. </p> <p>With $50,000 in prizes up for grabs, we\u2019re challenging you to use your modelling skills to your advantage, be that by building your first predictive racing model, improving on an existing design or have a go at something different by adapting your data skills from other fields. </p> <p>Time to get modelling!</p> <p>Progressive updates will be posted here after the conclusion of each race day and in the Betfair Quants Discord Server.</p> <p>For an invite to the discord server, please fill out the form here</p> <p>Please direct all questions and submissions to datathon@betfair.com.au.</p>"},{"location":"modelling/SpringRacingDatathon/#the-specifics","title":"The Specifics","text":"<p>The Terms and Conditions for the 2023 Spring Racing Data Modelling Competition can be viewed here</p>"},{"location":"modelling/SpringRacingDatathon/#prizes","title":"Prizes","text":"<p>$50,000 in prizes are up for grabs! See the list below for the prizes each placing at the end of the Competition will receive:</p> Place Prize 1 $25,000.00 2 $10,000.00 3 $5000.00 4 $2000.00 5 $2000.00 Other Prize Winners Prize Lowest Weekly Score Week 1 $1000.00 Lowest Weekly Score Week 2 $1000.00 Lowest Weekly Score Week 3 $1000.00 Lowest Weekly Score Week 4 $1000.00 Best New Modeller $2000.00 Total Prize Pool $50,000.00 <p>(for further information see the Terms and Conditions) </p> <p>Prize winners will be announced following the final day of racing, and all prizes will be distributed after the conclusion of the final day of racing. </p>"},{"location":"modelling/SpringRacingDatathon/#competition-rules","title":"Competition Rules","text":"<p>The 2023 Spring Racing Data Modelling Competition will see Competition Entrants provided with a bespoke set of historic form data in partnership with Punting Form </p> <p>The goal from there is to use the provided data (or any other data) to build your own model to predict the probability of a horse winning a given race on the nominated race day.</p> <p>The nominated race days and meetings are:</p> <ul> <li>Saturday October 28th, 2023 \u2013 Mooney Valley \u2013 ALL races; </li> <li>Saturday November 4th, 2023 \u2013 Flemington \u2013 ALL races;</li> <li>Saturday November 11th, 2023 \u2013 Flemington \u2013 ALL races; and </li> <li>Saturday November 18th, 2023 \u2013 Caulfield \u2013 ALL races. </li> </ul> <p>How you go about building the model is entirely up to you \u2013 do you want to build an Elo model, a regression-based model, a Machine Learning algorithm or something entirely different? Get as innovative with it as you want! </p> <p>Your set of predictions will be due before 10:59am AEDT on the nominated race day. All submissions must be emailed to datathon@betfair.com.au.</p> <p>See the Terms and Conditions for full Competition rules.</p>"},{"location":"modelling/SpringRacingDatathon/#submissions-judging","title":"Submissions &amp; Judging","text":"<p>The Datathon will be judged using the log loss method where the log loss score is calculated using the difference between the predicted probability and the actual outcome. The scoring calculation will involve finding the SUM of the log loss across all runners in each race and then finding the AVERAGE log loss of each race. The entrant with the lowest average log loss across all races over the nominated race days will be victorious.</p> <p></p> <p>One submission file template will be available to Competition Entrants on www.betfair.com.au/hub/datathon along with the updated data set by 11:59am on the day prior to the nominated race day. Submissions must follow the template set out in the submission file template provided and must be submitted in a csv format. </p> <p>For each race, each Competition Entrant is required to predict the probability of a horse winning the race. This prediction will then be used against the actual outcome to calculate the model\u2019s score. The sum of the probabilities for every race must equal 1 \u2013 being the number accepted winners. </p> <p>Please name submission file using the following formatting: </p> <p>\u2018Spring_Racing_Data_Modelling_Competition_2023_Submission_Form_{Model_Name}.csv\u2019; </p> <p>To submit your model entry, please send it through to datathon@betfair.com.au</p>"},{"location":"modelling/SpringRacingDatathon/#code-check","title":"Code Check","text":"<p>To be eligible for prize money, each Competition Entrant must provide proof that the code used to create their submission is unique and not similar to that of another Competition Entrant. </p> <p>The code requirements are:</p> <ul> <li>Either 50% of the code or 1000 lines of code (whichever is smaller)</li> <li>Feature engineering aspects may be redacted or altered at the discretion of the entrant</li> <li>Submitted in a .txt file with the first submission of the competition. If not submitted with the first submission, entrants have until close of the submissions on the second nominated race day</li> </ul>"},{"location":"modelling/SpringRacingDatathon/#submission-file-template","title":"Submission File Template","text":"<p>The submission file template will be loaded here by 11:59am on the day prior to the nominated race day.</p> <ul> <li>example file Caulfield Cup 21/10/2023</li> <li>Week 1 Submission File 28/10/2023</li> <li>Week 2 Submission File 04/11/2023</li> <li>Week 3 Submission File 11/11/2023</li> <li>Week 4 Submission File 18/11/2023</li> </ul>"},{"location":"modelling/SpringRacingDatathon/#dataset-updates","title":"Dataset updates","text":"<p>Dataset updates will be loaded to the dataset page (link included in your registration confirmation email) by 4:59pm AEDT each Friday prior to the nominated race day.</p>"},{"location":"modelling/SpringRacingDatathon/#final-results","title":"Final Results","text":"<p>Leaderboards will be posted here weekly as well as in the Betfair Quants Discord server.</p> <p>For an invite to the discord server, please fill out the form here</p> model_name Score Rank BSP 2.77788996997805 OnMyHighHorse 2.7906939396213 1 willingly 2.79479411408101 2 ShinyNewThings 2.7997219805638 3 melatonin 2.82023873698256 4 Sportpunter 2.82209518025333 5 BetfairHubPredictionsModel 2.82722068224616 ConcensusSCD2023 2.82801203747167 6 nihaoshijie 2.82882380798673 7 Elle-Macpherson 2.82949035955336 8 watptv 2.83220716456428 9 Doriemus 2.83280544288701 10 sebnatann 2.84644233918721 11 VinhoVerde 2.85267147439722 12 Cfrance 2.85483856884689 13 tomsguesses 2.85871078349823 14 RonW 2.86289564706468 15 Gee-Gee-Genius 2.86323498280765 16 BelleEpoque 2.87046918602378 17 HorsesAreJustLargeDogsRight 2.88255972209085 18 ANCR101 2.908669415987 19 Naomi 2.92736405744928 20 Vertex 2.94735261875303 21 Craigs-List 2.97002109980415 22 exogen 2.99524266470316 23 SpicyPredictions 2.99738014682078 24 Chautauqua 3.02877922721002 25 matouka 3.04024295542127 26 reliable-lumberjack 3.04760653609704 27 DamienThirst 3.11828100020267 28 236john-KvKid 3.14292359860204 29 High-Hat 3.15224272966616 30 RandomShrubbery 3.17516493828797 31 Moose-Ste 3.21855177030658 32 KeenAsABean 3.24194972021672 33 TopPunter 3.24471673034183 34 TheDartMethod 3.26158155505263 35 bruno 3.26421531746475 36 ReiningIn 3.3335306628469 37 Skunkworks 3.46770257357916 38 RockSystems 3.58239827522267 39 Hoover 3.73795524949571 40 Definitely-not-overfitted 3.87507506226626 41 Katana 4.49222287728695 42 HackerHandicapping 7.11891434429891 43"},{"location":"modelling/SpringRacingDatathon/#contestants-who-did-not-submit-every-week","title":"Contestants who did not submit every week","text":"model_name Score FastandCoefficient 3.87494725084764 jdo 4.55189918624135 LiverpoolCapper 2.92758899437985 boshea 2.73075899282043 Looney-mods 2.81198074377767 zazzage 2.86275935836261 grunet 2.94138126530589"},{"location":"modelling/brownlowModelTutorial/","title":"Modelling the Brownlow Medal in Python","text":"<pre><code># Import modules libraries\nimport pandas as pd\nimport h2o\nfrom h2o.automl import H2OAutoML\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport pickle\n\n# Change notebook settings\npd.options.display.max_columns = None\npd.options.display.max_rows = 300\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n</code></pre> <p>This walkthrough uses H2O's AutoML to automate machine learning. We have saved the models created by AutoML into the Github repo which can be loaded into the notebook to save you time training the model. However a drawback of H2O is that you can only load models if your currently installed verson of H2O is the same as the version used to create the model. You can check what version of H2O you have installed by running <code>h2o.__version__</code>:</p> <pre><code># Checking the current version of H2O\nprint(f'Current version of H2O installed: {h2o.__version__}')\n</code></pre> <pre>\n<code>Current version of H2O installed: 3.36.0.3\n</code>\n</pre> <p>If you have a different version of H2O installed you have two options. You can train the models yourself, all the code to do that is commented out in the notebook. Or you can pip uninstall H2O and then pip install H2O again, but specifically the 3.36.0.3 version. The code to do this is below and will only take 1 or 2 minutes:</p> <pre><code># # Uncomment the code below if you need to uninstall the current version of H2O and reinstall version 3.36.0.3\n# # The following command removes the H2O module for Python.\n# pip uninstall h2o\n\n# # Next, use pip to install this version of the H2O Python module.\n# pip install http://h2o-release.s3.amazonaws.com/h2o/rel-zorn/3/Python/h2o-3.36.0.3-py2.py3-none-any.whl\n</code></pre> <pre><code>brownlow_data = pd.read_csv('data/afl_brownlow_data.csv')\n\nbrownlow_data.tail(3)\n</code></pre> date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 76585 25/08/2018 2018 23 Optus Stadium 12587 9711 J Stephenson 35 Collingwood Fremantle Away 76 67 9 NaN 3 5 5 62.5 0 0 3 2 0 87 6 2 8 3 2 1 3 0 0 0 1 0 0 0 56 56 76586 25/08/2018 2018 23 Optus Stadium 12144 9711 J Thomas 24 Collingwood Fremantle Away 76 67 9 NaN 6 11 14 82.4 0 2 0 1 0 79 11 6 17 4 1 0 3 0 4 1 4 1 1 3 67 89 76587 25/08/2018 2018 23 Optus Stadium 11549 9711 T Varcoe 18 Collingwood Fremantle Away 76 67 9 NaN 7 4 7 53.8 0 1 0 2 0 73 6 7 13 1 0 0 3 0 2 0 4 0 1 1 45 53 <p>It looks like we've got about 76,000 rows of data and have stats like hitouts, clangers, effective disposals etc. Let's explore some certain scenarios. Using my domain knowledge of footy, I can hypothesise that if a player kicks 5 goals, he is pretty likely to poll votes. Similarly, if a player gets 30 possessions and 2+ goals, he is also probably likely to poll votes. Let's have a look at the mean votes for players for both of these situations. </p> <pre><code>brownlow_data.query('G &amp;gt;= 5').groupby('season').brownlow_votes.mean()\nprint(\"Mean votes if a player has kicked a bag:\", brownlow_data.query('G &amp;gt;= 5').brownlow_votes.mean())\n</code></pre> <pre>\n<code>season\n2010    1.420455\n2011    1.313433\n2012    1.413333\n2013    1.253731\n2014    1.915254\n2015    1.765625\n2016    1.788732\n2017    2.098361\n2018    0.000000\nName: brownlow_votes, dtype: float64</code>\n</pre> <pre>\n<code>Mean votes if a player has kicked a bag: 1.4708818635607321\n</code>\n</pre> <pre><code>brownlow_data.query('G &amp;gt;= 2 and D &amp;gt;= 30').groupby('season').brownlow_votes.mean()\nprint(\"Mean votes if a player has 30 possies and kicks 2+ goals:\", brownlow_data.query('G &amp;gt;= 2 and D &amp;gt;= 30').brownlow_votes.mean())\n</code></pre> <pre>\n<code>season\n2010    1.826923\n2011    1.756410\n2012    2.118421\n2013    2.000000\n2014    2.253731\n2015    2.047619\n2016    2.103448\n2017    2.050000\n2018    0.000000\nName: brownlow_votes, dtype: float64</code>\n</pre> <pre>\n<code>Mean votes if a player has 30 possies and kicks 2+ goals: 1.8741379310344828\n</code>\n</pre> <p>As suspected, the average votes for these two situations is 1.87! That's huge. Let's get an idea of the average votes for each player. It should be around 6/44, as there are always 6 votes per match and around 44 players per match.</p> <pre><code>brownlow_data.brownlow_votes.mean()\n</code></pre> <pre>\n<code>0.12347341475121326</code>\n</pre> <p>So the average vote is 0.12. Let's see how this changes is the player is a captain. I have collected data on if players are captains from wikipedia and collated it into a csv. Let's load this in and create a \"Is the player captain\" feature, then check the average votes for captains.</p> <pre><code>captains = pd.read_csv('data/captains.csv').set_index('player')\n\ndef is_captain_for_that_season(captains_df, player, year):\n\n    if player in captains_df.index:\n        # Get years they were captain\n        seasons = captains_df.loc[player].season.split('-')\n        if len(seasons) == 1:\n            seasons_captain = list(map(int, seasons))\n        elif len(seasons) == 2:\n            if seasons[1] == '':\n                seasons_captain = list(range(int(seasons[0]), 2019))\n            else:\n                seasons_captain = list(range(int(seasons[0]), int(seasons[1]) + 1))\n\n        if year in seasons_captain:\n            return 1\n    return 0\n\nbrownlow_data['is_captain'] = brownlow_data.apply(lambda x: is_captain_for_that_season(captains, x.player, x.season), axis='columns')\n</code></pre> <pre><code>brownlow_data.query('is_captain == 1').groupby('season').brownlow_votes.mean()\nprint(\"Mean votes if a player is captain:\", brownlow_data.query('is_captain == 1').brownlow_votes.mean())\n</code></pre> <pre>\n<code>season\n2010    0.408497\n2011    0.429936\n2012    0.274194\n2013    0.438725\n2014    0.519663\n2015    0.447222\n2016    0.347826\n2017    0.425806\n2018    0.000000\nName: brownlow_votes, dtype: float64</code>\n</pre> <pre>\n<code>Mean votes if a player is captain: 0.36661698956780925\n</code>\n</pre> <p>This is significantly higher than if they aren't captain. What would be interesting is to look at the average difference in votes between when they were captain and when they weren't, to try and find if there is a 'captain bias' in brownlow votes. Go ahead and try. For now, we're going to move onto feature creation</p> <p>First we will make features of ratios. What is important is not how many of a certain stat a player has, but how much of that stat a player has relative to everyone else in the same match. It doesn't matter if Dusty Martin has 31 possessions if Tom Mitchell has had 50 - Mitchell is probably more likely to poll (assuming all else is equal). So rather than using the actual number of possessions for example, we can divide these possessions by the total amount of possessions in the game. To do this we'll use pandas groupby and transform methods.</p> <pre><code>%%time\n\n# Get a list of stats of which to create ratios for\nratio_cols = ['CP', 'UP', 'ED', 'DE', 'CM', 'GA', 'MI5', 'one_perc', 'BO', 'TOG', \n               'K', 'HB', 'D', 'M', 'G', 'B', 'T', 'HO', 'I50', 'CL', 'CG', 'R50', \n               'FF', 'FA', 'AF', 'SC']\n\n# Create a ratios df\nratios = (brownlow_data.copy()\n          .loc[:, ['match_id'] + ratio_cols]\n          .groupby('match_id')\n          .transform(lambda x: x / x.sum()))\n\nfeature_cols = ['date', 'season', 'round', 'venue', 'ID', 'match_id', 'player', 'jumper_no', 'team', \n                'opposition', 'status', 'team_score', 'opposition_score', 'margin', 'brownlow_votes']\n\n# Create a features df - join the ratios to this df\nfeatures = (brownlow_data[feature_cols].copy()\n               .join(ratios))\n</code></pre> <pre>\n<code>Wall time: 17.1 s\n</code>\n</pre> <pre><code>features.head()\n</code></pre> date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 0 25/03/2010 2010 1 MCG 11559 5089 J Anderson 26 Carlton Richmond Away 120 64 56 0.0 0.024194 0.021186 0.020599 0.021931 0.00 0.000000 0.000000 0.000000 0.0 0.024411 0.017032 0.028481 0.022008 0.019802 0.000000 0.0000 0.029197 0.0 0.000000 0.000000 0.010204 0.014286 0.00000 0.025 0.020075 0.019353 1 25/03/2010 2010 1 MCG 4060 5089 E Betts 19 Carlton Richmond Away 120 64 56 0.0 0.044355 0.014831 0.024345 0.021804 0.05 0.133333 0.130435 0.033333 0.0 0.025243 0.021898 0.031646 0.026135 0.029703 0.111111 0.1875 0.043796 0.0 0.000000 0.014925 0.010204 0.000000 0.00000 0.000 0.034504 0.041125 2 25/03/2010 2010 1 MCG 3281 5089 P Bower 18 Carlton Richmond Away 120 64 56 0.0 0.016129 0.038136 0.044944 0.030602 0.10 0.000000 0.000000 0.100000 0.0 0.026352 0.036496 0.031646 0.034388 0.054455 0.000000 0.0000 0.036496 0.0 0.009804 0.000000 0.000000 0.014286 0.00000 0.000 0.037014 0.039311 3 25/03/2010 2010 1 MCG 4056 5089 A Carrazzo 44 Carlton Richmond Away 120 64 56 2.0 0.032258 0.069915 0.059925 0.025501 0.00 0.000000 0.000000 0.000000 0.0 0.024133 0.051095 0.060127 0.055021 0.024752 0.000000 0.0000 0.029197 0.0 0.078431 0.074627 0.040816 0.057143 0.04878 0.025 0.041092 0.040822 4 25/03/2010 2010 1 MCG 11535 5089 B Gibbs 4 Carlton Richmond Away 120 64 56 1.0 0.032258 0.031780 0.029963 0.022186 0.10 0.000000 0.130435 0.000000 0.0 0.023024 0.036496 0.025316 0.031637 0.029703 0.074074 0.0625 0.029197 0.0 0.009804 0.044776 0.010204 0.000000 0.02439 0.025 0.033250 0.033868 <pre><code>features['kicked_a_bag'] = brownlow_data.G.apply(lambda x: 1 if x &amp;gt;= 5 else 0)\n</code></pre> <pre><code>features['is_captain'] = features.apply(lambda x: is_captain_for_that_season(captains, x.player, x.season), axis='columns')\n</code></pre> <pre><code>features['team_won'] = np.where(features.margin &amp;gt; 0, 1, 0)\n</code></pre> <pre><code>features['got_30_possies_2_goals'] = np.where((brownlow_data.G &amp;gt;= 2) &amp;amp; (brownlow_data.D &amp;gt;= 30), 1, 0)\n</code></pre> <pre><code>afltables = pd.read_csv('data/afltables_stats.csv').query('Season &amp;gt;= 2000')\n\ndef replace_special_characters(name):\n    name = name.replace(\"'\", \"\").replace(\"-\", \" \").lower()\n    name_split = name.split()\n\n    if len(name_split) &amp;gt; 2:\n        first_name = name_split[0]\n        last_name = name_split[-1]\n        name = first_name + ' ' + last_name\n\n    name_split_2 = name.split()\n    name = name_split_2[0][0] + ' ' + name_split_2[1]\n    return name.title()\n\nafltables = (afltables.assign(player=lambda df: df['First.name'] + ' ' + df.Surname)\n                .assign(player=lambda df: df.player.apply(replace_special_characters))\n                .rename(columns={'Brownlow.Votes': 'brownlow_votes', 'Season': 'season', 'Playing.for': 'team'}))\n</code></pre> <pre><code>### Create Top 10 rank look up table\n\nbrownlow_votes_yearly = (afltables.groupby(['season', 'player', 'team'], as_index=False)\n                                  .brownlow_votes\n                                  .sum())\n\nbrownlow_votes_yearly['yearly_rank'] = (brownlow_votes_yearly.groupby('season')\n                                                             .brownlow_votes\n                                                             .rank(method='max', ascending=False))\n\n# Filter to only get a dataframe since 2000 and only the top 10 players from each season\nbrownlow_votes_top_10 = brownlow_votes_yearly.query('yearly_rank &amp;lt; 11 &amp;amp; season &amp;gt;= 2000')\n\nbrownlow_votes_top_10.head(3)\n\ndef how_many_times_top_10(top_10_df, player, year):\n    times = len(top_10_df[(top_10_df.player == player) &amp;amp; (top_10_df.season &amp;lt; year)])\n    return times\n\nfeatures['times_in_top_10'] = features.apply(lambda x: how_many_times_top_10(brownlow_votes_top_10, x.player, x.season), axis=1) \n</code></pre> season player team brownlow_votes yearly_rank 27 2000.0 A Koutoufides Carlton 19.0 4.0 36 2000.0 A Mcleod Adelaide 20.0 3.0 105 2000.0 B Ratten Carlton 18.0 6.0 <pre><code># Create a brownlow votes lookup table\nbrownlow_votes_lookup_table = (brownlow_data.groupby(['player', 'team', 'season'], as_index=False)\n                                    .brownlow_votes\n                                    .mean()\n                                    .assign(next_season=lambda df: df.season + 1)\n                                    .rename(columns={\n                                        'brownlow_votes': 'ave_votes_last_season'\n                                    }))\n\n# Have a look at Cripps to check if it's working\nbrownlow_votes_lookup_table[brownlow_votes_lookup_table.player == 'P Cripps']\n\n# Merge it to our features df\nfeatures_with_votes_last_season = (pd.merge(features, brownlow_votes_lookup_table.drop(columns='season'),\n\n                                             left_on=['player', 'team', 'season'], \n                                             right_on=['player', 'team', 'next_season'],\n                                             how='left')\n                                  .drop(columns=['next_season'])\n                                  .fillna(0))\n</code></pre> player team season ave_votes_last_season next_season 4377 P Cripps Carlton 2014 0.000000 2015 4378 P Cripps Carlton 2015 0.300000 2016 4379 P Cripps Carlton 2016 0.857143 2017 4380 P Cripps Carlton 2017 0.333333 2018 4381 P Cripps Carlton 2018 0.000000 2019 <pre><code>h2o.init()\n\n# Uncomment the code below if you want to train the models yourself - otherwise, we will load them in the load cell from disk\n\n\n## Join to our features df\n\n# aml_yearly_model_objects = {}\n# yearly_predictions_dfs = {}\n\n# feature_cols = ['margin', 'CP', 'UP', 'ED', 'DE',\n#        'CM', 'GA', 'MI5', 'one_perc', 'BO', 'TOG', 'K', 'HB', 'D', 'M', 'G',\n#        'B', 'T', 'HO', 'I50', 'CL', 'CG', 'R50', 'FF', 'FA', 'AF', 'SC']\n\n# for year in range(2011, 2018):\n#     # Filter the data to only include past data\n#     train_historic = brownlow_data[brownlow_data.season &amp;lt; year].copy()\n\n#     # Convert to an h2o frame\n#     train_h2o_historic = h2o.H2OFrame(train_historic)\n\n#     # Create an AutoML object\n#     aml = H2OAutoML(max_runtime_secs=30,\n#                    balance_classes=True,\n#                    seed=42)\n\n#     # Train the model\n#     aml.train(y='brownlow_votes', x=feature_cols, training_frame=train_h2o_historic)\n\n#     # save the model\n#     model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True)\n\n#     # Get model id\n#     model_name = aml.leaderboard[0, 'model_id']\n\n#     # Rename the model on disk\n#     os.rename(f'models/{model_name}', f'models/yearly_model_{year}')\n\n#     # Append the best model to a list\n#     aml_yearly_model_objects[year] = aml.leader\n\n#     # Make predictions on test set for that year\n#     test_historic = brownlow_data[brownlow_data.season == year].copy()\n#     test_h2o_historic = h2o.H2OFrame(test_historic)\n\n#     preds = aml.predict(test_h2o_historic).as_data_frame()\n#     test_historic['predicted_votes'] = preds.values\n\n#     # Convert negative predictions to 0\n#     test_historic['predicted_votes_neg_to_0'] = test_historic.predicted_votes.apply(lambda x: 0 if x &amp;lt; 0 else x)\n\n#     # Create a total match votes column - which calculates the number of votes predicted in each game when the predictions \n#     # are unscaled\n#     test_historic['unscaled_match_votes'] = test_historic.groupby('match_id').predicted_votes_neg_to_0.transform('sum')\n\n#     # Scale predictions\n#     test_historic['predicted_votes_scaled'] = test_historic.predicted_votes_neg_to_0 / test_historic.unscaled_match_votes * 6\n\n#     # Aggregate the predictions\n#     test_grouped = (test_historic.groupby(['player', 'team'], as_index=False)\n#                                  .sum()\n#                                  .sort_values(by='brownlow_votes', ascending=False)\n#                                  .assign(mae=lambda df: abs(df.predicted_votes_scaled - df.brownlow_votes)))\n\n#     test_grouped['error'] = test_grouped.predicted_votes_scaled - test_grouped.brownlow_votes\n#     test_grouped['next_year'] = year + 1\n\n#     # Add this years predictions df to a dictionary to use later\n#     yearly_predictions_dfs[year] = test_grouped\n\n# preds_errors = None\n\n# for key, value in yearly_predictions_dfs.items():\n#     if preds_errors is None:\n#         preds_errors = value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']]\n#     else:\n#         preds_errors = preds_errors.append(value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']], sort=True)\n\n# with open('data/prediction_errors_df.pickle', 'wb') as handle:\n#     pickle.dump(preds_errors, handle)\n</code></pre> <pre>\n<code>Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\nAttempting to start a local H2O server...\n; Java HotSpot(TM) Client VM (build 25.301-b09, mixed mode)\n  Starting server from C:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\Lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n  Ice root: C:\\Users\\zhoui\\AppData\\Local\\Temp\\tmp6umhk9gp\n  JVM stdout: C:\\Users\\zhoui\\AppData\\Local\\Temp\\tmp6umhk9gp\\h2o_ZhouI_started_from_python.out\n  JVM stderr: C:\\Users\\zhoui\\AppData\\Local\\Temp\\tmp6umhk9gp\\h2o_ZhouI_started_from_python.err\n</code>\n</pre> <pre>\n<code>C:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\h2o\\backend\\server.py:386: UserWarning:   You have a 32-bit version of Java. H2O works best with 64-bit Java.\n  Please download the latest 64-bit Java SE JDK from Oracle.\n\n  warn(\"  You have a 32-bit version of Java. H2O works best with 64-bit Java.\\n\"\n</code>\n</pre> <pre>\n<code>  Server is running at http://127.0.0.1:54325\nConnecting to H2O server at http://127.0.0.1:54325 ... successful.\n</code>\n</pre> H2O_cluster_uptime: 03 secs H2O_cluster_timezone: Australia/Sydney H2O_data_parsing_timezone: UTC H2O_cluster_version: 3.36.0.3 H2O_cluster_version_age: 5 days  H2O_cluster_name: H2O_from_python_ZhouI_yobrjv H2O_cluster_total_nodes: 1 H2O_cluster_free_memory: 247.5 Mb H2O_cluster_total_cores: 12 H2O_cluster_allowed_cores: 12 H2O_cluster_status: locked, healthy H2O_connection_url: http://127.0.0.1:54325 H2O_connection_proxy: {\"http\": null, \"https\": null} H2O_internal_security: False Python_version: 3.9.6 final <pre><code># Load predictions error df    \nwith open('data/prediction_errors_df.pickle', 'rb') as handle:\n    preds_errors = pickle.load(handle)\n\n# Look at last years predictions\npreds_errors.query('next_year == 2018').sort_values(by='brownlow_votes', ascending=False).head(20)\n</code></pre> brownlow_votes error next_year player predicted_votes_scaled season 139 36.0 -3.431041 2018 D Martin 32.568959 44374 486 33.0 0.065131 2018 P Dangerfield 33.065131 42357 619 25.0 0.243510 2018 T Mitchell 25.243510 44374 279 23.0 -9.267806 2018 J Kennedy 13.732194 38323 376 22.0 -7.621163 2018 L Franklin 14.378837 44374 278 21.0 -4.494170 2018 J Kelly 16.505830 42357 519 20.0 -2.909334 2018 R Sloane 17.090666 44374 410 19.0 -7.124114 2018 M Bontempelli 11.875886 44374 483 18.0 -3.734412 2018 O Wines 14.265588 44374 121 17.0 -4.034390 2018 D Beams 12.965610 38323 390 16.0 -2.797669 2018 L Parker 13.202331 44374 561 15.0 -5.458874 2018 S Pendlebury 9.541126 32272 463 15.0 -4.892862 2018 N Fyfe 10.107138 42357 42 15.0 -4.379072 2018 B Ebert 10.620928 44374 651 15.0 1.011272 2018 Z Merrett 16.011272 42357 578 14.0 2.069385 2018 T Adams 16.069385 44374 34 14.0 -4.886668 2018 B Brown 9.113332 44374 172 14.0 0.211867 2018 D Zorko 14.211867 42357 184 14.0 -0.620086 2018 G Ablett 13.379914 28238 389 14.0 -2.811169 2018 L Neale 11.188831 42357 <p>Look at that! A simple Machine Learning ensemble model, using AutoML predicted last year's winner! That's impressive. As we can see it also predicted Bontempelli would only score 11.26, when he actually scored 19 - a huge discrepency. Let's use this as a feature.</p> <pre><code>features_with_historic_perf_relative_to_model = \\\n    (features_with_votes_last_season.pipe(pd.merge, preds_errors[['player', 'next_year', 'error']],\n                                          left_on=['player', 'season'],\n                                          right_on=['player', 'next_year'],\n                                          how='left')\n                                     .fillna(0)\n                                     .rename(columns={'error': 'error_last_season'})\n                                     .drop_duplicates(subset=['player', 'round', 'SC']))\n</code></pre> <pre><code># Find number of players who vote when in top 15 SC\n\nbrownlow_data['SC_rank_match'] = brownlow_data.groupby('match_id').SC.rank(method='max', ascending=False)\n\nbrownlow_data.query('SC_rank_match &amp;gt; 20 and season &amp;gt; 2014').brownlow_votes.value_counts()\n</code></pre> <pre>\n<code>0.0    18330\n1.0       14\n2.0        8\n3.0        2\nName: brownlow_votes, dtype: int64</code>\n</pre> <p>Since 2014, there have only been 24 players who have voted and not been in the top 20 SC.</p> <pre><code>features_with_sc_rank = features_with_historic_perf_relative_to_model.copy()\n\nfeatures_with_sc_rank['SC_rank_match'] = features_with_sc_rank.groupby('match_id').SC.rank(method='max', ascending=False)\n\n# Filter out rows with a SC rank of below 20\nfeatures_with_sc_rank_filtered = features_with_sc_rank.query('SC_rank_match &amp;lt;= 20')\n</code></pre> <pre><code># Filter out 2010 and 2011 as we used these seasons to create historic model performance features\nfeatures_last_before_train = features_with_sc_rank_filtered.query('season != 2010 and season != 2011').reset_index(drop=True)\n\nfeatures_last_before_train.head(3)\n</code></pre> date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC kicked_a_bag is_captain team_won got_30_possies_2_goals times_in_top_10 ave_votes_last_season next_year error_last_season SC_rank_match 0 24/03/2012 2012 1 ANZ Stadium 11635 5343 C Bird 14 Sydney GWS Away 100 37 63 0.0 0.035842 0.041002 0.030769 0.019378 0.000000 0.083333 0.055556 0.019802 0.0 0.023064 0.034562 0.037383 0.035762 0.039409 0.105263 0.111111 0.049296 0.0000 0.060606 0.093333 0.063636 0.027778 0.025 0.075 0.039041 0.032662 0 0 1 0 0 0.052632 2012.0 0.136311 6.0 1 24/03/2012 2012 1 ANZ Stadium 1013 5343 J Bolton 24 Sydney GWS Away 100 37 63 0.0 0.039427 0.036446 0.028846 0.019607 0.000000 0.000000 0.000000 0.049505 0.0 0.024203 0.027650 0.040498 0.033113 0.009852 0.052632 0.000000 0.042254 0.0125 0.040404 0.066667 0.009091 0.000000 0.025 0.025 0.029819 0.038767 0 0 1 0 0 0.526316 2012.0 -0.833178 3.0 2 24/03/2012 2012 1 ANZ Stadium 1012 5343 A Goodes 37 Sydney GWS Away 100 37 63 0.0 0.032258 0.029613 0.028846 0.023332 0.037037 0.083333 0.055556 0.009901 0.0 0.026196 0.029954 0.024922 0.027815 0.029557 0.000000 0.055556 0.014085 0.0125 0.070707 0.040000 0.027273 0.041667 0.025 0.000 0.025822 0.027473 0 1 1 0 5 0.761905 2012.0 -5.718150 14.0 <pre><code>train_baseline = features_last_before_train.query(\"season &amp;lt; 2017\")\nholdout = features_last_before_train.query(\"season == 2017\")\n\nscale_cols = ['team_score', 'opposition_score', 'margin', 'CP', 'UP', 'ED', 'DE',\n       'CM', 'GA', 'MI5', 'one_perc', 'BO', 'K', 'HB', 'D', 'M', 'G',\n       'B', 'T', 'HO', 'I50', 'CL', 'CG', 'R50', 'FF', 'FA', 'AF', 'SC']\n\nother_feature_cols = ['is_captain', 'kicked_a_bag', 'team_won', 'got_30_possies_2_goals', 'times_in_top_10', \n                      'ave_votes_last_season', 'error_last_season', 'SC_rank_match']\n\n\nall_feature_cols = scale_cols + other_feature_cols\n\n# Scale features\nscaler = StandardScaler()\n\ntrain_baseline_scaled = train_baseline.copy()\ntrain_baseline_scaled[scale_cols] = scaler.fit_transform(train_baseline[scale_cols])\n\nholdout_scaled = holdout.copy()\nholdout_scaled[scale_cols] = scaler.transform(holdout[scale_cols])\n\n# Convert categorical columns to categoricals\ntrain_baseline_h2o = h2o.H2OFrame(train_baseline_scaled)\nholdout_h2o = h2o.H2OFrame(holdout_scaled)\n\nfor col in ['is_captain', 'kicked_a_bag', 'team_won', 'got_30_possies_2_goals']:\n    train_baseline_h2o[col] = train_baseline_h2o[col].asfactor()\n    holdout_h2o[col] = holdout_h2o[col].asfactor()\n</code></pre> <pre>\n<code>Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100%\nParse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100%\n</code>\n</pre> <p>Below I have commented out training and saving the 2017 model. Rather than training it again, we will just load it in. Uncomment this part out if you want to train it yourself.</p> <pre><code># aml_2017_model = H2OAutoML(max_runtime_secs = 60*30,\n#                    balance_classes=True,\n#                    seed=42)\n\n# aml_2017_model.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_baseline_h2o)\n</code></pre> <pre><code># # save the model\n# model_path = h2o.save_model(model=aml_2017_model.leader, path=\"models\", force=True)\n\n# # Get model id\n# model_name = aml_2017_model.leaderboard[0, 'model_id']\n\n# # Rename the model on disk\n# os.rename(f'models/{model_name}', f'models/brownlow_2017_model_v1')\n\n# Load model in\naml_2017_model = h2o.load_model('models/brownlow_2017_model_v1')\n</code></pre> <pre><code># Predict the 2017 brownlow count\npreds_final_2017_model = aml_2017_model.predict(holdout_h2o)\n\n\n# Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total\nholdout = (holdout.assign(predicted_votes=preds_final_2017_model.as_data_frame().values)\n                  .assign(predicted_votes_neg_to_0=lambda df: df.predicted_votes.apply(lambda x: 0 if x &amp;lt;0 else x))\n                  .assign(unscaled_match_votes=lambda df: df.groupby('match_id').predicted_votes_neg_to_0.transform('sum'))\n                  .assign(predicted_votes_scaled=lambda df: df.predicted_votes_neg_to_0 / df.unscaled_match_votes * 6))\n\n# Create an aggregate votes df and show the average SC points and goals scored\nagg_predictions_2017 = (holdout.groupby(['player', 'team'], as_index=False)\n                        .agg({\n                            'brownlow_votes': sum,\n                            'predicted_votes_scaled': sum,\n                            'SC': 'mean',\n                            'G': 'mean'})\n                        .sort_values(by='brownlow_votes', ascending=False)\n                        .assign(mae=lambda df: abs(df.brownlow_votes - df.predicted_votes_scaled))\n                        .reset_index(drop=True))\n</code></pre> <pre>\n<code>glm prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100%\n</code>\n</pre> <pre><code>agg_predictions_2017.head(15)\n</code></pre> player team brownlow_votes predicted_votes_scaled SC G mae 0 D Martin Richmond 36.0 25.007991 0.037862 0.064869 10.992009 1 P Dangerfield Geelong 33.0 25.568804 0.042441 0.070819 7.431196 2 T Mitchell Hawthorn 25.0 19.078639 0.036040 0.016928 5.921361 3 L Franklin Sydney 22.0 12.574692 0.034640 0.149203 9.425308 4 J Kelly GWS 21.0 15.198580 0.034652 0.033772 5.801420 5 R Sloane Adelaide 20.0 15.755535 0.037068 0.034821 4.244465 6 J Kennedy Sydney 20.0 11.065483 0.032014 0.030508 8.934517 7 M Bontempelli Western Bulldogs 19.0 13.836173 0.033233 0.040498 5.163827 8 D Beams Brisbane 17.0 10.073366 0.034848 0.044998 6.926634 9 O Wines Port Adelaide 16.0 10.511818 0.031601 0.021967 5.488182 10 N Fyfe Fremantle 15.0 12.782723 0.033761 0.031680 2.217277 11 S Pendlebury Collingwood 15.0 9.135615 0.033855 0.013660 5.864385 12 B Ebert Port Adelaide 15.0 7.941416 0.032795 0.008431 7.058584 13 L Parker Sydney 15.0 12.431929 0.031366 0.030311 2.568071 14 Z Merrett Essendon 15.0 15.017767 0.033737 0.015362 0.017767 <p>So whilst our model predicted Dangerfield to win, it was pretty damn accurate! Let's find the MAE for the top 100, 50, 25, and 10, and then compare it to 2018's MAE in week, when the Brownlow has been counted.</p> <pre><code>for top_x in [10, 25, 50, 100]:\n    temp_mae = round(agg_predictions_2017.iloc[:top_x].mae.mean(), 3)\n    print(f\"The Average Mean Absolute Error for the top {top_x} is {temp_mae}\")\n</code></pre> <pre>\n<code>The Average Mean Absolute Error for the top 10 is 7.033\nThe Average Mean Absolute Error for the top 25 is 4.962\nThe Average Mean Absolute Error for the top 50 is 3.744\nThe Average Mean Absolute Error for the top 100 is 2.696\n</code>\n</pre> <pre><code>train = features_last_before_train.query(\"season &amp;lt; 2018\")\ntest = features_last_before_train.query(\"season == 2018\")\n\n# Scale features\nscaler = StandardScaler()\n\ntrain_scaled = train.copy()\ntrain_scaled[scale_cols] = scaler.fit_transform(train[scale_cols])\n\ntest_scaled = test.copy()\ntest_scaled[scale_cols] = scaler.transform(test[scale_cols])\n</code></pre> <pre><code># Convert categorical columns to categoricals\n\ntrain_h2o = h2o.H2OFrame(train_scaled)\ntest_h2o = h2o.H2OFrame(test_scaled)\n\nfor col in ['is_captain', 'kicked_a_bag', 'team_won', 'got_30_possies_2_goals']:\n    train_h2o[col] = train_h2o[col].asfactor()\n    test_h2o[col] = test_h2o[col].asfactor()\n</code></pre> <pre>\n<code>Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100%\nParse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100%\n</code>\n</pre> <pre><code># # Train the model - this part is commented out as we will just load our model from disk\n\n# aml = H2OAutoML(max_runtime_secs = 60*30,\n#                    balance_classes=True,\n#                    seed=42)\n\n# aml.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_h2o)\n</code></pre> <pre><code># # save the model\n# model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True)\n\n# # Get model id\n# model_name = aml.leaderboard[0, 'model_id']\n\n# # Rename the model on disk\n# os.rename(f'models/{model_name}', f'models/brownlow_2018_model_v1')\n\n# Load model in\naml = h2o.load_model('models/brownlow_2018_model_v1')\n</code></pre> <pre><code># Predict the 2018 brownlow count\npreds_final_2018_model = aml.predict(test_h2o)\n\n\n# Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total\ntest = (test.assign(predicted_votes=preds_final_2018_model.as_data_frame().values)\n                  .assign(predicted_votes_neg_to_0=lambda df: df.predicted_votes.apply(lambda x: 0 if x &amp;lt;0 else x))\n                  .assign(unscaled_match_votes=lambda df: df.groupby('match_id').predicted_votes_neg_to_0.transform('sum'))\n                  .assign(predicted_votes_scaled=lambda df: df.predicted_votes_neg_to_0 / df.unscaled_match_votes * 6))\n\n# Create an aggregate votes df and show the average SC points and goals scored\nagg_predictions_2018 = (test.groupby(['player', 'team'], as_index=False)\n                        .agg({\n                            'predicted_votes_scaled': sum,\n                            'match_id': 'count'}) # shows how many games they played\n                        .sort_values(by='predicted_votes_scaled', ascending=False)\n                        .reset_index(drop=True))\n</code></pre> <pre>\n<code>stackedensemble prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100%\n</code>\n</pre> <pre><code># Show the top 25 predictions\n\nagg_predictions_2018.head(25)\n</code></pre> player team predicted_votes_scaled match_id 0 T Mitchell Hawthorn 36.577208 20 1 M Gawn Melbourne 20.686854 22 2 C Oliver Melbourne 19.705244 20 3 D Martin Richmond 19.674984 19 4 B Grundy Collingwood 19.136407 22 5 J Macrae Western Bulldogs 18.396814 17 6 P Dangerfield Geelong 18.207483 21 7 D Beams Brisbane 17.280375 15 8 A Gaff West Coast 16.217552 18 9 L Neale Fremantle 16.038782 21 10 E Yeo West Coast 15.866418 20 11 J Selwood Geelong 15.749850 18 12 D Heppell Essendon 15.073503 19 13 N Fyfe Fremantle 14.864971 11 14 Z Merrett Essendon 14.597278 18 15 S Sidebottom Collingwood 14.403396 18 16 J Kennedy Sydney 14.297521 16 17 M Crouch Adelaide 13.464205 16 18 G Ablett Geelong 13.403062 15 19 P Cripps Carlton 13.140569 21 20 R Laird Adelaide 13.016825 19 21 L Franklin Sydney 12.562711 13 22 S Coniglio GWS 12.484317 20 23 J Kelly GWS 12.245194 14 24 J Lloyd Sydney 12.019714 20 <pre><code>print(agg_predictions_2018.head(15))\n</code></pre> <pre>\n<code>           player              team  predicted_votes_scaled  match_id\n0      T Mitchell          Hawthorn               36.577208        20\n1          M Gawn         Melbourne               20.686854        22\n2        C Oliver         Melbourne               19.705244        20\n3        D Martin          Richmond               19.674984        19\n4        B Grundy       Collingwood               19.136407        22\n5        J Macrae  Western Bulldogs               18.396814        17\n6   P Dangerfield           Geelong               18.207483        21\n7         D Beams          Brisbane               17.280375        15\n8          A Gaff        West Coast               16.217552        18\n9         L Neale         Fremantle               16.038782        21\n10          E Yeo        West Coast               15.866418        20\n11      J Selwood           Geelong               15.749850        18\n12      D Heppell          Essendon               15.073503        19\n13         N Fyfe         Fremantle               14.864971        11\n14      Z Merrett          Essendon               14.597278        18\n</code>\n</pre> <p>Now that we have the top 25, let's also look at the top 3 from each team.</p> <pre><code>agg_predictions_2018.sort_values(by=['team', 'predicted_votes_scaled'], ascending=[True, False]).groupby('team').head(3)\n</code></pre> player team predicted_votes_scaled match_id 17 M Crouch Adelaide 13.464205 16 20 R Laird Adelaide 13.016825 19 42 B Gibbs Adelaide 8.976507 18 7 D Beams Brisbane 17.280375 15 48 D Zorko Brisbane 8.039436 14 58 S Martin Brisbane 6.835975 19 19 P Cripps Carlton 13.140569 21 51 K Simpson Carlton 7.497976 18 91 E Curnow Carlton 3.734706 19 4 B Grundy Collingwood 19.136407 22 15 S Sidebottom Collingwood 14.403396 18 26 A Treloar Collingwood 11.173963 11 12 D Heppell Essendon 15.073503 19 14 Z Merrett Essendon 14.597278 18 61 D Smith Essendon 6.367458 20 9 L Neale Fremantle 16.038782 21 13 N Fyfe Fremantle 14.864971 11 75 D Mundy Fremantle 4.895438 19 22 S Coniglio GWS 12.484317 20 23 J Kelly GWS 12.245194 14 25 C Ward GWS 11.251450 19 6 P Dangerfield Geelong 18.207483 21 11 J Selwood Geelong 15.749850 18 18 G Ablett Geelong 13.403062 15 76 J Witts Gold Coast 4.745802 13 79 J Lyons Gold Coast 4.482626 14 114 D Swallow Gold Coast 2.731594 15 0 T Mitchell Hawthorn 36.577208 20 41 L Breust Hawthorn 9.132324 16 50 J Gunston Hawthorn 7.641945 19 1 M Gawn Melbourne 20.686854 22 2 C Oliver Melbourne 19.705244 20 30 J Hogan Melbourne 10.510504 13 29 S Higgins North Melbourne 10.682736 19 40 B Brown North Melbourne 9.211568 13 43 B Cunnington North Melbourne 8.949073 17 28 O Wines Port Adelaide 10.948001 16 35 R Gray Port Adelaide 10.017038 17 53 J Westhoff Port Adelaide 7.425425 21 3 D Martin Richmond 19.674984 19 36 J Riewoldt Richmond 9.981867 15 57 K Lambert Richmond 6.897522 12 38 S Ross St Kilda 9.756485 17 46 J Steven St Kilda 8.103860 17 90 J Steele St Kilda 3.739189 16 16 J Kennedy Sydney 14.297521 16 21 L Franklin Sydney 12.562711 13 24 J Lloyd Sydney 12.019714 20 8 A Gaff West Coast 16.217552 18 10 E Yeo West Coast 15.866418 20 34 J Redden West Coast 10.200779 16 5 J Macrae Western Bulldogs 18.396814 17 39 M Bontempelli Western Bulldogs 9.317955 16 45 L Hunter Western Bulldogs 8.758631 17 <p>If you're looking for a round by round breakdown, just have a look at the test dataframe.</p> <pre><code>test[['date', 'round', 'player', 'team', 'opposition', 'margin', 'SC', 'predicted_votes_scaled']].tail(25)\n</code></pre> date round player team opposition margin SC predicted_votes_scaled 27236 24/08/2018 23 J Laverde Essendon Port Adelaide 22 0.028190 0.098214 27237 24/08/2018 23 Z Merrett Essendon Port Adelaide 22 0.030918 0.316848 27238 24/08/2018 23 D Parish Essendon Port Adelaide 22 0.030615 0.031097 27239 24/08/2018 23 A Saad Essendon Port Adelaide 22 0.026069 0.000000 27240 24/08/2018 23 D Smith Essendon Port Adelaide 22 0.028190 0.714852 27241 24/08/2018 23 D Zaharakis Essendon Port Adelaide 22 0.034859 0.331842 27242 25/08/2018 23 H Ballantyne Fremantle Collingwood -9 0.025773 0.594485 27243 25/08/2018 23 T Duman Fremantle Collingwood -9 0.038205 0.068379 27244 25/08/2018 23 J Hamling Fremantle Collingwood -9 0.024560 0.000000 27245 25/08/2018 23 B Hill Fremantle Collingwood -9 0.031837 0.755081 27246 25/08/2018 23 E Langdon Fremantle Collingwood -9 0.033354 0.420121 27247 25/08/2018 23 D Mundy Fremantle Collingwood -9 0.023954 0.000000 27248 25/08/2018 23 L Neale Fremantle Collingwood -9 0.042450 1.012576 27249 25/08/2018 23 S Switkowski Fremantle Collingwood -9 0.027289 0.000000 27250 25/08/2018 23 T Adams Collingwood Fremantle 9 0.030018 0.207985 27251 25/08/2018 23 M Cox Collingwood Fremantle 9 0.024864 0.017404 27252 25/08/2018 23 J Crisp Collingwood Fremantle 9 0.031534 0.022959 27253 25/08/2018 23 B Grundy Collingwood Fremantle 9 0.045482 1.118902 27254 25/08/2018 23 B Maynard Collingwood Fremantle 9 0.026076 0.000000 27255 25/08/2018 23 B Mihocek Collingwood Fremantle 9 0.025167 0.000000 27256 25/08/2018 23 S Pendlebury Collingwood Fremantle 9 0.029715 0.888746 27257 25/08/2018 23 T Phillips Collingwood Fremantle 9 0.028199 0.009869 27258 25/08/2018 23 S Sidebottom Collingwood Fremantle 9 0.039115 0.880034 27259 25/08/2018 23 B Sier Collingwood Fremantle 9 0.023044 0.002407 27260 25/08/2018 23 J Thomas Collingwood Fremantle 9 0.026986 0.001053 <p>And there we have it! In a single notebook we have made a fairly good Brownlow predictive model. Enjoy.</p>"},{"location":"modelling/brownlowModelTutorial/#modelling-the-brownlow","title":"Modelling the Brownlow","text":"<p>This walkthrough will provide a brief, yet effective tutorial on how to model the Brownlow medal. We will use data from 2010 to 2018, which includes Supercoach points and other useful stats.</p> <p>The output will be the number of votes predicted for each player in each match, and we will aggregate these to create aggregates for each team and for the whole competition. No doubt we'll have Mitchell right up the top, and if we don't, then we know we've done something wrong!</p>"},{"location":"modelling/brownlowModelTutorial/#eda","title":"EDA","text":""},{"location":"modelling/brownlowModelTutorial/#read-in-the-data","title":"Read in the data","text":"<p>I have collated this data using the fitzRoy R package and merging the afltables dataset with the footywire dataset, so that we can Supercoach and other advanced stats with Brownlow votes. Let's read in the data and have a sneak peak at what it looks like.</p>"},{"location":"modelling/brownlowModelTutorial/#exploring-votes-if-a-bag-is-kicked-5-goals","title":"Exploring votes if a bag is kicked (5+ goals)","text":""},{"location":"modelling/brownlowModelTutorial/#exploring-votes-if-the-player-has-30-possies-2-goals","title":"Exploring votes if the player has 30+ possies &amp; 2+ goals","text":""},{"location":"modelling/brownlowModelTutorial/#create-is-player-captain-feature","title":"Create Is Player Captain Feature","text":""},{"location":"modelling/brownlowModelTutorial/#feature-creation","title":"Feature Creation","text":"<p>Let's make a range of features, including: * Ratios of each statistic per game * If the player is a captain * If they kicked a bag (4/5+) * If they kicked 2 and had 30+ possies</p>"},{"location":"modelling/brownlowModelTutorial/#create-ratios-as-features","title":"Create Ratios As Features","text":""},{"location":"modelling/brownlowModelTutorial/#kicked-a-bag-feature","title":"Kicked A Bag Feature","text":""},{"location":"modelling/brownlowModelTutorial/#is-captain-feature","title":"Is Captain Feature","text":""},{"location":"modelling/brownlowModelTutorial/#won-the-game-feature","title":"Won the Game Feature","text":""},{"location":"modelling/brownlowModelTutorial/#30-2-goals-feature","title":"30+ &amp; 2+ Goals Feature","text":""},{"location":"modelling/brownlowModelTutorial/#previous-top-10-finish-feature","title":"Previous Top 10 Finish Feature","text":"<p>I have a strong feeling that past performance may be a predictor of future performance in the brownlow. For example, last year Dusty Martin won the Brownlow. The umpires may have a bias towards Dusty this year because he is known to be on their radar as being a good player. Let's create a feature which is categorical and is 1 if the player has previously finished in the top 10. Let's create a function for this and then apply it to the afltables dataset, which has data back to 1897. We will then create a lookup table for the top 10 for each season and merge this table with our current features df.</p>"},{"location":"modelling/brownlowModelTutorial/#average-brownlow-votes-per-game-last-season-feature","title":"Average Brownlow Votes Per Game Last Season Feature","text":""},{"location":"modelling/brownlowModelTutorial/#historic-performance-relative-to-model-feature","title":"Historic Performance Relative To Model Feature","text":"<p>It is well known that some players are good Brownlow performers. For whatever reason, they always poll much better than their stats may suggest. Lance Franklin and Bontempelli are probably in this category. Perhaps these players have an X-factor that Machine Learning models struggle to pick up on. To get around this, let's create a feature which looks at the player's performance relative to the model's prediction. To do this, we'll need to train and predict 7 different models - from 2011 to 2017.</p> <p>To create a model for each season, we will use h2o's AutoML. If you're new to h2o, please read about it here. It can be used in both R and Python.</p> <p>The metric we will use for loss in Mean Absolute Error (MAE).</p> <p>As we are using regression, some values are negative. We will convert these negative values to 0 as it doesn't make sense to poll negative brownlow votes. Similarly, some matches won't predict exactly 6 votes, so we will scale these predictions so that we predict exactly 6 votes for each match.</p> <p>So that you don't have to train these models yourself, I have saved the models and we will load them in. If you are keen to train the models yourself, simply uncomment out the code below and run the cell. To bulk uncomment, highlight the rows and press ctrl + '/'</p>"},{"location":"modelling/brownlowModelTutorial/#filtering-the-data-to-only-include-the-top-20-sc-for-each-match","title":"Filtering the data to only include the top 20 SC for each match","text":"<p>Logically, it is extremely unlikely that a player will poll votes if their Supercoach score is not in the top 20 players. By eliminating the other 20+ players, we can reduce the noise in the data, as we are almost certain the players won't poll from the bottom half. Let's explore how many players poll if they're not in the top 20, and then filter our df if this number is not significant.</p>"},{"location":"modelling/brownlowModelTutorial/#modeling-the-2017-brownlow","title":"Modeling The 2017 Brownlow","text":"<p>Now that we have all of our features, we can simply create a training set (2012-2016), and a test set (2017), and make our predictions for last year! We will use AutoML for this process again. Again, rather than waiting for the model to train, I will save the model so you can simply load it in. We will also scale our features. We can then see how our model went in predicting last year's brownlow, creating a baseline for this years' predictions. We will then predict this year's vote count.</p>"},{"location":"modelling/brownlowModelTutorial/#modelling-this-years-brownlow","title":"Modelling This Year's Brownlow","text":"<p>Let's now predict this year's vote count. These predictions will be on the front page of the GitHub.</p>"},{"location":"modelling/brownlowModelTutorial/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"modelling/cloudOrLocalMachine/","title":"Cloud Based Virtual Machines VS Local Machines in Machine Learning","text":""},{"location":"modelling/cloudOrLocalMachine/#introduction","title":"Introduction","text":"<p>In the world of machine learning, where datasets can become enormous and the computational power required to train a ML model increases, there is some debate circulating in the circles of the Automation world as to the benefits of using a local desktop or laptop (with or without GPU augmentation) versus utilisation of cloud serves (also known as \u201csomeone else\u2019s computer\u201d)</p> <p>Local machines, with their tangible presence and direct accessibility, have long been the cornerstone of ML models. From individual laptops to high-performance desktops, these setups offer users a sense of control and familiarity. However, as ML models grow in complexity and dataset sizes swell, the limitations of local hardware become increasingly apparent.</p> <p>On the other hand, cloud services emerge as a compelling alternative, promising virtually limitless computational resources on-demand. With services like Amazon Web Services (AWS), Google Cloud Platform (GCP), and Microsoft Azure, modellers can tap into vast arrays of CPUs, GPUs, and specialized accelerators, paying only for what they use. Yet, transitioning to the cloud introduces its own set of challenges, including cost management, security concerns, and navigating the myriad of available services.</p>"},{"location":"modelling/cloudOrLocalMachine/#understanding-your-requirements","title":"Understanding Your Requirements","text":"<p>To approach this, it is vital that you understand what you are trying to do and in what context a cloud based machine might be more functional than a local machine.</p> <p>One potential solution to this conundrum would be to initially utilise a cloud-based solution to understand the computational requirements of your use-case, as you can easily increase or decrease processing power as required (which is effectively the same as trying out many different physical graphics cards or CPUs without investing time or money in physically performing this task). Once you understand the requirements, you could then look at investing in your own local machine tailored to your requirements. </p> <p>How then do you assess what your requirements are? Well first you should consider how you would look to implement a betting model. </p> <ul> <li>If your requirements involve intensive in-situ calculations as well as dependency on low latency, then it\u2019s likely worth investing in that cloud server with high processing power. This could be something like calculating in-play probabilities on Horse Racing and then placing bets. </li> <li>If your requirements involve calculating probabilities for all races at the start of the day using a static dataset and then placing bets at BSP, a local machine should be sufficient for your requirements.</li> <li>For latency sensitive strategies which don\u2019t involve intensive in-situ calculations then a hybrid model could be considered (a basic cloud server for betting and local machine for processing)</li> </ul> <p></p>"},{"location":"modelling/cloudOrLocalMachine/#pros-cons","title":"Pros &amp; Cons","text":"<p>Let\u2019s discuss the nuances between using local machines and the cloud for ML tasks. By delving into the strengths, weaknesses, and trade-offs of each approach, we aim to equip modeller with the insights necessary to make informed decisions tailored to their specific needs and objectives. </p>"},{"location":"modelling/cloudOrLocalMachine/#local-machine","title":"Local Machine","text":"<p>Pros</p> <ul> <li>Control and Privacy: With a local machine, you have full control over your hardware and data, ensuring privacy and security, which is crucial when dealing with sensitive datasets or proprietary algorithms.</li> <li>Low User Latency: Working directly on a local machine often results in lower latency, especially for tasks that require frequent interaction or real-time feedback, such as model development and experimentation.</li> <li>No Dependency on Internet Connectivity: Since all computations are performed locally, you're not reliant on stable internet connectivity, making it suitable for environments with limited or intermittent internet access.</li> <li>One-time Cost: While purchasing a high-performance local machine can be initially expensive, there are no ongoing subscription fees, making it cost-effective over the long term, especially for users with consistent computational needs.</li> </ul> <p>Cons</p> <ul> <li>Limited Scalability: The computational power of a local machine is finite and may not scale easily to handle large datasets or complex models, limiting the scope of your machine learning experiments.</li> <li>Hardware Constraints: Upgrading hardware components, such as CPUs or GPUs, can be costly and time-consuming, potentially leading to bottlenecks in performance or compatibility issues with newer algorithms or frameworks.</li> <li>Maintenance Overhead: Maintaining hardware infrastructure, installing software dependencies, and troubleshooting technical issues fall entirely on the user, consuming valuable time and resources that could be dedicated to actual model development and research.</li> <li>Risk of Data Loss: Local machines are susceptible to hardware failures, theft, or natural disasters, putting your data at risk unless proper backup and disaster recovery measures are implemented.</li> </ul>"},{"location":"modelling/cloudOrLocalMachine/#cloud-computing","title":"Cloud Computing","text":"<p>Pros</p> <ul> <li>Scalability and Flexibility: Cloud platforms offer virtually unlimited scalability, allowing you to seamlessly scale up or down based on fluctuating computational demands, making it ideal for handling large-scale machine learning tasks or \u201cbursty\u201d workloads.</li> <li>Wide Range of Services: Cloud providers offer a diverse array of managed services, including pre-configured virtual machines, GPU instances, and specialized AI accelerators, streamlining the setup and deployment of machine learning workflows.</li> <li>Pay-as-you-go Pricing: Cloud computing follows a pay-as-you-go pricing model, where you only pay for the resources you consume, eliminating upfront hardware costs and providing cost-effective solutions for both small-scale experiments and large-scale production deployments.</li> <li>Collaboration and Accessibility: Cloud platforms facilitate seamless collaboration among team members, enabling real-time sharing of data, code, and models regardless of geographical location, fostering a more collaborative and productive research environment.</li> </ul> <p>Cons</p> <ul> <li>Potential Security Risks: Entrusting sensitive data to a third-party cloud provider raises concerns about data privacy, security breaches, and regulatory compliance, necessitating robust encryption, access controls, and compliance measures to mitigate risks.</li> <li>Vendor Lock-in: Adopting proprietary cloud services may result in vendor lock-in, where migrating to a different provider or transitioning back to on-premises infrastructure becomes challenging and costly due to dependencies on vendor-specific APIs and services.</li> <li>Latency and Network Overhead: Cloud computing relies on internet connectivity, which introduces latency and network overhead, particularly for data-intensive machine learning tasks, potentially impacting performance and responsiveness.</li> <li>Cost Management Complexity: While pay-as-you-go pricing offers cost flexibility, it also requires diligent monitoring and optimization to prevent cost overruns, as cloud bills can quickly escalate due to unexpected usage spikes, idle resources, or inefficient configurations.</li> </ul>"},{"location":"modelling/cloudOrLocalMachine/#cost-considerations","title":"Cost Considerations","text":"<p>This article maintains that building a local machine is cheaper than a cloud solution in the long run, especially when scaling up for models that require greater computing power. They performed a cost analysis to build your own local machine versus paying for cloud services (Analysis performed in 2018 in $USD)</p> <p></p> <p></p> <p>As you can see, costs can begin to scale quickly when using cloud-based services, however, this is dependent on your computing requirements. In addition, there may also be the requirement to pay for your data storage when you\u2019re not using the machine. </p>"},{"location":"modelling/cloudOrLocalMachine/#hardware","title":"Hardware","text":"<p>If you make the decision to go down the route of a local machine, then it is possible to find second hand machines available at the fraction of the cost of a ready made new machine or building your own from new parts. The caveat here being that it is the responsibility of the user to ensure that any second hand hardware is properly wiped to safeguard the user\u2019s data and privacy. </p> <p>Here are some aspects to consider when exploring this route:</p> <ul> <li>Cost Savings: Second-hand hardware typically comes at a fraction of the cost of new equipment. This can be particularly advantageous for those on a tight budget or individuals who are just starting out in machine learning and want to experiment without significant financial investment.</li> <li>Availability of High-End Components: As technology advances, high-performance hardware becomes more accessible in the second-hand market. You may find GPUs, CPUs, or even entire workstations with specifications that were considered top-of-the-line just a few years ago but are now available at significantly reduced prices.</li> <li>DIY Upgrades and Customization: Buying second-hand hardware allows for customization and upgrades tailored to your specific machine learning requirements. You can mix and match components, such as adding more RAM or swapping out GPUs, to create a setup optimized for your workload.</li> <li>Environmental Sustainability: Opting for second-hand hardware promotes sustainability by extending the lifespan of electronic devices and reducing electronic waste. By reusing existing equipment, you contribute to minimizing the environmental footprint associated with manufacturing new hardware.</li> <li>Risks and Caveats: While purchasing second-hand hardware can offer substantial cost savings, it also comes with certain risks and caveats. Older hardware may lack support for the latest software frameworks or may be prone to hardware failures due to wear and tear. Additionally, warranties and technical support may be limited or non-existent, requiring users to be proficient in troubleshooting and maintenance.</li> <li>Research and Due Diligence: Before making a purchase, it's essential to research the specifications, performance benchmarks, and compatibility of the second-hand hardware with your machine learning tasks. Look for reputable sellers or platforms that offer warranties, return policies, or verification of the hardware's condition to minimize the risk of receiving faulty or subpar equipment.</li> <li>Consider Future Scalability: While second-hand hardware may meet your current needs, consider your future scalability requirements. Will the hardware be able to accommodate larger datasets or more computationally intensive models as your projects grow? Planning for future upgrades or expansion can help ensure that your investment remains viable in the long term.</li> </ul>"},{"location":"modelling/cloudOrLocalMachine/#conclusion","title":"Conclusion","text":"<p>All in all, there is much to consider in this debate. Careful consideration of computing and betting requirements, budget and future-proofing should be entered into before making a decision on this front. However, whatever you have is a good place to start, so start there and only buy more when you run out of RAM, disk space or patience!</p> <p>As always, there are many wise minds in our Quants Discord server with experience and advice on implementing these solutions who are more than happy to share their expertise. If you\u2019d like to join the discussion, fill out the form here</p>"},{"location":"modelling/dataSources/","title":"Historical Data Sources","text":"<p>We know that your automated strategies and models are only as good as your data. We work hard to make sure you have access to the data you need to allow you to achieve what you're setting out to in your automation and modelling projects. There\u2019s a huge variety of historic pricing data available, and hopefully this page shows you how to access what you're looking for.</p> <p>For more information on how to use this data to make your own predictive model, take a look at our modelling section. </p>"},{"location":"modelling/dataSources/#historical-stream-api-data","title":"Historical Stream API data","text":"<p>Betfair UK give access to all the historical Stream API data since 2016. It is excellent to use in building models and back testing strategies, however isn't necessarily in an easily accessible format for everyone. </p>"},{"location":"modelling/dataSources/#what-you-need-to-know-about-this-data-source","title":"What you need to know about this data source:","text":"<ul> <li>JSON format, downloads as TAR files (zipped)</li> <li>Australian and overseas racing, plus soccer, tennis, cricket, golf and \u2018other sport\u2019 data</li> <li>All Exchange markets included since the Stream API was introduced in 2016</li> <li>Time-stamped odds and volume data</li> <li>Able to filter by Event ID, market type and other parameters </li> <li>3 tiers of access:<ul> <li>Basic free tier \u2013 1 minute intervals for odds, no volume (free)</li> <li>Advanced tier \u2013 1 second intervals for odds, volume included (cost associated)</li> <li>Pro tier \u2013 50 millisecond intervals for odds, volume included (cost associated)</li> </ul> </li> <li>Includes a Historic Data API endpoint for download management </li> </ul>"},{"location":"modelling/dataSources/#supporting-resources-to-help-you-access-this-data","title":"Supporting resources to help you access this data:","text":"<ul> <li>Historic Data FAQs &amp; sample data</li> <li>Historic Data Specifications</li> <li>Sample code for using the historic data download API</li> </ul>"},{"location":"modelling/dataSources/#tutorials-for-working-with-this-data","title":"Tutorials for working with this data","text":"<ul> <li>JSON to CSV in Python</li> <li>JSON to CSV | Revisited - where we make it 30 times faster</li> <li>Backtesting ratings using historic data in Python</li> <li>Automated betting angles: no modelling required</li> </ul>"},{"location":"modelling/dataSources/#historical-racing-data","title":"Historical racing data","text":"<p>This is an excellent resource if you are interested in racing and like to see market level data in a CSV format.</p>"},{"location":"modelling/dataSources/#what-you-need-to-know-about-this-data-source_1","title":"What you need to know about this data source:","text":"<ul> <li>CSV format</li> <li>Free to download</li> <li>All Australian and overseas races, dating back to the beginning of the Exchange</li> <li>Available as a single file per day, per country, win or place market</li> <li>Market snapshot by runner, including<ul> <li>Max and min matched prices and volume, pre-play and in-play</li> <li>Weighted average price, pre-play and in-play</li> <li>BSP</li> <li>Winner</li> </ul> </li> </ul> <p>If none of these options suit your needs please contact us at data@betfair.com.au to discuss other potential options.</p>"},{"location":"modelling/fasttrackTutorial/","title":"Greyhound form FastTrack API (Old)","text":"<pre><code># Import libraries\nimport betfairlightweight\nfrom betfairlightweight import filters\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom dateutil import tz\nimport math\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import zscore\nfrom sklearn.linear_model import LogisticRegression\nimport fasttrack as ft\n</code></pre> <pre><code>seckey = \"your_security_key\"\ngreys = ft.Fasttrack(seckey)\n</code></pre> <pre>\n<code>Valid Security Key\n</code>\n</pre> <pre><code>track_codes = greys.listTracks()\ntrack_codes.head()\n</code></pre> track_name track_code state 0 Albury 223 NSW 1 Armidale 225 NSW 2 Bathurst 226 NSW 3 Broke Hill 227 NSW 4 Bulli 202 NSW <p>Later on in this tutorial, we will be building a greyhound model on QLD tracks only so let's create a list of the QLD FastTrack track codes which will be used later to filter our data downloads for only QLD tracks.</p> <pre><code>tracks_filter = list(track_codes[track_codes['state'] == 'QLD']['track_code'])\ntracks_filter\n</code></pre> <pre>\n<code>['400',\n '409',\n '401',\n '402',\n '403',\n '404',\n '405',\n '406',\n '407',\n '408',\n '410',\n '411',\n '412',\n '414',\n '413']</code>\n</pre> <pre><code>race_details, dog_results = greys.getRaceResults('2018-01-01', '2021-06-15', tracks_filter)\n</code></pre> <pre>\n<code>Getting meets for each date ..\n</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1262/1262 [10:34&lt;00:00,  1.99it/s]\n</code>\n</pre> <pre>\n<code>Getting historic results details ..\n</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2045/2045 [22:22&lt;00:00,  1.52it/s]\n</code>\n</pre> <pre><code>race_details.head()\n</code></pre> @id RaceNum RaceName RaceTime Distance RaceGrade Track date 0 285107231 1 UBET - DOWNLOAD THE APP 06:24PM 520m Maiden Albion Park 01 Jan 18 1 285107232 2 THIRTY TALKS @ STUD 06:47PM 600m Restricted Win Albion Park 01 Jan 18 2 285107233 3 BOX 1 PHOTOGRAPHY 07:02PM 331m Grade 5 Albion Park 01 Jan 18 3 285107234 4 ASPLEY LEAGUES CLUB 07:26PM 395m Mixed 4/5 Albion Park 01 Jan 18 4 285107235 5 TWITTER @ BRISGREYS 07:52PM 520m Mixed 3/4 Albion Park 01 Jan 18 <pre><code>dog_results.head()\n</code></pre> @id Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney RaceId TrainerId TrainerName 0 124886323 1 MERLOT HAYZE 2 2 27.5 $5.10 None 3.00 None 32 0 32 5.76 30.46 1260.00 285107231 12979 T Trigg 1 1362060038 2 SPIN THAT WHEEL 1 1 28.4 $2.70F None 3.00 3.14 11 0 11 5.67 30.68 360.00 285107231 160421 C Schmidt 2 1770370034 3 SOMERVILLE 8 8 32.7 $11.70 None 6.25 3.29 23 0 23 5.75 30.91 180.00 285107231 69795 L Green 3 108391387 4 SYFY LEGEND 6 6 30.4 $8.30 None 15.75 9.43 54 0 54 5.81 31.57 0.00 285107231 82013 S Kleinhans 4 2032540059 5 GET MESSI 5 5 34.4 $10.20 None 17.25 1.57 46 0 46 5.80 31.68 0.00 285107231 87148 S Lawrance <p>Here we do some basic data manipulation and cleansing to get variables into format that we can work with. Also adding on a few variables that will be handy down the track. Nothing too special here.</p> <pre><code>race_details['Distance'] = race_details['Distance'].apply(lambda x: int(x.replace(\"m\", \"\")))\nrace_details = race_details.rename(columns = {'@id': 'FastTrack_RaceId'})\nrace_details['date_dt'] = pd.to_datetime(race_details['date'], format = '%d %b %y')\nrace_details['trackdist'] = race_details['Track'] + race_details['Distance'].astype(str)\n\ndog_results = dog_results.rename(columns = {'@id': 'FastTrack_DogId', 'RaceId': 'FastTrack_RaceId'})\ndog_results['StartPrice'] = dog_results['StartPrice'].apply(lambda x: None if x  == None \n    else float(x.replace('$', '').replace('F', '')))\ndog_results = dog_results[~dog_results['Box'].isnull()]\ndog_results = dog_results.merge(\n    race_details[['FastTrack_RaceId', 'Distance', 'RaceGrade', 'Track', 'date_dt', 'trackdist']], \n    how = 'left',\n    on = 'FastTrack_RaceId'\n)\ndog_results['RunTime'] = dog_results['RunTime'].astype(float)\ndog_results['Prizemoney'] = dog_results['Prizemoney'].astype(float)\ndog_results['win'] = dog_results['Place'].apply(lambda x: 1 if x in ['1', '1='] else 0)\n\nprint(\"Number of races in dataset: \" + str(dog_results['FastTrack_RaceId'].nunique()))\n</code></pre> <pre>\n<code>Number of races in dataset: 20760\n</code>\n</pre> <pre><code>dog_results = dog_results.sort_values(by = ['FastTrack_DogId', 'date_dt'])\ndog_results = dog_results.set_index('date_dt')\n\n# Normalise the runtimes for each trackdist so we can compare runs across different track distance combinations. \n# We are making an unrealistic assumption that a dog that can run a good time  on one trackdistance can run a \n# good time on a different trackdistance\ndog_results['RunTime_norm'] = dog_results.groupby('trackdist')['RunTime'].transform(lambda x: zscore(x, nan_policy = 'omit'))\n\n# Feature 1 - Total prize money won over the last 365 Days\ndog_results['Prizemoney_365D'] = dog_results.groupby('FastTrack_DogId')['Prizemoney'].apply(lambda x: x.rolling(\"365D\").sum().shift(1))\ndog_results['Prizemoney_365D'].fillna(0, inplace = True)\n\n# Feature 2 - Number of runs over the last 365D\ndog_results['runs_365D'] = dog_results.groupby('FastTrack_DogId')['win'].apply(lambda x: x.rolling(\"365D\").count().shift(1))\ndog_results['runs_365D'].fillna(0, inplace = True)\n\n# Feature 3 - win % over the last 365D\ndog_results['wins_365D'] = dog_results.groupby('FastTrack_DogId')['win'].apply(lambda x: x.rolling(\"365D\").sum().shift(1))\ndog_results['wins_365D'].fillna(0, inplace = True)\ndog_results['win%_365D'] = dog_results['wins_365D'] / dog_results['runs_365D']\n\n# Feature 4 - Best runtime over the last 365D\ndog_results['RunTime_norm_best_365D'] = dog_results.groupby('FastTrack_DogId')['RunTime_norm'].apply(lambda x: x.rolling(\"365D\").min().shift(1))\n\n# Feature 5 - Median runtime over the last 365D\ndog_results['RunTime_norm_median_365D'] = dog_results.groupby('FastTrack_DogId')['RunTime_norm'].apply(lambda x: x.rolling(\"365D\").median().shift(1))\n\ndog_results.head(10)\n</code></pre> FastTrack_DogId Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 ... Track trackdist win RunTime_norm Prizemoney_365D runs_365D wins_365D win%_365D RunTime_norm_best_365D RunTime_norm_median_365D date_dt 2018-04-08 -2143477289 3 SUNBURNT SWAMPY 3 3 31.6 11.2 None 4.75 1.86 ... Albion Park Albion Park331 0 0.856147 0.0 0.0 0.0 NaN NaN NaN 2018-04-15 -2143477289 6 SUNBURNT SWAMPY 4 4 31.1 38.9 None 12.75 0.14 ... Albion Park Albion Park331 0 0.991574 175.0 1.0 0.0 0.0 0.856147 0.856147 2018-04-22 -2143477289 6 SUNBURNT SWAMPY 5 5 30.7 29.1 None 9.50 4.57 ... Albion Park Albion Park331 0 1.194715 175.0 2.0 0.0 0.0 0.856147 0.923861 2018-07-15 -2143477289 3 SUNBURNT SWAMPY 3 3 31.9 38.1 None 10.00 0.00 ... Albion Park Albion Park331 0 0.675578 175.0 3.0 0.0 0.0 0.856147 0.991574 2018-09-02 -2143477289 6 SUNBURNT SWAMPY 2 2 32.8 11.7 None 8.25 3.57 ... Albion Park Albion Park331 0 0.607864 350.0 4.0 0.0 0.0 0.675578 0.923861 2018-09-09 -2143477289 7 SUNBURNT SWAMPY 6 6 32.6 41.0 None 12.75 3.71 ... Albion Park Albion Park331 0 1.262428 350.0 5.0 0.0 0.0 0.607864 0.856147 2018-09-16 -2143477289 4 SUNBURNT SWAMPY 1 1 32.3 18.0 None 1.50 0.43 ... Albion Park Albion Park331 0 -0.385268 350.0 6.0 0.0 0.0 0.607864 0.923861 2018-10-14 -2143477289 5 SUNBURNT SWAMPY 8 8 32.3 5.5 None 11.25 1.29 ... Albion Park Albion Park331 0 1.217286 350.0 7.0 0.0 0.0 -0.385268 0.856147 2018-11-18 -2143477289 7 SUNBURNT SWAMPY 3 3 32.8 21.0 None 9.25 1.71 ... Albion Park Albion Park331 0 1.262428 350.0 8.0 0.0 0.0 -0.385268 0.923861 2019-05-26 -2143477289 4 SUNBURNT SWAMPY 7 7 31.7 71.0 None 11.00 1.86 ... Albion Park Albion Park331 0 0.517579 350.0 9.0 0.0 0.0 -0.385268 0.991574 <p>10 rows \u00d7 31 columns</p> <p>Convert all features into Z-scores within each race so that the features are on a relative basis when fed into the model</p> <pre><code>dog_results = dog_results.sort_values(by = ['date_dt', 'FastTrack_RaceId'])\n\nfor col in ['Prizemoney_365D', 'runs_365D', 'win%_365D',\n            'RunTime_norm_best_365D', 'RunTime_norm_median_365D']:\n    dog_results[col + '_Z'] = dog_results.groupby('FastTrack_RaceId')[col].transform(lambda x: zscore(x, ddof = 1))\n\ndog_results['runs_365D_Z'].fillna(0, inplace = True)\ndog_results['win%_365D_Z'].fillna(0, inplace = True)\n</code></pre> <pre><code>dog_results = dog_results.reset_index()\ndog_results = dog_results.sort_values(by = ['date_dt', 'FastTrack_RaceId'])\n\n# Only keep data aFter 2019\nmodel_df = dog_results[dog_results['date_dt'] &amp;gt;= '2019-01-01']\nfeature_cols = ['Prizemoney_365D_Z', 'runs_365D_Z', 'win%_365D_Z',\n                'RunTime_norm_best_365D_Z', 'RunTime_norm_median_365D_Z']\nmodel_df = model_df[['date_dt', 'FastTrack_RaceId', 'DogName', 'win', 'StartPrice'] + feature_cols]\n\n# Only train model off of races where each dog has a value for each feature\nraces_exclude = model_df[model_df.isnull().any(axis = 1)]['FastTrack_RaceId'].drop_duplicates()\nmodel_df = model_df[~model_df['FastTrack_RaceId'].isin(races_exclude)]\n\n# checking if any null values\nmodel_df.drop(columns = 'StartPrice').isnull().values.any()\n</code></pre> <pre>\n<code>False</code>\n</pre> <p>We will use pre-2021 as our train dataset and post-2021 as our test dataset which gives us an approximate 80/20 split of train to test data.</p> <p>Note that one issue with training our model this way is that we are training each dog result individually and not in conjunction with the other dogs in the race. Therefore the probabilities are not guaranteed to add up to 1.</p> <pre><code># Split the data into train and test data\ntrain_data = model_df[model_df['date_dt'] &amp;lt; '2021-01-01'].reset_index(drop = True)\ntest_data = model_df[model_df['date_dt'] &amp;gt;= '2021-01-01'].reset_index(drop = True)\n\ntrain_x, train_y = train_data[feature_cols], train_data['win']\ntest_x, test_y = test_data[feature_cols], test_data['win']\n\nlogit_model = LogisticRegression()\nlogit_model.fit(train_x, train_y)\n\ntest_data['prob_unscaled'] = logit_model.predict_proba(test_x)[:,1]\ntest_data.groupby('FastTrack_RaceId')['prob_unscaled'].sum()\n</code></pre> <pre>\n<code>FastTrack_RaceId\n626218700    0.840901\n626218701    0.731972\n626218702    0.754034\n626218703    0.986967\n626218704    0.990238\n               ...   \n680757815    1.178215\n680757816    0.847067\n680757817    1.043633\n680757818    0.805511\n680757819    0.782609\nName: prob_unscaled, Length: 2491, dtype: float64</code>\n</pre> <p>To correct for this, we'll apply a scaling factor to the model's raw outputs to force them to sum to 1. A better way to do this would be to use a conditional logistic regression which in the training process would ensure probabilities sum to unity.</p> <pre><code># Scale the raw model output so they sum to unity\ntest_data['prob_scaled'] = test_data.groupby('FastTrack_RaceId')['prob_unscaled'].apply(lambda x: x / sum(x))\ntest_data.groupby('FastTrack_RaceId')['prob_scaled'].sum()\n</code></pre> <pre>\n<code>FastTrack_RaceId\n626218700    1.0\n626218701    1.0\n626218702    1.0\n626218703    1.0\n626218704    1.0\n            ... \n680757815    1.0\n680757816    1.0\n680757817    1.0\n680757818    1.0\n680757819    1.0\nName: prob_scaled, Length: 2491, dtype: float64</code>\n</pre> <p>As a rudimentary check, let's see how many races the model correctly predicts using the highest probability in a given race as our pick. We'll also do the same for the starting price odds as a comparison.</p> <p>The model predicts the winner in 33% of races which is not great given the starting price predicts it in 41.7% of races ... but it will do for our purposes!</p> <pre><code># Create a boolean column for whether a dog has the higehst model prediction in a race. Do the same for the starting price \n# as a comparison\ntest_data['model_win_prediction'] = test_data.groupby('FastTrack_RaceId')['prob_scaled'].apply(lambda x: x == max(x))\ntest_data['odds_win_prediction'] = test_data.groupby('FastTrack_RaceId')['StartPrice'].apply(lambda x: x == min(x))\n\nprint(\"Model predicts the winner in {:.2%} of races\".format(\n    len(test_data[(test_data['model_win_prediction'] == True) &amp;amp; (test_data['win'] == 1)]) / test_data['FastTrack_RaceId'].nunique()\n    ))\nprint(\"Starting Price Odds predicts the winner in {:.2%} of races\".format(\n    len(test_data[(test_data['odds_win_prediction'] == True) &amp;amp; (test_data['win'] == 1)]) / test_data['FastTrack_RaceId'].nunique()\n    ))\n</code></pre> <pre>\n<code>Model predicts the winner in 32.96% of races\nStarting Price Odds predicts the winner in 41.75% of races\n</code>\n</pre> <pre><code>qld_races_today, qld_dogs_today = greys.getBasicFormat('2021-06-16', tracks_filter)\nqld_races_today.head()\n</code></pre> <pre>\n<code>Getting meets for each date ..\n</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  2.08it/s]\n</code>\n</pre> <pre>\n<code>Getting dog lineups ..\n</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01&lt;00:00,  1.43it/s]\n</code>\n</pre> @id RaceNum RaceName RaceTime RaceTimeDateUTC Distance RaceGrade PrizeMoney1 PrizeMoney2 PrizeMoney3 ... Handicap TAB GradeCode VICGREYS RaceComment Track Date Quali TipsComments_Bet TipsComments_Tips 0 680665206 1 TAB ORIGIN GREYHOUNDS TOMORROW 03:32PM 16 Jun 21 05:32AM 395m Novice Non Penalty $1750 $500 $250 ... None None NNP None \"\" Albion Park 16 Jun 21 None None None 1 680665207 2 TERRY HILL VS BEN HANNANT 03:52PM 16 Jun 21 05:52AM 395m Maiden Heat $1750 $500 $250 ... None None MH None \"\" Albion Park 16 Jun 21 None None None 2 680665208 3 QLD VS NSW TOMORROW @BRISGREYS 04:17PM 16 Jun 21 06:17AM 395m Maiden Heat $1750 $500 $250 ... None None MH None \"\" Albion Park 16 Jun 21 None None None 3 680665209 4 ORIGIN SPRINT TOMORROW NIGHT 04:38PM 16 Jun 21 06:38AM 395m Maiden Heat $1750 $500 $250 ... None None MH None \"\" Albion Park 16 Jun 21 None None None 4 680665210 5 BEN HANNANT?S QLD MAROONS 04:57PM 16 Jun 21 06:57AM 395m Grade 5 Heat $1750 $500 $250 ... None None 5H None \"\" Albion Park 16 Jun 21 None None None <p>5 rows \u00d7 27 columns</p> <p>Creat a list of the QLD tracks running today which will be used later when we fetch the Betfair data</p> <pre><code># Qld tracks running today\nqld_tracks_today = list(qld_races_today['Track'].unique())\nqld_tracks_today\n</code></pre> <pre>\n<code>['Albion Park', 'Ipswich']</code>\n</pre> <pre><code>my_username = \"your_username\"\nmy_password = \"your_password\"\nmy_app_key = \"your_app_key\"\n\ntrading = betfairlightweight.APIClient(my_username, my_password, app_key=my_app_key)\ntrading.login_interactive()\n</code></pre> <pre>\n<code>&lt;LoginResource&gt;</code>\n</pre> <p>Next, we'll call the list_events operation which will return all the greyhound events in Australia over the next 24 hours.</p> <pre><code># Create the market filter\ngreyhounds_event_filter = filters.market_filter(\n    event_type_ids=[4339],\n    market_countries=['AU'],\n    market_start_time={\n        'to': (datetime.utcnow() + timedelta(days=1)).strftime(\"%Y-%m-%dT%TZ\")\n    }\n)\n\n# Get a list of all greyhound events as objects\ngreyhounds_events = trading.betting.list_events(\n    filter=greyhounds_event_filter\n)\n\n# Create a DataFrame with all the events by iterating over each event object\ngreyhounds_events_today = pd.DataFrame({\n    'Event Name': [event_object.event.name for event_object in greyhounds_events],\n    'Event ID': [event_object.event.id for event_object in greyhounds_events],\n    'Event Venue': [event_object.event.venue for event_object in greyhounds_events],\n    'Country Code': [event_object.event.country_code for event_object in greyhounds_events],\n    'Time Zone': [event_object.event.time_zone for event_object in greyhounds_events],\n    'Open Date': [event_object.event.open_date for event_object in greyhounds_events],\n    'Market Count': [event_object.market_count for event_object in greyhounds_events]\n})\n\ngreyhounds_events_today.head()\n</code></pre> Event Name Event ID Event Venue Country Code Time Zone Open Date Market Count 0 Bend (AUS) 16th Jun 30618018 Bendigo AU Australia/Sydney 2021-06-16 01:37:00 36 1 WPrk (AUS) 16th Jun 30618017 Wentworth Park AU Australia/Sydney 2021-06-16 09:05:00 40 2 MBdg (AUS) 16th Jun 30618832 Murray Bridge AU Australia/Adelaide 2021-06-16 01:55:00 36 3 Cran (AUS) 16th Jun 30618160 Cranbourne AU Australia/Sydney 2021-06-16 08:44:00 34 4 Ball (AUS) 16th Jun 30618165 Ballarat AU Australia/Sydney 2021-06-16 08:58:00 60 <p>Next, let's fetch the market ids. As we know the meets we're interested in today, let's restrict the market pull request for only the QLD tracks that are running today.</p> <pre><code>greyhounds_events_today = greyhounds_events_today[greyhounds_events_today['Event Venue'].isin(qld_tracks_today)]\ngreyhounds_events_today.head()\n</code></pre> Event Name Event ID Event Venue Country Code Time Zone Open Date Market Count 7 Ipsw (AUS) 16th Jun 30618813 Ipswich AU Australia/Queensland 2021-06-16 08:55:00 40 9 APrk (AUS) 16th Jun 30618188 Albion Park AU Australia/Queensland 2021-06-16 05:32:00 27 <pre><code>market_catalogue_filter = filters.market_filter(\n    event_ids = list(greyhounds_events_today['Event ID']),\n    market_type_codes = ['WIN']\n)\n\nmarket_catalogue = trading.betting.list_market_catalogue(\n    filter=market_catalogue_filter,\n    max_results='1000',\n    sort='FIRST_TO_START',\n    market_projection=['MARKET_START_TIME', 'MARKET_DESCRIPTION', 'RUNNER_DESCRIPTION', 'EVENT', 'EVENT_TYPE']\n)\n\nwin_markets = []\nrunners = []\n\nfor market_object in market_catalogue:\n    # win_markets_df.append({\n    #     'Event Name': market_object.event.name,\n    #     'Event ID': market_object.event.id,\n    #     'Event Venue': market_object.event_venue,\n    #     'Market Name': market_object.market_name,\n    #     'Market ID': market_object.market_id,\n    #     'Market start time': market_object.market_start_time,\n    #     'Total Matched': market_object.total_matched\n    #     })\n    win_markets.append({\n        'event_name': market_object.event.name,\n        'event_id': market_object.event.id,\n        'event_venue': market_object.event.venue,\n        'market_name': market_object.market_name,\n        'market_id': market_object.market_id,\n        'market_start_time': market_object.market_start_time,\n        'total_matched': market_object.total_matched\n        })\n\n    for runner in market_object.runners:\n        runners.append({\n            'market_id': market_object.market_id,\n            'runner_id': runner.selection_id,\n            'runner_name': runner.runner_name\n            })\n\nwin_markets_df = pd.DataFrame(win_markets)\nrunners_df = pd.DataFrame(runners)\n</code></pre> <p>For matching purposes, we'll need to extract the race number from the market_name. Also let's add another field 'local_start_time' as the market_start_time field is in UTC format.</p> <pre><code># Extract race number from market name\nwin_markets_df['race_number'] = win_markets_df['market_name'].apply(\n    lambda x: x[1:3].strip() if x[0] == 'R' else None)\n\n# Functions that returns the time from a newly specified timezone given a time and an old timezone\ndef change_timezone(time, oldtz, newtz):\n    from_zone = tz.gettz(oldtz)\n    to_zone = tz.gettz(newtz)\n    newtime = time.replace(tzinfo = from_zone).astimezone(to_zone).replace(tzinfo = None)\n    return newtime\n\n# Add in a local_start_time variable\nwin_markets_df['local_start_time'] = win_markets_df['market_start_time'].apply(lambda x: \\\n                    change_timezone(x, 'UTC', 'Australia/Sydney'))\n\nwin_markets_df.head()\n</code></pre> event_name event_id event_venue market_name market_id market_start_time total_matched race_number local_start_time 0 APrk (AUS) 16th Jun 30618188 Albion Park R1 395m Nvce 1.184472300 2021-06-16 05:32:00 0.0 1 2021-06-16 15:32:00 1 APrk (AUS) 16th Jun 30618188 Albion Park R2 395m Heat 1.184472302 2021-06-16 05:52:00 0.0 2 2021-06-16 15:52:00 2 APrk (AUS) 16th Jun 30618188 Albion Park R3 395m Heat 1.184472304 2021-06-16 06:17:00 0.0 3 2021-06-16 16:17:00 3 APrk (AUS) 16th Jun 30618188 Albion Park R4 395m Heat 1.184472306 2021-06-16 06:38:00 0.0 4 2021-06-16 16:38:00 4 APrk (AUS) 16th Jun 30618188 Albion Park R5 395m Heat 1.184472308 2021-06-16 06:57:00 0.0 5 2021-06-16 16:57:00 <p>To match the dog names from Betfair and FastTrack, we'll also need to remove the rug number from the start of the runner_name in the runners_df DataFrame.</p> <pre><code># Remove dog number from runner_name\nrunners_df['runner_name'] = runners_df['runner_name'].apply(lambda x: x[(x.find(\" \") + 1):].upper())\n\n# Merge on the race number and event venue onto runners_df\nrunners_df = runners_df.merge(\n     win_markets_df[['market_id', 'event_venue', 'race_number']],\n     how = 'left',\n     on = 'market_id')\nrunners_df.head()\n</code></pre> market_id runner_id runner_name event_venue race_number 0 1.184472300 36594055 LITTLE MISS VANE Albion Park 1 1 1.184472300 39860314 MULGOWIE BELLE Albion Park 1 2 1.184472300 39860315 NIGHT CAPERS Albion Park 1 3 1.184472300 38079770 WRONG GIRL HARRY Albion Park 1 4 1.184472300 37616746 IM ON FIRE Albion Park 1 <pre><code>qld_races_today = qld_races_today.rename(columns = {'@id': 'FastTrack_RaceId'})\nqld_races_today = qld_races_today[['FastTrack_RaceId', 'Date', 'Track', 'RaceNum', 'RaceName', \n                                   'RaceTime', 'Distance', 'RaceGrade']]\nqld_dogs_today = qld_dogs_today.rename(columns = {'@id': 'FastTrack_DogId', 'RaceId': 'FastTrack_RaceId'})\nqld_dogs_today = qld_dogs_today.merge(\n    qld_races_today[['FastTrack_RaceId', 'Track', 'RaceNum']],\n    how = 'left',\n    on = 'FastTrack_RaceId'\n    )\nqld_dogs_today['DogName_bf'] = qld_dogs_today['DogName'].apply(lambda x: x.replace(\"'\", \"\").replace(\".\", \"\").replace(\"Res\", \"\").strip())\n</code></pre> <p>Now we can merge on the FastTrack and Betfair lineup dataframes by dog name, track and race number. We'll check that all selections have been matched by making sure there are no null dog ids.</p> <pre><code># Match on the fastTrack dogId to the runners_df\nrunners_df = runners_df.merge(\n    qld_dogs_today[['DogName_bf', 'Track', 'RaceNum', 'FastTrack_DogId']],\n    how = 'left',\n    left_on = ['runner_name', 'event_venue', 'race_number'],\n    right_on = ['DogName_bf', 'Track', 'RaceNum'],\n    ).drop(['DogName_bf', 'Track', 'RaceNum'], axis = 1)\n\n# Check all betfair selections are matched to a fastTrack dogId by checking if there are any null dogIds\nrunners_df['FastTrack_DogId'].isnull().any()\n</code></pre> <pre>\n<code>False</code>\n</pre> <pre><code>runners_df.head()\n</code></pre> market_id runner_id runner_name event_venue race_number FastTrack_DogId 0 1.184472300 36594055 LITTLE MISS VANE Albion Park 1 434800466 1 1.184472300 39860314 MULGOWIE BELLE Albion Park 1 510731455 2 1.184472300 39860315 NIGHT CAPERS Albion Park 1 415994834 3 1.184472300 38079770 WRONG GIRL HARRY Albion Park 1 443645048 4 1.184472300 37616746 IM ON FIRE Albion Park 1 448841452 <pre><code>runners_historicdata = dog_results[dog_results['FastTrack_DogId'].isin(runners_df['FastTrack_DogId'])]\nrunners_historicdata = runners_historicdata.sort_values(by = ['FastTrack_DogId', 'date_dt'])\nrunners_historicdata = runners_historicdata[runners_historicdata['date_dt'] &amp;gt;= (datetime.now() - timedelta(days = 365))]\n</code></pre> <p>Next we create the features. As our trained model requires a non-null value in each of the features, we'll exclude all markets where at least one dog has a null feature.</p> <pre><code># Create the feature variables over the last 365 days\nrunners_features = runners_historicdata.groupby('FastTrack_DogId').agg(\n    Prizemoney_365D = ('Prizemoney', 'sum'),\n    RunTime_norm_best_365D = ('RunTime_norm', 'min'),\n    RunTime_norm_median_365D = ('RunTime_norm', 'median'),\n    runs_365D = ('FastTrack_RaceId', 'count'),\n    wins_365D = ('win', 'sum')\n    ).reset_index()\n\nrunners_features['win%_365D'] = runners_features['wins_365D'] / runners_features['runs_365D']\nrunners_features = runners_features.drop('wins_365D', axis = 1)\n\nrunners_df = runners_df.merge(runners_features,\n                              how = 'left',\n                              on = 'FastTrack_DogId')\n\n# Only run on races where every dog has non-null features\nmarkets_exclude = runners_df[runners_df.isnull().any(axis = 1)]['market_id'].drop_duplicates()\nrunners_df = runners_df[~runners_df['market_id'].isin(markets_exclude)]\n\nprint(\"{0} markets are excluded\".format(str(len(markets_exclude))))\n\n# Convert the feature variables into Z-scores\nfor col in ['Prizemoney_365D', 'runs_365D', 'win%_365D',\n            'RunTime_norm_best_365D', 'RunTime_norm_median_365D']:\n    runners_df[col + '_Z'] = runners_df.groupby('market_id')[col].transform(\n        lambda x: zscore(x, ddof = 1))\n\nrunners_df['runs_365D_Z'].fillna(0, inplace = True)\nrunners_df['win%_365D_Z'].fillna(0, inplace = True)\n</code></pre> <pre>\n<code>6 markets are excluded\n</code>\n</pre> <p>Attach the model output onto the <code>runners_df</code> DataFrame. We will also scale the probabilities to sum to unity (same as what we did when assessing the trained model outputs in step 2).</p> <p>Let's also add a column for model fair odds which is just the reciprocal of the <code>prob_scaled</code>. We'll also add another column for the minimum back odds we're willing to take assuming we'd only bet off a 10% model overlay.</p> <pre><code>runners_df['prob_unscaled'] = logit_model.predict_proba(runners_df[feature_cols])[:,1]\nrunners_df['prob_scaled'] = runners_df.groupby('market_id')['prob_unscaled'].apply(lambda x: x / sum(x))\nrunners_df['model_fairodds'] = 1 / runners_df['prob_scaled']\nrunners_df['min_odds'] = (0.1 + 1) / runners_df['prob_scaled']\nrunners_df[['market_id', 'runner_name', 'event_venue', 'prob_scaled', 'model_fairodds', 'min_odds']].head()\n</code></pre> market_id runner_name event_venue prob_scaled model_fairodds min_odds 0 1.184472300 LITTLE MISS VANE Albion Park 0.056184 17.798518 19.578370 1 1.184472300 MULGOWIE BELLE Albion Park 0.376277 2.657620 2.923382 2 1.184472300 NIGHT CAPERS Albion Park 0.325158 3.075425 3.382967 3 1.184472300 WRONG GIRL HARRY Albion Park 0.152564 6.554620 7.210082 4 1.184472300 IM ON FIRE Albion Park 0.089817 11.133812 12.247193 <pre><code>market_id = win_markets_df['market_id'][0]\nbet_df = runners_df[runners_df['market_id'] == market_id].reset_index(drop = True)\nbet_df\n</code></pre> market_id runner_id runner_name event_venue race_number FastTrack_DogId Prizemoney_365D RunTime_norm_best_365D RunTime_norm_median_365D runs_365D win%_365D Prizemoney_365D_Z runs_365D_Z win%_365D_Z RunTime_norm_best_365D_Z RunTime_norm_median_365D_Z prob_unscaled prob_scaled model_fairodds min_odds 0 1.184472300 36594055 LITTLE MISS VANE Albion Park 1 434800466 2175.0 -0.049480 0.538154 12.0 0.083333 -0.114708 0.70791 -0.774043 1.632343 0.269920 0.037524 0.056184 17.798518 19.578370 1 1.184472300 39860314 MULGOWIE BELLE Albion Park 1 510731455 1850.0 -1.029058 -0.897651 2.0 0.500000 -0.818108 -0.97759 1.600323 -0.174870 -1.638741 0.251304 0.376277 2.657620 2.923382 2 1.184472300 39860315 NIGHT CAPERS Albion Park 1 415994834 2500.0 -1.513865 0.231713 5.0 0.200000 0.588691 -0.47194 -0.109221 -1.069288 -0.137442 0.217164 0.325158 3.075425 3.382967 3 1.184472300 38079770 WRONG GIRL HARRY Albion Park 1 443645048 1750.0 -1.151977 0.743528 4.0 0.250000 -1.034539 -0.64049 0.175703 -0.401644 0.542930 0.101893 0.152564 6.554620 7.210082 4 1.184472300 37616746 IM ON FIRE Albion Park 1 448841452 2865.0 -0.926976 1.059778 16.0 0.062500 1.378664 1.38211 -0.892762 0.013458 0.963332 0.059986 0.089817 11.133812 12.247193 <p>One thing we have to ensure is that the odds that we place adhere to the betfair price increments stucture. For example odds of 19.578370 are not valid odds to place a bet on. If we were to try we would get an <code>INVALID_ODDS</code> error. For more information on valid price increments click here.</p> <p>We'll create a function that rounds odds up to the nearest valid price increment and apply this to our <code>min_odds</code> field. </p> <pre><code>def roundUpOdds(odds):\n    if odds &amp;lt; 2:\n        return math.ceil(odds * 100) / 100\n    elif odds &amp;lt; 3:\n        return math.ceil(odds * 50) / 50\n    elif odds &amp;lt; 4:\n        return math.ceil(odds * 20) / 20\n    elif odds &amp;lt; 6:\n        return math.ceil(odds * 10) / 10\n    elif odds &amp;lt; 10:\n        return math.ceil(odds * 5) / 5\n    elif odds &amp;lt; 20:\n        return math.ceil(odds * 2) / 2\n    elif odds &amp;lt; 30:\n        return math.ceil(odds * 1) / 1\n    elif odds &amp;lt; 50:\n        return math.ceil(odds * 0.5) / 0.5\n    elif odds &amp;lt; 100:\n        return math.ceil(odds * 0.2) / 0.2\n    elif odds &amp;lt; 1000:\n        return math.ceil(odds * 0.1) / 0.1\n    else:\n        return odds\n\nbet_df['min_odds'] = bet_df['min_odds'].apply(lambda x: roundUpOdds(x))\nbet_df\n</code></pre> market_id runner_id runner_name event_venue race_number FastTrack_DogId Prizemoney_365D RunTime_norm_best_365D RunTime_norm_median_365D runs_365D win%_365D Prizemoney_365D_Z runs_365D_Z win%_365D_Z RunTime_norm_best_365D_Z RunTime_norm_median_365D_Z prob_unscaled prob_scaled model_fairodds min_odds 0 1.184472300 36594055 LITTLE MISS VANE Albion Park 1 434800466 2175.0 -0.049480 0.538154 12.0 0.083333 -0.114708 0.70791 -0.774043 1.632343 0.269920 0.037524 0.056184 17.798518 20.00 1 1.184472300 39860314 MULGOWIE BELLE Albion Park 1 510731455 1850.0 -1.029058 -0.897651 2.0 0.500000 -0.818108 -0.97759 1.600323 -0.174870 -1.638741 0.251304 0.376277 2.657620 2.94 2 1.184472300 39860315 NIGHT CAPERS Albion Park 1 415994834 2500.0 -1.513865 0.231713 5.0 0.200000 0.588691 -0.47194 -0.109221 -1.069288 -0.137442 0.217164 0.325158 3.075425 3.40 3 1.184472300 38079770 WRONG GIRL HARRY Albion Park 1 443645048 1750.0 -1.151977 0.743528 4.0 0.250000 -1.034539 -0.64049 0.175703 -0.401644 0.542930 0.101893 0.152564 6.554620 7.40 4 1.184472300 37616746 IM ON FIRE Albion Park 1 448841452 2865.0 -0.926976 1.059778 16.0 0.062500 1.378664 1.38211 -0.892762 0.013458 0.963332 0.059986 0.089817 11.133812 12.50 <p>Now that we have valid minimum odds that we want to bet on for each selection, we'll start betting. The following function will place a standard limit bet on Betfair on the specified <code>market_id</code> and <code>selection_id</code> at the specified size and price.</p> <pre><code># Create a function to place a bet using betfairlightweight\ndef placeBackBet(instance, market_id, selection_id, size, price):\n    order_filter = filters.limit_order(\n        size = size,\n        price = price,\n        persistence_type = \"LAPSE\"\n    )\n    instructions_filter = filters.place_instruction(\n        selection_id = str(selection_id),\n        order_type = \"LIMIT\",\n        side = \"BACK\",\n        limit_order = order_filter\n    )\n    order  = instance.betting.place_orders(\n        market_id = market_id,\n        instructions = [instructions_filter]\n    )\n    print(\"Bet Place on selection {0} is {1}\".format(str(selection_id), order.__dict__['_data']['status']))\n    return order\n</code></pre> <p>Now let's loop through the runners in <code>bet_df</code> and place a bet of $5 on each runner at the minimum odds we're willing to take.</p> <pre><code>for selection_id, min_odds in zip(bet_df['runner_id'], bet_df['min_odds']):\n    placeBackBet(trading, market_id, selection_id, 5, min_odds)\n</code></pre> <pre>\n<code>Bet Place on selection 36594055 is SUCCESS\nBet Place on selection 39860314 is SUCCESS\nBet Place on selection 39860315 is SUCCESS\nBet Place on selection 38079770 is SUCCESS\nBet Place on selection 37616746 is SUCCESS\n</code>\n</pre> <p>And success! We have downloaded historical greyhound form data from FastTrack, built a simple model, and bet off this model using the Betfair API.</p>"},{"location":"modelling/fasttrackTutorial/#greyhound-form-fasttrack-tutorial","title":"Greyhound form FastTrack tutorial","text":"<p>| Building a model from greyhound historic data to place bets on Betfair</p>"},{"location":"modelling/fasttrackTutorial/#workshop","title":"Workshop","text":""},{"location":"modelling/fasttrackTutorial/#overview","title":"Overview","text":"<p>This tutorial will walk through how to retrieve historic greyhound form data from FastTrack by accessing their Data Download Centre (DDC). We will then build a simple model on the data to demonstrate how we can then easily start betting on Betfair using the Betfair API. The tutorial will be broken up into four sections:</p> <ol> <li>Download historic greyhound data from FastTrack DDC</li> <li>Build a simple machine learning model</li> <li>Retrieve today's race lineups from FastTrack and Betfair API</li> <li>Run model on today's lineups and start betting</li> </ol>"},{"location":"modelling/fasttrackTutorial/#requirements","title":"Requirements","text":"<ul> <li>You will need a Betfair API app key. If you don't have one please follow the steps outlined on the The Automation Hub</li> <li>You will need your own FastTrack security key. Please note - The FastTrack DDC has been moved across to the new Topaz API as of December 2023. This means that, while we can source a key for you, the code in this tutorial will not work for the Topaz API. We will be updating this tutorial in early 2024 to reflect the new Topaz API nomenclature and documentation. (Additionally, only Australia and New Zealand customers are eligible for a free FastTrack key). If you would like to be considered for a FastTrack Topaz key, please email data@betfair.com.au.</li> <li>This notebook and accompanying files is shared on <code>betfair-downunder</code>'s Github.</li> <li>You can watch our workshop working through this tutorial on YouTube.</li> </ul>"},{"location":"modelling/fasttrackTutorial/#1-download-historic-greyhound-data-from-fasttrack","title":"1. Download historic greyhound data from FastTrack","text":""},{"location":"modelling/fasttrackTutorial/#create-a-fasttrack-object","title":"Create a FastTrack object","text":"<p>Enter in your FastTrack security key. Create a Fastrack object with this key which will also check whether the key is valid. If the key is vaid, a \"Valid Security Key\" message will be printed. The created 'greys' object will allow us to call a bunch of functions that interact with the FastTrack DDC.</p>"},{"location":"modelling/fasttrackTutorial/#find-a-list-of-greyhound-tracks-and-fasttrack-track-codes","title":"Find a list of greyhound tracks and FastTrack track codes","text":"<p>Call the listTracks function which creates a DataFrame containing all the greyhound tracks, their track codes and their state.</p>"},{"location":"modelling/fasttrackTutorial/#call-the-getraceresults-function","title":"Call the getRaceResults function","text":"<p>Call the getRaceResults function which will retrieve race details and historic results for all races between two dates. The function takes in two parameters and one optional third parameter. Two DataFrames are returned, the first contains all the details for each race and the second contains the dog results for each race.</p> <p><code>getRaceResults(dt_start, dt_end, tracks = None)</code></p> <ul> <li><code>dt_start</code>: the start date of the results you want to retrieve (str yyyy-mm-dd) </li> <li><code>dt_end</code>: the end date of the results you want to retrieve (str yyyy-mm-dd) </li> <li><code>tracks</code>: optional parameter which will restrict the download to only races in this list. If left blank, all tracks will be downloaded (list of str)</li> </ul> <p>In this example, we'll retrieve data from 2018-01-01 to 2021-06-15 and restrict the download to our <code>tracks_filter</code> list which contains only the QLD track codes.</p>"},{"location":"modelling/fasttrackTutorial/#2-build-a-simple-machine-learning-model","title":"2. Build a simple machine learning model","text":"<p>* NOTE: This model is not profitable. It is provided for educational purposes only. *</p>"},{"location":"modelling/fasttrackTutorial/#construct-some-simple-features","title":"Construct some simple features","text":"<p>We'll start by constructing some simple features. Normally we'd explore the data, but the objective of this tutorial is to demonstrate how to connect to FastTrack and Betfair so we'll skip the exploration step and jump straight to model building to generate some probability outputs.</p>"},{"location":"modelling/fasttrackTutorial/#train-the-model","title":"Train the model","text":"<p>Next, we'll train our model. To keep things simple, we'll choose a Logistic Regression from the sklearn package.</p> <p>For modelling purposes, we'll only keep data after 2019 as our features use the last 365 days of history so data in 2018 won't capture an entire 365 day period. Also we'll only keep races where each dog has a value for each feature. The last piece of code is to just double check the DataFrame has no null values.</p>"},{"location":"modelling/fasttrackTutorial/#3-retrieve-todays-race-lineups","title":"3. Retrieve today's race lineups","text":""},{"location":"modelling/fasttrackTutorial/#retrieve-todays-lineups-from-fasttrack","title":"Retrieve today's lineups from FastTrack","text":"<p>Now that we have trained our model. We want to get today's races from FastTrack and run the model over it. </p> <p>We have two options from FastTrack:</p> <ul> <li>Basic Plus Format: Contains basic information about the dog lineups such as box, best time, trainer, owner, ratings, speed ratings ...</li> <li>Full Plus Format: Contains everything in the basic format with additional information such as previous start information. </li> </ul> <p><code>getBasicFormat(dt, tracks = None)</code></p> <p><code>getFullFormat(dt, tracks = None)</code></p> <p>The calls will return two dataframes, one with the race information and one with the individual dog information. Again, the tracks parameter is optional and if left blank, all tracks will be returned.</p> <p>As we are only after the dog lineups to run our model on, let's just grab the basic format and again only restrict for QLD tracks.</p>"},{"location":"modelling/fasttrackTutorial/#retrieve-todays-lineups-from-the-betfair-api","title":"Retrieve today's lineups from the Betfair API","text":"<p>The FastTrack lineups contain all the dogs in a race, including reserves and scratched dogs. As we only want to run our model on final lineups, we'll need to connect to the Betfair API to update our lineups for any scratchings.</p> <p>Let's first login to the Betfair API. Enter in your username, password and API key and create a betfairlightweight object.</p>"},{"location":"modelling/fasttrackTutorial/#merge-race-lineups-from-fasttrack-and-betfair","title":"Merge race lineups from FastTrack and Betfair","text":"<p>Before we can merge, we'll need to do some minor formatting changes to the FastTrack names so we can match onto the Betfair names. Betfair excludes all apostrophes and full stops in their naming convention so we'll create a betfair equivalent dog name on the dataset removing these characters. We'll also tag on the race number to the lineups dataset for merging purposes as well.</p>"},{"location":"modelling/fasttrackTutorial/#4-run-model-on-todays-lineups-and-start-betting","title":"4. Run model on today's lineups and start betting","text":""},{"location":"modelling/fasttrackTutorial/#create-model-features-for-the-runners","title":"Create model features for the runners","text":"<p>First we have to create the same model features we used in our logistic regression model on the dogs in the runners_df DataFrame. As our features use historic data over the last 365 days, we'll need to filter our historic results dataset (created in step 1) for only the dog ids we are interested in and only over the last 365 days.</p>"},{"location":"modelling/fasttrackTutorial/#now-we-can-start-betting","title":"Now we can start betting!","text":"<p>Now we can start betting! For demonstration, we'll only bet on one market, but it's just as easy to set it up to bet on all markets based on your model probabilities. Let's take the first market only and create a separate DataFrame from <code>runners_df</code> with only those runners in that market.</p>"},{"location":"modelling/fasttrackTutorial/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"modelling/greyhoundDatathon/","title":"Betfair\u2019s 2024 Greyhound Racing Datathon","text":""},{"location":"modelling/greyhoundDatathon/#registrations","title":"Registrations","text":"<p>Registrations have closed.</p>"},{"location":"modelling/greyhoundDatathon/#the-competition","title":"The Competition","text":"<p>Do you think you have what it takes to predict the outcome of a greyhound race?</p> <p>Betfair is giving you the chance to show off your data modelling skills by building a predictive model for a set of Victorian greyhound race meetings.</p> <p>With $5,000 in prizes up for grabs, we\u2019re challenging you to use your modelling skills to your advantage, be that by building your first predictive racing model, improving on an existing design or have a go at something different by adapting your data skills from other fields.</p> <p>Submissions will be daily for the duration of the competition and will be due 60 minutes before the first race of the day.</p> <p>Time to get modelling!</p> <p>Progressive updates will be posted here after the conclusion of each race day and in the Betfair Quants Discord Server.</p> <p>For an invite to the discord server, please fill out the form here</p> <p>Please direct all questions and submissions to datathon@betfair.com.au.</p>"},{"location":"modelling/greyhoundDatathon/#the-specifics","title":"The specifics","text":"<p>The Terms and Conditions for the 2024 Greyhound Racing Datathon can be viewed here</p>"},{"location":"modelling/greyhoundDatathon/#prizes","title":"Prizes","text":"<p>$5,000 in prizes are up for grabs! See the list below for the prizes each placing at the end of the competition will receive:</p> Place Prize 1 $2,500.00 2 $1,000.00 3 $500.00 4 $250.00 5 $250.00 6 $100.00 7 $100.00 8 $100.00 9 $100.00 10 $100.00 Total Prize Pool $5,000.00 <p>Prize winners will be announced at the end of the count and prizes will be distributed in the following days</p>"},{"location":"modelling/greyhoundDatathon/#competition-rules","title":"Competition Rules","text":"<p>The 2024 Greyhound Racing Datathon will see Competition Entrants provided with a set of historical pricing data pertaining to Victorian Greyhound Racing. Additional data can be found here</p> <p>The goal from there is to use the provided data (or any other data) to build your own model to predict the probability of a dog winning a given race on the nominated race day.</p> <p>The nominated race days and meetings are:</p> <ul> <li>Monday May 13th, 2024 \u2013 Traralgon, Warrnambool &amp; Sandown Park \u2013 ALL races; and</li> <li>Tuesday May 14th, 2024 \u2013 Horsham, Geelong &amp; Warragul \u2013 ALL races; and</li> <li>Wednesday May 15th, 2024 \u2013 Bendigo, Ballarat &amp; The Meadows \u2013 ALL races; and </li> <li>Thursday May 16th, 2024 \u2013 Warragul, Warrnambool &amp; Sandown Park \u2013 ALL races; and  </li> <li>Friday May 17th, 2024 \u2013 Bendigo, Geelong &amp; Traralgon \u2013 ALL races; and</li> <li>Saturday May 18th, 2024 \u2013 Warragul &amp; The Meadows \u2013 ALL races; and </li> <li>Sunday May 19th, 2024 \u2013 Ballarat &amp; Sale \u2013 ALL races. </li> </ul> <p>How you go about building the model is entirely up to you \u2013 do you want to build an Elo model, a regression-based model, a Machine Learning algorithm or something entirely different? Get as innovative with it as you want! </p> <p>Your set of predictions will be due 60 minutes prior to the first race on the nominated race day. All submissions must be emailed to datathon@betfair.com.au.</p> <p>See the Terms and Conditions for full Competition rules.</p>"},{"location":"modelling/greyhoundDatathon/#submissions-judging","title":"Submissions &amp; Judging","text":"<p>The Datathon will be judged using the log loss method where the log loss score is calculated using the difference between the predicted probability and the actual outcome. The scoring calculation will involve finding the SUM of the log loss across all runners in each race and then finding the AVERAGE log loss of each race. The entrant with the lowest average log loss across all races over the nominated race days will be victorious.</p> <p></p> <p>One submission file template will be available to Competition Entrants on this page by 11:59am on the nominated race day. Submissions must follow the template set out in the submission file template provided and must be submitted in a csv format. </p> <p>For each race, each Competition Entrant is required to predict the probability of a dog winning the race. This prediction will then be used against the actual outcome to calculate the model\u2019s score. The sum of the probabilities for every race must equal 1 \u2013 being the number of accepted winners. </p>"},{"location":"modelling/greyhoundDatathon/#submission-file-template","title":"Submission File Template","text":"<p>The submission file template will be loaded here by 11:59am on the nominated race day.</p> <p>Please name submission file using the following formatting:   - \u2018Greyhound_Racing_Datathon_2024_Submission_Form_{Model_Name}.csv\u2019; </p> <ul> <li>Day 1 - submission file - 13/05/2024</li> <li>Day 2 - submission file - 14/05/2024</li> <li>Day 3 - submission file - 15/05/2024</li> <li>Day 4 - submission file - 16/05/2024</li> <li>Day 5 - submission file - 17/05/2024</li> <li>Day 6 - submission file - 18/05/2024</li> <li>Day 7 - submission file - 19/05/2024</li> </ul>"},{"location":"modelling/greyhoundDatathon/#historic-form-data","title":"Historic Form Data","text":"<p>All entrants received a bulk historic data file upon registration</p> <ul> <li>Form Data Update</li> <li>Pricing Data Update</li> </ul> <p>To submit your model entry, please send it through to datathon@betfair.com.au</p>"},{"location":"modelling/greyhoundDatathon/#leaderboard-final","title":"Leaderboard - Final","text":"Model Log Loss Rank PrizeRank BSP 2.3723 1 selling-full-rune-200k 2.3888 2 1 Mach7 2.4258 3 2 RapidRacer 2.4303 4 3 willingly 2.4436 5 4 TinHare 2.4438 6 5 InLimbo 2.4496 7 6 RIPPED 2.4579 8 7 YaLikeDags 2.4594 9 8 FirstDayFudge 2.4652 10 9 Analytique 2.479 11 10 Cortina 2.4829 12 11 NormalChannels 2.5061 13 12 blahboy 2.515 14 13 WisdomOfTheCrowd 2.5226 15 EbbingFlowing 2.525 16 14 RandomShrubbery 2.5442 17 15 Nightingale 2.5511 18 16 ScrapingTheDulux 2.5679 19 17 PlusEvOnly 2.5832 20 18 Iggy 2.5937 21 CanGetThirsty 2.6179 22 19 BiggyModel 2.6245 23 Greys-V1 2.6251 24 20 Vertex 2.6413 25 21 Mutt-Maestro 2.6427 26 22 DreamWeaver 2.7206 27 GradientGlider 2.7217 28 Purrfection 2.7256 29 LightningBolter 2.7266 30 LogisticalLegend 2.7371 31 LeafyDecisions 2.7453 32 XtremeRollers 2.7517 33 BelowAverage 2.767 34 23 crowbar 2.7975 35 24 HarbourBoostinator 2.8245 36 Flash-Reality 2.8792 37 25 FashionModel 2.933 38 26 Katana 2.937 39 27 <p>The following models are ineligible for prize money:</p> <ul> <li>BSP (This is the actual Betfair Starting Price)</li> <li>Iggy (This is the Betfair Hub Greyhound Predictions Model)</li> <li>Wisdom of the Crowd (This is the average of all submitted models)</li> <li>All other models with no value in the prize rank column</li> </ul>"},{"location":"modelling/greyhoundDatathon/#faqs","title":"FAQs","text":"<p>Will I receive a confirmation email once I submit my entry? No</p> <p>If I notice an error with my submission, can I resubmit? Yes, only the last entry received before the submission deadline will be considered for scoring</p> <p>How is the log loss calculated for a runner assigned a probability of exactly zero or exactly one? A placeholder value of 0.000001 or 0.999999 (respectively) will be assigned in place of these values</p> <p>What happens if the race is a dead heat? The race will be removed from the competition scoring</p> <p>What happens if there is a scratching after the submission deadline? All probabilities for remaining runners will be renormalised so the field is equal to 1</p> <p>What happens if there is a scratching before the submission deadline? The runner may be removed from the submission or assigned a null or 0 probability. If a probability is entered, this probability will be removed and remaining probabilities will be renormalised as per the above</p>"},{"location":"modelling/greyhoundDatathon/#results","title":"Results","text":"<p>Leaderboards will be posted here daily as well as in the Betfair Quants Discord server.</p> <p>For an invite to the discord server, please fill out the form here</p>"},{"location":"modelling/greyhoundModellingPython/","title":"Greyhound modelling in Python (Old)","text":"<p>This tutorial was written by Bruno Chauvet and was originally published on Github. It is shared here with his permission. </p> <p>This tutorial follows on logically from the Greyhound form Fasttrack tutorial we shared previously. </p> <p>As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements!</p> <pre><code># Import libraries\nimport os\nimport sys\n\n# Allow imports from src folder\nmodule_path = os.path.abspath(os.path.join('../src'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nfrom dateutil import tz\nfrom pandas.tseries.offsets import MonthEnd\nfrom sklearn.preprocessing import MinMaxScaler\nimport itertools\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport fasttrack as ft\n\nfrom dotenv import load_dotenv\nload_dotenv()\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code># Validate FastTrack API connection\napi_key = os.getenv('FAST_TRACK_API_KEY', '&lt;replace key=\"\" with=\"\" your=\"\"&gt;')\nclient = ft.Fasttrack(api_key)\ntrack_codes = client.listTracks()\n</code></pre> <pre>\n<code>Valid Security Key\n</code>\n</pre> <pre><code># Import race data excluding NZ races\nau_tracks_filter = list(track_codes[track_codes['state'] != 'NZ']['track_code'])\n\n# Time window to import data\n# First day of the month 46 months back from now\ndate_from = (datetime.today() - relativedelta(months=46)).replace(day=1).strftime('%Y-%m-%d')\n# First day of previous month\ndate_to = (datetime.today() - relativedelta(months=1)).replace(day=1).strftime('%Y-%m-%d')\n\n# Dataframes to populate data with\nrace_details = pd.DataFrame()\ndog_results = pd.DataFrame()\n\n# For each month, either fetch data from API or use local CSV file if we already have downloaded it\nfor start in pd.date_range(date_from, date_to, freq='MS'):\n    start_date = start.strftime(\"%Y-%m-%d\")\n    end_date = (start + MonthEnd(1)).strftime(\"%Y-%m-%d\")\n    try:\n        filename_races = f'FT_AU_RACES_{start_date}.csv'\n        filename_dogs = f'FT_AU_DOGS_{start_date}.csv'\n\n        filepath_races = f'../data/{filename_races}'\n        filepath_dogs = f'../data/{filename_dogs}'\n\n        print(f'Loading data from {start_date} to {end_date}')\n        if os.path.isfile(filepath_races):\n            # Load local CSV file\n            month_race_details = pd.read_csv(filepath_races) \n            month_dog_results = pd.read_csv(filepath_dogs) \n        else:\n            # Fetch data from API\n            month_race_details, month_dog_results = client.getRaceResults(start_date, end_date, au_tracks_filter)\n            month_race_details.to_csv(filepath_races, index=False)\n            month_dog_results.to_csv(filepath_dogs, index=False)\n\n        # Combine monthly data\n        race_details = race_details.append(month_race_details, ignore_index=True)\n        dog_results = dog_results.append(month_dog_results, ignore_index=True)\n    except:\n        print(f'Could not load data from {start_date} to {end_date}')\n</code></pre> <pre>\n<code>Loading data from 2018-11-01 to 2018-11-30\nLoading data from 2018-12-01 to 2018-12-31\nLoading data from 2019-01-01 to 2019-01-31\nLoading data from 2019-02-01 to 2019-02-28\nLoading data from 2019-03-01 to 2019-03-31\nLoading data from 2019-04-01 to 2019-04-30\nLoading data from 2019-05-01 to 2019-05-31\nLoading data from 2019-06-01 to 2019-06-30\nLoading data from 2019-07-01 to 2019-07-31\nLoading data from 2019-08-01 to 2019-08-31\nLoading data from 2019-09-01 to 2019-09-30\nLoading data from 2019-10-01 to 2019-10-31\nLoading data from 2019-11-01 to 2019-11-30\nLoading data from 2019-12-01 to 2019-12-31\nLoading data from 2020-01-01 to 2020-01-31\nLoading data from 2020-02-01 to 2020-02-29\nLoading data from 2020-03-01 to 2020-03-31\nLoading data from 2020-04-01 to 2020-04-30\nLoading data from 2020-05-01 to 2020-05-31\nLoading data from 2020-06-01 to 2020-06-30\nLoading data from 2020-07-01 to 2020-07-31\nLoading data from 2020-08-01 to 2020-08-31\nLoading data from 2020-09-01 to 2020-09-30\nLoading data from 2020-10-01 to 2020-10-31\nLoading data from 2020-11-01 to 2020-11-30\nLoading data from 2020-12-01 to 2020-12-31\nLoading data from 2021-01-01 to 2021-01-31\nLoading data from 2021-02-01 to 2021-02-28\nLoading data from 2021-03-01 to 2021-03-31\nLoading data from 2021-04-01 to 2021-04-30\nLoading data from 2021-05-01 to 2021-05-31\nLoading data from 2021-06-01 to 2021-06-30\nLoading data from 2021-07-01 to 2021-07-31\nLoading data from 2021-08-01 to 2021-08-31\nLoading data from 2021-09-01 to 2021-09-30\nLoading data from 2021-10-01 to 2021-10-31\nLoading data from 2021-11-01 to 2021-11-30\nLoading data from 2021-12-01 to 2021-12-31\nLoading data from 2022-01-01 to 2022-01-31\nLoading data from 2022-02-01 to 2022-02-28\nLoading data from 2022-03-01 to 2022-03-31\nLoading data from 2022-04-01 to 2022-04-30\nLoading data from 2022-05-01 to 2022-05-31\nLoading data from 2022-06-01 to 2022-06-30\nLoading data from 2022-07-01 to 2022-07-31\nLoading data from 2022-08-01 to 2022-08-31\n</code>\n</pre> <p>To better understand the data we retrieved, let's print the first few rows</p> <pre><code># Race data\nrace_details.head()\n</code></pre> @id RaceNum RaceName RaceTime Distance RaceGrade Track date 0 278896185 1 TRIPLE M BENDIGO 93.5 02:54PM 425m Grade 6 Bendigo 01 Dec 17 1 278896189 2 GOLDEN CITY CONCRETE PUMPING 03:17PM 500m Mixed 6/7 Bendigo 01 Dec 17 2 275589809 3 RAILWAY STATION HOTEL FINAL 03:38PM 500m Mixed 6/7 Final Bendigo 01 Dec 17 3 278896183 4 MCIVOR RD VETERINARY CLINIC 03:58PM 425m Grade 5 Bendigo 01 Dec 17 4 278896179 5 GRV VIC BRED SERIES HT1 04:24PM 425m Grade 5 Heat Bendigo 01 Dec 17 <pre><code># Individual dogs results\ndog_results.head()\n</code></pre> @id Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney RaceId TrainerId TrainerName 0 124886334 1 VANDA MICK 2.0 2 32.0 $2.80F NaN 0.49 NaN S/231 0 NaN 6.79 24.66 NaN 278896185 66993 M Ellis 1 2027130024 2 DYNA ZAD 7.0 7 24.2 $6.60 NaN 0.49 0.49 M/843 4 NaN 6.95 24.69 NaN 278896185 115912 M Delbridge 2 1448760015 3 KLONDIKE GOLD 4.0 4 33.3 $16.60 NaN 1.83 1.34 M/422 0 NaN 6.81 24.79 NaN 278896185 94459 R Hayes 3 1449650024 4 FROSTY TIARA 3.0 3 26.8 $22.00 NaN 2.94 1.11 S/114 0 NaN 6.75 24.86 NaN 278896185 87428 R Morgan 4 118782592 5 GNOCCHI 1.0 1 29.6 $8.60 NaN 6.50 3.56 S/355 0 NaN 6.80 25.11 NaN 278896185 138164 J La Rosa <p>From the FastTrack documentation, this is what each variable represents:</p> Variable Description Box Integer value between 1 and 8 Rug This is an integer value between 1 and 10 Weight This is a decimal value to 1 decimal place StartPrice This is an integer value &gt; 0 that is prefixed by the character \"$\" Handicap Empty = Not a Handicapped Race \"Y\" = Handicapped Race Margin1 This is a decimal value to two decimal places representing a dogs margin from the winning dog, in the case of the winning dog, it is the margin to the second dog. Margin2 This is a decimal value to two decimal places representing a dogs margin from the dog in front if it, in the case of the winning dog this value is empty. PIR This is a dogs place at each of the split points in a race. The string representation is \\/\\\\\\The Speed values will be one of the following:S = \u201cSlow Start\u201dM = \u201cMedium Start\u201dF = \u201cFast Start\u201dE.g. S/444 = Slow start and was placed 4th at each of the racesthree split points Checks Whether the dog was checked in running by other dogs, and the number of lengths lost as a result of the checking, ie. C1 SplitMargin This is a decimal value to two decimal places representing a dogs time at the first split marker. RunTime This is a decimal value to two decimal places representing a dogs running time for a race PriceMoney This is an integer value &gt; 0 with a \u201c$\u201d character prefix. Maximum of 8 values <pre><code># Clean up the race dataset\nrace_details = race_details.rename(columns = {'@id': 'FastTrack_RaceId'})\nrace_details['Distance'] = race_details['Distance'].apply(lambda x: int(x.replace(\"m\", \"\")))\nrace_details['date_dt'] = pd.to_datetime(race_details['date'], format = '%d %b %y')\n</code></pre> <pre><code># Clean up the dogs results dataset\ndog_results = dog_results.rename(columns = {'@id': 'FastTrack_DogId', 'RaceId': 'FastTrack_RaceId'})\n\n# Combine dogs results with race attributes\ndog_results = dog_results.merge(\n    race_details[['FastTrack_RaceId', 'Distance', 'RaceGrade', 'Track', 'date_dt']], \n    how = 'left',\n    on = 'FastTrack_RaceId'\n)\n\n# Convert StartPrice to probability\ndog_results['StartPrice'] = dog_results['StartPrice'].apply(lambda x: None if x is None else float(x.replace('$', '').replace('F', '')) if isinstance(x, str) else x)\ndog_results['StartPrice_probability'] = (1 / dog_results['StartPrice']).fillna(0)\ndog_results['StartPrice_probability'] = dog_results.groupby('FastTrack_RaceId')['StartPrice_probability'].apply(lambda x: x / x.sum())\n\n# Discard entries without results (scratched or did not finish)\ndog_results = dog_results[~dog_results['Box'].isnull()]\ndog_results['Box'] = dog_results['Box'].astype(int)\n\n# Clean up other attributes\ndog_results['RunTime'] = dog_results['RunTime'].astype(float)\ndog_results['SplitMargin'] = dog_results['SplitMargin'].astype(float)\ndog_results['Prizemoney'] = dog_results['Prizemoney'].astype(float).fillna(0)\ndog_results['Place'] = pd.to_numeric(dog_results['Place'].apply(lambda x: x.replace(\"=\", \"\") if isinstance(x, str) else 0), errors='coerce').fillna(0)\ndog_results['win'] = dog_results['Place'].apply(lambda x: 1 if x == 1 else 0)\n</code></pre> <p>The cell below shows some normalisation techniques. Why normalise data? Microsoft Azure has an excellent article on why this technique is often applied.</p> <ul> <li>Apply <code>Log base 10</code> transformation to <code>Prizemoney</code> and <code>Place</code></li> <li>Apply <code>inverse</code> transformation to <code>Place</code></li> <li>Combine <code>RunTime</code> and <code>Distance</code> to generate <code>Speed</code> value</li> </ul> <pre><code># Normalise some of the raw values\ndog_results['Prizemoney_norm'] = np.log10(dog_results['Prizemoney'] + 1) / 12\ndog_results['Place_inv'] = (1 / dog_results['Place']).fillna(0)\ndog_results['Place_log'] = np.log10(dog_results['Place'] + 1).fillna(0)\ndog_results['RunSpeed'] = (dog_results['RunTime'] / dog_results['Distance']).fillna(0)\n</code></pre> <pre><code># Calculate median winner time per track/distance\nwin_results = dog_results[dog_results['win'] == 1]\nmedian_win_time = pd.DataFrame(data=win_results[win_results['RunTime'] &amp;gt; 0].groupby(['Track', 'Distance'])['RunTime'].median()).rename(columns={\"RunTime\": \"RunTime_median\"}).reset_index()\nmedian_win_split_time = pd.DataFrame(data=win_results[win_results['SplitMargin'] &amp;gt; 0].groupby(['Track', 'Distance'])['SplitMargin'].median()).rename(columns={\"SplitMargin\": \"SplitMargin_median\"}).reset_index()\nmedian_win_time.head()\n</code></pre> Track Distance RunTime_median 0 Albion Park 331 19.180 1 Albion Park 395 22.860 2 Albion Park 520 30.220 3 Albion Park 600 35.100 4 Albion Park 710 42.005 <pre><code># Calculate track speed index\nmedian_win_time['speed_index'] = (median_win_time['RunTime_median'] / median_win_time['Distance'])\nmedian_win_time['speed_index'] = MinMaxScaler().fit_transform(median_win_time[['speed_index']])\nmedian_win_time.head()\n</code></pre> Track Distance RunTime_median speed_index 0 Albion Park 331 19.180 0.471787 1 Albion Park 395 22.860 0.460736 2 Albion Park 520 30.220 0.497773 3 Albion Park 600 35.100 0.556644 4 Albion Park 710 42.005 0.657970 <pre><code># Compare dogs finish time with median winner time\ndog_results = dog_results.merge(median_win_time, on=['Track', 'Distance'], how='left')\ndog_results = dog_results.merge(median_win_split_time, on=['Track', 'Distance'], how='left')\n\n# Normalise time comparison\ndog_results['RunTime_norm'] = (dog_results['RunTime_median'] / dog_results['RunTime']).clip(0.9, 1.1)\ndog_results['RunTime_norm'] = MinMaxScaler().fit_transform(dog_results[['RunTime_norm']])\ndog_results['SplitMargin_norm'] = (dog_results['SplitMargin_median'] / dog_results['SplitMargin']).clip(0.9, 1.1)\ndog_results['SplitMargin_norm'] = MinMaxScaler().fit_transform(dog_results[['SplitMargin_norm']])\ndog_results.head()\n</code></pre> FastTrack_DogId Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 ... win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm 0 124886334 1.0 VANDA MICK 2 2 32.0 2.8 NaN 0.49 NaN ... 1 0.0 1.000000 0.301030 0.058024 24.21 0.321642 6.63 0.408759 0.382180 1 2027130024 2.0 DYNA ZAD 7 7 24.2 6.6 NaN 0.49 0.49 ... 0 0.0 0.500000 0.477121 0.058094 24.21 0.321642 6.63 0.402795 0.269784 2 1448760015 3.0 KLONDIKE GOLD 4 4 33.3 16.6 NaN 1.83 1.34 ... 0 0.0 0.333333 0.602060 0.058329 24.21 0.321642 6.63 0.383017 0.367841 3 1449650024 4.0 FROSTY TIARA 3 3 26.8 22.0 NaN 2.94 1.11 ... 0 0.0 0.250000 0.698970 0.058494 24.21 0.321642 6.63 0.369268 0.411111 4 118782592 5.0 GNOCCHI 1 1 29.6 8.6 NaN 6.50 3.56 ... 0 0.0 0.200000 0.778151 0.059082 24.21 0.321642 6.63 0.320789 0.375000 <p>5 rows \u00d7 34 columns</p> <pre><code># Calculate box winning percentage for each track/distance\nbox_win_percent = pd.DataFrame(data=dog_results.groupby(['Track', 'Distance', 'Box'])['win'].mean()).rename(columns={\"win\": \"box_win_percent\"}).reset_index()\n# Add to dog results dataframe\ndog_results = dog_results.merge(box_win_percent, on=['Track', 'Distance', 'Box'], how='left')\n# Display example of barrier winning probabilities\ndisplay(box_win_percent.head(8))\n</code></pre> Track Distance Box box_win_percent 0 Albion Park 331 1 0.195652 1 Albion Park 331 2 0.153472 2 Albion Park 331 3 0.125446 3 Albion Park 331 4 0.124615 4 Albion Park 331 5 0.116135 5 Albion Park 331 6 0.105144 6 Albion Park 331 7 0.104770 7 Albion Park 331 8 0.115095 <pre><code># Generate rolling window features\ndataset = dog_results.copy()\ndataset = dataset.set_index(['FastTrack_DogId', 'date_dt']).sort_index()\n\n# Use rolling window of 28, 91 and 365 days\nrolling_windows = ['28D', '91D', '365D']\n# Features to use for rolling windows calculation\nfeatures = ['RunTime_norm', 'SplitMargin_norm', 'Place_inv', 'Place_log', 'Prizemoney_norm']\n# Aggregation functions to apply\naggregates = ['min', 'max', 'mean', 'median', 'std']\n# Keep track of generated feature names\nfeature_cols = ['speed_index', 'box_win_percent']\n\nfor rolling_window in rolling_windows:\n        print(f'Processing rolling window {rolling_window}')\n\n        rolling_result = (\n            dataset\n            .reset_index(level=0)\n            .groupby('FastTrack_DogId')[features]\n            .rolling(rolling_window)\n            .agg(aggregates)\n            .groupby(level=0)\n            .shift(1)\n        )\n\n        # Generate list of rolling window feature names (eg: RunTime_norm_min_365D)\n        agg_features_cols = [f'{f}_{a}_{rolling_window}' for f, a in itertools.product(features, aggregates)]\n        # Add features to dataset\n        dataset[agg_features_cols] = rolling_result\n        # Keep track of generated feature names\n        feature_cols.extend(agg_features_cols)\n</code></pre> <pre>\n<code>Processing rolling window 28D\nProcessing rolling window 91D\nProcessing rolling window 365D\n</code>\n</pre> <pre><code># Replace missing values with 0\ndataset.fillna(0, inplace=True)\ndisplay(dataset.head(8))\n</code></pre> Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR ... Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D FastTrack_DogId date_dt -2143487296 2017-12-14 6.0 JEWELLED COIN 7 7 26.6 13.1 0.0 8.25 0.14 55 ... 0.000000 0.000000 0.000000 0.000000 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 2017-12-21 4.0 JEWELLED COIN 5 5 26.8 9.7 0.0 13.50 3.00 555 ... 0.845098 0.845098 0.845098 0.845098 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 2017-12-26 3.0 JEWELLED COIN 3 3 27.1 21.5 0.0 6.75 2.29 642 ... 0.698970 0.845098 0.772034 0.772034 0.103328 0.0 0.142298 0.071149 0.071149 0.100620 2017-12-30 7.0 JEWELLED COIN 7 9 26.4 48.1 0.0 21.75 2.29 7777 ... 0.602060 0.845098 0.715376 0.698970 0.122347 0.0 0.204697 0.115665 0.142298 0.104915 2018-01-02 8.0 JEWELLED COIN 5 5 26.8 32.7 0.0 15.75 0.00 888 ... 0.602060 0.903090 0.762305 0.772034 0.137070 0.0 0.204697 0.086749 0.071149 0.103357 2018-01-08 4.0 JEWELLED COIN 1 1 27.2 2.5 0.0 6.50 1.29 5443 ... 0.602060 0.954243 0.800692 0.845098 0.146490 0.0 0.204697 0.069399 0.000000 0.097556 2018-01-10 2.0 JEWELLED COIN 5 5 27.3 8.5 0.0 2.00 2.14 442 ... 0.602060 0.954243 0.783738 0.772034 0.137448 0.0 0.204697 0.080926 0.069282 0.091711 2018-01-17 2.0 JEWELLED COIN 3 3 27.4 7.3 0.0 5.25 5.14 433 ... 0.477121 0.954243 0.739936 0.698970 0.170804 0.0 0.204697 0.098329 0.138563 0.095547 <p>8 rows \u00d7 108 columns</p> <p>As we use up to a year of data to generate our feature set, we exclude the first year of the dataset from our training dataset</p> <pre><code># Only keep data after 2018-12-01\nmodel_df = dataset.reset_index()\nfeature_cols = np.unique(feature_cols).tolist()\nmodel_df = model_df[model_df['date_dt'] &amp;gt;= '2018-12-01']\nmodel_df = model_df[['date_dt', 'FastTrack_RaceId', 'DogName', 'win', 'StartPrice_probability'] + feature_cols]\n\n# Only train model off of races where each dog has a value for each feature\nraces_exclude = model_df[model_df.isnull().any(axis = 1)]['FastTrack_RaceId'].drop_duplicates()\nmodel_df = model_df[~model_df['FastTrack_RaceId'].isin(races_exclude)]\n</code></pre> <pre><code>from matplotlib import pyplot\nfrom matplotlib.pyplot import figure\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Split the data into train and test data\ntrain_data = model_df[model_df['date_dt'] &amp;lt; '2021-01-01'].reset_index(drop = True).sample(frac=1)\ntest_data = model_df[model_df['date_dt'] &amp;gt;= '2021-01-01'].reset_index(drop = True)\n\n# Use our previously built features set columns as Training vector\n# Use win flag as Target vector\ntrain_x, train_y = train_data[feature_cols], train_data['win']\ntest_x, test_y = test_data[feature_cols], test_data['win']\n\n# Build a LogisticRegression model\nmodel = LogisticRegression(verbose=0, solver='saga', n_jobs=-1)\n\n# Train the model\nprint(f'Training on {len(train_x):,} samples with {len(feature_cols)} features')\nmodel.fit(train_x, train_y)\n</code></pre> <pre>\n<code>Training on 630,306 samples with 77 features\n</code>\n</pre> <pre>\n<code>LogisticRegression(n_jobs=-1, solver='saga')</code>\n</pre> <p>Now that we have trained our model, we can generate predictions on the test dataset</p> <pre><code># Generate runner win predictions\ndog_win_probabilities = model.predict_proba(test_x)[:,1]\ntest_data['prob_LogisticRegression'] = dog_win_probabilities\n# Normalise probabilities\ntest_data['prob_LogisticRegression'] = test_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / sum(x))\n</code></pre> <pre><code># Create a boolean column for whether a dog has the higehst model prediction in a race\ntest_dataset_size = test_data['FastTrack_RaceId'].nunique()\nodds_win_prediction = test_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x == max(x))\nodds_win_prediction_percent = len(test_data[(odds_win_prediction == True) &amp;amp; (test_data['win'] == 1)]) / test_dataset_size\nprint(f\"LogisticRegression strike rate: {odds_win_prediction_percent:.2%}\")\n</code></pre> <pre>\n<code>LogisticRegression strike rate: 32.57%\n</code>\n</pre> <pre><code>from sklearn.metrics import brier_score_loss\n\nbrier_score = brier_score_loss(test_data['win'], test_data['prob_LogisticRegression'])\nprint(f'LogisticRegression Brier score: {brier_score:.8}')\n</code></pre> <pre>\n<code>LogisticRegression Brier score: 0.11074995\n</code>\n</pre> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\nbins = 100\n\nsns.displot(data=[test_data['prob_LogisticRegression'], test_data['StartPrice_probability']], kind=\"hist\",\n             bins=bins, height=7, aspect=2)\nplt.title('StartPrice vs LogisticRegression probabilities distribution')\nplt.xlabel('Probability')\nplt.show()\n</code></pre> <p>Probabilities generated by the logistic regression model follow a slightly different distribution. <code>Scikit-learn</code> framework offers various hyper parameters to fine tune a model and achieve better performances.</p> <pre><code>from sklearn.calibration import calibration_curve\n\nbins = 100\nfig = plt.figure(figsize=(12, 9))\n\n# Generate calibration curves based on our probabilities\ncal_y, cal_x = calibration_curve(test_data['win'], test_data['prob_LogisticRegression'], n_bins=bins)\n\n# Plot against reference line\nplt.plot(cal_x, cal_y, marker='o', linewidth=1)\nplt.plot([0, 1], [0, 1], '--', color='gray')\nplt.title(\"LogisticRegression calibration curve\");\n</code></pre> <p>A model is perfectly calibrated if the grouped values (bins) follow the dotted line. Our model generates probabilities that need to be calibrated. To get our model to generate more accurate probabilities, we would need to generate better features, test various modelling approaches and calibrate generated probabilities.</p> <pre><code>from matplotlib import pyplot\nfrom matplotlib.pyplot import figure\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Gradient Boosting Machines libraries\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n# Common models parameters\nverbose       = 0\nlearning_rate = 0.1\nn_estimators  = 100\n\n# Train different types of models\nmodels = {\n    'LogisticRegression':         LogisticRegression(verbose=0, solver='saga', n_jobs=-1),\n    'GradientBoostingClassifier': GradientBoostingClassifier(verbose=verbose, learning_rate=learning_rate, n_estimators=n_estimators, max_depth=3, max_features=0.25),\n    'RandomForestClassifier':     RandomForestClassifier(verbose=verbose, n_estimators=n_estimators, max_depth=8, max_features=0.5, n_jobs=-1),\n    'LGBMClassifier':             LGBMClassifier(verbose=verbose, learning_rate=learning_rate, n_estimators=n_estimators, force_col_wise=True),\n    'XGBClassifier':              XGBClassifier(verbosity=verbose, learning_rate=learning_rate, n_estimators=n_estimators, objective='binary:logistic', use_label_encoder=False),\n    'CatBoostClassifier':         CatBoostClassifier(verbose=verbose, learning_rate=learning_rate, n_estimators=n_estimators)\n}\n\nprint(f'Training on {len(train_x):,} samples with {len(feature_cols)} features')\nfor key, model in models.items():\n    print(f'Fitting model {key}')\n    model.fit(train_x, train_y)\n</code></pre> <pre>\n<code>Training on 630,306 samples with 77 features\nFitting model LogisticRegression\nFitting model GradientBoostingClassifier\nFitting model RandomForestClassifier\nFitting model LGBMClassifier\nFitting model XGBClassifier\nFitting model CatBoostClassifier\n</code>\n</pre> <pre><code># Calculate probabilities for each model on the test dataset\nprobs_columns = ['StartPrice_probability']\nfor key, model in models.items():\n    probs_column_key = f'prob_{key}'\n    # Calculate runner win probability\n    dog_win_probs = model.predict_proba(test_x)[:,1]\n    test_data[probs_column_key] = dog_win_probs\n    # Normalise probabilities\n    test_data[probs_column_key] = test_data.groupby('FastTrack_RaceId')[f'prob_{key}'].apply(lambda x: x / sum(x))\n    probs_columns.append(probs_column_key)\n</code></pre> <pre><code># Create a boolean column for whether a dog has the higehst model prediction in a race.\n# Do the same for the starting price as a comparison\ntest_dataset_size = test_data['FastTrack_RaceId'].nunique()\nodds_win_prediction = test_data.groupby('FastTrack_RaceId')['StartPrice_probability'].apply(lambda x: x == max(x))\nodds_win_prediction_percent = len(test_data[(odds_win_prediction == True) &amp;amp; (test_data['win'] == 1)]) / test_dataset_size\nbrier_score = brier_score_loss(test_data['win'], test_data['StartPrice_probability'])\nprint(f'Starting Price                strike rate: {odds_win_prediction_percent:.2%} Brier score: {brier_score:.8}')\n\nfor key, model in models.items():\n    predicted_winners = test_data.groupby('FastTrack_RaceId')[f'prob_{key}'].apply(lambda x: x == max(x))\n    strike_rate = len(test_data[(predicted_winners == True) &amp;amp; (test_data['win'] == 1)]) / test_data['FastTrack_RaceId'].nunique()\n    brier_score = brier_score_loss(test_data['win'], test_data[f'prob_{key}'])\n    print(f'{key.ljust(30)}strike rate: {strike_rate:.2%} Brier score: {brier_score:.8}')\n</code></pre> <pre>\n<code>Starting Price                strike rate: 42.24% Brier score: 0.1008106\nLogisticRegression            strike rate: 32.57% Brier score: 0.11074995\nGradientBoostingClassifier    strike rate: 33.31% Brier score: 0.1105322\nRandomForestClassifier        strike rate: 33.24% Brier score: 0.11110442\nLGBMClassifier                strike rate: 33.40% Brier score: 0.11024272\nXGBClassifier                 strike rate: 33.45% Brier score: 0.11019414\nCatBoostClassifier            strike rate: 33.33% Brier score: 0.11038785\n</code>\n</pre> <pre><code># Display and format sample results\ndef highlight_max(s, props=''):\n    return np.where(s == np.nanmax(s.values), props, '')\ndef highlight_min(s, props=''):\n    return np.where(s == np.nanmin(s.values), props, '')\n\ntest_data[probs_columns].sample(20).style \\\n    .bar(color='#FFA07A', vmin=0.01, vmax=0.25, axis=1) \\\n    .apply(highlight_max, props='color:red;', axis=1) \\\n    .apply(highlight_min, props='color:blue;', axis=1)\n</code></pre> StartPrice_probability prob_LogisticRegression prob_GradientBoostingClassifier prob_RandomForestClassifier prob_LGBMClassifier prob_XGBClassifier prob_CatBoostClassifier 103796 0.168011 0.229477 0.213527 0.198978 0.212484 0.201237 0.216716 148438 0.099749 0.094426 0.128795 0.157217 0.111212 0.064149 0.125434 47999 0.013440 0.063647 0.070135 0.096175 0.077220 0.079547 0.079643 226804 0.331895 0.074406 0.093237 0.087665 0.070858 0.074815 0.092281 14964 0.025668 0.031759 0.028652 0.037364 0.028186 0.025395 0.025159 139208 0.537243 0.269383 0.304885 0.300394 0.286015 0.301443 0.296410 43027 0.257494 0.137246 0.114401 0.113054 0.093332 0.108154 0.126703 151933 0.254678 0.280246 0.204094 0.188197 0.211623 0.196675 0.219325 75586 0.126036 0.106833 0.100814 0.115142 0.111017 0.118383 0.107643 114240 0.530708 0.162428 0.159064 0.129457 0.188566 0.172335 0.153399 73858 0.158025 0.228463 0.233774 0.228696 0.221210 0.217358 0.230763 174698 0.040139 0.039564 0.045854 0.048556 0.040328 0.039350 0.042728 121778 0.036096 0.084464 0.092160 0.097381 0.084467 0.088923 0.089644 227215 0.046275 0.088997 0.090732 0.109514 0.109888 0.109474 0.105836 10731 0.074040 0.106214 0.076128 0.080542 0.070221 0.072588 0.080146 88484 0.064234 0.120542 0.104910 0.117783 0.134223 0.145592 0.114074 104690 0.064302 0.145388 0.126006 0.141474 0.129206 0.120031 0.128490 201852 0.602269 0.367840 0.394838 0.364498 0.401240 0.387149 0.393707 218537 0.073158 0.121725 0.115405 0.113319 0.116149 0.098413 0.112932 119928 0.310934 0.191449 0.176635 0.162701 0.187676 0.182536 0.154486 <p>We now have built a simple feature set and trained models using various classification techniques. To improve our model's performance, one should build a more advanced feature set and fine tune the model's hyper parameters.</p> <pre><code>from sklearn.preprocessing import normalize\n\ntotal_feature_importances = []\n\n# Individual models feature importance\nfor key, model in models.items():\n    figure(figsize=(10, 24), dpi=80)\n    if isinstance(model, LogisticRegression):\n        feature_importance = model.coef_[0]\n    else:\n        feature_importance = model.feature_importances_\n\n    feature_importance = normalize(feature_importance[:,np.newaxis], axis=0).ravel()\n    total_feature_importances.append(feature_importance)\n    pyplot.barh(feature_cols, feature_importance)\n    pyplot.xlabel(f'{key} Features Importance')\n    pyplot.show()\n\n# Overall feature importance\navg_feature_importances = np.asarray(total_feature_importances).mean(axis=0)\nfigure(figsize=(10, 24), dpi=80)\npyplot.barh(feature_cols, avg_feature_importances)\npyplot.xlabel('Overall Features Importance')\npyplot.show()\n</code></pre> <p>This notebook shows an approach to building a simple feature set and training fundamental models using various classification techniques. Analysing these models using metrics such as strike rate and Beyer score indicates that these models have poor performances compared to market starting prices and therefore probably should not be used for betting. To improve your model's performances, the logical next steps would generally be to create new features to add to the dataset, apply various normalisation and standardisation techniques and fine-tune hyper-parameters when training models. This process does take time and effort to get right but is an approach that can be both fun, challenging and sometimes rewarding.</p> <pre><code># Import libraries\nimport os\nimport sys\n\n# Allow imports from src folder\nmodule_path = os.path.abspath(os.path.join('../src'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nfrom dateutil import tz\nfrom pandas.tseries.offsets import MonthEnd\nfrom sklearn.preprocessing import MinMaxScaler\nimport itertools\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport fasttrack as ft\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Validate FastTrack API connection\napi_key = os.getenv('FAST_TRACK_API_KEY', '&lt;replace key=\"\" with=\"\" your=\"\"&gt;')\nclient = ft.Fasttrack(api_key)\ntrack_codes = client.listTracks()\n\n# Import race data excluding NZ races\nau_tracks_filter = list(track_codes[track_codes['state'] != 'NZ']['track_code'])\n\n# Time window to import data\n# First day of the month 46 months back from now\ndate_from = (datetime.today() - relativedelta(months=46)).replace(day=1).strftime('%Y-%m-%d')\n# First day of previous month\ndate_to = (datetime.today() - relativedelta(months=1)).replace(day=1).strftime('%Y-%m-%d')\n\n# Dataframes to populate data with\nrace_details = pd.DataFrame()\ndog_results = pd.DataFrame()\n\n# For each month, either fetch data from API or use local CSV file if we already have downloaded it\nfor start in pd.date_range(date_from, date_to, freq='MS'):\n    start_date = start.strftime(\"%Y-%m-%d\")\n    end_date = (start + MonthEnd(1)).strftime(\"%Y-%m-%d\")\n    try:\n        filename_races = f'FT_AU_RACES_{start_date}.csv'\n        filename_dogs = f'FT_AU_DOGS_{start_date}.csv'\n\n        filepath_races = f'../data/{filename_races}'\n        filepath_dogs = f'../data/{filename_dogs}'\n\n        print(f'Loading data from {start_date} to {end_date}')\n        if os.path.isfile(filepath_races):\n            # Load local CSV file\n            month_race_details = pd.read_csv(filepath_races) \n            month_dog_results = pd.read_csv(filepath_dogs) \n        else:\n            # Fetch data from API\n            month_race_details, month_dog_results = client.getRaceResults(start_date, end_date, au_tracks_filter)\n            month_race_details.to_csv(filepath_races, index=False)\n            month_dog_results.to_csv(filepath_dogs, index=False)\n\n        # Combine monthly data\n        race_details = race_details.append(month_race_details, ignore_index=True)\n        dog_results = dog_results.append(month_dog_results, ignore_index=True)\n    except:\n        print(f'Could not load data from {start_date} to {end_date}')\n\n## Cleanse and normalise the data\n# Clean up the race dataset\nrace_details = race_details.rename(columns = {'@id': 'FastTrack_RaceId'})\nrace_details['Distance'] = race_details['Distance'].apply(lambda x: int(x.replace(\"m\", \"\")))\nrace_details['date_dt'] = pd.to_datetime(race_details['date'], format = '%d %b %y')\n# Clean up the dogs results dataset\ndog_results = dog_results.rename(columns = {'@id': 'FastTrack_DogId', 'RaceId': 'FastTrack_RaceId'})\n\n# Combine dogs results with race attributes\ndog_results = dog_results.merge(\n    race_details[['FastTrack_RaceId', 'Distance', 'RaceGrade', 'Track', 'date_dt']], \n    how = 'left',\n    on = 'FastTrack_RaceId'\n)\n\n# Convert StartPrice to probability\ndog_results['StartPrice'] = dog_results['StartPrice'].apply(lambda x: None if x is None else float(x.replace('$', '').replace('F', '')) if isinstance(x, str) else x)\ndog_results['StartPrice_probability'] = (1 / dog_results['StartPrice']).fillna(0)\ndog_results['StartPrice_probability'] = dog_results.groupby('FastTrack_RaceId')['StartPrice_probability'].apply(lambda x: x / x.sum())\n\n# Discard entries without results (scratched or did not finish)\ndog_results = dog_results[~dog_results['Box'].isnull()]\ndog_results['Box'] = dog_results['Box'].astype(int)\n\n# Clean up other attributes\ndog_results['RunTime'] = dog_results['RunTime'].astype(float)\ndog_results['SplitMargin'] = dog_results['SplitMargin'].astype(float)\ndog_results['Prizemoney'] = dog_results['Prizemoney'].astype(float).fillna(0)\ndog_results['Place'] = pd.to_numeric(dog_results['Place'].apply(lambda x: x.replace(\"=\", \"\") if isinstance(x, str) else 0), errors='coerce').fillna(0)\ndog_results['win'] = dog_results['Place'].apply(lambda x: 1 if x == 1 else 0)\n\n# Normalise some of the raw values\ndog_results['Prizemoney_norm'] = np.log10(dog_results['Prizemoney'] + 1) / 12\ndog_results['Place_inv'] = (1 / dog_results['Place']).fillna(0)\ndog_results['Place_log'] = np.log10(dog_results['Place'] + 1).fillna(0)\ndog_results['RunSpeed'] = (dog_results['RunTime'] / dog_results['Distance']).fillna(0)\n\n## Generate features using raw data\n# Calculate median winner time per track/distance\nwin_results = dog_results[dog_results['win'] == 1]\nmedian_win_time = pd.DataFrame(data=win_results[win_results['RunTime'] &amp;gt; 0].groupby(['Track', 'Distance'])['RunTime'].median()).rename(columns={\"RunTime\": \"RunTime_median\"}).reset_index()\nmedian_win_split_time = pd.DataFrame(data=win_results[win_results['SplitMargin'] &amp;gt; 0].groupby(['Track', 'Distance'])['SplitMargin'].median()).rename(columns={\"SplitMargin\": \"SplitMargin_median\"}).reset_index()\nmedian_win_time.head()\n\n# Calculate track speed index\nmedian_win_time['speed_index'] = (median_win_time['RunTime_median'] / median_win_time['Distance'])\nmedian_win_time['speed_index'] = MinMaxScaler().fit_transform(median_win_time[['speed_index']])\nmedian_win_time.head()\n\n# Compare dogs finish time with median winner time\ndog_results = dog_results.merge(median_win_time, on=['Track', 'Distance'], how='left')\ndog_results = dog_results.merge(median_win_split_time, on=['Track', 'Distance'], how='left')\n\n# Normalise time comparison\ndog_results['RunTime_norm'] = (dog_results['RunTime_median'] / dog_results['RunTime']).clip(0.9, 1.1)\ndog_results['RunTime_norm'] = MinMaxScaler().fit_transform(dog_results[['RunTime_norm']])\ndog_results['SplitMargin_norm'] = (dog_results['SplitMargin_median'] / dog_results['SplitMargin']).clip(0.9, 1.1)\ndog_results['SplitMargin_norm'] = MinMaxScaler().fit_transform(dog_results[['SplitMargin_norm']])\ndog_results.head()\n\n# Calculate box winning percentage for each track/distance\nbox_win_percent = pd.DataFrame(data=dog_results.groupby(['Track', 'Distance', 'Box'])['win'].mean()).rename(columns={\"win\": \"box_win_percent\"}).reset_index()\n# Add to dog results dataframe\ndog_results = dog_results.merge(box_win_percent, on=['Track', 'Distance', 'Box'], how='left')\n# Display example of barrier winning probabilities\ndisplay(box_win_percent.head(8))\n\n# Generate rolling window features\ndataset = dog_results.copy()\ndataset = dataset.set_index(['FastTrack_DogId', 'date_dt']).sort_index()\n\n# Use rolling window of 28, 91 and 365 days\nrolling_windows = ['28D', '91D', '365D']\n# Features to use for rolling windows calculation\nfeatures = ['RunTime_norm', 'SplitMargin_norm', 'Place_inv', 'Place_log', 'Prizemoney_norm']\n# Aggregation functions to apply\naggregates = ['min', 'max', 'mean', 'median', 'std']\n# Keep track of generated feature names\nfeature_cols = ['speed_index', 'box_win_percent']\n\nfor rolling_window in rolling_windows:\n        print(f'Processing rolling window {rolling_window}')\n\n        rolling_result = (\n            dataset\n            .reset_index(level=0)\n            .groupby('FastTrack_DogId')[features]\n            .rolling(rolling_window)\n            .agg(aggregates)\n            .shift(1)\n        )\n\n        # Generate list of rolling window feature names (eg: RunTime_norm_min_365D)\n        agg_features_cols = [f'{f}_{a}_{rolling_window}' for f, a in itertools.product(features, aggregates)]\n        # Add features to dataset\n        dataset[agg_features_cols] = rolling_result\n        # Keep track of generated feature names\n        feature_cols.extend(agg_features_cols)\n\n# Replace missing values with 0\ndataset.fillna(0, inplace=True)\ndisplay(dataset.head(8))\n\n# Only keep data after 2018-12-01\nmodel_df = dataset.reset_index()\nfeature_cols = np.unique(feature_cols).tolist()\nmodel_df = model_df[model_df['date_dt'] &amp;gt;= '2018-12-01']\nmodel_df = model_df[['date_dt', 'FastTrack_RaceId', 'DogName', 'win', 'StartPrice_probability'] + feature_cols]\n\n# Only train model off of races where each dog has a value for each feature\nraces_exclude = model_df[model_df.isnull().any(axis = 1)]['FastTrack_RaceId'].drop_duplicates()\nmodel_df = model_df[~model_df['FastTrack_RaceId'].isin(races_exclude)]\n\n## Build and train Regression models\nfrom matplotlib import pyplot\nfrom matplotlib.pyplot import figure\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Split the data into train and test data\ntrain_data = model_df[model_df['date_dt'] &amp;lt; '2021-01-01'].reset_index(drop = True).sample(frac=1)\ntest_data = model_df[model_df['date_dt'] &amp;gt;= '2021-01-01'].reset_index(drop = True)\n\n# Use our previously built features set columns as Training vector\n# Use win flag as Target vector\ntrain_x, train_y = train_data[feature_cols], train_data['win']\ntest_x, test_y = test_data[feature_cols], test_data['win']\n\n# Build a LogisticRegression model\nmodel = LogisticRegression(verbose=0, solver='saga', n_jobs=-1)\n\n# Train the model\nprint(f'Training on {len(train_x):,} samples with {len(feature_cols)} features')\nmodel.fit(train_x, train_y)\n\n# Generate runner win predictions\ndog_win_probabilities = model.predict_proba(test_x)[:,1]\ntest_data['prob_LogisticRegression'] = dog_win_probabilities\n# Normalise probabilities\ntest_data['prob_LogisticRegression'] = test_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / sum(x))\n\n# Create a boolean column for whether a dog has the higehst model prediction in a race\ntest_dataset_size = test_data['FastTrack_RaceId'].nunique()\nodds_win_prediction = test_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x == max(x))\nodds_win_prediction_percent = len(test_data[(odds_win_prediction == True) &amp;amp; (test_data['win'] == 1)]) / test_dataset_size\nprint(f\"LogisticRegression strike rate: {odds_win_prediction_percent:.2%}\")\n\nfrom sklearn.metrics import brier_score_loss\n\nbrier_score = brier_score_loss(test_data['win'], test_data['prob_LogisticRegression'])\nprint(f'LogisticRegression Brier score: {brier_score:.8}')\n\n# Predictions distribution\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nbins = 100\n\nsns.displot(data=[test_data['prob_LogisticRegression'], test_data['StartPrice_probability']], kind=\"hist\",\n             bins=bins, height=7, aspect=2)\nplt.title('StartPrice vs LogisticRegression probabilities distribution')\nplt.xlabel('Probability')\nplt.show()\n\n# Predictions calibration\nfrom sklearn.calibration import calibration_curve\n\nbins = 100\nfig = plt.figure(figsize=(12, 9))\n\n# Generate calibration curves based on our probabilities\ncal_y, cal_x = calibration_curve(test_data['win'], test_data['prob_LogisticRegression'], n_bins=bins)\n\n# Plot against reference line\nplt.plot(cal_x, cal_y, marker='o', linewidth=1)\nplt.plot([0, 1], [0, 1], '--', color='gray')\nplt.title(\"LogisticRegression calibration curve\");\n\n# Other classification models\nfrom matplotlib import pyplot\nfrom matplotlib.pyplot import figure\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Gradient Boosting Machines libraries\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n# Common models parameters\nverbose       = 0\nlearning_rate = 0.1\nn_estimators  = 100\n\n# Train different types of models\nmodels = {\n    'LogisticRegression':         LogisticRegression(verbose=0, solver='saga', n_jobs=-1),\n    'GradientBoostingClassifier': GradientBoostingClassifier(verbose=verbose, learning_rate=learning_rate, n_estimators=n_estimators, max_depth=3, max_features=0.25),\n    'RandomForestClassifier':     RandomForestClassifier(verbose=verbose, n_estimators=n_estimators, max_depth=8, max_features=0.5, n_jobs=-1),\n    'LGBMClassifier':             LGBMClassifier(verbose=verbose, learning_rate=learning_rate, n_estimators=n_estimators, force_col_wise=True),\n    'XGBClassifier':              XGBClassifier(verbosity=verbose, learning_rate=learning_rate, n_estimators=n_estimators, objective='binary:logistic', use_label_encoder=False),\n    'CatBoostClassifier':         CatBoostClassifier(verbose=verbose, learning_rate=learning_rate, n_estimators=n_estimators)\n}\n\nprint(f'Training on {len(train_x):,} samples with {len(feature_cols)} features')\nfor key, model in models.items():\n    print(f'Fitting model {key}')\n    model.fit(train_x, train_y)\n\n# Calculate probabilities for each model on the test dataset\nprobs_columns = ['StartPrice_probability']\nfor key, model in models.items():\n    probs_column_key = f'prob_{key}'\n    # Calculate runner win probability\n    dog_win_probs = model.predict_proba(test_x)[:,1]\n    test_data[probs_column_key] = dog_win_probs\n    # Normalise probabilities\n    test_data[probs_column_key] = test_data.groupby('FastTrack_RaceId')[f'prob_{key}'].apply(lambda x: x / sum(x))\n    probs_columns.append(probs_column_key)\n\n# Calculate model strike rate and Brier score across models\n# Create a boolean column for whether a dog has the higehst model prediction in a race.\n# Do the same for the starting price as a comparison\ntest_dataset_size = test_data['FastTrack_RaceId'].nunique()\nodds_win_prediction = test_data.groupby('FastTrack_RaceId')['StartPrice_probability'].apply(lambda x: x == max(x))\nodds_win_prediction_percent = len(test_data[(odds_win_prediction == True) &amp;amp; (test_data['win'] == 1)]) / test_dataset_size\nbrier_score = brier_score_loss(test_data['win'], test_data['StartPrice_probability'])\nprint(f'Starting Price                strike rate: {odds_win_prediction_percent:.2%} Brier score: {brier_score:.8}')\n\nfor key, model in models.items():\n    predicted_winners = test_data.groupby('FastTrack_RaceId')[f'prob_{key}'].apply(lambda x: x == max(x))\n    strike_rate = len(test_data[(predicted_winners == True) &amp;amp; (test_data['win'] == 1)]) / test_data['FastTrack_RaceId'].nunique()\n    brier_score = brier_score_loss(test_data['win'], test_data[f'prob_{key}'])\n    print(f'{key.ljust(30)}strike rate: {strike_rate:.2%} Brier score: {brier_score:.8}')\n\n# Visualise model predictions\n# Display and format sample results\ndef highlight_max(s, props=''):\n    return np.where(s == np.nanmax(s.values), props, '')\ndef highlight_min(s, props=''):\n    return np.where(s == np.nanmin(s.values), props, '')\n\ntest_data[probs_columns].sample(20).style \\\n    .bar(color='#FFA07A', vmin=0.01, vmax=0.25, axis=1) \\\n    .apply(highlight_max, props='color:red;', axis=1) \\\n    .apply(highlight_min, props='color:blue;', axis=1)\n\n## Display models feature importance\nfrom sklearn.preprocessing import normalize\n\ntotal_feature_importances = []\n\n# Individual models feature importance\nfor key, model in models.items():\n    figure(figsize=(10, 24), dpi=80)\n    if isinstance(model, LogisticRegression):\n        feature_importance = model.coef_[0]\n    else:\n        feature_importance = model.feature_importances_\n\n    feature_importance = normalize(feature_importance[:,np.newaxis], axis=0).ravel()\n    total_feature_importances.append(feature_importance)\n    pyplot.barh(feature_cols, feature_importance)\n    pyplot.xlabel(f'{key} Features Importance')\n    pyplot.show()\n\n# Overall feature importance\navg_feature_importances = np.asarray(total_feature_importances).mean(axis=0)\nfigure(figsize=(10, 24), dpi=80)\npyplot.barh(feature_cols, avg_feature_importances)\npyplot.xlabel('Overall Features Importance')\npyplot.show()\n</code></pre>"},{"location":"modelling/greyhoundModellingPython/#greyhound-modelling-in-python","title":"Greyhound modelling in Python","text":"<p>| Building a Greyhound Racing model with Scikit-learn Logistic Regression and Ensemble Learning</p>"},{"location":"modelling/greyhoundModellingPython/#workshop","title":"Workshop","text":""},{"location":"modelling/greyhoundModellingPython/#overview","title":"Overview","text":"<p>This tutorial will walk you through the different steps required to generate Greyhound racing winning probabilities</p> <ol> <li>Download historic greyhound data from FastTrack API</li> <li>Cleanse and normalise the data</li> <li>Generate features using raw data</li> <li>Build and train classification models</li> <li>Evaluate models' performances</li> <li>Evaluate feature importance</li> </ol>"},{"location":"modelling/greyhoundModellingPython/#requirements","title":"Requirements","text":"<ul> <li>You will need a Betfair API app key. If you don't have one please follow the steps outlined on the The Automation Hub</li> <li>You will need your own FastTrack security key. Please note - The FastTrack DDC has been moved across to the new Topaz API as of December 2023. This means that, while we can source a key for you, the code in this tutorial will not work for the Topaz API. We will be updating this tutorial in early 2024 to reflect the new Topaz API nomenclature and documentation. (Additionally, only Australia and New Zealand customers are eligible for a free FastTrack key). If you would like to be considered for a FastTrack Topaz key, please email data@betfair.com.au.</li> <li>This notebook and accompanying files is shared on <code>betfair-downunder</code>'s Github.</li> </ul>"},{"location":"modelling/greyhoundModellingPython/#note-fasttrack-api-key","title":"Note - FastTrack API key","text":"<p>If you follow README instructions to run this notebook locally, you should have configured a <code>.env</code> file with your FastTrack API key. Otherwise you can set your API key below.</p>"},{"location":"modelling/greyhoundModellingPython/#1-download-historic-greyhound-data-from-fasttrack-api","title":"1. Download historic greyhound data from FastTrack API","text":"<p>The cell below downloads FastTrack AU race data for the past few months. Data is cached locally in the <code>data</code> folder so it can easily be reused for further processing. Depending on the amount of data to retrieve, this can take a few hours.</p>"},{"location":"modelling/greyhoundModellingPython/#2-cleanse-and-normalise-the-data","title":"2. Cleanse and normalise the data","text":"<p>Here we do some basic data manipulation and cleansing to get variables into format that we can work with</p>"},{"location":"modelling/greyhoundModellingPython/#3-generate-features-using-raw-data","title":"3. Generate features using raw data","text":""},{"location":"modelling/greyhoundModellingPython/#calculate-median-winner-time-by-trackdistance","title":"Calculate median winner time by track/distance","text":"<p>To compare individual runner times, we extract the median winner time for each Track/Distance and use it as a reference time.</p>"},{"location":"modelling/greyhoundModellingPython/#calculate-track-speed-index","title":"Calculate Track speed index","text":"<p>Some tracks are run faster than other, we calculate here a <code>speed_index</code> using the track reference time over the travelled distance. The lower the <code>speed_index</code>, the faster the track is. We use <code>MinMaxScaler</code> to scale <code>speed_index</code> values between zero and one.</p>"},{"location":"modelling/greyhoundModellingPython/#compare-individual-times-with-track-reference-time","title":"Compare individual times with track reference time","text":"<p>For each dog result, we compare the runner time with the reference time using the formula <code>(track reference time) / (runner time)</code> and normalise the result. The higher the value, the quicker the dog was.</p>"},{"location":"modelling/greyhoundModellingPython/#barrier-winning-probabilities","title":"Barrier winning probabilities","text":"<p>The barrier dogs start from play a big part in the race so we calculate the winning percentage for each <code>barrier/track/distance</code></p>"},{"location":"modelling/greyhoundModellingPython/#generate-time-based-features","title":"Generate time-based features","text":"<p>Now that we have a set of basic features for individual dog results, we need to aggregate them into a single feature vector.</p> <p>To do so, we calculate the <code>min</code>, <code>max</code>, <code>mean</code>, <code>median</code>, <code>std</code> of features previously caculated over different time windows <code>28</code>, <code>91</code> and <code>365</code> days:</p> <ul> <li><code>RunTime_norm</code></li> <li><code>SplitMargin_norm</code></li> <li><code>Place_inv</code></li> <li><code>Place_log</code></li> <li><code>Prizemoney_norm</code></li> </ul> <p>This will allow us to create a short, medium and long term measures of our features.</p> <p>Depending on the dataset size, this can take several minutes.</p>"},{"location":"modelling/greyhoundModellingPython/#4-build-and-train-regression-models","title":"4. Build and train regression models","text":""},{"location":"modelling/greyhoundModellingPython/#logistic-regression","title":"Logistic regression","text":"<p>The dataset is split between training and validation:</p> <ul> <li>train dataset: from 2018-12-01 to 2020-12-31</li> <li>validation dataset: from 2021-01-01</li> </ul> <p>The next cell trains a LogisticRegression model and uses the <code>win</code> flag (<code>0=lose, 1=win</code>) as a target.</p>"},{"location":"modelling/greyhoundModellingPython/#5-evaluate-model-predictions","title":"5. Evaluate model predictions","text":""},{"location":"modelling/greyhoundModellingPython/#model-strike-rate","title":"Model strike rate","text":"<p>Knowing how often a model correctly predicts the winner is one of the most important metrics</p>"},{"location":"modelling/greyhoundModellingPython/#brier-score","title":"Brier score","text":"<p>The Brier score measures the mean squared difference between the predicted probability and the actual outcome. The smaller the Brier score loss, the better.</p>"},{"location":"modelling/greyhoundModellingPython/#predictions-distribution","title":"Predictions' distribution","text":"<p>To get a better feel of what our models are predicting, we can plot the generated probabilities' distribution and compare them with Start Prices probabilities' distribution.</p>"},{"location":"modelling/greyhoundModellingPython/#predictions-calibration","title":"Predictions calibration","text":"<p>We want to ensure that probabilities generated by our model match real world probabilities. Calibration curves help us understand if a model needs to be calibrated.</p>"},{"location":"modelling/greyhoundModellingPython/#compare-other-types-of-classification-models","title":"Compare other types of classification models","text":"<p>The next cell trains different classification models using <code>Scikit-learn</code> unified API:</p> <ul> <li>GradientBoostingClassifier</li> <li>RandomForestClassifier</li> <li>LGBMClassifier</li> <li>XGBClassifier</li> <li>CatBoostClassifier</li> </ul> <p>Depending on dataset size and compute capacity, this can take several minutes</p>"},{"location":"modelling/greyhoundModellingPython/#calculate-models-strike-rate-and-brier-score","title":"Calculate models strike rate and Brier score","text":"<p>Here we compare the strike rate of the different models' predictions with the start price strike rate.</p>"},{"location":"modelling/greyhoundModellingPython/#visualise-models-predictions","title":"Visualise models predictions","text":"<p>Here we generate some probabilities using our trained models' and compare them with the start price.</p> <p>In blue the lowest prediction and in red the highest prediction generated by the different models.</p>"},{"location":"modelling/greyhoundModellingPython/#6-display-models-features-importance","title":"6. Display models' features' importance","text":""},{"location":"modelling/greyhoundModellingPython/#conclusion","title":"Conclusion","text":""},{"location":"modelling/greyhoundModellingPython/#complete-code","title":"Complete Code","text":"<p>Run the code from your ide by using <code>py &lt;filename&gt;.py</code>, making sure you amend the path to point to your input data. </p> <p>Download from Github </p>"},{"location":"modelling/greyhoundModellingPython/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"modelling/greyhoundModellingPythonREADME/","title":"Betfair Greyhound Modelling","text":""},{"location":"modelling/greyhoundModellingPythonREADME/#project-setup","title":"Project Setup","text":"<p>This project assume you have a developmemnt environment with Python and pip installed</p> <p>Clone project <pre><code>git clone https://github.com/BrunoChauvet/greyhound-modelling.git\n</code></pre></p> <p>Install python libraries <pre><code>cd greyhound-modellin\npip install --user -r requirements.txt\n</code></pre></p> <p>Create a configuration file named <code>.env</code> in the rot folder and set required values: <pre><code>FAST_TRACK_API_KEY=&lt;your key&gt;\n</code></pre></p> <p>Launch notebook <pre><code>jupyter notebook\n</code></pre></p>"},{"location":"modelling/greyhoundModellingPythonREADME/#notebooks","title":"Notebooks","text":"<p>Below is the list of notebooks with detailed examples</p>"},{"location":"modelling/greyhoundModellingPythonREADME/#logistic-regression","title":"Logistic Regression","text":"<p>logistic_regression provides a step-by-step tutorial from fetching Greyhound racing data from FastTrack API to generating win probabilities using Scikit-learn and various Regression techniques.</p>"},{"location":"modelling/greyhoundModellingPythonREADME/#feature-importance","title":"Feature Importance","text":"<p>TODO</p>"},{"location":"modelling/greyhoundModellingPythonREADME/#models-ensemble","title":"Models Ensemble","text":"<p>TODO</p>"},{"location":"modelling/howToModel/","title":"Intro to modelling","text":"<p>Want to learn how to create your own predictive model using sports or racing data, but you don\u2019t know where to start? We\u2019re here to help.</p> <p>The Data Scientists at Betfair have put together the first few steps we suggest you take to get you started on your data modelling journey. We also run occasional data modelling workshops to help you get the basics down \u2013 reach out and let us know if you\u2019re interested in being notified about upcoming data events.</p>"},{"location":"modelling/howToModel/#choose-your-language","title":"Choose your language","text":"<p>There are lots of programming languages to choose from. For our data modelling workshops we work in R and Python, as they\u2019re both relatively easy to learn and designed for working with data.</p> <p>If you\u2019re new to these languages, here are some resources that will help get you set up.</p>"},{"location":"modelling/howToModel/#language-1-r","title":"Language 1: R","text":"<ul> <li>What is R?</li> <li>Download and install R \u2013 get the language set up on your computer</li> <li>Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R</li> <li>Take a look at the some of the existing R libraries you can use if you want to connect to our API, including abettor and our Data Scientists\u2019 R repo.</li> </ul>"},{"location":"modelling/howToModel/#language-2-python","title":"Language 2: Python","text":"<ul> <li>What is Python?</li> <li>Download and install Anaconda Distribution \u2013 this will install Python and a heap of data science packages along with it</li> </ul>"},{"location":"modelling/howToModel/#find-a-data-source","title":"Find a data source","text":"<p>Finding quality data is crucial to being able to create a successful model. We have lots of historical Exchange data that we\u2019re happy to share, and there are lots of other sources of sports or racing specific data available online, depending on what you\u2019re looking for.</p> <p>For our workshops we use historical NBA odds data from the Exchange (which you can download directly from here, along with NBA game data from a variety of sources including:</p> <ul> <li>ESPN.com</li> <li>NBA.com</li> <li>basketball-reference.com</li> <li>Stattleship\u2019s API</li> </ul>"},{"location":"modelling/howToModel/#learn-to-program","title":"Learn to Program","text":"<p>Okay, so easier said than done, but you don't actually need a high level of programming knowledge to be able to build a decent model, and there are so many excellent resources available online that the barrier to entry is much lower than it's been in the past.</p> <p>These are some of our favourites if you want to learn to use R or Python for data modelling:</p> <ul> <li>Dataquest \u2013 free coding resource for learning both Python and R for data science</li> <li>Datacamp \u2013 another popular free resource to learn both R and Python for data science</li> <li>Codeacademy \u2013 free online programming courses with community engagement</li> </ul> <p>We've also shared a R repo for connecting with our API, which might make that part of the learning process easier for you, if you go down that path.</p>"},{"location":"modelling/howToModel/#learn-how-to-model-data","title":"Learn how to model data","text":"<p>We\u2019ve put together some articles to give you an introduction to some of the different approaches you can take to modelling data, but again there are also lots of resources available online. Here are some good places to start:</p> <ul> <li>Work through the modelling tutorials we've put together using AFL and soccer data</li> <li>This Introduction to Tennis Modelling gives a good overview of ranking-based models, regression-based models and point-based models</li> <li>How we used ELO and machine learning as different approaches to modelling the World Cup</li> </ul>"},{"location":"modelling/howToModel/#get-your-hands-dirty","title":"Get your hands dirty","text":"<p>The best way to learn is by doing. Make sure you have a solid foundation knowledge to work from, then get excited, get your hands dirty and see what you can create! Here are a final few thoughts to help you decide where to from here:</p> <ul> <li>Make sure you\u2019ve got your Betfair Basics knowledge solid including back betting, lay betting and reading exchange markets</li> <li>Learn about the importance of ratings and prices and get inspired by the models created by our Data Scientists</li> <li>Consider how you could use our API in building and automating your model</li> <li>Find out about how successful some of our customers have been in their modelling journeys</li> </ul>"},{"location":"modelling/howToModelTheAusOpen/","title":"How to model the Australian Open","text":"<p>Betfair\u2019s Data Scientists Team put together a collection of articles on How to Build a Model and in preparation for the previous Betfair Aus Open Datathon.</p> <p>This article will outline their thought process and share their approach. Subsequent articles will be posted with code examples that outline how this approach can be put into practice.</p>"},{"location":"modelling/howToModelTheAusOpen/#tools-for-creating-our-model","title":"Tools for creating our model","text":"<p>We will be providing a step by step tutorial in two languages \u2013 Python and R.</p> <ul> <li>Tutorial in Python</li> <li>Tutorial in R</li> </ul> <p>These are the two most popular languages used in data science nowadays. Both code examples will follow identical approaches.</p>"},{"location":"modelling/howToModelTheAusOpen/#tournament-structure","title":"Tournament structure","text":"<p>The Datathon structure requires contestants to predict every possible tournament match-up only using data that is available at the start of the tournament.</p> <p>This means we can\u2019t use information from previous rounds (data from Round 1 matches for potential Round 2 matches and so on). For example, if we were to just train our model on all the tennis matches in the data set, our model would have been trained assuming that it had the result from previous rounds in the Australian Open.</p> <p>But this isn\u2019t the case, so we need to account for this nuance of the competition. We need to ensure that we don\u2019t include previous round data from the same tournament in the way we structure our features for predicting results.</p>"},{"location":"modelling/howToModelTheAusOpen/#how-to-set-up-data-and-features","title":"How to set up data and features","text":"<p>In predictive modelling language \u2013 features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options</p> <p>Training the model</p> <ul> <li>We can train the model on every tennis match in the data set or</li> <li>We can only train the model on Australian Open matches</li> </ul> <p>Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure.</p> <p>Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on.</p> <p>In the end, we decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface \u2013 hard court.</p> <p>Next decision is to decide the features (or the metrics we feed into the model, which makes the decision on who the winner is going to be).</p> <p>We don\u2019t have a definitive list of features that we will use, but we will most likely keep the number of features quite low (between 4-5).</p> <p>Features set</p> <p>Likely features may include:</p> <ul> <li>ELO</li> <li>First serve %</li> <li>Winners-unforced error ratio</li> </ul> <p>We will also use the difference between opponents' statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns. This will reduce the dimensionality of the model.</p> <p>A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, the difference in ELO rating etc.</p>"},{"location":"modelling/howToModelTheAusOpen/#target-variable","title":"Target variable","text":"<p>Our target variable (what we are trying to predict) is whether player A wins or not against player B. In machine learning terms this is a classification problem.</p> <p>The output will be a probability number between 0 and 1. A number closer to 0 means Player B is likely to win, and a number closer to 1 will mean Player A is likely to win.</p> <p>Another positive of a probabilistic outcome is that they can easily be converted to odds, and can also be compared with the historical Betfair odds that have been provided, and test if our model would have been profitable for previous seasons.</p>"},{"location":"modelling/howToModelTheAusOpen/#sports-modelling-nuances","title":"Sports modelling nuances","text":"<p>Sports data is inherently complex to model. Generally when predicting something, like \u201cwill it rain today\u201d, you have information for that day, such as the temperature, which you can use in formulating your prediction.</p> <p>However with sports data, you cannot use the majority of information that is provided in the raw dataset, such as aces, winners, etc, as this will create what is called feature leakage \u2013 using data from after the event, which you won\u2019t have access to before the event, to predict the result.</p> <p>You will also need to use historic results in such a way that will have predictive power for the sports event that you are trying to predict. This means that you run into little nuances like needing to use rolling averages, as well as whether to model each match on a single row or multiple rows. In the next article in this series, we will show you how we tackle this problem using code examples that anyone can replicate.</p>"},{"location":"modelling/howToModelTheAusOpen/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"modelling/soccerEloTutorialR/","title":"Using an Elo approach to model soccer in R","text":"<p>This tutorial was written as part of the 2021 Euro &amp; Copa America Datathon competition</p>"},{"location":"modelling/soccerEloTutorialR/#elo-modelling","title":"Elo modelling","text":"<p>Elo modelling is a commonly-used approach towards creating rating systems in sport. Originally devised by Arpad Elo for ranking chess players, the popularity of Elo modelling has grown massively since its first official use in the 1960s to the point where it is now widely used to model just about every professional sporting code across the globe.</p> <p>From a set of Elo ratings we can construct win/loss probabilities for given match-ups between two teams using a simple formula which takes the difference between the two teams' ratings and outputs the probability of each team winning or losing the match. A very basic yet very effective approach!</p> <p>To read more the inner workings of Elo models, take a read of this article for a detailed run-down of how the mathematics behind an Elo rating system works in the context of tennis matches.</p>"},{"location":"modelling/soccerEloTutorialR/#elo-models-soccer","title":"Elo Models &amp; Soccer","text":"<p>Elo-based systems lend themselves particularly well to modelling soccer, so much so that publicly available Elo ratings systems (www.eloratings.net for international soccer for example)  have been adopted by professional bodies to help seed tournaments and create fairer fixtures.</p> <p>This tutorial aims to serve as guide of how to build a basic soccer Elo model with a particular focus on the 2021 editions of the Euro and Copa America, as they served as the subject of Betfair's Datathon.</p> <p>To follow along with this tutorial you will need two things:</p> <ol> <li>This code is written in R, and hence you will need to have R and RStudio running on your system if you wish to follow along.</li> <li>You will also need the historical data set provided for Betfair's 2021 Euro &amp; Copa America Datathon. For full access to the data set including international soccer fixtures from 2014 to 2021, please reach out to datathon@betfair.com.au for data access.</li> </ol> <p>Let's get started!</p>"},{"location":"modelling/soccerEloTutorialR/#load-packages-import-data","title":"Load Packages &amp; Import Data","text":"<p>The first thing to do is load the packages that will be required to run the Elo model and to read in the historic data from wherever it is stored on your machine.</p> <p>The main package to take notice of here is the <code>elo</code> package. As the name suggests, this package is for running Elo models. The <code>MLmetrics</code> package will be used towards the end of the tutorial to back-test the accuracy of our model.</p> <pre><code>library(readr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(elo)\nlibrary(MLmetrics)\n\nraw_data &lt;- read_csv(\"datathon_initial_form_data.csv\")\n</code></pre> <p>When exploring a new data set, it is always good practice to first get an idea of the what is included in the data set.</p> <pre><code>colnames(raw_data)\n</code></pre> <p>Something noticeable is that the data set does not have a feature included to indicate the result of each match. Let's start by creating two new columns: one showing the result from the home team's perspective and another from the away team's perspective.</p> <p>We'll also create a feature to show the margin of the match, i.e. the absolute goal difference between the two sides.</p> <pre><code>raw_data &lt;- raw_data %&gt;% \n  mutate(home_result = case_when(home_team_goal_count &gt; away_team_goal_count ~ 1,\n                                 home_team_goal_count &lt; away_team_goal_count ~ 0,\n                                 home_team_goal_count == away_team_goal_count ~ 0.5),\n  away_result = case_when(home_team_goal_count &lt; away_team_goal_count ~ 1,\n                          home_team_goal_count &gt; away_team_goal_count ~ 0,\n                          home_team_goal_count == away_team_goal_count ~ 0.5),\n  margin = abs(home_team_goal_count - away_team_goal_count))\n</code></pre> <p>Let's also make sure the date_GMT column is formatted correctly as a date. We can do that using the <code>parse_date_time</code> function from the <code>lubridate</code> package.</p> <pre><code>raw_data &lt;- raw_data %&gt;% \n  mutate(date_GMT = parse_date_time(raw_data$date_GMT , \"mdY_HM\"))\n</code></pre>"},{"location":"modelling/soccerEloTutorialR/#running-the-elo-model","title":"Running the Elo Model","text":"<p>Now comes the fun part - setting the Elo model in action!</p> <p>There are a number of steps we could have taken before getting to this point to make the model a touch more complex (and possibly more accurate as a result), however in the interest of keeping this tutorial as accessible and easy to follow as possible let's jump straight into running an Elo model based on our data set.</p> <p>To run the Elo model we will need to use the <code>elo.run</code> function from the <code>elo</code> package. The <code>elo.run</code> function requires we input a formula as an argument which tells the function which columns list the two teams, which column is our target/outcome variable, as well as any other features we want to include in our model.</p> <p>We can use the <code>home_result</code> column we've created to identify the result from the home team's perspective.</p> <p>We can also input a k value which essentially dictates the maximum number of Elo points that can be won or lost in a single match.</p> <p>Here we could set k to be a constant (such as 30), however to add an extra layer of (very slight) complexity, we can instead choose to use a variable k value which is dictated by the margin of the match using a formula of <code>k = 30 + 30*margin</code>. This means that for every goal the margin increases by, the k value will increase by 30, starting with a base k value of 30 for draws (<code>margin equals 0</code>). The idea here is simply to help account for and reward/punish teams winning/losing by bigger margins compared to closer matches.</p> <pre><code>elo_model &lt;- elo.run(data = raw_data,\n                     formula = home_result ~ home_team_name + away_team_name + k(30 + 30*margin))\n</code></pre> <p>Let's take a look at the last few matches in our training sample to get a better view of the Elo model in action.</p> <pre><code>elo_results &lt;- elo_model %&gt;%\n  as.data.frame()\n\nelo_results %&gt;% tail(n = 10)\n</code></pre> <p>We can also now check out what the latest Elo rankings for each team in the data set! Let's look at the top 20.</p> <p>Keep in mind that teams start with an Elo rating of 1500, so any team with an Elo greater 1500 can be considered above average, while those with Elo ratings below 1500 are therefore below average.</p> <pre><code>final_elos &lt;- final.elos(elo_model)\nfinal_elos %&gt;% sort(decreasing = TRUE) %&gt;% head(n = 20)\n</code></pre>"},{"location":"modelling/soccerEloTutorialR/#accounting-for-draws","title":"Accounting for Draws","text":"<p>Something that we need to acknowledge both generally and also in relation to the Euro &amp; Copa America Datathon is that Elo models are best suited to sporting contexts with binary outcomes - i.e. win or lose - however in most soccer fixture we also have the possibility of a draw to account for. We are still able to ensure that Elo ratings update appropriately after a draw using 0.5 wins to denote the situation in which a draw occurs, but when it comes to making probabilistic win predictions using the Elo ratings (see p.A column above for example) things get a little more confusing.</p> <p>For the Euro &amp; Copa America Datathon, we also need to consider the fact that draws are a possibility during group stage match-ups, therefore we need to develop a workaround option to include draw probabilities.</p> <p>One way to do this - and the method that will be adopted for this tutorial - is to find the historic rate at which two teams of a certain Elo prediction split ending up drawing their matches. For example, how often do matches in which an Elo model deems Team A an 80% chance of winning and Team B a 20% chance of winning result in a draw? How about a 70%-30% split? We can find these draw rates for a range of probability points between 0 and 1 and use them to redistribute win/loss probabilities accordingly.</p> <p>Let's start the process by finding historic draw rates. We will bucket matches at 5% increments according to the home team's probability of winning the match according to the model.</p> <pre><code>draw_rates &lt;- data.frame(win_prob = elo_model$elos[,3],\n                        win_loss_draw = elo_model$elos[,4]) %&gt;%\n  mutate(prob_bucket = abs(round((win_prob)*20)) / 20) %&gt;%   # Round the predicted win probabilities to the nearest 0.05\n  group_by(prob_bucket) %&gt;%\n  summarise(draw_prob = sum(ifelse(win_loss_draw == 0.5, 1, 0)) / n())   # Calculate the rate their was a draw for this win prob - elo package codes a draw as a 0.5\n\ndraw_rates %&gt;% head(n=20)\n</code></pre> <p>We now have data which will help us deem how likely a match-up between two teams is to end up in a draw!</p> <p>The next step is to merge this data in with our existing data set. We also need to include the win/loss probabilities for each match that we've already found using our Elo model so that we may tweak them to include for the possibility of a draw. We'll also pull in the actual Elo ratings for each team for completeness.</p> <pre><code>data_with_probabilities &lt;- raw_data %&gt;% \n  select(tournament, date_GMT, home_team_name, away_team_name, home_result, away_result) %&gt;%   # Remove some redundant columns\n  mutate(home_elo = elo_results$elo.A - elo_results$update.A,    # Add in home team's elo rating (need to subtract the points update to obtain pre-match rating)\n         away_elo = elo_results$elo.B - elo_results$update.B,    # Add in away team's elo rating (need to subtract the points update to obtain pre-match rating)\n         home_prob = elo_results$p.A,                            # Add in home team's win/loss probability\n         away_prob = 1 - home_prob) %&gt;%                          # Add in away team's win/loss probability\n  mutate(prob_bucket = round(20*home_prob)/20) %&gt;%               # Bucket the home team's win/loss probability into a rounded increment of 0.05\n  left_join(draw_rates, by = \"prob_bucket\") %&gt;%                  # Join in our historic draw rates using the probability buckets\n    relocate(draw_prob, .after = home_prob) %&gt;% \n  select(-prob_bucket)\n</code></pre> <p>Having now brought the draw probability for each match into the data frame, we need to redistribute the win and loss probabilities so that <code>Pr(win) + Pr(draw) + Pr(loss)</code> sums to exactly 1. We can do this by simply subtracting the draw probability from each of the win and loss probabilities in a proportional manner. See below:</p> <pre><code>data_with_probabilities &lt;- data_with_probabilities %&gt;% \n  mutate(home_prob = home_prob - home_prob * draw_prob,          # Redistribute home team's probabilities proportionally to create win/draw/loss probabilities\n         away_prob = away_prob - away_prob * draw_prob)          # Redistribute away team's probabilities proportionally to create win/draw/loss probabilities\n\ndata_with_probabilities %&gt;% \n  select(home_team_name, away_team_name, home_prob, draw_prob, away_prob) %&gt;% \n  tail(n=10)\n</code></pre> <p>And there you have it! We've now come up with win, draw and loss probabilities for each match-up in our data set!</p> <p>Keep in mind that if we were to be focusing on knockout matches (i.e. where no draws are possible), we could have just skipped the previous few steps as we already had binary win-loss probabilities as direct outputs from our Elo model.</p>"},{"location":"modelling/soccerEloTutorialR/#back-testing","title":"Back Testing","text":"<p>The last step in our modelling process is to back test against a subset of our data to get an idea of our model's accuracy.</p> <p>We can use the <code>MLmetrics</code> package to run a log loss function on our subset. </p> <p>Let's look at how the model performed when we limit the data set to include only the most recent matches from early 2021.</p> <pre><code>matches_2021 &lt;- data_with_probabilities %&gt;% \n  filter(year(date_GMT) == 2021) %&gt;%                      # Filter down to only 2021 matches\n  mutate(home_win = ifelse(home_result == 1, 1, 0),       # Include new columns which show the true outcome of the match\n         draw = ifelse(home_result == 0.5, 1, 0),\n         away_win = ifelse(away_result == 1, 1, 0)) %&gt;% \n  select(date_GMT, home_team_name, away_team_name, home_prob, draw_prob, away_prob, home_win, draw, away_win)\n\n\n# Run the multinomial log loss function from MLmetrics to output a log loss score for our sample\nMultiLogLoss(\n  y_pred = matches_2021[,c(\"home_prob\", \"draw_prob\", \"away_prob\")] %&gt;% as.matrix(),\n  y_true = matches_2021[,c(\"home_win\", \"draw\", \"away_win\")] %&gt;% as.matrix()\n)\n</code></pre> <p>A pretty good result for such a simple Elo model!</p>"},{"location":"modelling/soccerEloTutorialR/#making-future-predictions","title":"Making Future Predictions","text":"<p>Okay, so now our Elo model is set in place, we have the latest set of Elo ratings for each team and we've back tested our model. We can now apply our model to future matches to obtain probabilities for match-ups that are yet to occur.</p> <p>Again we can do this using the <code>elo</code> package. This time we will use the function <code>elo.prob</code>, which takes two teams and outputs the probability of the first team winning the match-up. Like before, this function only considers win/loss outcomes to be possible, so if we were to also be looking to generate draw probabilities for a future match-up, we can just go through the exact same process as we did previously (i.e. make a binary win/loss prediction, merge in historic draw rates for various probabilities, redistribute accordingly).</p> <p>Let's just keep this simple for now though and focus on win and loss probabilities. We'll put together a small dataframe of matches and see what our model thinks - we've gone for hypothetical match-ups of Brazil v Argentina, England v France, Spain v Germany and an obligatory 2006 World Cup rematch of Australia v Italy.</p> <pre><code>future_matches &lt;- data.frame(\n  team_a = c(\"Brazil\", \"England\", \"Spain\", \"Australia\"),\n  team_b = c(\"Argentina\", \"France\", \"Germany\", \"Italy\"))  %&gt;% \n  mutate(elo_a = final_elos[team_a],\n         elo_b = final_elos[team_b],\n         team_a_win_prob = elo.prob(elo.A = elo_a,\n                                    elo.B = elo_b)\n  )\n\nfuture_matches\n</code></pre>"},{"location":"modelling/soccerEloTutorialR/#conclusions-areas-for-improvement","title":"Conclusions &amp; Areas for Improvement","text":"<p>Elo modelling can be a surprisingly accurate modelling technique given how simple it is to implement. This tutorial gives a very basic framework from which you are free to build a more intricate model with more detailed inputs and features.</p> <p>Some things that you might want to consider adding to this Elo model:</p> <ul> <li>Home ground advantage</li> <li>Key match statistics (i.e. shots on target, possession %, etc.)</li> <li>Whether the match was a dead rubber (teams may take the foot off the gas if they don't need to win to advance to the next stage of a tournament)</li> <li>Selected team line-ups (were key players missing?)</li> </ul> <p>Remember, an Elo model can be as complex or as simple as you want it to be - in some cases it might be better to keep it basic!</p> <p>We hope you've found this tutorial useful - if you have any questions regarding predictive data modelling please reach out to automation@betfair.com.au.</p>"},{"location":"modelling/soccerEloTutorialR/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"modelling/soccerModellingELO2022WorldCup/","title":"World Cup 2022 Elo Rating System Tutorial","text":""},{"location":"modelling/soccerModellingELO2022WorldCup/#introduction","title":"Introduction","text":"<p>The 2022 FIFA World Cup is nearly upon us - 64 matches of top shelf international football between 32 qualifying nations from around the globe in what is sure to be a spectacle of the best footballing nations from around the globe.</p> <p>One of the reasons we're excited for this year's World Cup is that the tournament provides a unique opportunity for us analytical types to try our hand at modelling international football over an extended run of matches in a high-exposure environment where we know betting market liquidity will be strong. While international football fixtures will often see teams field sub-full-strength sides for friendly matches and minor fixtures, the World Cup sees teams roll out full strength (or near enough to) line-ups, meaning we as fans are treated to the absolute best each competing country has to offer - this factor makes the World Cup an even more exciting prospect for predictive modelling.</p> <p>Just to make things even better, Betfair are hosting another Datathon to celebrate the 2022 FIFA World Cup! Betfair's Datathons are a series of data modelling competitions run for major sporting tournaments which aim to provide a platform for both aspiring and seasoned data modellers to test their wares at applying their predictive modelling nous in the sporting context.</p> <p>Building your own predictive model is a very rewarding and fun exercise, not to mention there is prize money for top performers. While there are many ways to go about creating predictive models, this tutorial aims to give readers a gentle introduction into one of the simpler approaches - Elo Rating Systems.</p>"},{"location":"modelling/soccerModellingELO2022WorldCup/#what-are-elo-rating-systems","title":"What are Elo Rating Systems?","text":"<p>Elo models (or Elo rating systems) are an approach to statistical learning which see every player or team in a league, competition or code given a dynamic numeric rating which indicates the given player or team's relative ability. From here, we are then able to use these ratings to construct win/loss probabilities for given match-ups between teams using a simple formula which takes the difference between the two teams' ratings and outputs the probability of each team winning or losing the match.</p> <p>Originally devised by their namesake Arpad Elo for the purpose of ranking chess players in the 1960s, Elo rating systems have become a common fixture in the wider world of sport, including in football. You may have seen examples of Elo rating systems for international football already without knowing it - FIFA's own international team ratings are a form of Elo ratings, as are the widely followed World Footall Elo Ratings which will serve as somewhat of a guide for how we go about building the Elo rating system within this tutorial.</p>"},{"location":"modelling/soccerModellingELO2022WorldCup/#how-can-i-build-an-elo-rating-system-for-international-football-and-the-world-cup","title":"How can I build an Elo Rating System for International Football and the World Cup?","text":"<p>Building an Elo rating system for international football really only requires one thing - an expansive and complete (or as near as possible) history of international match results which can be used to train/develop our ratings. Where other forms of modelling may need detailed statistics to create an informative and accurate model (e.g. time in possession, shots, passing %, corners etc.), one of the great things about Elo modelling is that in it's most simplistic form it really only needs to know three things in order to run:</p> <ul> <li>Each match's competing teams</li> <li>The result, i.e. the winner, loser or whether it was a draw</li> <li>The final score</li> </ul> <p>Only requiring these basic details makes Elo modelling a very popular choice for international football as there are fairly limited resources for detailed in-match statistics, and even fewer that are free to the public. Fortunately, there are extensive data sets of basic match results for international football out there for free on the internet if you know where to find them! This data set from Kaggle which covers just about every single international football fixtures since 1872 will serve as the training data set for both this tutorial as well as the 2022 World Cup Datathon.</p> <p>Let's get into building our Elo model - this tutorial uses R, however Python and other similar programming languages are just as capable of implementing Elo-style approaches.</p>"},{"location":"modelling/soccerModellingELO2022WorldCup/#importing-and-manipulating-data","title":"Importing and Manipulating Data","text":"<p>Start off by installing the packages we'll need. <code>tidyverse</code> will be used for basic data reading and manipulation, <code>formattable</code> is a useful package for working with different data formats, <code>lubridate</code> for dates, <code>showtext</code> for importing fonts for visualisations, while the <code>elo</code> package does most of the heavy lifting here relating to the inner workings of the Elo rating system itself.</p> <pre><code>packages &lt;- c(\"tidyverse\", \"elo\", \"formattable\", \"lubridate\", \"showtext\")\ninstall.packages(setdiff(packages, rownames(installed.packages())))\n\nlibrary(tidyverse)\nlibrary(formattable)\nlibrary(elo)\nlibrary(lubridate)\nlibrary(showtext)\n</code></pre> <p>Once the required packages are installed and loaded into our R environment we can import the historic match results data. In this case I've downloaded the Kaggle data set to my local machine so that I can read it in as a CSV - you should be able to do the same using a free Kaggle account. Alternatively this is the same data set you'll receive through participating in the Betfair World Cup Datathon.</p> <pre><code>match_results &lt;- read_csv(\"results.csv\")\n</code></pre> <p>Let's get an idea of what's included in the data set:</p> <pre><code>match_results %&gt;% glimpse\n</code></pre> <pre><code>## Rows: 44,060\n## Columns: 9\n## $ date       &lt;date&gt; 1872-11-30, 1873-03-08, 1874-03-07, 1875-03-06, 1876-03-04\u2026\n## $ home_team  &lt;chr&gt; \"Scotland\", \"England\", \"Scotland\", \"England\", \"Scotland\", \"\u2026\n## $ away_team  &lt;chr&gt; \"England\", \"Scotland\", \"England\", \"Scotland\", \"England\", \"W\u2026\n## $ home_score &lt;dbl&gt; 0, 4, 2, 2, 3, 4, 1, 0, 7, 9, 2, 5, 0, 5, 2, 5, 0, 1, 1, 0,\u2026\n## $ away_score &lt;dbl&gt; 0, 2, 1, 2, 0, 0, 3, 2, 2, 0, 1, 4, 3, 4, 3, 1, 1, 6, 5, 13\u2026\n## $ tournament &lt;chr&gt; \"Friendly\", \"Friendly\", \"Friendly\", \"Friendly\", \"Friendly\",\u2026\n## $ city       &lt;chr&gt; \"Glasgow\", \"London\", \"Glasgow\", \"London\", \"Glasgow\", \"Glasg\u2026\n## $ country    &lt;chr&gt; \"Scotland\", \"England\", \"Scotland\", \"England\", \"Scotland\", \"\u2026\n## $ neutral    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL\u2026\n</code></pre> <p>The data contains information regarding who played, when and where the match took place, each team's score and where applicable, the tournament during which the match took place.</p> <p>To run an Elo model, we'll need to add a couple more features to our data set, namely an indicator for each match's outcome as well as the final margin. The historic data also has a couple of different naming conventions for some nations which we should now address.</p> <pre><code>match_results_wrangled &lt;- match_results %&gt;% \n\n  # Rename United States and South Korea to USA and Korea Republic respectively to reflect current naming. We need to do this to both 'home_team' and 'away_team' columns\n  mutate(home_team = case_when(home_team == \"United States\" ~ \"USA\",\n                               home_team == \"South Korea\" ~ \"Korea Republic\",\n                               TRUE ~ home_team),\n         away_team = case_when(away_team == \"United States\" ~ \"USA\",\n                               away_team == \"South Korea\" ~ \"Korea Republic\",\n                               TRUE ~ away_team),\n\n         # Add a new column for match result from both the home team and away team's perspectives. 1 = Win, 0.5 = Draw, 0 = Loss.\n         home_result = case_when(home_score &gt; away_score ~ 1,\n                                 home_score &lt; away_score ~ 0,\n                                 home_score == away_score ~ 0.5),\n         away_result = case_when(home_score &lt; away_score ~ 1,\n                                 home_score &gt; away_score ~ 0,\n                                 home_score == away_score ~ 0.5),\n\n         # Add a margin feature\n         margin = abs(home_score - away_score)) %&gt;% \n\n  # If there are any matches missing score data, we don't want to use them for our Elo model - drop any data where we are missing a match margin\n  drop_na(margin)\n</code></pre>"},{"location":"modelling/soccerModellingELO2022WorldCup/#emulating-the-world-football-elo-ratings-model","title":"Emulating the World Football Elo Ratings Model","text":"<p>Having now wrangled our data into a usable format, we could now in theory jump straight into running our Elo model over the data using the <code>elo</code> package, but before we do that there are a few other things we should first consider. To build the most accurate model possible, we would need to trial-and-error at tweaking the model's parameters and inputs. What K value should we use? How much do we account for home ground advantage? Should any consideration be given to how much a team wins or loses a match by in adjusting their Elo rating?</p> <p>You can read about these nuanced inputs into Elo Rating Systems and others here - for simplicity however, we will be aiming to emulate the ratings system adopted for the well-renowned World Football Elo Ratings.</p> <p>As of the 2022 World Cup, World Football Elo Ratings uses the following input parameters into it's Elo model:</p> <ul> <li>K (a weighting factor which determines how many rating points teams can gain or lose from a single match) is variable based on the significance of the tournament. K is first defined by the following: 60 for World Cup finals, 50 for continental championship finals and major intercontinental tournaments, 40 for World Cup and continental qualifiers and major tournaments, 30 for all other tournaments and finally, 20 for friendly matches.</li> <li>K then undergoes an adjustment for the match goal difference - K is multipled by 1.5 for matches decided by two goals, 1.75 where decided by 3 goals, or by the rule 1.75 + (N-3)/8 where decided by four or more goals, with N indicating the margin.</li> <li>Teams playing at home also get a 100-point Elo ratings boost when calculating win/loss probabilities (as well as being factored into ratings adjustments).</li> </ul> <p>Let's now add features to our data set to allow for these factors to be included in our Elo model.</p> <pre><code>final_dataset &lt;- match_results_wrangled %&gt;% \n\n  # Add a column to show 100 where there is a home ground advantage or 0 when played at a neutral venue. Our Elo model will call on this column for HGA.\n  mutate(hga = 100*!neutral,\n\n         # Allocate each tournament within the data to a K-weighting according to the above definition. Note that this section is quite open to interpretation.\n         tournament_weight = case_when(tournament == \"FIFA World Cup\" ~ 60,\n                                       tournament %in% c(\"Confederations Cup\", \"African Cup of Nations\", \"Copa Am\u00e9rica\", \n                                                     \"UEFA Euro\", \"AFC Asian Cup\") ~ 50,\n                                       str_detect(tolower(tournament), \"qualification\") ~ 40,\n                                       str_detect(tolower(tournament), \" cup\") ~ 40,\n                                       tournament %in% c(\"UEFA Nations League\", \"CONCACAF Nations League\") ~ 40,\n                                       tournament == \"Friendly\" ~ 20,\n                                       TRUE ~ 30),\n\n         # Use our 'margin' feature to create the goal difference K-multiplier.\n         goal_diff_multiplier = case_when(margin &lt;= 1 ~ 1,\n                                          margin == 2 ~ 1.5,\n                                          margin == 3 ~ 1.75, \n                                          margin &gt;= 4 ~ 1.75 + (margin-3)/8),\n\n         # Combine the tournament weight and goal difference features to obtain our final K value for each match.\n         k = tournament_weight*goal_diff_multiplier,\n\n         # Also add a 'match_id' feaature to identify each match by number - we'll use this later.\n         match_id = row_number()) %&gt;% \n  relocate(match_id)\n</code></pre>"},{"location":"modelling/soccerModellingELO2022WorldCup/#running-the-elo-model","title":"Running the Elo Model","text":"<p>Our data is now ready to be run through the Elo model. This involves using the <code>elo.run()</code> function from the <code>elo</code> package, which then runs through each match in the historic data set, running and updating each team's Elo rating as it goes. By the end, we should have an Elo rating for each team as of the most recent international football match.</p> <p>Using the <code>formula</code> argument to <code>elo.run()</code> we can include for the nuances of the World Football Elo Ratings system. Use <code>adjust()</code> to include the home ground advantage factor, and <code>k()</code> to allow for our variable k-value.</p> <pre><code>elo_model &lt;- elo.run(data = final_dataset,\n                     formula = home_result ~ adjust(home_team, hga) + away_team + k(k))\n</code></pre> <p>The Elo model is stored as <code>elo_model</code>, which we can then call on to see the model's final ratings, as below:</p> <pre><code>final_elos &lt;- final.elos(elo_model) %&gt;% \n  enframe(name = \"team\", value = \"elo\") %&gt;% \n  arrange(desc(elo))\n\n# See the top 10 rated nations\nfinal_elos %&gt;% slice_head(n = 10)\n</code></pre> <pre><code>## # A tibble: 10 \u00d7 2\n##    team          elo\n##    &lt;chr&gt;       &lt;dbl&gt;\n##  1 Brazil      2226.\n##  2 Argentina   2194.\n##  3 Spain       2091.\n##  4 Netherlands 2083.\n##  5 Belgium     2077.\n##  6 Portugal    2049.\n##  7 Italy       2045.\n##  8 France      2042.\n##  9 Denmark     2014.\n## 10 Germany     2008.\n</code></pre> <p>To gain a better understanding of how our final Elo ratings are distributed, we can visualise all team ratings in a histogram.</p> <p><pre><code># Load Google fonts from https://fonts.google.com/\nfont_add_google(name = \"Poor Story\", family = \"poor_story\")\nshowtext_auto()\n\n# Visualise final Elo ratings\nfinal_elos %&gt;% \n  ggplot(aes(x = elo)) +\n  geom_histogram(binwidth = 100, fill = \"#ffb80c\", color = \"white\") +\n  theme_minimal() +\n  labs(x = \"Elo Rating\",\n       y = \"# of Teams\",\n       title = \"Distribution of Final Elo Ratings\") +\n  theme(panel.background = element_rect(fill = \"transparent\", colour = NA),\n        plot.background = element_rect(fill = \"transparent\", colour = NA),\n        text = element_text(color = \"black\", family = \"poor_story\"),\n        axis.text = element_text(color = \"black\", size = 14),\n        plot.title = element_text(color = \"black\", size = 24, hjust = 0.5),\n        axis.title = element_text(family = \"poor_story\", size = 16),\n        plot.margin = unit(c(1,1,1,1), \"cm\"))\n</code></pre> </p> <p>We can see that the final Elo ratings are somewhat normally distributed, ranging from below 1000 at the bottom end up to above 2000 for the absolute best teams as we saw above. In line with most Elo models, teams start with an intial Elo rating of 1500 which is also approximately at the centre of the distribution of teams' final Elo ratings.</p> <p>Since we are only aiming to recreate an existing rating system here, this tutorial won't go too far into testing the Elo model's accuracy, but be aware that the <code>elo</code> package also includes some helpful helper functions for model evaluation, including Brier scores.</p> <pre><code>brier(elo_model)\n</code></pre> <pre><code>## [1] 0.1419991\n</code></pre>"},{"location":"modelling/soccerModellingELO2022WorldCup/#using-the-elo-model-to-predict-world-cup-results","title":"Using the Elo Model to Predict World Cup Results","text":"<p>Now that we have our Elo model and final team ratings, we can turn our attention toward future matches and making predictions using our ratings.</p> <p>Start off by reading in the file.</p> <pre><code>world_cup_matchups &lt;- read_csv(\"dummy_submission_file.csv\")\n\nworld_cup_matchups %&gt;% glimpse\n</code></pre> <pre><code>## Rows: 544\n## Columns: 6\n## $ home_team      &lt;chr&gt; \"Qatar\", \"Senegal\", \"Qatar\", \"Netherlands\", \"Ecuador\", \u2026\n## $ away_team      &lt;chr&gt; \"Ecuador\", \"Netherlands\", \"Senegal\", \"Ecuador\", \"Senega\u2026\n## $ stage          &lt;chr&gt; \"Group\", \"Group\", \"Group\", \"Group\", \"Group\", \"Group\", \"\u2026\n## $ home_team_prob &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,\u2026\n## $ draw_prob      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,\u2026\n## $ away_team_prob &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,\u2026\n</code></pre> <p>We have information relating to the teams playing and the stage of the World Cup tournament during which the match will be played - now all we need to do is fill in the \"home_team_prob\", \"draw_prob\" and \"away_team_prob\" columns with the relevant probabilities using predictions based on our Elo ratings.</p>"},{"location":"modelling/soccerModellingELO2022WorldCup/#accounting-for-draws","title":"Accounting for Draws","text":"<p>Here is where things get a touch messy - Elo systems were originally devised to accommodate for two-player games of chess where winning or losing were the only two outcomes possible. For the group stage of the World Cup, drawn matches are also a possible outcome which means we need to accommodate for this in our predictions.</p> <p>While there is no particularly obvious way to go about this, we can make do by using our historical match data to see how often teams draw for various Elo rating matchups. For example, at what rate does a team with a 400 Elo-point advantage draw with their opponent? In this instance, a 400 point advantage would see the better ranked team have a 91% chance of winning, but we would need to adjust that probability to include for a third possible outcome - the draw.</p> <p>Let's find the draw rates for various differentials in win probability.</p> <pre><code># Start by taking our Elo model, wrangling the match-by-match rating updates within the stored model and joining to our historic data set.\nelo_results &lt;- elo_model %&gt;% \n  as_tibble() %&gt;% \n  mutate(match_id = row_number()) %&gt;% \n  select(match_id, \n         home_update = update.A,\n         away_update = update.B,\n         home_elo = elo.A,\n         away_elo = elo.B) %&gt;% \n\n  # Join onto our historic data set using the match_id columns we created as a join key\n  right_join(final_dataset, by = \"match_id\")\n</code></pre> <p>We can now find the home and away win probabilities for each historic match and bucket them into groups at 5% increments according to how close the home and away win probabilities were. E.g. a team with a 90% chance to win coming up against a team with a 10% chance to win would have a 80% differential in win probability.</p> <pre><code>draw_rates &lt;- elo_results %&gt;%\n  mutate(home_elo_pre_match = home_elo - home_update,\n         away_elo_pre_match = away_elo - away_update,\n         home_prob = elo.prob(home_elo_pre_match, away_elo_pre_match),\n         away_prob = 1 - home_prob,\n         prob_diff = abs(home_prob - away_prob),\n         prob_diff_bucket = round(20*prob_diff)/20) %&gt;% # Bucket into 20 groups at 5% increments between 0% and 100%\n  filter(year(date) &gt;= 2005) %&gt;%  # Filter down to the past 15 years to only bucket matches once some fairly decent Elo ratings have been established\n  group_by(prob_diff_bucket) %&gt;%\n  summarise(draw_rate = sum(home_result == 0.5)/n())\n\ndraw_rates %&gt;% slice_head(n = 10)\n</code></pre> <pre><code>## # A tibble: 10 \u00d7 2\n##    prob_diff_bucket draw_rate\n##               &lt;dbl&gt;     &lt;dbl&gt;\n##  1             0        0.320\n##  2             0.05     0.296\n##  3             0.1      0.289\n##  4             0.15     0.271\n##  5             0.2      0.285\n##  6             0.25     0.256\n##  7             0.3      0.273\n##  8             0.35     0.270\n##  9             0.4      0.276\n## 10             0.45     0.240\n</code></pre> <p>We can visualise the draw rates for win probability differentials to get a better idea of how often draws happen in various scenarios.</p> <p><pre><code>draw_rates %&gt;%\n  mutate(draw_rate = percent(draw_rate, digits = 0),\n         prob_diff_bucket = percent(prob_diff_bucket, digits = 0)) %&gt;%\n  ggplot(aes(x = prob_diff_bucket, y = draw_rate)) +\n  geom_col(fill = \"#ffb80c\") +\n  geom_text(aes(label = draw_rate, y = draw_rate + 0.01), \n            size = 3.5, family = \"poor_story\") +\n  theme_minimal() +\n  scale_y_continuous(labels = scales::percent) +\n  scale_x_continuous(labels = scales::percent, breaks = scales::pretty_breaks()) +\n  labs(y = \"Historic Draw Rate\", \n       x = \"Difference in Win Probability Between Home &amp; Away Sides\",\n       title = \"Draw Frequency by Heaviness of Favouritism\") +\n  theme(panel.background = element_rect(fill = \"transparent\", colour = NA),\n        plot.background = element_rect(fill = \"transparent\", colour = NA),\n        text = element_text(color = \"black\", family = \"poor_story\"),\n        axis.text = element_text(color = \"black\", size = 12),\n        plot.title = element_text(color = \"black\", size = 24, hjust = 0.5),\n        axis.title = element_text(family = \"poor_story\", size = 16),\n        plot.margin = unit(c(1,1,1,1), \"cm\"))\n</code></pre> </p> <p>We can see that draws happen in more than 30% of matches where there has been a ~0% difference in win probability between the two sides (i.e. it was very close to a 50-50 match-up) - this is the highest draw rate, as one would expect. The draw rate falls away from there, to the point where only 3.5% of matches with a 95% win probability differential end in draws. Applying these draw rates to our group stage fixture predictions should allow for our Elo model to overcome the three-outcome nature of football.</p>"},{"location":"modelling/soccerModellingELO2022WorldCup/#visualising-elo-ratings-through-time","title":"Visualising Elo Ratings Through Time","text":"<p>Something we can do to help us better understand how the Elo model develops final team ratings is to visualise how a team's rating has developed over time. We know that the rating system dictates that all teams start with an initial rating of 1500, so we should expect to see teams move upward or down from there as a starting point depending on their fortunes. </p> <p>Let's look at how Australia's Elo rating has changed through time.</p> <p><pre><code>elo_results %&gt;% \n  select(match_id, date, home_team, home_elo) %&gt;% rename(team = home_team, elo = home_elo) %&gt;% \n  bind_rows(elo_results %&gt;% select(match_id, date, away_team, away_elo) %&gt;% rename(team = away_team, elo = away_elo)) %&gt;% \n  arrange(match_id) %&gt;% \n  filter(team == \"Australia\") %&gt;% \n  ggplot(aes(x = date, y = elo, group = 1)) +\n  geom_hline(yintercept = 1500, linetype = \"dashed\", color = \"black\") +\n  geom_line(color = \"#ffb80c\", size = 1) +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"Elo Rating\",\n       title = \"Australia's Historic Elo Rating\") + \n  theme(panel.background = element_rect(fill = \"transparent\", colour = NA),\n        plot.background = element_rect(fill = \"transparent\", colour = NA),\n        text = element_text(color = \"black\", family = \"poor_story\"),\n        axis.text = element_text(color = \"black\", size = 12),\n        plot.title = element_text(color = \"black\", size = 24, hjust = 0.5),\n        axis.title = element_text(family = \"poor_story\", size = 16),\n        plot.margin = unit(c(1,1,1,1), \"cm\"))\n</code></pre> </p> <p>After a slow few decades initially with very few matches played, the Socceroos' Elo eventuall reached a peak rating of 1954 in June of 2001 (after a win against New Zealand) and has now settled back in the low-1800s range as of late 2022.</p>"},{"location":"modelling/soccerModellingELO2022WorldCup/#making-predictions","title":"Making Predictions","text":"<p>Time to get back to making predictions - again using the <code>elo</code> package, the <code>elo.calc()</code> function can be used here to find the probability that a team with rating X will beat a team with rating Y. From there we can overlay our draw rates and adjust as necessary to ensure all outcomes sum to equal 100%.</p> <pre><code>world_cup_predictions &lt;- world_cup_matchups %&gt;%\n  # Join on final home and away team Elo ratings ahead of calculating win probabilities\n  left_join(final_elos %&gt;% rename(home_elo = elo), by = c(\"home_team\" = \"team\")) %&gt;%\n  left_join(final_elos %&gt;% rename(away_elo = elo), by = c(\"away_team\" = \"team\")) %&gt;%\n\n  # Calculate win probabilities\n  mutate(home_team_prob = elo.prob(home_elo, away_elo),\n         away_team_prob = elo.prob(away_elo, home_elo),\n\n         # Just as we did with finding draw rates, group each match into probability differential buckets so that we can apply the correct draw rate\n         prob_diff = abs(home_team_prob - away_team_prob),\n         prob_diff_bucket = round(20*prob_diff)/20) %&gt;%\n\n  # Join on our draw rates data frame - this essentially gives us the historic draw rate for the given match-up's probability split\n  left_join(draw_rates, by = \"prob_diff_bucket\") %&gt;%\n\n  # update the draw_prob column accordingly for group stage matches. Set draw_prob to 0 for knockout matches where draws aren't possible.\n  mutate(draw_prob = case_when(stage == \"Group\" ~ draw_rate,\n                               stage == \"Knockout\" ~ 0),\n\n         # Adjust home_team_prob and away_team_prob columns proportionally to allow for draw probability\n         home_team_prob = home_team_prob * (1 - draw_prob),\n         away_team_prob = away_team_prob * (1 - draw_prob)) %&gt;%\n\n  # Remove redundant columns\n  select(home_team:away_team_prob)\n\n# View 5 group stage matches and 5 knockout matches\nworld_cup_predictions %&gt;% group_by(stage) %&gt;% slice_head(n = 5)\n\nmodel_name = \"Elo_Tutorial\"\nfile_name=paste(model_name,\"_submission_file.csv\",sep=\"\")\nwrite.csv(world_cup_predictions,file_name)\n</code></pre> <pre><code>## # A tibble: 10 \u00d7 6\n## # Groups:   stage [2]\n##    home_team   away_team   stage    home_team_prob draw_prob away_team_prob\n##    &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n##  1 Qatar       Ecuador     Group             0.209     0.276          0.515\n##  2 Senegal     Netherlands Group             0.155     0.213          0.632\n##  3 Qatar       Senegal     Group             0.276     0.256          0.468\n##  4 Netherlands Ecuador     Group             0.560     0.240          0.199\n##  5 Ecuador     Senegal     Group             0.424     0.285          0.291\n##  6 Qatar       Senegal     Knockout          0.371     0              0.629\n##  7 Qatar       Netherlands Knockout          0.126     0              0.874\n##  8 Qatar       Ecuador     Knockout          0.289     0              0.711\n##  9 Qatar       England     Knockout          0.218     0              0.782\n## 10 Qatar       USA         Knockout          0.321     0              0.679\n</code></pre> <p>And there you have it - we've successfully created a model for predicting future match outcomes for the 2022 FIFA World Cup!</p> <p>We hope you've found this tutorial useful - if you have any questions regarding predictive data modelling please reach out to automation@betfair.com.au.</p>"},{"location":"modelling/soccerModellingELO2022WorldCup/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"modelling/soccerModellingELO2022WorldCup/#complete-code","title":"Complete Code","text":"<pre><code>packages &lt;- c(\"tidyverse\", \"elo\", \"formattable\", \"lubridate\", \"showtext\")\ninstall.packages(setdiff(packages, rownames(installed.packages())))\n\nlibrary(tidyverse)\nlibrary(formattable)\nlibrary(elo)\nlibrary(lubridate)\nlibrary(showtext)\n\n\nmatch_results &lt;- read_csv(\"results.csv\")\nmatch_results %&gt;% glimpse\nmatch_results_wrangled &lt;- match_results %&gt;% \n\n  # Rename United States and South Korea to USA and Korea Republic respectively to reflect current naming. We need to do this to both 'home_team' and 'away_team' columns\n  mutate(home_team = case_when(home_team == \"United States\" ~ \"USA\",\n                               home_team == \"South Korea\" ~ \"Korea Republic\",\n                               TRUE ~ home_team),\n         away_team = case_when(away_team == \"United States\" ~ \"USA\",\n                               away_team == \"South Korea\" ~ \"Korea Republic\",\n                               TRUE ~ away_team),\n\n         # Add a new column for match result from both the home team and away team's perspectives. 1 = Win, 0.5 = Draw, 0 = Loss.\n         home_result = case_when(home_score &gt; away_score ~ 1,\n                                 home_score &lt; away_score ~ 0,\n                                 home_score == away_score ~ 0.5),\n         away_result = case_when(home_score &lt; away_score ~ 1,\n                                 home_score &gt; away_score ~ 0,\n                                 home_score == away_score ~ 0.5),\n\n         # Add a margin feature\n         margin = abs(home_score - away_score)) %&gt;% \n\n  # If there are any matches missing score data, we don't want to use them for our Elo model - drop any data where we are missing a match margin\n  drop_na(margin)\nfinal_dataset &lt;- match_results_wrangled %&gt;% \n\n  # Add a column to show 100 where there is a home ground advantage or 0 when played at a neutral venue. Our Elo model will call on this column for HGA.\n  mutate(hga = 100*!neutral,\n\n         # Allocate each tournament within the data to a K-weighting according to the above definition. Note that this section is quite open to interpretation.\n         tournament_weight = case_when(tournament == \"FIFA World Cup\" ~ 60,\n                                       tournament %in% c(\"Confederations Cup\", \"African Cup of Nations\", \"Copa Am\u00e9rica\", \n                                                         \"UEFA Euro\", \"AFC Asian Cup\") ~ 50,\n                                       str_detect(tolower(tournament), \"qualification\") ~ 40,\n                                       str_detect(tolower(tournament), \" cup\") ~ 40,\n                                       tournament %in% c(\"UEFA Nations League\", \"CONCACAF Nations League\") ~ 40,\n                                       tournament == \"Friendly\" ~ 20,\n                                       TRUE ~ 30),\n\n         # Use our 'margin' feature to create the goal difference K-multiplier.\n         goal_diff_multiplier = case_when(margin &lt;= 1 ~ 1,\n                                          margin == 2 ~ 1.5,\n                                          margin == 3 ~ 1.75, \n                                          margin &gt;= 4 ~ 1.75 + (margin-3)/8),\n\n         # Combine the tournament weight and goal difference features to obtain our final K value for each match.\n         k = tournament_weight*goal_diff_multiplier,\n\n         # Also add a 'match_id' feaature to identify each match by number - we'll use this later.\n         match_id = row_number()) %&gt;% \n  relocate(match_id)\nelo_model &lt;- elo.run(data = final_dataset,\n                     formula = home_result ~ adjust(home_team, hga) + away_team + k(k))\nfinal_elos &lt;- final.elos(elo_model) %&gt;% \n  enframe(name = \"team\", value = \"elo\") %&gt;% \n  arrange(desc(elo))\n\n# See the top 10 rated nations\nfinal_elos %&gt;% slice_head(n = 10)\n# Load Google fonts from https://fonts.google.com/\nfont_add_google(name = \"Poor Story\", family = \"poor_story\")\nshowtext_auto()\n\n# Visualise final Elo ratings\nfinal_elos %&gt;% \n  ggplot(aes(x = elo)) +\n  geom_histogram(binwidth = 100, fill = \"#ffb80c\", color = \"white\") +\n  theme_minimal() +\n  labs(x = \"Elo Rating\",\n       y = \"# of Teams\",\n       title = \"Distribution of Final Elo Ratings\") +\n  theme(panel.background = element_rect(fill = \"transparent\", colour = NA),\n        plot.background = element_rect(fill = \"transparent\", colour = NA),\n        text = element_text(color = \"black\", family = \"poor_story\"),\n        axis.text = element_text(color = \"black\", size = 14),\n        plot.title = element_text(color = \"black\", size = 24, hjust = 0.5),\n        axis.title = element_text(family = \"poor_story\", size = 16),\n        plot.margin = unit(c(1,1,1,1), \"cm\"))\n\nbrier(elo_model)\nworld_cup_matchups &lt;- read_csv(\"dummy_submission_file.csv\")\n\nworld_cup_matchups %&gt;% glimpse\n\n# Start by taking our Elo model, wrangling the match-by-match rating updates within the stored model and joining to our historic data set.\nelo_results &lt;- elo_model %&gt;% \n  as_tibble() %&gt;% \n  mutate(match_id = row_number()) %&gt;% \n  select(match_id, \n         home_update = update.A,\n         away_update = update.B,\n         home_elo = elo.A,\n         away_elo = elo.B) %&gt;% \n\n  # Join onto our historic data set using the match_id columns we created as a join key\n  right_join(final_dataset, by = \"match_id\")\ndraw_rates &lt;- elo_results %&gt;%\n  mutate(home_elo_pre_match = home_elo - home_update,\n         away_elo_pre_match = away_elo - away_update,\n         home_prob = elo.prob(home_elo_pre_match, away_elo_pre_match),\n         away_prob = 1 - home_prob,\n         prob_diff = abs(home_prob - away_prob),\n         prob_diff_bucket = round(20*prob_diff)/20) %&gt;% # Bucket into 20 groups at 5% increments between 0% and 100%\n  filter(year(date) &gt;= 2005) %&gt;%  # Filter down to the past 15 years to only bucket matches once some fairly decent Elo ratings have been established\n  group_by(prob_diff_bucket) %&gt;%\n  summarise(draw_rate = sum(home_result == 0.5)/n())\n\ndraw_rates %&gt;% slice_head(n = 10)\n\ndraw_rates %&gt;%\n  mutate(draw_rate = percent(draw_rate, digits = 0),\n         prob_diff_bucket = percent(prob_diff_bucket, digits = 0)) %&gt;%\n  ggplot(aes(x = prob_diff_bucket, y = draw_rate)) +\n  geom_col(fill = \"#ffb80c\") +\n  geom_text(aes(label = draw_rate, y = draw_rate + 0.01), \n            size = 3.5, family = \"poor_story\") +\n  theme_minimal() +\n  scale_y_continuous(labels = scales::percent) +\n  scale_x_continuous(labels = scales::percent, breaks = scales::pretty_breaks()) +\n  labs(y = \"Historic Draw Rate\", \n       x = \"Difference in Win Probability Between Home &amp; Away Sides\",\n       title = \"Draw Frequency by Heaviness of Favouritism\") +\n  theme(panel.background = element_rect(fill = \"transparent\", colour = NA),\n        plot.background = element_rect(fill = \"transparent\", colour = NA),\n        text = element_text(color = \"black\", family = \"poor_story\"),\n        axis.text = element_text(color = \"black\", size = 12),\n        plot.title = element_text(color = \"black\", size = 24, hjust = 0.5),\n        axis.title = element_text(family = \"poor_story\", size = 16),\n        plot.margin = unit(c(1,1,1,1), \"cm\"))\n\nelo_results %&gt;% \n  select(match_id, date, home_team, home_elo) %&gt;% rename(team = home_team, elo = home_elo) %&gt;% \n  bind_rows(elo_results %&gt;% select(match_id, date, away_team, away_elo) %&gt;% rename(team = away_team, elo = away_elo)) %&gt;% \n  arrange(match_id) %&gt;% \n  filter(team == \"Australia\") %&gt;% \n  ggplot(aes(x = date, y = elo, group = 1)) +\n  geom_hline(yintercept = 1500, linetype = \"dashed\", color = \"black\") +\n  geom_line(color = \"#ffb80c\", size = 1) +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"Elo Rating\",\n       title = \"Australia's Historic Elo Rating\") + \n  theme(panel.background = element_rect(fill = \"transparent\", colour = NA),\n        plot.background = element_rect(fill = \"transparent\", colour = NA),\n        text = element_text(color = \"black\", family = \"poor_story\"),\n        axis.text = element_text(color = \"black\", size = 12),\n        plot.title = element_text(color = \"black\", size = 24, hjust = 0.5),\n        axis.title = element_text(family = \"poor_story\", size = 16),\n        plot.margin = unit(c(1,1,1,1), \"cm\"))\n\nworld_cup_predictions &lt;- world_cup_matchups %&gt;%\n  # Join on final home and away team Elo ratings ahead of calculating win probabilities\n  left_join(final_elos %&gt;% rename(home_elo = elo), by = c(\"home_team\" = \"team\")) %&gt;%\n  left_join(final_elos %&gt;% rename(away_elo = elo), by = c(\"away_team\" = \"team\")) %&gt;%\n\n  # Calculate win probabilities\n  mutate(home_team_prob = elo.prob(home_elo, away_elo),\n         away_team_prob = elo.prob(away_elo, home_elo),\n\n         # Just as we did with finding draw rates, group each match into probability differential buckets so that we can apply the correct draw rate\n         prob_diff = abs(home_team_prob - away_team_prob),\n         prob_diff_bucket = round(20*prob_diff)/20) %&gt;%\n\n  # Join on our draw rates data frame - this essentially gives us the historic draw rate for the given match-up's probability split\n  left_join(draw_rates, by = \"prob_diff_bucket\") %&gt;%\n\n  # update the draw_prob column accordingly for group stage matches. Set draw_prob to 0 for knockout matches where draws aren't possible.\n  mutate(draw_prob = case_when(stage == \"Group\" ~ draw_rate,\n                               stage == \"Knockout\" ~ 0),\n\n         # Adjust home_team_prob and away_team_prob columns proportionally to allow for draw probability\n         home_team_prob = home_team_prob * (1 - draw_prob),\n         away_team_prob = away_team_prob * (1 - draw_prob)) %&gt;%\n\n  # Remove redundant columns\n  select(home_team:away_team_prob)\n\n# View 5 group stage matches and 5 knockout matches\nworld_cup_predictions %&gt;% group_by(stage) %&gt;% slice_head(n = 5)\nmodel_name = \"Elo_Tutorial\"\nfile_name=paste(model_name,\"_submission_file.csv\",sep=\"\")\nwrite.csv(world_cup_predictions,file_name)\n</code></pre>"},{"location":"modelling/soccerModellingTutorialPython/","title":"How to model Soccer: Python Tutorial","text":""},{"location":"modelling/soccerModellingTutorialPython/#the-task","title":"The Task","text":"<p>This notebook will outline how to train a classification model to predict the outcome of a soccer match using a dataset provided by https://www.football-data.co.uk/</p> <ol> <li>Reading data from file and get a raw dataset</li> <li>Data cleaning and feature engineering</li> <li>Training a model</li> <li>The tutorial covers the thought process of manipulating the dataset (why and how), some simple data cleaning, feature engineering and training a classification model.</li> </ol> <p>The tutorial DOES NOT delve deep into the fundamentals of machine learning, advanced feature engineering or model tuning.</p> <p>There are some helpful hints along the way though.</p> <pre><code># import required libraries\n\nimport numpy as np\nimport pandas as pd\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre>"},{"location":"modelling/soccerModellingTutorialPython/#read-data-from-file-and-get-a-raw-dataset","title":"Read data from file and get a raw dataset","text":""},{"location":"modelling/soccerModellingTutorialPython/#change-the-data-types-date-column","title":"Change the data types - date column.","text":"<p>We need the date column in good order for our tutorial. Here's the data set we're using for this tutorial.</p> <p>In general, it's a good idea to evaluate data types of all columns that we work with to ensure they are correct.</p> <pre><code>df = pd.read_csv('soccerData.csv')\n</code></pre> <pre><code>df['date']= pd.to_datetime(df['date'])\n</code></pre>"},{"location":"modelling/soccerModellingTutorialPython/#get-data-columns-and-create-raw-dataset","title":"Get data columns and create raw dataset","text":"<p>For this tutorial, let's take only a few stats columns to work with.</p> <p>Typically we would explore all features and then decide which data to discard.</p> <ol> <li>Goal counts</li> <li>Half Time Goal Counts</li> <li>Corners</li> <li>Total shots</li> <li>Shots on target</li> <li>Fouls</li> <li>Yellow Cards</li> <li>Red Cards</li> </ol> <pre><code>raw_match_stats = df[[\n                'date',\n                'match_id',\n                'home_team_name',\n                'away_team_name',\n                'home_team_goal_count', \n                'away_team_goal_count',\n                'home_team_half_time_goal_count',\n                'away_team_half_time_goal_count',\n                'home_team_shots',\n                'away_team_shots',\n                'home_team_shots_on_target',\n                'away_team_shots_on_target',\n                'home_team_fouls',\n                'away_team_fouls',\n                'home_team_corner_count',\n                'away_team_corner_count',\n                'home_team_yellow',\n                'away_team_yellow',\n                'home_team_red',\n                'away_team_red'\n                ]]\n</code></pre>"},{"location":"modelling/soccerModellingTutorialPython/#clean-data","title":"Clean data","text":"<p>As a cleaning step, we order our data by date and drop rows with NA values.</p> <pre><code>raw_match_stats = raw_match_stats.sort_values(by=['date'], ascending=False)\n\nraw_match_stats = raw_match_stats.dropna(inplace=True)\n</code></pre>"},{"location":"modelling/soccerModellingTutorialPython/#raw-dataset","title":"Raw dataset","text":"<p>This raw dataset is structured so that each match has an individual row and stats for both teams are on that row with columns titles \"home\" and \"away\".</p> <p>Our goal is to build a machine learning (ML) model that can predict the result of a soccer match. Given that we have some match stats, we will aim to use that information to predict a WIN, LOSS or DRAW.</p> <pre><code>raw_match_stats\n</code></pre> date match_id home_team_name away_team_name home_team_goal_count away_team_goal_count home_team_half_time_goal_count away_team_half_time_goal_count home_team_shots away_team_shots home_team_shots_on_target away_team_shots_on_target home_team_fouls away_team_fouls home_team_corner_count away_team_corner_count home_team_yellow away_team_yellow home_team_red away_team_red 6/11/2023 222305 Tottenham Chelsea 1 4 1 1 8 17 5 8 12 21 1 6 1 5 2 0 5/11/2023 222304 Luton Liverpool 1 1 0 0 8 24 5 6 7 13 4 7 1 1 0 0 5/11/2023 222303 Nottm Forest Aston Villa 2 0 1 0 5 13 3 3 6 9 0 10 1 1 0 0 4/11/2023 222299 Everton Brighton 1 1 1 0 10 7 4 2 15 5 3 3 4 2 0 0 4/11/2023 222296 Fulham Man Utd 0 1 0 0 18 12 3 5 9 15 9 4 5 2 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 19/08/2000 213457 Chelsea West Ham 4 2 1 0 17 12 10 5 19 14 7 7 1 2 0 0 19/08/2000 213462 Liverpool Bradford 1 0 0 0 16 3 10 2 8 8 6 1 1 1 0 0 19/08/2000 213463 Sunderland Arsenal 1 0 0 0 8 14 2 7 10 21 2 9 3 1 0 1 19/08/2000 213464 Tottenham Ipswich 3 1 2 1 20 15 6 5 14 13 3 4 0 0 0 0 19/08/2000 213456 Charlton Man City 4 0 2 0 17 8 14 4 13 12 6 6 1 2 0 0"},{"location":"modelling/soccerModellingTutorialPython/#data-cleaning-and-feature-engineering","title":"Data cleaning and feature engineering","text":""},{"location":"modelling/soccerModellingTutorialPython/#target-variable-match-result","title":"Target variable - Match Result","text":"<p>Our machine learning model aims to predict the result of a match. This \"result\" is called the \"target variable\". Our dataset has no columns showing the match result. We will create two columns for the results for each team. One of these would become the target variable for our ML model.</p> <pre><code># create results columns for both home and away teams (3 - win, 1 = Draw, 0 = Loss).\n\nraw_match_stats.loc[raw_match_stats['home_team_goal_count'] == raw_match_stats['away_team_goal_count'], 'home_team_result'] = 1\nraw_match_stats.loc[raw_match_stats['home_team_goal_count'] &gt; raw_match_stats['away_team_goal_count'], 'home_team_result'] = 3\nraw_match_stats.loc[raw_match_stats['home_team_goal_count'] &lt; raw_match_stats['away_team_goal_count'], 'home_team_result'] = 0\n\nraw_match_stats.loc[raw_match_stats['home_team_goal_count'] == raw_match_stats['away_team_goal_count'], 'away_team_result'] = 1\nraw_match_stats.loc[raw_match_stats['home_team_goal_count'] &gt; raw_match_stats['away_team_goal_count'], 'away_team_result'] = 0\nraw_match_stats.loc[raw_match_stats['home_team_goal_count'] &lt; raw_match_stats['away_team_goal_count'], 'away_team_result'] = 3\n</code></pre>"},{"location":"modelling/soccerModellingTutorialPython/#average-pre-match-stats-ten-match-average","title":"Average pre-match stats - Ten match average","text":"<p>Great! Now we have a dataset with many rows of data, with each row representing match stats and the match result (this would become our target variable).</p> <p>But our goal is to build an ML model that predicts the match result prior to the start of a match. Are the stats from that match what we need to build this ML model? No! When predicting a match outcome BEFORE the start of the match, we are forced to rely on match stats available to us from previous matches.</p> <p>Therefore, we need a dataset with the match result (target variable) and stats for each team heading into that match. For this tutorial, we will look at the average stats for each team in the ten matches preceding each match.</p> <p>Lets look at how we can get the average stats for the previous 10 matches for each team at each match.</p> <ol> <li>Split the raw_match_stats to two datasets (home_team_stats and away_team_stats).</li> <li>Stack these two datasets so that each row is the stats for a team for one match (team_stats_per_match).</li> <li>At each row of this dataset, get the team name, find the stats for that team during the last 10 matches, and average these stats (avg_stats_per_team).</li> <li>Add these stats to the team_stats_per_match dataset.</li> </ol> <p>Why did we chose ten matches? Why not 15? Should we average over a time period (matches in the last year perhaps?) rather than a number? What's the least number of matches available for each competing team in the dataset? These are all interesting questions that may improve our model.</p> <pre><code># Split the raw_match_stats to two datasets (home_team_stats and away_team_stats)\n\nhome_team_stats = raw_match_stats[[\n 'date',\n 'match_id',\n 'home_team_name',\n 'home_team_goal_count',\n 'home_team_half_time_goal_count',\n 'home_team_corner_count',\n 'home_team_shots',\n 'home_team_shots_on_target',\n 'home_team_fouls',\n 'home_team_yellow',\n 'home_team_red',\n 'home_team_result',\n 'away_team_goal_count',\n 'away_team_half_time_goal_count',\n 'away_team_corner_count',\n 'away_team_shots',\n 'away_team_shots_on_target',\n 'away_team_fouls',\n 'away_team_yellow',\n 'away_team_red']]\n\nhome_team_stats = home_team_stats.rename(columns={'home_team_name':'name',\n                                                'home_team_goal_count':'goalsScored',\n                                                'home_team_half_time_goal_count':'halfTimeGoalsScored',\n                                                'home_team_corner_count':'cornerCount',\n                                                'home_team_shots':'shots',\n                                                'home_team_shots_on_target':'shotsOnTarget',\n                                                'home_team_fouls':'foulsConceded',\n                                                'home_team_yellow':'yellowConceded',\n                                                'home_team_red':'redConceded',\n                                                'home_team_result':'result',\n                                                'away_team_goal_count':'goalsConceded',\n                                                'away_team_half_time_goal_count':'halfTimeGoalsConceded',\n                                                'away_team_corner_count':'cornersConceded',\n                                                'away_team_shots':'shotsConceded',\n                                                'away_team_shots_on_target':'shotsOnTargetConceded',\n                                                'away_team_fouls':'foulsReceived',\n                                                'away_team_yellow':'yellowOpponent',\n                                                'away_team_red':'redOpponent'})\n\naway_team_stats = raw_match_stats[[\n 'date',\n 'match_id',\n 'away_team_name',\n 'away_team_goal_count',\n 'away_team_half_time_goal_count',\n 'away_team_corner_count',\n 'away_team_shots',\n 'away_team_shots_on_target',\n 'away_team_fouls',\n 'away_team_yellow',\n 'away_team_red',\n 'away_team_result',\n 'home_team_goal_count',\n 'home_team_half_time_goal_count',\n 'home_team_corner_count',\n 'home_team_shots',\n 'home_team_shots_on_target',\n 'home_team_fouls',\n 'home_team_yellow',\n 'home_team_red',]]\n\naway_team_stats = away_team_stats.rename(columns={'away_team_name':'name',\n                                                'away_team_goal_count':'goalsScored',\n                                                'away_team_half_time_goal_count':'halfTimeGoalsScored',\n                                                'away_team_corner_count':'cornerCount',\n                                                'away_team_shots':'shots',\n                                                'away_team_shots_on_target':'shotsOnTarget',\n                                                'away_team_fouls':'foulsConceded',\n                                                'away_team_yellow':'yellowConceded',\n                                                'away_team_red':'redConceded',\n                                                'away_team_result':'result',\n                                                'home_team_goal_count':'goalsConceded',\n                                                'home_team_half_time_goal_count':'halfTimeGoalsConceded',\n                                                'home_team_corner_count':'cornersConceded',\n                                                'home_team_shots':'shotsConceded',\n                                                'home_team_shots_on_target':'shotsOnTargetConceded',\n                                                'home_team_fouls':'foulsReceived',\n                                                'home_team_yellow':'yellowOpponent',\n                                                'home_team_red':'redOpponent'})\n\n# add an additional column to denote whether the team is playing at home or away - this will help us later\nhome_team_stats['home_or_away']='Home'\naway_team_stats['home_or_away']='Away'\n\n# stack these two datasets so that each row is the stats for a team for one match (team_stats_per_match)\nteam_stats_per_match = pd.concat([home_team_stats,away_team_stats])\n</code></pre> <pre><code># At each row of this dataset, get the team name, find the stats for that team during the last 10 matches, and average these stats (avg_stats_per_team). \n\navg_stat_columns = ['goals_per_match','corners_per_match','shots_per_match','shotsOnTarget_per_match','fouls_per_match', 'possession_per_match']\nstats_list = []\nfor index, row in team_stats_per_match.iterrows():\n    team_stats_last_five_matches = team_stats_per_match.loc[(team_stats_per_match['name']==row['name']) &amp; (team_stats_per_match['date_GMT']&lt;row['date_GMT'])].sort_values(by=['date_GMT'], ascending=False)\n    stats_list.append(team_stats_last_five_matches.iloc[0:5,:].mean(axis=0).values[0:6])\n\navg_stats_per_team = pd.DataFrame(stats_list, columns=avg_stat_columns)\n</code></pre> <pre><code># At each row of this dataset, get the team name, find the stats for that team during the last 5 matches, and average these stats (avg_stats_per_team). \n\navg_lastTen_stat_columns = [\n                    'average_goalsScored_last_ten',\n                    'average_halfTimeGoalsScored_last_ten',\n                    'average_cornerCount_last_ten',\n                    'average_shots_last_ten',\n                    'average_shotsOnTarget_last_ten',\n                    'average_foulsConceded_last_ten',\n                    'average_yellowConceded_last_ten',\n                    'average_redConceded_last_ten',\n                    'average_result_last_ten',\n                    'average_goalsConceded_last_ten',\n                    'average_halfTimeGoalsConceded_last_ten',\n                    'average_cornersConceded_last_ten',\n                    'average_shotsConceded_last_ten',\n                    'average_shotsOnTargetConceded_last_ten',\n                    'average_foulsReceived_last_ten',\n                    'average_yellowOpponent_last_ten',\n                    'average_redOpponent_last_ten'\n                    ]\n\nlastTen_stats_list = []\nfor index, row in team_stats_per_match.iterrows():\n    team_stats_last_ten_matches = team_stats_per_match.loc[(team_stats_per_match['name']==row['name']) &amp; (team_stats_per_match['date']&lt;row['date'])].sort_values(by=['date'], ascending=False)\n    lastTen_stats_list.append(team_stats_last_ten_matches.iloc[0:10,3:-1].mean(axis=0).values[0:18])\n\navg_lastTen_stats_per_team = pd.DataFrame(lastTen_stats_list, columns=avg_lastTen_stat_columns)\n</code></pre>"},{"location":"modelling/soccerModellingTutorialPython/#average-pre-match-stats-last-5-home-and-last-5-away-matches","title":"Average Pre-Match Stats Last 5 Home and Last 5 Away Matches","text":"<p>Often teams can play better at home than away so it's important to consider whether they are playing at home or away. For example in the AFL, the Brisbane Lions are known to be almost unbeatable at the Gabba (there's a reason it's called the Gabbatoir). As such we need to add some additional features.</p> <p><pre><code>avg_lastFiveHome_stat_columns=[\n                    'average_goalsScored_last_five_home',\n                    'average_halfTimeGoalsScored_last_five_home',\n                    'average_cornerCount_last_five_home',\n                    'average_shots_last_five_home',\n                    'average_shotsOnTarget_last_five_home',\n                    'average_foulsConceded_last_five_home',\n                    'average_yellowConceded_last_five_home',\n                    'average_redConceded_last_five_home',\n                    'average_result_last_five_home',\n                    'average_goalsConceded_last_five_home',\n                    'average_halfTimeGoalsConceded_last_five_home',\n                    'average_cornersConceded_last_five_home',\n                    'average_shotsConceded_last_five_home',\n                    'average_shotsOnTargetConceded_last_five_home',\n                    'average_foulsReceived_last_five_home',\n                    'average_yellowOpponent_last_five_home',\n                    'average_redOpponent_last_five_home'\n                    ]\n\nlastFive_Home_stats_list = []\nteam_stats_L5_home_matches = team_stats_per_match[team_stats_per_match['home_or_away'] == 'Home']\nfor index, row in team_stats_L5_home_matches.iterrows():\n    team_stats_last_five_home_matches = team_stats_L5_home_matches.loc[(team_stats_L5_home_matches['name']==row['name']) &amp; (team_stats_L5_home_matches['date']&lt;row['date'])].sort_values(by=['date'], ascending=False)\n    lastFive_Home_stats_list.append(team_stats_last_five_home_matches.iloc[0:5,3:-1].mean(axis=0).values[0:18])\n\navg_lastFiveHome_stats_per_team = pd.DataFrame(lastFive_Home_stats_list, columns=avg_lastFiveHome_stat_columns)\n</code></pre> <pre><code>team_stats_L5_home_matches = pd.concat([team_stats_L5_home_matches.reset_index(drop=True), avg_lastFiveHome_stats_per_team], axis=1, ignore_index=False)\n</code></pre> <pre><code>avg_lastFiveAway_stat_columns=[\n                    'average_goalsScored_last_five_away',\n                    'average_halfTimeGoalsScored_last_five_away',\n                    'average_cornerCount_last_five_away',\n                    'average_shots_last_five_away',\n                    'average_shotsOnTarget_last_five_away',\n                    'average_foulsConceded_last_five_away',\n                    'average_yellowConceded_last_five_away',\n                    'average_redConceded_last_five_away',\n                    'average_result_last_five_away',\n                    'average_goalsConceded_last_five_away',\n                    'average_halfTimeGoalsConceded_last_five_away',\n                    'average_cornersConceded_last_five_away',\n                    'average_shotsConceded_last_five_away',\n                    'average_shotsOnTargetConceded_last_five_away',\n                    'average_foulsReceived_last_five_away',\n                    'average_yellowOpponent_last_five_away',\n                    'average_redOpponent_last_five_away'\n                    ]\n\nlastFive_away_stats_list = []\nteam_stats_L5_away_matches = team_stats_per_match[team_stats_per_match['home_or_away'] == 'Away']\nfor index, row in team_stats_L5_away_matches.iterrows():\n    team_stats_last_five_away_matches = team_stats_L5_away_matches.loc[(team_stats_L5_away_matches['name']==row['name']) &amp; (team_stats_L5_away_matches['date']&lt;row['date'])].sort_values(by=['date'], ascending=False)\n    lastFive_away_stats_list.append(team_stats_last_five_away_matches.iloc[0:5,3:-1].mean(axis=0).values[0:18])\n\navg_lastFiveAway_stats_per_team = pd.DataFrame(lastFive_away_stats_list, columns=avg_lastFiveAway_stat_columns)\nteam_stats_L5_away_matches = pd.concat([team_stats_L5_away_matches.reset_index(drop=True), avg_lastFiveAway_stats_per_team], axis=1, ignore_index=False)\n</code></pre> <pre><code>team_stats_L5_home_matches.columns = team_stats_L5_home_matches.columns[:2].tolist() + ['team_1_'+str(col) for col in team_stats_L5_home_matches.columns[2:]]\nteam_stats_L5_away_matches.columns = team_stats_L5_away_matches.columns[:2].tolist() + ['team_2_'+str(col) for col in team_stats_L5_away_matches.columns[2:]]\n</code></pre> <pre><code>home_and_away_stats = pd.merge(team_stats_L5_home_matches,team_stats_L5_away_matches,how='left',on=['date','match_id'])\n</code></pre></p>"},{"location":"modelling/soccerModellingTutorialPython/#reshape-average-pre-match-stats","title":"Reshape average pre-match stats","text":"<p>Now that we have the average stats for each team going into every match, we can create a dataset similar to the raw_match_stats, where each row represents both teams from one match.</p> <ol> <li>Re-segment the home and away teams (name Team 1 and Team 2 rather than home and away).</li> <li>Combine at each match to get a dataset with a row representing each match.</li> </ol> <pre><code>team_stats_per_match = pd.concat([team_stats_per_match.reset_index(drop=True), avg_lastTen_stats_per_team], axis=1, ignore_index=False)\n# Re-segment the home and away teams.\nhome_team_stats = team_stats_per_match.iloc[:int(team_stats_per_match.shape[0]/2),:]\naway_team_stats = team_stats_per_match.iloc[int(team_stats_per_match.shape[0]/2):,:]\n\nhome_team_stats.columns = home_team_stats.columns[:2].tolist() + ['team_1_'+str(col) for col in home_team_stats.columns[2:]]\naway_team_stats.columns = away_team_stats.columns[:2].tolist() + ['team_2_'+str(col) for col in away_team_stats.columns[2:]]\n\n# Combine at each match to get a dataset with a row representing each match. \n# drop the NA rows (earliest match for each team, i.e no previous stats)\naway_team_stats = away_team_stats.iloc[:, 2:]\nmatch_stats = pd.concat([home_team_stats, away_team_stats.reset_index(drop=True)], axis=1, ignore_index=False)\nmatch_stats = match_stats.dropna().reset_index(drop=True)\n</code></pre> <pre><code>match_stats=pd.merge(match_stats,home_and_away_stats,how='left',on=['date',\n                                                                    'match_id',\n                                                                    'team_1_name',\n                                                                    'team_1_goalsScored',\n                                                                    'team_1_halfTimeGoalsScored',\n                                                                    'team_1_cornerCount',\n                                                                    'team_1_shots',\n                                                                    'team_1_shotsOnTarget',\n                                                                    'team_1_foulsConceded',\n                                                                    'team_1_yellowConceded',\n                                                                    'team_1_redConceded',\n                                                                    'team_1_result',\n                                                                    'team_1_goalsConceded',\n                                                                    'team_1_halfTimeGoalsConceded',\n                                                                    'team_1_cornersConceded',\n                                                                    'team_1_shotsConceded',\n                                                                    'team_1_shotsOnTargetConceded',\n                                                                    'team_1_foulsReceived',\n                                                                    'team_1_yellowOpponent',\n                                                                    'team_1_redOpponent',\n                                                                    'team_1_home_or_away',\n                                                                    'team_2_name',\n                                                                    'team_2_goalsScored',\n                                                                    'team_2_halfTimeGoalsScored',\n                                                                    'team_2_cornerCount',\n                                                                    'team_2_shots',\n                                                                    'team_2_shotsOnTarget',\n                                                                    'team_2_foulsConceded',\n                                                                    'team_2_yellowConceded',\n                                                                    'team_2_redConceded',\n                                                                    'team_2_result',\n                                                                    'team_2_goalsConceded',\n                                                                    'team_2_halfTimeGoalsConceded',\n                                                                    'team_2_cornersConceded',\n                                                                    'team_2_shotsConceded',\n                                                                    'team_2_shotsOnTargetConceded',\n                                                                    'team_2_foulsReceived',\n                                                                    'team_2_yellowOpponent',\n                                                                    'team_2_redOpponent',\n                                                                    'team_2_home_or_away'])\n</code></pre>"},{"location":"modelling/soccerModellingTutorialPython/#train-ml-model","title":"Train ML model","text":"<p>In our ML model, we will use the raw Team 1 and Team 2 average stats as features.</p> <p>Some questions we could ask ourselves about this dataset are: (and there is no easy answer without some experimentation)</p> <p>Would we be better off using the differential between the teams as features?</p> <p>Can we generate any other useful features from the dataset provided?</p> <p>Do we need to weigh the home and away teams because home teams win more often?</p> <p>In this tutorial we will:</p> <ol> <li>Train a model using 68 feature columns</li> <li>Use a datetime split in training/test data (a random 80/20 split could also be used)</li> <li>Use accuracy to evaluate our models</li> </ol> <p>It's probably worth evaluating multiple models (several models explained in this tutorial), perhaps use k-fold cross validation, and use metrics other than accuracy to evaluate a model (check the commented out code).</p> <pre><code># import required libraries\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support as score, confusion_matrix, roc_auc_score, classification_report, log_loss\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\n</code></pre>"},{"location":"modelling/soccerModellingTutorialPython/#get-data-from-our-dataset","title":"Get data from our dataset","text":"<ol> <li>Team_1_result column - target variable</li> <li>The raw stats of the two teams (68 columns) - features</li> </ol> <p>Do we need to scale or normalize the feature columns in order for it to make mathematical sense to a ML model? This depends on the type of model we are training, but it's definitely worth investigating in order to achieve a high performing model.</p> <p>We should also investigate the dataset to check if it's balanced on all classes or if it's skewed towards a particular class (i.e are there an equal number of wins, losses and draws?). If not, would this affect model performance?</p> <pre><code>match_stats.dropna(inplace=True)\n\n# Define features\nfeatures = ['team_1_average_goalsScored_last_ten',\n            'team_1_average_halfTimeGoalsScored_last_ten',\n            'team_1_average_cornerCount_last_ten',\n            'team_1_average_shots_last_ten',\n            'team_1_average_shotsOnTarget_last_ten',\n            'team_1_average_foulsConceded_last_ten',\n            'team_1_average_yellowConceded_last_ten',\n            'team_1_average_redConceded_last_ten',\n            'team_1_average_result_last_ten',\n            'team_1_average_goalsConceded_last_ten',\n            'team_1_average_halfTimeGoalsConceded_last_ten',\n            'team_1_average_cornersConceded_last_ten',\n            'team_1_average_shotsConceded_last_ten',\n            'team_1_average_shotsOnTargetConceded_last_ten',\n            'team_1_average_foulsReceived_last_ten',\n            'team_1_average_yellowOpponent_last_ten',\n            'team_1_average_redOpponent_last_ten',\n            'team_2_average_goalsScored_last_ten',\n            'team_2_average_halfTimeGoalsScored_last_ten',\n            'team_2_average_cornerCount_last_ten',\n            'team_2_average_shots_last_ten',\n            'team_2_average_shotsOnTarget_last_ten',\n            'team_2_average_foulsConceded_last_ten',\n            'team_2_average_yellowConceded_last_ten',\n            'team_2_average_redConceded_last_ten',\n            'team_2_average_result_last_ten',\n            'team_2_average_goalsConceded_last_ten',\n            'team_2_average_halfTimeGoalsConceded_last_ten',\n            'team_2_average_cornersConceded_last_ten',\n            'team_2_average_shotsConceded_last_ten',\n            'team_2_average_shotsOnTargetConceded_last_ten',\n            'team_2_average_foulsReceived_last_ten',\n            'team_2_average_yellowOpponent_last_ten',\n            'team_2_average_redOpponent_last_ten',\n            'team_1_average_goalsScored_last_five_home',\n            'team_1_average_halfTimeGoalsScored_last_five_home',\n            'team_1_average_cornerCount_last_five_home',\n            'team_1_average_shots_last_five_home',\n            'team_1_average_shotsOnTarget_last_five_home',\n            'team_1_average_foulsConceded_last_five_home',\n            'team_1_average_yellowConceded_last_five_home',\n            'team_1_average_redConceded_last_five_home',\n            'team_1_average_result_last_five_home',\n            'team_1_average_goalsConceded_last_five_home',\n            'team_1_average_halfTimeGoalsConceded_last_five_home',\n            'team_1_average_cornersConceded_last_five_home',\n            'team_1_average_shotsConceded_last_five_home',\n            'team_1_average_shotsOnTargetConceded_last_five_home',\n            'team_1_average_foulsReceived_last_five_home',\n            'team_1_average_yellowOpponent_last_five_home',\n            'team_1_average_redOpponent_last_five_home',\n            'team_2_average_goalsScored_last_five_away',\n            'team_2_average_halfTimeGoalsScored_last_five_away',\n            'team_2_average_cornerCount_last_five_away',\n            'team_2_average_shots_last_five_away',\n            'team_2_average_shotsOnTarget_last_five_away',\n            'team_2_average_foulsConceded_last_five_away',\n            'team_2_average_yellowConceded_last_five_away',\n            'team_2_average_redConceded_last_five_away',\n            'team_2_average_result_last_five_away',\n            'team_2_average_goalsConceded_last_five_away',\n            'team_2_average_halfTimeGoalsConceded_last_five_away',\n            'team_2_average_cornersConceded_last_five_away',\n            'team_2_average_shotsConceded_last_five_away',\n            'team_2_average_shotsOnTargetConceded_last_five_away',\n            'team_2_average_foulsReceived_last_five_away',\n            'team_2_average_yellowOpponent_last_five_away',\n            'team_2_average_redOpponent_last_five_away'\n]\n</code></pre>"},{"location":"modelling/soccerModellingTutorialPython/#split-test-and-training-data","title":"Split test and training data","text":"<p>We train a model on the training data, and then use test data to evaluate the performance of that model.</p> <pre><code>train_data = match_stats[match_stats['date'] &lt; '2018-07-01']\ntest_data = match_stats[match_stats['date'] &gt;= '2018-07-01']\n\nX_train = train_data[features]\nX_test = test_data[features]\nY_train = train_data['team_1_result']\nY_test = test_data['team_1_result']\n</code></pre>"},{"location":"modelling/soccerModellingTutorialPython/#name-and-define-classifiers","title":"Name and define classifiers","text":"<pre><code>names = [\"Nearest Neighbors\", \"Logistic Regression\",\"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\"]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    LogisticRegression(),\n    SVC(kernel=\"linear\", C=0.025, probability=True),\n    SVC(gamma=2, C=1, probability=True),\n    GaussianProcessClassifier(1.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()]\n</code></pre>"},{"location":"modelling/soccerModellingTutorialPython/#iterate-through-all-classifiers-and-get-their-accuracy-score","title":"Iterate through all classifiers and get their accuracy score","text":"<p>We can use the best performing model to make our predictions.</p> <p>There are several other metrics in the code that have been commented out which might provide helpful insights on model performance.</p> <pre><code>for name, clf in zip(names, classifiers):\n    # Fit the classifier on the training data and make predictions\n    clf.fit(X_train, Y_train)\n    test_data[name + '_team_1_result'] = clf.predict(X_test)\n    accuracy = clf.score(X_test, Y_test)\n\n    # prediction_proba = clf.predict_proba(X_test)\n    # logloss = log_loss(y_test,prediction_proba)\n    # precision, recall, fscore, support = score(y_test, prediction)\n    # conf_martrix = confusion_matrix(y_test, prediction)\n    # clas_report = classification_report(y_test, prediction)\n\n    print(name, accuracy)\n\n# Export the predictions to a CSV file\ntest_data.to_csv('predictions.csv', index=False)\n</code></pre> <pre><code>Nearest Neighbors 0.43313373253493015\nLogistic Regression 0.5359281437125748\nLinear SVM 0.532435129740519\nRBF SVM 0.4431137724550898\nGaussian Process 0.4286427145708583\nDecision Tree 0.499001996007984\nRandom Forest 0.49001996007984033\nNeural Net 0.5069860279441117\nAdaBoost 0.499500998003992\nNaive Bayes 0.5149700598802395\nQDA 0.4865269461077844\n</code></pre>"},{"location":"modelling/soccerModellingTutorialPython/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"modelling/soccerModellingTutorialR/","title":"How to model the 2021 Euro &amp; Copa America: R Tutorial","text":""},{"location":"modelling/soccerModellingTutorialR/#the-task","title":"The Task","text":"<p>This notebook will outline how to train a simple classification model to predict the outcome of a soccer match using the dataset provided for the datathon.</p> <ol> <li>Reading data from file and get a raw dataset</li> <li>Data cleaning and feature engineering</li> <li>Training a model</li> </ol> <p>The tutorial covers the thought process of manipulating the dataset (why and how), some simple data cleaning, feature engineering and training a classification model.</p> <p>The tutorial DOES NOT delve deep into the fundamentals of machine learning, advanced feature engineering or model tuning.</p> <p>There are some helpful hints along the way though.</p> <pre><code># import required libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(caTools) # calculate rolling average \n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#read-data-from-file-and-get-a-raw-dataset","title":"Read data from file and get a raw dataset","text":""},{"location":"modelling/soccerModellingTutorialR/#change-the-data-types-date-column","title":"Change the data types - date column.","text":"<p>We need the date column in good order for our tutorial. Here's a sample of the data set we're using for this tutorial.</p> <p>In general, it's a good idea to evaluate data types of all columns that we work with to ensure they are correct.</p> <pre><code>df = read_csv(\"SoccerData.csv\", guess_max = 2000)\ndf = df %&gt;% mutate(date_GMT = parse_date_time(date_GMT, '%b %d %Y - %I:%M%p', tz = \"GMT\"))\n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#get-data-columns-and-create-raw-dataset","title":"Get data columns and create raw dataset","text":"<p>For this tutorial, let's take only a few stats columns to work with.</p> <p>Typically we would explore all features and then decide which data to discard.</p> <ol> <li>Goal counts</li> <li>Corners</li> <li>Total shots</li> <li>Shots on target</li> <li>Fouls</li> <li>Possession</li> </ol> <pre><code>raw_match_stats = df %&gt;%\n  select(\n  date_GMT,\n  home_team_name,\n  away_team_name,\n  home_team_goal_count,\n  away_team_goal_count,\n  home_team_corner_count,\n  away_team_corner_count,\n  home_team_shots,\n  away_team_shots,\n  home_team_shots_on_target,\n  away_team_shots_on_target,\n  home_team_fouls,\n  away_team_fouls,\n  home_team_possession,\n  away_team_possession\n  )\n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#clean-data","title":"Clean data","text":"<p>As a cleaning step, we order our data by date and drop rows with NA values.</p> <pre><code>raw_match_stats = raw_match_stats %&gt;%\n  arrange(desc(date_GMT)) %&gt;%\n  drop_na() %&gt;%\n  mutate(game_id = 1:n())\n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#raw-dataset","title":"Raw dataset","text":"<p>This raw dataset is structured so that each match has an individual row and stats for both teams are on that row with columns titles \"home\" and \"away\".</p> <p>Our goal is to build a machine learning (ML) model that can predict the result of a soccer match. Given that we have some match stats, we will aim to use that information to predict a WIN, LOSS or DRAW.</p> <pre><code>raw_match_stats\n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#data-cleaning-and-feature-engineering","title":"Data cleaning and feature engineering","text":""},{"location":"modelling/soccerModellingTutorialR/#target-variable-match-result","title":"Target variable - Match Result","text":"<p>Our machine learning model aims to predict the result of a match. This \"result\" is called the \"target variable\". Our dataset has no columns showing the match result. We will create two columns for the results for each team. One of these would become the target variable for our ML model.</p> <pre><code># create results columns for both home and away teams (W - win, D = Draw, L = Loss).\nraw_match_stats = raw_match_stats %&gt;%\n  mutate(home_team_result = case_when(home_team_goal_count == away_team_goal_count ~ \"D\",\n                                      home_team_goal_count &gt; away_team_goal_count ~ \"W\",\n                                      home_team_goal_count &lt; away_team_goal_count ~ \"L\"),\n         away_team_result = case_when(home_team_goal_count == away_team_goal_count ~ \"D\",\n                                      home_team_goal_count &gt; away_team_goal_count ~ \"L\",\n                                      home_team_goal_count &lt; away_team_goal_count ~ \"W\"))\n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#average-pre-match-stats-five-match-average","title":"Average pre-match stats - Five match average","text":"<p>Great! Now we have a dataset with many rows of data, with each row representing match stats and the match result (this would become our target variable).</p> <p>But our goal is to build an ML model that predicts the match result prior to the start of a match. Are the stats from that match what we need to build this ML model? No! When predicting a match outcome BEFORE the start of the match, we are forced to rely on match stats available to us from previous matches.</p> <p>Therefore, we need a dataset with the match result (target variable) and stats for each team heading into that match. For this tutorial, we will look at the average stats for each team in the five matches preceding each match.</p> <p>Lets look at how we can get the average stats for the previous 5 matches for each team at each match.</p> <ol> <li>Split the raw_match_stats to two datasets (home_team_stats and away_team_stats).</li> <li>Stack these two datasets so that each row is the stats for a team for one match (team_stats_per_match).</li> <li>At each row of this dataset, get the team name, find the stats for that team during the 4. last 5 matches, and average these stats (avg_stats_per_team). Add these stats to the team_stats_per_match dataset.</li> </ol> <p>Why did we chose five matches? Why not 10? Should we average over a time period (matches in the last year perhaps?) rather than a number? What's the least number of matches available for each competing team in the dataset? These are all interesting questions that may improve our model.</p> <pre><code># Split the raw_match_stats to two datasets (home_team_stats and away_team_stats). \nhome_team_stats = raw_match_stats %&gt;%\n  select(game_id,\n         date_GMT,\n         starts_with(\"home_team\"))\naway_team_stats = raw_match_stats %&gt;%\n  select(game_id,\n         date_GMT,\n         starts_with(\"away_team\"))\n# rename \"home_team\" and \"away_team\" columns\nhome_team_stats = rename_with(home_team_stats, ~gsub(\"home_team_\", \"\", .x))\naway_team_stats = rename_with(away_team_stats, ~gsub(\"away_team_\", \"\", .x))\n# Stack these two datasets so that each row is the stats for a team for one match (team_stats_per_match). \nteam_stats_per_match = home_team_stats %&gt;%\n  bind_rows(away_team_stats)\n</code></pre> <pre><code># At each row of this dataset, get the team name, find the stats for that team during the last 5 matches, and average these stats (avg_stats_per_team). \navg_stats_per_team &lt;- team_stats_per_match %&gt;%\n  group_by(name) %&gt;%\n  filter(n() &gt; 1) %&gt;%     # remove countries with only 1 match\n  arrange(date_GMT) %&gt;%\n  mutate_at(\n  vars(\n  goal_count,\n  corner_count,\n  shots,\n  shots_on_target,\n  fouls,\n  possession\n  ),\n  ## Columns for which we want a rolling mean\n  .funs = ~ runmean(\n  x = dplyr::lag(.x),\n  k = 5,\n  endrule = \"mean\",\n  align = \"right\"\n  )## Rolling mean for last 5 matches\n  ) %&gt;%\n  rename(\n  goals_per_match = goal_count,\n  corners_per_match = corner_count,\n  shots_per_match = shots,\n  shotsOnTarget_per_match = shots_on_target,\n  fouls_per_match = fouls,\n  possession_per_match = possession\n  )\n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#reshape-average-pre-match-stats","title":"Reshape average pre-match stats","text":"<p>Now that we have the average stats for each team going into every match, we can create a dataset similar to the raw_match_stats, where each row represents both teams from one match.</p> <ol> <li>Re-segment the home and away teams (name Team 1 and Team 2 rather than home and away).</li> <li>Combine at each match to get a dataset with a row representing each match.</li> </ol> <pre><code># Add these stats to the home/away dataset\nhome_team_stats &lt;- home_team_stats %&gt;% \n  left_join(avg_stats_per_team)  %&gt;%\n  arrange(game_id) %&gt;%\n  rename_with(~paste0(\"team_1_\", .))\naway_team_stats &lt;- away_team_stats %&gt;% \n  left_join(avg_stats_per_team)  %&gt;%\n  arrange(game_id) %&gt;%\n  rename_with(~paste0(\"team_2_\", .))\n</code></pre> <pre><code># Combine at each match to get a dataset with a row representing each match. \n# drop the NA rows ( earliest match for each team, i.e no previous stats)\nmatch_stats = home_team_stats %&gt;%\n  bind_cols(away_team_stats) %&gt;%\n  drop_na()\n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#find-the-difference-of-stats-between-teams","title":"Find the difference of stats between teams","text":"<p>In our ML model, we will take the difference between Team 1 and Team 2 average stats as features. 6 new columns are created for this.</p> <p>Would we be better off using the raw stats for each team as features?</p> <p>Can we generate any other useful features from the dataset provided?</p> <p>Do we need to weigh the home and away teams because home teams win more often?</p> <pre><code># create columns with average stat differences between the two teams\nmatch_stats = match_stats %&gt;%\n  mutate(goals_per_match_diff = team_1_goals_per_match - team_2_goals_per_match,\n         corners_per_match_diff = team_1_corners_per_match - team_2_corners_per_match,\n         shots_per_match_diff = team_1_shots_per_match - team_2_shots_per_match,\n         shotsOnTarget_per_match_diff = team_1_shotsOnTarget_per_match - team_2_shotsOnTarget_per_match,\n         fouls_per_match_diff = team_1_fouls_per_match - team_2_fouls_per_match,\n         possession_per_match_diff = team_1_possession_per_match - team_2_possession_per_match)\n</code></pre> <pre><code>match_stats\n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#train-ml-model","title":"Train ML model","text":"<p>In this tutorial we will:</p> <ol> <li>Train a model using 6 feature columns</li> <li>Use an 80/20 split in training/test data</li> <li>Use accuracy to evaluate our models</li> </ol> <p>It's probably worth evaluating multiple models (several models explained in this tutorial), perhaps use k-fold cross validation, and use metrics other than accuracy to evaluate a model (check the commented out code).</p> <pre><code>library(tidymodels)\nlibrary(discrim)\nlibrary(kknn) # kknn\nlibrary(kernlab) # svm\nlibrary(naivebayes) #naive bayes\nlibrary(rpart) # decision tree\nlibrary(nnet) # logistic regression and neural net\nlibrary(ranger) # random forest (modified by multiclass)\nlibrary(xgboost) # xgboost\n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#get-data-from-our-dataset","title":"Get data from our dataset","text":"<ol> <li>Team_1_result column - target variable</li> <li>The difference of stats between teams (6 columns) - features</li> </ol> <p>Do we need to scale or normalize the feature columns in order for it to make mathematical sense to a ML model? This depends on the type of model we are training, but it's definitely worth investigating in order to achieve a high performing model.</p> <p>We should also investigate the dataset to check if it's balanced on all classes or if it's skewed towards a particular class (i.e are there an equal number of wins, losses and draws?). If not, would this affect model performance?</p> <pre><code># form a dataset with only target and features\nmodel_data = match_stats %&gt;%\n  mutate(target = factor(recode(team_1_result, W = 0, D = 1, L = 2))) %&gt;%\n  select(\n    target,\n    goals_per_match_diff,\n    corners_per_match_diff,\n    shots_per_match_diff,\n    shotsOnTarget_per_match_diff,\n    fouls_per_match_diff,\n    possession_per_match_diff\n  )\n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#split-test-and-training-data","title":"Split test and training data","text":"<p>We train a model on the training data, and then use test data to evaluate the performance of that model.</p> <pre><code># For more details of using tidymodels package, check https://www.tidymodels.org/\n# split test and train data\nset.seed(2021)\ndata_split &lt;- initial_split(model_data, strata = target, prop = 0.8)\n# create a recipe for data pre-processing\nmodel_recipe &lt;- \n  recipe(target ~ ., data = model_data) %&gt;%\n  step_zv(all_predictors())  %&gt;%\n  step_corr(all_predictors()) %&gt;%\n  prep()\n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#name-and-define-classifiers","title":"Name and define classifiers","text":"<pre><code># Define the models\n## knn model\nknn_model &lt;- \n  nearest_neighbor() %&gt;% \n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"classification\")\n## svm model\nsvm_model &lt;- \n  svm_rbf() %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"classification\") \n## logistic regression model\nlr_model &lt;- \n  multinom_reg() %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"classification\")\n## naive bayes\nnb_model &lt;- \n  naive_Bayes() %&gt;% \n  set_engine(\"naivebayes\") %&gt;% \n  set_mode(\"classification\")\n## decision tree model\ntree_model &lt;- \n  decision_tree() %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n# random forest\nrf_model &lt;- \n  rand_forest() %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"classification\")\n# boosted tree (xgboost)\nxgb_model &lt;- \n  boost_tree() %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"classification\")\n# single layer neural network\nnn_model &lt;- \n  mlp() %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"classification\")\n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#iterate-through-all-classifiers-and-get-their-accuracy-score","title":"Iterate through all classifiers and get their accuracy score","text":"<p>We can use the best performing model to make our predictions.</p> <p>There are several other metrics in the code that have been commented out which might provide helpful insights on model performance.</p> <pre><code>accuracy_results &lt;-\n  map_dfr(\n    .x = list(knn_model,\n              lr_model,\n              nb_model,\n              nn_model,\n              rf_model,\n              svm_model,\n              tree_model,\n              xgb_model),\n    .f = function(the_model) {\n      # create a work flow with recipe and model\n      the_workflow &lt;- workflow() %&gt;%\n        add_recipe(model_recipe) %&gt;%\n        add_model(the_model)\n\n      # fit the model with training data and evaluate with test data\n      the_fit &lt;- the_workflow %&gt;%\n        last_fit(data_split)\n\n      tibble(model_name = class(the_model)[1],\n             accuracy = the_fit %&gt;% collect_metrics() %&gt;% filter(.metric == \"accuracy\") %&gt;% select(.estimate) %&gt;% pull)\n    }\n  )\n## Displaying accuracy of the models set in descending order\naccuracy_results %&gt;%\n  arrange(desc(accuracy))\n</code></pre>"},{"location":"modelling/soccerModellingTutorialR/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"modelling/testfile/","title":"Testfile","text":""},{"location":"modelling/testfile/#_","title":"_","text":"TRADE AND BET ON BETFAIR USING OUR API OR CHOOSE FROM A CATALOGUE OF THIRD PARTY BETTING TOOLS              The Betfair Exchange is a unique platform for peer-to-peer trading and wagering.             We welcome and support winning clients and provide tools for your automation journey.          Join Now Get Your API Key Not in Australia or New Zealand? Trade and bet on Betfair using our API or choose from a catalogue of third party betting tools Join Now Get Your API Key Not in Australia or New Zealand? Trade and bet on Betfair using our API or choose from a catalogue of third party betting tools Join Now Get Your API Key Not in Australia or New Zealand? <p>REAL-TIME DATA</p> <p>Get access to up-to-date data on the Betfair Exchange, including prices, volumes, and market trends.</p> <p>LIGHTNING FASTEXECUTION</p> <p>Place bets and trades quickly and efficiently, with sub-second response times.</p> <p>AUTOMATION</p> <p>Automate your betting strategies and reduce manual intervention, by using our API to place bets and trades automatically.</p> <p>CUSTOMISATION</p> <p>Develop your own trading and betting applications, tailored to your specific needs and preferences.</p> <p>INTEGRATION</p> <p>Integrate Betfair data into your existing systems and workflows and enhance your trading and betting capabilities.</p> <p>REAL-TIME DATA</p> <p>Get access to up-to-date data on the Betfair Exchange, including prices, volumes, and market trends.</p> <p>LIGHTNING FASTEXECUTION</p> <p>Place bets and trades quickly and efficiently, with sub-second response times.</p> <p>AUTOMATION</p> <p>Automate your betting strategies and reduce manual intervention, by using our API to place bets and trades automatically.</p> <p>CUSTOMISATION</p> <p>Develop your own trading and betting applications, tailored to your specific needs and preferences.</p> <p>INTEGRATION</p> <p>Integrate Betfair data into your existing systems and workflows and enhance your trading and betting capabilities.</p> REAL TIME AND HISTORICAL DATA In addition to a real-time data streaming service, we provide access to a database of historical market data. The Betfair database currently holds almost 1.5TB of raw and preprocessed market data available in JSON format.  Our Data Availability Listing REAL TIME AND HISTORICAL DATA In addition to a real-time data streaming service, we provide access to a database of historical market data. The Betfair database currently holds almost 1.5TB of raw and preprocessed market data available in JSON format.  Our Data Availability Listing DEDICATED TEAM We are a service-oriented team dedicated to supporting those looking to create and automate a betting or trading strategy. Whether you're a seasoned quant or brand new to Betfair, we're here to help. 12.4% 76K 50ms 1450GB ADVANTAGE FOR BSP AGAINST BEST TOTE AUSTRALIAN RACING EVENTS PER YEAR STREAM UPDATE FREQUENCY OF HISTORICAL PRICING DATA 12.4% ADVANTAGE FOR BSP AGAINST BEST TOTE 76K AUSTRALIAN RACING EVENTS PER YEAR 50ms STREAM UPDATE FREQUENCY 1450GB OF HISTORICAL PRICING DATA Connect to the API in your language of choice              Helper libraries are available for most programming languages, provided either by Betfair or supported by the community so you can focus on the most important aspects instead of wasting time connecting the pieces together.              Connect to the API in your language of choice              Helper libraries are available for most programming languages, provided either by Betfair or supported by the community so you can focus on the most important aspects instead of wasting time connecting the pieces together.              C++ Clojure Node.js Perl Scala Python C# PHP Ruby Java Javascript R C++ Clojure Node.js Perl Scala Python C# PHP Ruby Java Javascript R RESOURCE CATALOGUE API Tutorials <p>Betfair has a set of customer-facing transactional APIs to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software setups or tapping existing libraries.</p> Learn More Historic Pricing Data <p>We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport. Australian and New Zealand customers can reach out directly for access to our curated database.</p> Learn More Data Modelling <p>Many of our most successful customers use predictive models as the basis for their betting strategies.  We have a series of modelling tutorials created by community members ranging from racing to sports. We hold regular datathons where cash prizes can be won.</p> Learn More Automation Tools <p>There are many applications that have been built by external developers which offer exciting features including one-click bets or tools for fully automating your strategies on the exchange. We've developed reviews and tutorials for some of the most popular tools.</p> Learn More Inspiration and Information <p>There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy. We're always looking for new and interesting content.</p> Learn More Quants Community <p>betfair quants\u00a0is really active Betfair-owned Discord server for people interested in modelling and automation on the Exchange. Our resident experts are always willing and able to share their expertise and experience. Please\u00a0reach out\u00a0if you'd like an invitation.</p> Learn More API Tutorials <p>Betfair has a set of customer-facing transactional APIs to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software setups or tapping existing libraries.</p> Learn More Historic Pricing Data <p>We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport. Australian and New Zealand customers can reach out directly for access to our curated database.</p> Learn More Data Modelling <p>Many of our most successful customers use predictive models as the basis for their betting strategies.  We have a series of modelling tutorials created by community members ranging from racing to sports. We hold regular datathons where cash prizes can be won.</p> Learn More Automation Tools <p>There are many applications that have been built by external developers which offer exciting features including one-click bets or tools for fully automating your strategies on the exchange. We've developed reviews and tutorials for some of the most popular tools.</p> Learn More Inspiration &amp; Info <p>There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy. We're always looking for new and interesting content.</p> Learn More Quants Community <p>betfair quants\u00a0is really active Betfair-owned Discord server for people interested in modelling and automation on the Exchange. Our resident experts are always willing and able to share their expertise and experience. Please\u00a0reach out\u00a0if you'd like an invitation.</p> Learn More <p>For one-on-one support, AUS and NZ customers can contact us</p> Contact us Get Your API Key <p></p>"},{"location":"modelling/topazTutorial/","title":"Greyhound Modelling in Python using the Topaz API","text":"<p>Building a greyhound racing model using Python and Machine Learning</p> <p>This tutorial is a refreshed version of our previous tutorials utilising the new version of the FastTrack API (now called Topaz).  Topaz is a product provided to Betfair Australia &amp; New Zealand customers by Greyhound Racing Victoria (GRV). </p> <p>If you would like your own Topaz API key, please contact us here.  Access can only be provided to Betfair Australia or New Zealand customers with active accounts</p>"},{"location":"modelling/topazTutorial/#overview","title":"Overview","text":"<p>This tutorial will walk you through the different steps required to generate Greyhound racing winning probabilities</p> <ol> <li>Download historic greyhound data from Topaz API</li> <li>Cleanse and normalise the data</li> <li>Generate features using raw data</li> <li>Build and train classification models</li> <li>Evaluate models' performances</li> <li>Evaluate feature importance</li> </ol>"},{"location":"modelling/topazTutorial/#requirements","title":"Requirements","text":"<ul> <li>Coding environment which supports Jupyter Notebooks (e.g. Visual Studio Code)</li> <li>Betfair API Key. If you don't have one please follow the steps outlined on the The Automation Hub</li> <li>Topaz API Key. If you would like to be considered for a Topaz key, please email data@betfair.com.au (Australian/New Zealand customers only).</li> <li>Python Topaz API wrapper. To install this package using pip, type 'pip install topaz_api' into your terminal</li> </ul>"},{"location":"modelling/topazTutorial/#historic-data","title":"Historic Data","text":"<p>To get started on building our own Topaz model, first we need to download the historic data from the Topaz API. The API has rate limits in place and so for the purposes of a bulk download of historic data, we will need to implement some way of handling these rate limits in order for us to download the data with a high degree of completeness. After all, the maxim 'Garbage In, Garbage Out' in regards to modelling holds true. If you don't feed your model good, complete data, then you won't have much success.</p> <p>In the code block below, there are multiple instances of retries and programmed sleep functions to allow the rate limits to reset. We are also requesting the races for each state in blocks of 7 days.</p> <p>NOTE: For state / date-range combinations where there are genuinely no races that occurred, the function will continuously error until it reaches the maximum specified retries before continuing to the next block. This may occur during the pandemic shutdown period in 2020 or the NSW Greyhound racing ban in 2017 or for time periods with the 'NT' jurisdiction where greyhound meetings may be up to 14 days apart.</p> Downloading Historic Data<pre><code>import pandas as pd\nfrom tqdm import tqdm\nimport time\nfrom datetime import datetime, timedelta\nfrom topaz import TopazAPI\nfrom sklearn.preprocessing import MinMaxScaler \nimport numpy as np\nimport requests\nimport os\nimport itertools\n\n# Insert your TOPAZ API key \nTOPAZ_API_KEY = ''\n\n# Define the states you require\nJURISDICTION_CODES = ['NSW','QLD','WA','TAS','NT','NZ','VIC','SA']\n\n'''\nIt is pythonic convention to define hard-coded variables (like credentials) in all caps. Variables whose value may change in use should be defined in lowercase with underscore spacing\n'''\n\n# Define the start and end date for the historic data download\nstart_date = datetime(2020,1,1)\nend_date = (datetime.today() - timedelta(days=1))\n\n\ndef generate_date_range(start_date, end_date):\n    ''' \n    Here we generate our date range for passing into the Topaz API.\n    This is necessary due to the rate limits implemented in the Topaz API. \n    Passing a start date and end date 12 months apart in the API call will cause significant amounts of data to be omitted, so we need to generate a range to pass much smaller date ranges in the Topaz API calls\n    '''\n    # Initialise the date_range list\n    date_range = []\n\n    current_date = start_date\n\n    while current_date &lt;= end_date:\n        date_range.append(current_date.strftime(\"%Y-%m-%d\"))\n        current_date += timedelta(days=1)\n\n    return date_range\n\ndef define_topaz_api(api_key):\n    '''\n    This is where we define the Topaz API and pass our credentials\n    '''\n    topaz_api = TopazAPI(api_key)\n\n    return topaz_api\n\ndef download_topaz_data(topaz_api,date_range,codes,datatype,number_of_retries,sleep_time):\n    '''\n    The parameters passed here are:\n        1. The TopazAPI instance with our credentials\n        2. Our date range that we generated which is in the format of a list of strings\n        3. Our list of JURISDICTION_CODES\n        4. Our datatype (either 'HISTORICAL' or 'UPCOMING'). \n            It is important to separate these in our database to ensure that the upcoming races with lots of empty fields (like margin) do not contaminate our historical dataset.\n    '''\n\n    # Iterate over 10-day blocks\n    for i in range(0, len(date_range), 10):\n        start_block_date = date_range[i]\n        print(start_block_date)\n        end_block_date = date_range[min(i + 9, len(date_range) - 1)]  # Ensure the end date is within the range\n\n\n        for code in codes:\n            # initialise the race list\n            all_races = []\n\n            print(code)\n\n            retries = number_of_retries  # Number of retries\n            '''\n            In this code block we are attempting to download a list of raceIds by passing our JURISDICTION_CODES and date range.\n            The sleep functions are included to allow time for the rate limits to reset and any other errors to clear.\n            The Topaz API does not return much detail in error messages and through extensive trial and error, we have found the best way to resolve errors is simply to pause for a short time and then try again.\n            After 10 retries, the function will move to the next block. This will usually only occur if there actually were zero races for that JURISDICTION_CODE and date_range or there is a total Topaz API outage.\n            '''\n            while retries &gt; 0:\n                try:\n                    races = topaz_api.get_races(from_date=start_block_date, to_date=end_block_date, owning_authority_code=code)\n                    all_races.append(races)\n                    break  # Break out of the loop if successful\n                except requests.HTTPError as http_err:\n                    if http_err.response.status_code == 429:\n                        retries -= 1\n                        if retries &gt; 0:\n                            print(f\"Rate limited. Retrying in {sleep_time * 5/60} minutes...\")\n                            time.sleep(sleep_time * 5)\n                        else:\n                            print(\"Max retries reached. Moving to the next block.\")\n                    else:\n                        print(f\"Error fetching races for {code}: {http_err.response.status_code}\")\n                        retries -= 1\n                        if retries &gt; 0:\n                            print(f\"Retrying in {sleep_time} seconds...\")\n                            time.sleep(sleep_time)\n                        else:\n                            print(\"Max retries reached. Moving to the next block.\")\n\n            try:\n                all_races_df = pd.concat(all_races, ignore_index=True)\n            except ValueError:\n                continue\n\n            # Extract unique race IDs\n            race_ids = list(all_races_df['raceId'].unique())\n\n            # Define the file path\n            file_path = code + '_DATA_'+datatype+'.csv'\n\n            # Use tqdm to create a progress bar\n            for race_id in tqdm(race_ids, desc=\"Processing races\", unit=\"race\"):\n                result_retries = number_of_retries\n                '''\n                Here we utilise the retries function again to maximise the chances of our data being complete.\n                We spend some time here gathering the splitPosition and splitTime. While these fields are not utilised in the completed model, the process to extract this data has been included here for the sake of completeness only, as it is not straightforward.\n\n                '''\n                while result_retries &gt; 0:\n                    # Check if we already have a file for this jurisdiction\n                    file_exists = os.path.isfile(file_path)\n                    # Set the header_param to the opposite Bool\n                    header_param = not file_exists\n\n                    try:\n                        # Get race run data\n                        race_run = topaz_api.get_race_runs(race_id=race_id)\n                        # Get the race result data\n                        race_result_json = topaz_api.get_race_result(race_id=race_id)\n\n                        # Flatten the JSON response into a dataframe\n                        race_result = pd.json_normalize(race_result_json)\n\n                        # Separate the split times and flatten them\n                        split_times_df = pd.DataFrame(race_result['splitTimes'].tolist(),index=race_result.index)\n                        splits_dict = split_times_df.T.stack().to_frame()\n                        splits_dict.reset_index(drop=True, inplace= True)\n                        splits_normalised = pd.json_normalize(splits_dict[0])\n\n                        if len(splits_normalised) &gt; 0:\n\n                            # Create a dataframe from the first split\n                            first_split = splits_normalised[splits_normalised['splitTimeMarker'] == 1]\n                            first_split = first_split[['runId','position','time']]\n                            first_split = first_split.rename(columns={'position':'firstSplitPosition','time':'firstSplitTime'})\n\n                            # Create a dataframe from the second split\n                            second_split = splits_normalised[splits_normalised['splitTimeMarker'] == 2]\n                            second_split = second_split[['runId','position','time']]\n                            second_split = second_split.rename(columns={'position':'secondSplitPosition','time':'secondSplitTime'})\n\n                            # Create a dataframe from the runIds, then merge the first and second split dataframes\n                            split_times = splits_normalised[['runId']]\n                            split_times = pd.merge(split_times,first_split,how='left',on=['runId'])\n                            split_times = pd.merge(split_times,second_split,how='left',on=['runId'])\n\n                            # Attach the split times to the original race_run dataframe\n                            race_run = pd.merge(race_run,split_times,how='left',on=['runId'])\n\n                    except requests.HTTPError as http_err:\n                        if http_err.response.status_code == 404:\n                            break\n\n                    except Exception:\n                        result_retries -= 1\n                        if result_retries &gt; 0:\n                            time.sleep(sleep_time)\n                        else:\n                            break\n\n                    finally:\n                        race_run.drop_duplicates(inplace=True)\n                        race_run.to_csv(file_path, mode='a', header=header_param, index=False)\n                        break\n</code></pre> <p>The Topaz race results endpoint requires the passing of a jurisdiction ID as a parameter to pull the results. Simply passing a date range will return no data. This is why we have looped over all jurisdictions in 10 day blocks.</p> <p>Bulk downloading the historical data in this way may take a while depending on how many years of data you are requesting. Bruno, the author of one of our previous greyhound modelling tutorials, uses 6 years worth of data for his backtesting. This however is computationally expensive to process in a machine learning model, so we suggest 2-3 years for those just starting out. </p> <p>NOTE: In the above code we have exported each state separately to its own csv file. This will keep each file under a million rows ensuring that you can manually inspect the data by opening the file in Excel. This is not required (we will pull in each file to our model before we begin to process the data)</p>"},{"location":"modelling/topazTutorial/#cleaning-the-data","title":"Cleaning the data","text":"<p>Let's pull all of our state files and get cleaning on this data!</p> Cleaning The Data<pre><code>def collect_topaz_data(api_key,codes,start_date,end_date,datatype,number_of_retries,sleep_time):\n    '''\n    This function here combines our three previously defined functions into one neat function in the correct order of execution.\n    As the data is written to csv files, we do not need to return anything at the end of the function. This also means that it is not necessary to define a variable as the output of a function\n    '''\n    date_range = generate_date_range(start_date,end_date)\n\n    topaz_api = define_topaz_api(api_key)\n\n    download_topaz_data(topaz_api,date_range,codes,datatype,number_of_retries,sleep_time)\n\n''' \nThe word 'HISTORICAL' is used here because we are gathering data for races that have already resulted and we want to store the data separately to any data that we download for future races where we will use the word 'UPCOMING'\nThis is because many of the datapoints will be blank for future races and will corrupt our historical data\n\nWhen calling a function that we have defined with arguments, we will need to pass parameters into the function for it to run. \nThe parameter names don't necessarily need to match between the function definition and the function call, because it will use the position of the parameter for this purpose.\nHowever, the data types should match, otherwise the function may fail. (i.e. don't pass a 'string parameter' into a function where a 'list' is expected)\n'''\n\ncollect_topaz_data(TOPAZ_API_KEY,JURISDICTION_CODES,start_date,end_date,'HISTORICAL',10,30)\n\n'''\n\nThis is what the data may look like once exported\n\n[\n{ \"trackCode\":\"GAW\"  },\n{ \"track\":\"Gawler\"  },\n{ \"distance\":\"531\"  },\n{ \"raceId\":\"184204888\"  },\n{ \"meetingDate\":\"2017-01-01T00:00:00.000Z\"  },\n{ \"raceTypeCode\":\"6\"  },\n{ \"raceType\":\"Grade 6\"  },\n{ \"runId\":\"184931091\"  },\n{ \"dogId\":\"641470451\"  },\n{ \"dogName\":\"CHANCE IT\"  },\n{ \"weightInKg\":\"26.4\"  },\n{ \"incomingGrade\":\"5\"  },\n{ \"outgoingGrade\":\"5\"  },\n{ \"gradedTo\":\"Field\"  },\n{ \"rating\":\"89\"  },\n{ \"raceNumber\":\"6\"  },\n{ \"boxNumber\":\"8\"  },\n{ \"boxDrawnOrder\":\"\"  },\n{ \"rugNumber\":\"8\"  },\n{ \"startPrice\":\"18\"  },\n{ \"place\":\"1\"  },\n{ \"unplaced\":\"\"  },\n{ \"unplacedCode\":\"\"  },\n{ \"scratched\":\"FALSE\"  },\n{ \"prizeMoney\":\"630\"  },\n{ \"resultTime\":\"31.73\"  },\n{ \"resultMargin\":\"0.0035\"  },\n{ \"resultMarginLengths\":\"0.25L\"  },\n{ \"startPaceCode\":\"\"  },\n{ \"jumpCode\":\"\"  },\n{ \"runLineCode\":\"\"  },\n{ \"firstSecond\":\"BOPA ALLEN\"  },\n{ \"colourCode\":\"BK\"  },\n{ \"sex\":\"Bitch\"  },\n{ \"comment\":\"\"  },\n{ \"ownerId\":\"2069410086\"  },\n{ \"trainerId\":\"-146035\"  },\n{ \"ownerName\":\"Bewley Hosking\"  },\n{ \"ownerState\":\"SA\"  },\n{ \"trainerName\":\"Kevin Bewley\"  },\n{ \"trainerSuburb\":\"Lewiston\"  },\n{ \"trainerState\":\"SA\"  },\n{ \"trainerDistrict\":\"\"  },\n{ \"trainerPostCode\":\"5501\"  },\n{ \"isQuad\":\"\"  },\n{ \"isBestBet\":\"\"  },\n{ \"damId\":\"-390299\"  },\n{ \"damName\":\"TA TA\"  },\n{ \"sireId\":\"-339746\"  },\n{ \"sireName\":\"MAGIC SPRITE\"  },\n{ \"dateWhelped\":\"2013-08-08T00:00:00.000Z\"  },\n{ \"totalFormCount\":\"0\"  },\n{ \"last5\":\"4-2-6-2-3\"  },\n{ \"isLateScratching\":\"FALSE\"  },\n{ \"bestTime\":\"NBT\"  },\n{ \"bestFinishTrackAndDistance\":\"31.5\"  },\n{ \"pir\":\"0\"  },\n{ \"careerPrizeMoney\":\"8935\"  },\n{ \"averageSpeed\":\"63.885\"  },\n{ \"firstSplitPosition\":\"1\"  },\n{ \"firstSplitTime\":\"3.21\"  },\n{ \"secondSplitPosition\":\"1\"  },\n{ \"secondSplitTime\":\"16.32\"  }\n]\n\n'''\ndef load_topaz_data(codes,datatype):\n    '''\n    This function here will loop over our previously written csv files and load them into one dataframe.\n    Due to the time taken to gather data from the Topaz API and the sheer size of the files, it makes sense to store the Topaz data locally as csv files locally rather than as a variable in the operating environment.\n    If the python kernel needs to be reset for whatever reason, all stored variables are lost - so storing the data in csv files means we can just load up the files directly rather than spending another 24 hours re-downloading all the data\n    '''\n    # initialise the dataframe\n    topaz_data_all = pd.DataFrame()\n\n    # loop over all csv files\n    for code in codes:\n        try:\n            state_data = pd.read_csv(code+'_DATA_'+datatype+'.csv',low_memory=False,parse_dates=['meetingDate','dateWhelped'])\n        except FileNotFoundError:\n            pass \n        # Add the state as a column\n        state_data['state']=code\n        # Concatenate the dataframes\n        topaz_data_all = pd.concat([topaz_data_all,state_data])\n\n    return topaz_data_all\n\n'''\nBecause this new function returns a dataframe as the output, we need to define the output of the function as a named variable. This name can be different to the name of the variable named after the 'return' operation.\nSimply calling the function without assigning the output as a variable will result in the function's output not being defined which will affect our downstream operations.\n'''\n\ntopaz_data_all = load_topaz_data(JURISDICTION_CODES,'HISTORICAL')\n\ndef discard_using_dates(dataset,start_date):\n\n    dataset['meetingDateNaive'] = pd.to_datetime(dataset['meetingDate'], format='%Y-%m-%dT%H:%M:%S.%fZ', utc=True).dt.tz_localize(None)\n    dataset = dataset[dataset['meetingDateNaive'] &gt;= start_date]\n    dataset = dataset.drop(columns=['meetingDateNaive'])\n\n    return dataset\n\ntopaz_data_all = discard_using_dates(topaz_data_all,start_date)\n\ndef discard_scratched_runners_data(topaz_data_all):\n    '''\n    This function will discard all scratched runners and abandoned races by dropping all rows where place = None\n    Note that this should only be done for past races, as upcoming races will also have place = None, because they have not been run yet, and so dropping these would remove all upcoming races\n    '''\n    # Discard scratched runners and abandoned races\n    topaz_data_all.dropna(subset=['place'], how='all', inplace=True)\n\n    return topaz_data_all\n\nTOPAZ_COLUMNS_TO_KEEP = ['state',    \n                        'track',\n                        'distance',\n                        'raceId',\n                        'meetingDate',\n                        'raceTypeCode',\n                        'runId',\n                        'dogId',\n                        'dogName',\n                        'weightInKg',\n                        'gradedTo',\n                        'rating',\n                        'raceNumber',\n                        'boxNumber',\n                        'rugNumber',\n                        'sex',\n                        'trainerId',\n                        'trainerState',\n                        'damId',\n                        'damName',\n                        'sireId',\n                        'sireName',\n                        'dateWhelped',\n                        'last5',\n                        'pir',\n                        'place',\n                        'prizeMoney',\n                        'resultTime',\n                        'resultMargin']\n\ndef discard_unnecessary_columns(topaz_data_all,columns):\n    '''\n    This function serves to keep only the subset of columns that we will use for our model.\n    Columns have been discarded because:\n     - All entries are identical or null\n     - Entries are duplicates of another column - simply being abbreviated in this column\n     - Entries contain data leakage which is data that has been added after the race has been run and would not be available for a live model\n     - Significant amounts of missing data\n\n    Duplicate rows have also been discarded. These rows may exist if the collect_topaz_data function has been run for the same date more than once.\n    Each race a dog runs will have a unique runId which is why this column is ideal for identifying duplicates\n    '''\n    # Keep the required columns\n    topaz_data = topaz_data_all[columns]\n\n    # Drop duplicate runIds\n    topaz_data = topaz_data.drop_duplicates(subset=['runId'])\n\n    # Reset the index\n    topaz_data.reset_index(drop=True, inplace=True)\n\n    return topaz_data\n\n'''\nFor all function where the defined variable name is the same as the parameter name included in the function call, this means we are doing an 'inplace' modification of the existing dataframe\nThe variable here is being mutated and will not be the same after the application of the function. This means that if the function is run twice, the second run may fail because the variable has changed\n'''\ntopaz_data_all = discard_scratched_runners_data(topaz_data_all)\n\ntopaz_data = discard_unnecessary_columns(topaz_data_all,TOPAZ_COLUMNS_TO_KEEP)\n\n# Let's correct the track names to align with Betfair names (Not all tracks in Topaz are on the exchange due to being closed or not hosting TAB-meetings)\n# We're not using the NZ tracks in this tutorial, however they are included below for completeness\n# The \"Straight\" tracks at Murray Bridge and Richmond are not differentiated on Betfair but we can treat these later.\n\nTRACK_DICTIONARY= {\n                    'Auckland (NZ)':'Manukau',\n                    'Christchurch (NZ)':'Addington',\n                    'Dport @ HOB':'Hobart',\n                    'Dport @ LCN':'Launceston',\n                    'Meadows (MEP)':'The Meadows',\n                    'Otago (NZ)':'Forbury Park',\n                    'Palmerston Nth (NZ)':'Manawatu',\n                    'Sandown (SAP)':'Sandown Park',\n                    'Southland (NZ)':'Ascot Park',\n                    'Tokoroa (NZ)':'Tokoroa',\n                    'Waikato (NZ)':'Cambridge',\n                    'Wanganui (NZ)':'Hatrick',\n                    'Taranaki (NZ)':'Taranaki',\n                    'Ashburton (NZ)':'Ashburton',\n                    'Richmond (RIS)':'Richmond',\n                    'Murray Bridge (MBR)':'Murray Bridge',\n                    'Murray Bridge (MBS)':'Murray Bridge'\n                    }\n\ndef clean_track_data(topaz_data, track_dict):\n    '''\n    This function replaces the track names in Topaz with the track names as they are displayed on the Betfair Exchange.\n    This is important for performing our historical backtesting later on\n    '''\n    topaz_data['track'] = topaz_data['track'].replace(track_dict)\n\n    return topaz_data\n\ntopaz_data = clean_track_data(topaz_data,TRACK_DICTIONARY)\n\ndef correct_result_margin(topaz_data):\n    '''\n    resultMargin has the same value for 1st and 2nd placed dogs, but should be 0 for the 1st placed dog.\n    This function simply replaces the entered value for resultMargin with 0 if the place value = 1\n    '''\n    # Replace margin to make sense for our model\n    topaz_data.loc[topaz_data['place'] == 1, ['resultMargin']] = 0\n\n    return topaz_data\n\n# Here we will mutate the dataframe to make the correction in resultMargin\ntopaz_data = correct_result_margin(topaz_data)\n</code></pre> <p>Here we've cleaned our dataset as much as we can. Next it's on to the feature creation!</p>"},{"location":"modelling/topazTutorial/#create-the-features","title":"Create the features","text":"<p>Creating Basic Features<pre><code>def generate_dogAge(topaz_data):\n    '''\n    This function creates a dogAge variable by subtracting the race date from the dateWhelped (i.e. date of birth).\n    '''\n    # Subtract dateWhelped from meetingDate\n    topaz_data['dogAge'] = (topaz_data['meetingDate'] - topaz_data['dateWhelped']).dt.days\n\n    return topaz_data\n\n# Here we will mutate the dataframe to calculate the dog's age\ntopaz_data = generate_dogAge(topaz_data)\n\nLAST_FIVE_RACE_COLUMNS = ['posL1R', 'posL2R', 'posL3R','posL4R', 'posL5R']\n\ndef extract_form(topaz_data):\n\n    ''' \n    This function splits the last5 variable, which is a column of 5 characters or less that contain the dogs finishing position in the last 5 races. \n    First we define it as a string in order to extract all the characters, as some of them may be letters (e.g. F = Fell, D = Disqualified)\n\n    We then create five separate columns for each race, and fill any blank cells with 10. Cells may be blank because the dog could have run fewer than 5 previous races.\n    We have chosen 10 as the padding value because a greyhound can never finish in 10th place owing to the maximum field size of 8 in greyhound racing.\n    '''\n    # Ensure variable is a string\n    topaz_data['last5'] = topaz_data['last5'].astype(str)\n    # Function to extract numbers from the 'last5' column\n    def extract_numbers(row):\n        try:\n            numbers = list(map(int, row.split('-')))\n            # If there are fewer than 5 numbers, pad with tens\n            numbers += [10] * (5 - len(numbers))\n            return numbers\n        except ValueError:\n            # Handle the case where the string cannot be split into integers\n            return [10, 10, 10, 10, 10]\n\n    # Apply the function to create new columns for each position\n    topaz_data[LAST_FIVE_RACE_COLUMNS] = topaz_data['last5'].apply(extract_numbers).apply(pd.Series)\n\n    return topaz_data\n\ntopaz_data = extract_form(topaz_data)\n\ndef generate_winPercentage(topaz_data):\n    '''\n    This function generates a percentage of the dog's previous 5 races in which it has either won or placed in the top 3. \n    For generating the win percentage we take the number of '1's from the the position columns we generated previously and divides by the number of columns that are not equal to 10 which we previously defined as races that did not exist\n    For generating the top 3 place percentage by dividing the number of places which are less than or equal to 3.\n    '''\n    # Generate win percentage from the last 5 races\n    topaz_data['lastFiveWinPercentage'] = ((topaz_data[LAST_FIVE_RACE_COLUMNS] == 1).sum(axis=1)) / ((topaz_data[LAST_FIVE_RACE_COLUMNS] != 10).sum(axis=1))\n\n    # Fill any empty values with 0\n    topaz_data['lastFiveWinPercentage'].fillna(0,inplace=True)\n\n    # Generate top 3 place percentage from the last 5 races\n    topaz_data['lastFivePlacePercentage'] = ((topaz_data[LAST_FIVE_RACE_COLUMNS] &lt;= 3).sum(axis=1)) / ((topaz_data[LAST_FIVE_RACE_COLUMNS] != 10).sum(axis=1))\n\n    # Fill any empty values with 0\n    topaz_data['lastFivePlacePercentage'].fillna(0,inplace=True)\n\n    return topaz_data\n\ntopaz_data = generate_winPercentage(topaz_data)\n\ndef generate_finishingPlaceMovement(topaz_data):\n    '''\n    This function attempts to determine if the dog has a tendency to fall back in the field towards the end of the race or it finishes very strongly by finding the difference between its final place and its field position at the last split.\n    We first fill empty rows and convert to a string, before extracting the second last character from the string. \n    Some tracks have much more detailed position data but these make up a minority of races and so using the fourth or fifth last position in-running would omit most of the data and so is not overly useful for modelling purposes\n\n    If the pir variable only has 1 character, then it corresponds to the final placing and so the fillna operation used on the 2ndLastPIR variable simply fills its final place, assigning a finishingPlaceMovement of 0\n    '''\n\n    # fill missing values with the place value\n    topaz_data['pir'] = topaz_data['pir'].fillna(topaz_data['place'])\n\n    # fill any missing values with 8 and force into a string format\n    topaz_data['pir'] = topaz_data['pir'].fillna(8).astype(str)\n\n    # replace any non-integer values with '8'\n    topaz_data['pir'] = topaz_data['pir'].str.replace('[fFTDPS.]', '8', regex=True)\n\n    # Extract the second last letter and create a new column '2ndLastPIR'\n    topaz_data['2ndLastPIR'] = topaz_data['pir'].apply(lambda x: x[-2] if len(x) &gt;= 2 else None)\n\n    # Fill any empty values for 2ndLastPIR with the dog's place\n    topaz_data['2ndLastPIR'] = topaz_data['2ndLastPIR'].fillna(topaz_data['place'])\n\n    # fill any missing values with 8\n    topaz_data['2ndLastPIR'] = topaz_data['2ndLastPIR'].fillna(8)\n\n    # Convert the column to integer format\n    topaz_data['2ndLastPIR'] = topaz_data['2ndLastPIR'].astype(int)\n\n    # force pir back to integer format\n    topaz_data['pir'] = topaz_data['pir'].astype(int)\n\n    # Create a feature that calculates places gained/conceded in the home straight\n    topaz_data['finishingPlaceMovement'] = topaz_data['2ndLastPIR'] - topaz_data['place']\n\n    return topaz_data\n\ntopaz_data = generate_finishingPlaceMovement(topaz_data)\n\ndef scale_values(topaz_data):\n\n    '''\n    This function scales the 'dogAge' and 'weightInKg' columns based on the 'raceId' groups using Min-Max scaling. \n    Additionally, it applies logarithmic transformations to 'prizeMoney','place', and 'resultMargin' columns. \n    These transformations are performed to ensure that the data follows a more Gaussian distribution and meets the assumptions of certain statistical analyses. \n    '''\n    # Scale existing values using MinMaxScaler\n    topaz_data['dogAgeScaled'] = topaz_data.groupby('raceId')['dogAge'].transform(lambda x: MinMaxScaler().fit_transform(x.values.reshape(-1, 1)).flatten())\n    topaz_data['weightInKgScaled'] = topaz_data.groupby('raceId')['weightInKg'].transform(lambda x: MinMaxScaler().fit_transform(x.values.reshape(-1, 1)).flatten())\n\n    # Transform existing values using log functions\n    topaz_data['prizemoneyLog'] = np.log10(topaz_data['prizeMoney'] + 1)\n    topaz_data['placeLog'] = np.log10(topaz_data['place'] + 1)\n    topaz_data['marginLog'] = np.log10(topaz_data['resultMargin'] + 1)\n\n    return topaz_data\n\ntopaz_data = scale_values(topaz_data)\n\ndef generate_runTimeNorm(topaz_data):\n    '''\n    This function finds the rolling 365 day median win time for each track/distance combination by creating a dataframe that contains only winning runs\n    The median time is then compared to the dog's actual result time and then a normalised value is attained.\n    The purpose of the clip(0.9,1.1) is to ensure that very high or very low values (which may be the result of dirty data) do not create erroneous measurements and are therefore clipped to be between 0.9 and 1.1\n\n    The speed index is used to account for differences in the speed of each track, and will assign different values based on whether a track is fast or slow using the MinMaxScaler\n    '''\n\n    # Calculate median winner time per track/distance\n    win_results = topaz_data[topaz_data['place'] == 1]\n\n    # Find the median for each unique combination of track, distance and meetingDate\n    grouped_data = win_results.groupby(['track', 'distance', 'meetingDate'])['resultTime'].median().reset_index()\n\n    # Find the rolling 365 median of the median values\n    median_win_time = pd.DataFrame(grouped_data.groupby(['track', 'distance']).apply(lambda x: x.sort_values('meetingDate').set_index('meetingDate')['resultTime'].shift(1).rolling('365D', min_periods=1).median())).reset_index()\n\n    # Rename the column\n    median_win_time.rename(columns={\"resultTime\": \"runTimeMedian\"},inplace=True)\n\n    # Calculate the speedIndex for the track\n    median_win_time['speedIndex'] = (median_win_time['runTimeMedian'] / median_win_time['distance'])\n\n    # Apply the MinMaxScaler\n    median_win_time['speedIndex'] = MinMaxScaler().fit_transform(median_win_time[['speedIndex']])\n\n    # Merge with median winner time\n    topaz_data = topaz_data.merge(median_win_time, how='left', on=['track', 'distance','meetingDate'])\n\n    # Normalise time comparison\n    topaz_data['runTimeNorm'] = (topaz_data['runTimeMedian'] / topaz_data['resultTime']).clip(0.9, 1.1)\n\n    # Fill the empty values with 1\n    topaz_data['runTimeNorm'].fillna(1, inplace=True)\n\n    # Apply the MinMaxScaler\n    topaz_data['runTimeNorm'] = topaz_data.groupby('raceId')['runTimeNorm'].transform(lambda x: MinMaxScaler().fit_transform(x.values.reshape(-1, 1)).flatten())\n\n    return topaz_data\n\ntopaz_data = generate_runTimeNorm(topaz_data)\n\ndef generate_adjacentVacantBox_state(topaz_data):\n    '''\n    This function determines whether the dog has at least one vacant box directly next to them at the beginning of the race. \n    In greyhound racing, especially for very short races, a dog with more room to move at the beginning of the race is less likely to be bumped (or 'checked') if it has a vacant box next to it. \n    This is also true for dogs in box 1 as there is a lot of room adjacent to box 1. \n\n    We find this by ordering by raceId and boxNumber and determining if the box number above or below is filled. \n    Box 8 is counted as always having the box above filled as Box 8 is hard-up on the fence and so the dog does not have room to move here.\n    We then determine if the dog has adjacent space by generating the variable 'hasAtLeast1VacantBox'\n    '''\n\n    # Sort the DataFrame by 'RaceId' and 'Box'\n    topaz_data = topaz_data.sort_values(by=['raceId', 'boxNumber'])\n\n    # Check if there is an entry equal to boxNumber plus or minues one\n    topaz_data['hasEntryBoxNumberPlus1'] = (topaz_data.groupby('raceId')['boxNumber'].shift(1) == topaz_data['boxNumber'] + 1) | (topaz_data['boxNumber'] == 8)\n    topaz_data['hasEntryBoxNumberMinus1'] = (topaz_data.groupby('raceId')['boxNumber'].shift(-1) == topaz_data['boxNumber'] - 1)\n\n    # Convert boolean values to 1\n    topaz_data['hasEntryBoxNumberPlus1'] = topaz_data['hasEntryBoxNumberPlus1'].astype(int)\n    topaz_data['hasEntryBoxNumberMinus1'] = topaz_data['hasEntryBoxNumberMinus1'].astype(int)\n\n    # Calculate 'hasAtLeast1VacantBox'\n    topaz_data['hasAtLeast1VacantBox'] = (topaz_data['hasEntryBoxNumberPlus1'] + topaz_data['hasEntryBoxNumberMinus1']&gt; 0).astype(int)\n\n    return topaz_data\n\ntopaz_data = generate_adjacentVacantBox_state(topaz_data)\n\ndef generate_boxWinPercentage(topaz_data):\n    ''' \n    This function creates a rolling win percentage for each combination of 'track', 'distance', 'boxNumber', 'hasAtLeast1VacantBox', 'meetingDate'\n    The purpose of this is to help the model determine if the box placement and presence of vacant adjacent boxes can give the dog an improved chance of winning the race\n    The shift(1) is used to ensure that the results of that meetingDate are excluded from the calculation to prevent data leakage\n    '''\n    # Create a win column\n    topaz_data['win'] = topaz_data['place'].apply(lambda x: 1 if x == 1 else 0)\n\n    # Find the mean (win percentage) for each 'track', 'distance', 'boxNumber', 'hasAtLeast1VacantBox', 'meetingDate' combination\n    grouped_data = topaz_data.groupby(['track', 'distance', 'boxNumber', 'hasAtLeast1VacantBox', 'meetingDate'])['win'].mean().reset_index()\n\n    # set the index to be the meeting date for the rolling window calculation\n    grouped_data.set_index('meetingDate', inplace=True)\n\n    # Apply rolling mean calculation to the aggregated data\n    box_win_percent = grouped_data.groupby(['track', 'distance', 'boxNumber', 'hasAtLeast1VacantBox']).apply(lambda x: x.sort_values('meetingDate')['win'].shift(1).rolling('365D', min_periods=1).mean()).reset_index()\n\n    # Reset index and rename columns\n    box_win_percent.columns = ['track', 'distance', 'boxNumber', 'hasAtLeast1VacantBox', 'meetingDate', 'rolling_box_win_percentage']\n\n    # Add to dog results dataframe\n    topaz_data = topaz_data.merge(box_win_percent, on=['track', 'distance', 'meetingDate','boxNumber','hasAtLeast1VacantBox'], how='left')\n\n    return topaz_data\n\ntopaz_data = generate_boxWinPercentage(topaz_data)\n</code></pre> Now we've created some basic features, it's time to create our big group of features where we iterate over different subsets, variables and time points to generate a large number of different features</p> Creating Bulk Features<pre><code># Calculate values for dog, trainer, dam and sire\nSUBSETS = ['dog', 'trainer', 'dam', 'sire']\n\n# Use rolling window of 28, 91 and 365 days\nROLLING_WINDOWS = ['28D','91D', '365D']\n\n# Features to use for rolling windows calculation\nFEATURES = ['runTimeNorm', 'placeLog', 'prizemoneyLog', 'marginLog','finishingPlaceMovement']\n\n# Aggregation functions to apply\nAGGREGATES = ['min', 'max', 'mean', 'median', 'std']\n\ndef generate_rollingFeatures(topaz_data):\n\n    '''\n    This function generates rolling window features for subsets including dog, trainer, dam, and sire.\n    The rolling windows used are defined as 28 days, 91 days, and 365 days. \n    Features such as 'runTimeNorm','placeLog', 'prizemoneyLog', 'marginLog', and 'finishingPlaceMovement' are considered for rolling window calculation.\n    The aggregation functions applied include 'min', 'max', 'mean', 'median', and 'std'.\n\n    The function iterates through each subset, calculates rolling window features for each specified rolling window, and adds them to the dataset. \n    The generated feature names indicate the subset, feature, aggregation function, and rolling window used for calculation.\n\n    Finally, the function returns the dataset with generated features added and a list of column names representing the generated rolling window features.\n    The function enables the creation of a bulk set of features to be created using statistical aggregates, which will be entrusted to the algorithm to sort through which ones have significance\n\n    We create a copy of the topaz_data dataframe rather than modifying it directly in case of issues with this function. It means that this cell can simply be rerun instead of having to reload and reprocess the entire database\n    '''\n    # Create a copy of the dataframe and ensure meetingDate is in datetime format\n    dataset = topaz_data.copy()\n    dataset['meetingDate'] = pd.to_datetime(dataset['meetingDate'])\n\n    # Keep track of generated feature names\n    feature_cols = []\n\n    for i in SUBSETS:\n\n        # Add the Id string\n        idnumber = i + 'Id'\n\n        # Create a subset dataframe\n        subset_dataframe = dataset[['meetingDate',idnumber] + FEATURES]\n\n        # intitialise the dataframe\n        average_df = pd.DataFrame()\n\n        for feature in FEATURES:\n\n            # Find the mean value for the meetingDate\n            feature_average_df = subset_dataframe.groupby([idnumber, 'meetingDate'])[feature].mean().reset_index()\n\n            # Rename the feature column to indicate it's the average of that feature\n            feature_average_df.rename(columns={feature: f'{feature}{i}DayAverage'}, inplace=True)\n\n            # If average_df is empty, assign the feature_average_df to it\n            if average_df.empty:\n                average_df = feature_average_df\n            else:\n                # Otherwise, merge feature_average_df with average_df\n                average_df = pd.merge(average_df, feature_average_df, on=[idnumber, 'meetingDate'])\n\n        # Create a list from the column names\n        column_names = average_df.columns.tolist()\n\n        # Columns to exclude\n        columns_to_exclude = [idnumber,'meetingDate']\n\n        # Exclude specified columns from the list\n        column_names_filtered = [col for col in column_names if col not in columns_to_exclude]\n\n        # Drop any duplicate rows (for when any dam/sire has more than one progeny race in one day or an owner/trainer has more than one dog race in one day)\n        average_df.drop_duplicates(inplace=True)\n\n        # convert meetingDate to datetime format\n        average_df['meetingDate'] = pd.to_datetime(average_df['meetingDate'])\n\n        # set the index before applying the rolling windows\n        average_df = average_df.set_index([idnumber, 'meetingDate']).sort_index() \n\n        for rolling_window in ROLLING_WINDOWS:\n            print(f'Processing {i} rolling window {rolling_window} days')\n\n            rolling_result = (\n                average_df\n                .reset_index(level=0)\n                .groupby(idnumber)[column_names_filtered]\n                .rolling(rolling_window)\n                .agg(AGGREGATES)\n                .groupby(level=0)\n                .shift(1)\n            )\n\n            # Generate list of rolling window feature names (eg: RunTime_norm_min_365D)\n            agg_features_cols = [f'{i}_{f}_{a}_{rolling_window}' for f, a in itertools.product(FEATURES, AGGREGATES)]\n\n            # Add features to dataset\n            average_df[agg_features_cols] = rolling_result\n\n            # Keep track of generated feature names\n            feature_cols.extend(agg_features_cols)\n\n            # fill empty values with 0\n            average_df.fillna(0, inplace=True)\n\n        average_df.reset_index(inplace=True)\n        dataset = pd.merge(dataset,average_df,on=[idnumber, 'meetingDate'])\n\n    return dataset, feature_cols\n\ndataset, feature_cols = generate_rollingFeatures(topaz_data)\n\nfrom dateutil.relativedelta import relativedelta\n\ndef generate_feature_list(feature_cols):\n    '''\n    This function converts the numpy series to a list for later use\n    '''\n    feature_cols = np.unique(feature_cols).tolist()\n\n    return feature_cols\n\nfeature_cols = generate_feature_list(feature_cols)\n\nDATASET_COLUMNS_TO_KEEP = [\n                    'meetingDate',\n                    'state',\n                    'track',\n                    'distance',\n                    'raceId',\n                    'raceTypeCode',\n                    'raceNumber',\n                    'boxNumber',\n                    'rugNumber',\n                    'runId',\n                    'dogId',\n                    'dogName',\n                    'weightInKg',\n                    'sex',\n                    'trainerId',\n                    'trainerState',\n                    'damId',\n                    'damName',\n                    'sireId',\n                    'sireName',\n                    'win',\n                    'speedIndex',\n                    'dogAgeScaled',\n                    'lastFiveWinPercentage',\n                    'lastFivePlacePercentage',\n                    'weightInKgScaled',\n                    'rolling_box_win_percentage',\n                    'hasAtLeast1VacantBox']\n\ndef generate_modelling_dataset(dataset, feature_cols):\n    '''\n    This function extends the list of feature columns from the generate_rollingFeatures function by adding the features we previously created, and then keeps only the columns specified to prepare the dataset for training by the algorithm\n    '''\n\n    # Trim the dataset\n    dataset = dataset[DATASET_COLUMNS_TO_KEEP + feature_cols]\n\n    # Extend the feature_cols list\n    feature_cols.extend(['speedIndex',\n                'dogAgeScaled',\n                'lastFiveWinPercentage',\n                'lastFivePlacePercentage',\n                'weightInKgScaled',\n                'rolling_box_win_percentage',\n                'hasAtLeast1VacantBox'])\n\n    # Fill any missing values with 0 as a final step before training\n    dataset = dataset.fillna(0,inplace=True)\n\n    # Drop any duplicate runIds\n    dataset.drop_duplicates(subset=['runId'],inplace=True)\n\n    return dataset, feature_cols\n\ndiscard_before_date = start_date + relativedelta(years=1)\n\ndataset = discard_using_dates(dataset,discard_before_date)\n\ndataset, feature_cols = generate_modelling_dataset(dataset, feature_cols)\n</code></pre> <p>Now that we've created our features, lets feed them into our Machine Learning Algorithm to generate some probabilities!</p>"},{"location":"modelling/topazTutorial/#training","title":"Training","text":"Training the model<pre><code>from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\n\n# Gradient Boosting Machines libraries\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n# Common models parameters\nverbose       = 0\nlearning_rate = 0.1\nn_estimators  = 500\n\ndef define_models():\n    '''\n    This function defines a number of machine learning algorithms with some suggested hyperparameters.\n    The purpose of this is to demonstrate the differences in accuracy as well as training times for each different algorithm specifically in relation to this dataset\n    '''\n    # Train different types of models\n    models = {\n        'GradientBoostingClassifier': GradientBoostingClassifier(verbose=verbose, learning_rate=learning_rate, n_estimators=n_estimators, max_depth=8, max_features=8),\n        'RandomForestClassifier':     RandomForestClassifier(verbose=verbose, n_estimators=n_estimators, max_depth=20, max_features=8, min_samples_leaf=5,min_samples_split=10),\n        'LGBMClassifier':             LGBMClassifier(learning_rate=learning_rate, n_estimators=n_estimators, force_col_wise=True,num_leaves=80,verbose=-1),\n        'XGBClassifier':              XGBClassifier(verbosity=verbose, learning_rate=learning_rate, n_estimators=n_estimators, max_depth = 8, subsample = 1.0, objective='binary:logistic'),\n        'CatBoostClassifier':         CatBoostClassifier(verbose=verbose, learning_rate=learning_rate, n_estimators=n_estimators, depth=8, l2_leaf_reg=10),\n        'KNeighborsClassifier':       KNeighborsClassifier(n_neighbors=15,weights='distance',p=2),\n        'DecisionTreeClassifier':     DecisionTreeClassifier(max_depth=20,min_samples_split=10,min_samples_leaf=5,max_features=8),\n        'MLPClassifier':              MLPClassifier(hidden_layer_sizes=100,activation='relu',solver='adam', alpha=0.0001),\n        'AdaBoostClassifier':         AdaBoostClassifier(n_estimators=n_estimators,learning_rate=learning_rate,algorithm='SAMME'),\n        'GuassianNB':                 GaussianNB(),\n        'QuadDiscriminantAnalysis':   QuadraticDiscriminantAnalysis(),\n        'LogisticsRegression':        LogisticRegression(verbose=0, solver='liblinear',max_iter=5000000, C=10,penalty='l2')\n    }\n\n    return models\n\nmodels = define_models()\n\ndef remove_races(dataset):\n    '''\n    This function discards Qualifying Trials and any races which may only have 1 runner like Match Races which are outside the scope of this model\n    '''\n    #remove trial races\n    final_dataset = dataset[dataset['raceTypeCode'] != 'QT']\n\n    # Remove rows where raceId appears only once\n    counts = final_dataset['raceId'].value_counts()\n    repeated_raceIds = counts[counts &gt; 1].index\n    final_dataset = final_dataset[final_dataset['raceId'].isin(repeated_raceIds)]\n\n    return final_dataset\n\nfinal_dataset = remove_races(dataset)\n\ndef downcast_data(final_dataset):\n    '''\n    This function downcasts int64 and float64 to int32 and float32 respectively in order to reduce the memory usage of the model during the training process\n    For systems with high RAM capacity this may not be required. Feel free to comment out this function, however it may result in memory allocation failures\n    '''\n    # Loop through each column and convert int64 to int32 and float64 to float32\n    for col in final_dataset.columns:\n        if final_dataset[col].dtype == 'int64':\n            final_dataset.loc[:, col] = final_dataset[col].astype('int32')\n        elif final_dataset[col].dtype == 'float64':\n            final_dataset.loc[:, col] = final_dataset[col].astype('float32')\n\n    return final_dataset\n\nfinal_dataset = downcast_data(final_dataset)\n\ndef train_test_split(final_dataset,end_date):\n    '''\n    This function splits the dataset into a training set and a test set for the purposes of model training.\n    This is to enable testing of the trained model on an unseen test set to establish statistical metrics regarding its accuracy\n    There are other ways to split a dataset into test and training sets, however this method is useful for the purposes of backtesting profitability\n    '''\n    final_dataset['meetingDateNaive'] = pd.to_datetime(final_dataset['meetingDate'], format='%Y-%m-%dT%H:%M:%S.%fZ', utc=True).dt.tz_localize(None)\n    # Split the data into train and test data\n    train_data = final_dataset[final_dataset['meetingDateNaive'] &lt; end_date - relativedelta(years=1)].reset_index(drop=True)\n    test_data = final_dataset[final_dataset['meetingDateNaive'] &gt;= end_date - relativedelta(years=1)].reset_index(drop=True)\n\n    train_data.drop(columns=['meetingDateNaive'], inplace=True)\n    test_data.drop(columns=['meetingDateNaive'], inplace=True)\n\n    return test_data, train_data\n\ntest_data, train_data = train_test_split(final_dataset,end_date)\n\ndef generate_xy(test_data, train_data, feature_cols):\n    '''\n    This function separates the target column 'win' from the actual features of the dataset and also seperates the training features from the race metadata which is not being used for the training (e.g. raceId)\n    '''\n    train_x, train_y = train_data[feature_cols], train_data['win']\n    test_x, test_y = test_data[feature_cols], test_data['win']\n\n    return train_x, train_y, test_x, test_y\n\ntrain_x, train_y, test_x, test_y = generate_xy(test_data, train_data, feature_cols)\n\nimport pickle\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import brier_score_loss\nimport time\n\ndef train_models(models,train_x, train_y, test_x, test_y, test_data):\n    '''\n    This function loads each individual machine learning algorithm and trains each model on the training dataset.\n    It begins by initialising an empty list and passing over each model by training it and then normalising the probabilities to 1 for each race field.\n    This normalisation could not be performed if a different method was used to generate the train/test split that resulted in incomplete race fields being included\n\n    Once normalised, then the log loss and brier score for each model is calculated and printed. These scores are not stored, merely displayed as indicators (along with the training time for each algorithm)\n    Finally the trained model (also called a pickle) is 'dumped' (i.e. saved) as a _pickle.dat file\n\n    '''\n    probs_columns = []\n\n    for key, model in models.items():\n\n        prob_col_key = f'prob_{key}'\n        print(f'Fitting model {key}')\n\n        # time and train the model\n        start_time = time.time()  # Record start time\n        model.fit(train_x, train_y)\n        end_time = time.time()  # Record end time\n        print(f\"Training time for {key}: {end_time - start_time} seconds\")\n\n        # Calculate runner win probability\n        dog_win_probs = model.predict_proba(test_x)[:, 1]\n        test_data[prob_col_key] = dog_win_probs\n\n        # Normalise probabilities\n        test_data[prob_col_key] = test_data.groupby('raceId', group_keys=False)[f'prob_{key}'].apply(lambda x: x / sum(x))\n\n        # Assign the dog with the highest probability a value of 1 as the predicted race winner\n        test_data['predicted_winner'] = test_data.groupby('raceId', group_keys=False)[f'prob_{key}'].apply(lambda x: x == max(x))\n\n        # Calculate log loss\n        test_loss = log_loss(test_y, test_data[prob_col_key])\n        print(f\"Log loss for {key} on test data: {test_loss}\")\n\n        # Calculate Brier Score\n        brier_score = brier_score_loss(test_y, test_data[prob_col_key])\n        print(f\"Brier score for {key} on test data: {brier_score}\")\n\n        # Add the probability column key to the list probs_columns\n        probs_columns.append(prob_col_key)\n\n        # Dump the pickle file\n        with open(key + '_.pickle', 'wb') as f:\n            pickle.dump(model, f)\n\n    return test_data, probs_columns\n\nall_models_test_data, probs_columns = train_models(models,train_x, train_y, test_x, test_y, test_data)\n\nBACKTESTING_COLUMNS = ['meetingDate','state','track','race_number','boxNumber','rugNumber']\n\nBACKTESTING_COLUMNS = ['meetingDate','state','track','race_number','boxNumber','rugNumber']\n\ndef trim_and_export_test_data(all_models_test_data):\n    '''\n    This functions trims all the model features from the test dataset and outputs only the required the race information in addition to the win probability for each machine learning model to a csv file for the purposes of backtesting\n    '''\n    trimmed_test_data=all_models_test_data[BACKTESTING_COLUMNS + probs_columns]\n\n    trimmed_test_data.to_csv('multiple_algorithms_predictions.csv',index=False)\n\ntrim_and_export_test_data(all_models_test_data)\n</code></pre> <p>Alright, now that we've trained the models, let's have a look at the log loss calculations for each model and the time taken to train each model</p> Model Training Time Log Loss Brier Score AdaBoostClassifier 14287 0.7561 0.1367 RandomForestClassifier 4575 0.7495 0.1365 GradientBoostingClassifier 2799 0.7488 0.1364 MLPClassifier 2468 0.7464 0.1364 Logistics Regression 448 0.7466 0.1364 CatBoostClassifier 304 0.7453 0.1364 XGBClassifier 194 0.7494 0.1364 LGBMClassifier 144 0.7476 0.1364 QuadDiscriminantAnalysis 50 1.2495 0.1364 DecisionTreeClassifier 16 1.6094 0.1366 GuassianNB 8 1.5375 0.1364 KNeighborsClassifier 3 1.1889 0.1366 <p>As we can see some models took a lot longer than others to train, with the longest being the AdaBoostClassifier taking close to 4 hours and KNeighbours Classifier at 3 seconds.  We've only used one set of parameters for training each model. You could try using a GridSearch to find the best parameters for each model.</p> <p>Here's a great explanation of the differences between log loss and Brier score for machine learning models - more info</p> Plotting feature importance<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_feature_importance(models):\n    \"\"\"\n    This function iterates through each model in the provided dictionary and plots the feature importance if available.\n    If the model has a 'feature_importances_' attribute, it plots the feature importance either using 'plot_importance' method (if available) or a horizontal bar chart. Each plot is saved as a JPG file with a filename indicating the model name.\n    If a model does not have feature importance information, a message is printed indicating that.\n    \"\"\"\n    for key, model in models.items():\n\n        if hasattr(model, 'feature_importances_'):\n            plt.figure(figsize=(10, 70))\n            if hasattr(model, 'plot_importance'):\n                model.plot_importance(ax=plt.gca(), importance_type='gain', title=f'Feature Importance - {key}')\n            else:\n                feature_importance = model.feature_importances_\n                sorted_idx = np.argsort(feature_importance)\n                plt.barh(train_x.columns[sorted_idx], feature_importance[sorted_idx])\n                plt.xlabel(\"Feature Importance\")\n                plt.ylabel(\"Features\")\n                plt.title(f\"Feature Importance - {key}\")\n\n            # Save plot to JPG file\n            plt.savefig(f'feature_importance_{key}.jpg')\n\n            plt.close()  # Close the plot to release memory\n        else:\n            print(f\"No feature importance available for {key}\")\n\nplot_feature_importance(models)\n</code></pre> <p></p>"},{"location":"modelling/topazTutorial/#grid-search","title":"Grid Search","text":"<p>For our first round of training our model, we have used some suggested parameters for each diifferent type of machine learning model. However, perhaps these parameters aren't suited to our use case. In this case we should use a technique called 'Grid Search' to find the best hyperparameters for one of the machine learning models (in this case we'll use LGBM). A grid search methodically tests different hyperparameter combinations, checking each one's performance through cross-validation. This helps pinpoint the best setup that maximises the model's performance metric, like accuracy or area under the curve (AUC).</p> <p>With LightGBM, which relies heavily on hyperparameters like learning rate, maximum tree depth, and regularization parameters, a grid search is essential. By trying out various combinations of these hyperparameters, we can fine-tune the model for superior predictive performance. LightGBM's speed advantage makes grid search even more appealing. Since it's built for efficiency, the computational burden of grid searching is reduced compared to other algorithms. This means we can explore a wide range of hyperparameters without waiting forever. As above, the AdaBoost Classifier almost 4 hours to train just once, so for this model would not be appropriate for a Grid Search. LGBM took only 150 seconds, so a grid search of 3 folds for 27 different hyperparameter combinations (81 fits) would take around 200 minutes - much more acceptable.</p> <p>Grid Search for LGBM<pre><code>from sklearn.model_selection import GridSearchCV\n\n# Define parameter grid for LGBMClassifier. (More parameters and parameters can be added if desired)\nparam_grid = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'n_estimators': [100, 200, 500],\n    'num_leaves': [50, 80, 120]\n}\n\ndef LGBM_GridSearch(train_x, train_y):\n\n    '''\n    This function demonstrates how to perform a gridSearch of different combinations of hyperparameter functions to find an optimal combination for a particular machine learning model\n    We have performed the wider model training first in order to find which model has a short enough training time to be a candidate for gridSearch. \n    Models with training times of 2 hours per fit won't be suitable for gridSearch unless the user has a lot of patience.\n\n    It will perform the grid search and return the parameters associated with the fit with the most favourable neg_log_loss score\n    '''\n\n    # Initialize LGBMClassifier\n    lgbm = LGBMClassifier(force_col_wise=True, verbose=-1)\n\n    # Initialize GridSearchCV\n    grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, scoring='neg_log_loss', cv=3, verbose=3)\n\n    # Fit GridSearchCV\n    grid_search.fit(train_x, train_y)\n\n    # Print results\n    print(\"GridSearchCV Results:\")\n    print(\"Best parameters found:\", grid_search.best_params_)\n    print(\"Best negative log loss found:\", grid_search.best_score_)\n\n    lgbm_best_params = grid_search.best_params_\n\n    return lgbm_best_params\n\nlgbm_best_params = LGBM_GridSearch(train_x, train_y)\n\ndef apply_best_GridSearch_params(lgbm_best_params,train_x, train_y):\n    '''\n    This function takes the previously defined best parameters, trains the model using those parameters and then outputs the pickle file.\n    '''\n    # Define the model with best parameters\n    best_model = LGBMClassifier(force_col_wise=True, verbose=3, n_estimators=lgbm_best_params['n_estimators'],learning_rate=lgbm_best_params['learning_rate'],num_leaves=lgbm_best_params['num_leaves'])\n\n    # Train the model on best parameters\n    best_model.fit(train_x, train_y)\n\n    # Dump the pickle for the best model\n    with open('best_lgbm_model.pickle', 'wb') as f:\n        pickle.dump(best_model, f)\n\n    return best_model\n\nbest_model = apply_best_GridSearch_params(lgbm_best_params,train_x, train_y)\n\ndef best_model_predictions(best_model,test_data):\n    '''\n    This function generates a column of probabilities for each runner, before normalising the probabilities across each race.\n    Normalising the probabilities is essential for use in a betting model as the sum of the probabilities should be equal to 1 given that each race has 1 winner.\n\n    Unnecessary columns are dropped and then the dataframe is exported to a csv file\n    '''\n\n    # Predict probabilities using the best model\n    test_data['win_probability'] = best_model.predict_proba(test_x)[:, 1]\n\n    # Normalise probabilities across each race\n    test_data['win_probability'] = test_data.groupby('raceId', group_keys=False)['win_probability'].apply(lambda x: x / sum(x))\n\n    # Keep only required columns\n    test_data = test_data[BACKTESTING_COLUMNS + ['win_probability']]\n\n    # Export DataFrame to CSV\n    test_data.to_csv('lgbm_gridSearch_predictions.csv', index=False)\n\n    return test_data\n\ntest_data = best_model_predictions(best_model,test_data)\n</code></pre> <pre><code>Fitting 3 folds for each of 27 candidates, totalling 81 fits\n[CV 1/3] END learning_rate=0.01, n_estimators=100, num_leaves=50;, score=-0.387 total time=  36.8s\n[CV 2/3] END learning_rate=0.01, n_estimators=100, num_leaves=50;, score=-0.387 total time=  39.6s\n[CV 3/3] END learning_rate=0.01, n_estimators=100, num_leaves=50;, score=-0.386 total time=  47.6s\n[CV 1/3] END learning_rate=0.01, n_estimators=100, num_leaves=80;, score=-0.387 total time= 1.0min\n[CV 2/3] END learning_rate=0.01, n_estimators=100, num_leaves=80;, score=-0.386 total time=  53.8s\n[CV 3/3] END learning_rate=0.01, n_estimators=100, num_leaves=80;, score=-0.385 total time= 1.0min\n\n#\n# Output trimmed\n#\n\nGridSearchCV Results:\nBest parameters found: {'learning_rate': 0.01, 'n_estimators': 500, 'num_leaves': 120}\nBest negative log loss found: -0.3791625785797655\n</code></pre></p>"},{"location":"modelling/topazTutorial/#analysing-profitability","title":"Analysing profitability","text":"<p>Let's explore now how we can take our model's probability predictions and compare them to actual market data. We'll join them onto the Betfair BSP datasets and then apply some segmentation to see if we can choose a segment for a staking strategy. For this profitability analysis, we have used proportional staking where the model's probability is multipled by 100 to calculate a stake (e.g. 0.07 x 100 = $7) and then bets placed at BSP. </p> <p>It's important to be cautious when segmenting the data, especially if more than one segment is applied, that the set of datapoints is not too small to expose the strategy to overfitting. E.g. Picking races in QLD between 320m and 330m on a Monday night would most likely result in overfitting and unreproducable results. This page here has a great tool to help assess if the strategy is overfitted.</p> Joining model predictions with Betfair data<pre><code>BETFAIR_RESULTS_COLUMNS = ['LOCAL_MEETING_DATE',\n                        'SCHEDULED_RACE_TIME',\n                        'ACTUAL_OFF_TIME',\n                        'TRACK',\n                        'STATE_CODE',\n                        'RACE_NO',\n                        'WIN_MARKET_ID',\n                        'DISTANCE',\n                        'RACE_TYPE',\n                        'SELECTION_ID',\n                        'TAB_NUMBER',\n                        'SELECTION_NAME',\n                        'WIN_RESULT',\n                        'WIN_BSP',\n                        'BEST_AVAIL_BACK_AT_SCHEDULED_OFF']\n\ndef retrieve_betfair_results(end_date):\n    '''\n    This function retrieves the Betfair Historical results csv files using a series of urls in a consistent format.\n    Each csv file is retrieved and then concatenated into a single dataframe before being trimming by dropping unnecessary columns and dropping rows with None in particular columns as well as duplicates\n\n    A runner may have None for BSP if it was a very late scratching and was not able to be removed before commencement of the race.\n    Additionally, some NZ results are known as not having had BSP offered due to lack of vision\n\n    A runner may have None for for 'BEST_AVAIL_BACK_AT_SCHEDULED_OFF' and a value for BSP if another runner was a very late scratching and all bets in the market at the time had to be voided and manual BSP reconciliation required.\n\n    These datasets are also known to have duplicate values, so as an additional cleaning step, duplicate values were dropped\n    '''\n\n    # Define the start and end date for iteration\n    start_results_date = end_date - timedelta(years=1)\n    end_results_date = datetime.today() - timedelta(months=1)\n\n    # List to store DataFrames\n    result_dataframes = []\n\n    # Define our date variable that will be changed\n    current_date = start_results_date\n\n    # Initialise the while loop to ensure we cover our whole date range\n    while current_date &lt;= end_results_date:\n        # Generate the URL\n        url = f'https://betfair-datascientists.github.io/data/assets/ANZ_Greyhounds_{current_date.year}_{current_date.month:02d}.csv'\n\n        try:\n            # Read CSV data into a DataFrame directly from the URL\n            df = pd.read_csv(url)\n            result_dataframes.append(df)\n            print(f\"Processed: {url}\")\n        except Exception as e:\n            print(f\"Failed to fetch data from: {url}, Error: {e}\")\n\n        # Move to the next month\n        current_date = current_date.replace(day=1) + pd.DateOffset(months=1)\n\n    # Concatenate all DataFrames into one\n    betfair_results = pd.concat(result_dataframes, ignore_index=True)\n\n    # Keep only required columns\n    betfair_results= betfair_results[BETFAIR_RESULTS_COLUMNS]\n\n    # Drop rows with None in WIN_BSP or BEST_AVAIL_BACK_AT_SCHEDULED_OFF\n    betfair_results = betfair_results.dropna(subset=['WIN_BSP'])\n    betfair_results = betfair_results.dropna(subset=['BEST_AVAIL_BACK_AT_SCHEDULED_OFF'])\n\n    # Drop any duplicate rows\n    betfair_results.drop_duplicates(inplace=True)\n\n    # Ensure LOCAL_MEETING_DATE is in datetime format\n    betfair_results['LOCAL_MEETING_DATE']=pd.to_datetime(betfair_results['LOCAL_MEETING_DATE']).dt.date\n\n    return betfair_results\n\nbetfair_results = retrieve_betfair_results(end_date)\n\ndef create_backtesting_dataset(betfair_results,best_model_test_data):\n    '''\n    This function merges together our model predictions with the retrieved Betfair data\n    '''\n\n    # merge dataframes\n    backtesting = pd.merge(betfair_results,\n                        best_model_test_data,\n                        how='left',\n                        left_on=['LOCAL_MEETING_DATE','TRACK','RACE_NO','TAB_NUMBER'],\n                        right_on=['meetingDate','track','raceNumber','rugNumber'])\n\n    # Drop Betfair races where our model didn't have a prediction\n    backtesting.dropna(subset=['win_probability'],inplace=True)\n\n    return backtesting\n\nbacktesting = create_backtesting_dataset(betfair_results,test_data)\n\ndef create_backtesting_dataset(betfair_results,best_model_test_data):\n    '''\n    This function merges together our model predictions with the retrieved Betfair data\n    '''\n\n    # merge dataframes\n    backtesting = pd.merge(betfair_results,\n                        best_model_test_data,\n                        how='left',\n                        left_on=['LOCAL_MEETING_DATE','TRACK','RACE_NO','TAB_NUMBER'],\n                        right_on=['meetingDate','track','raceNumber','rugNumber'])\n\n    # Drop Betfair races where our model didn't have a prediction\n    backtesting.dropna(subset=['win_probability'],inplace=True)\n\n    return backtesting\n\nbacktesting = create_backtesting_dataset(betfair_results,test_data)\n\ndef generate_backtesting_columns(backtesting):\n    '''\n    This function generates additional columns for the purposes of profitability testing.\n\n    Model Rank: This function ranks each runner within the race in descending order by win_probability\n    Model Value: Implied value of the model's probability against the best available odds at the scheduled off - These odds are used as a signal only\n    Proportional Back Stake: A Profit/Loss column for each runner before commission\n\n    '''\n    # Rank each runner by the model's probability\n    backtesting['MODEL_RANK'] = backtesting.groupby('WIN_MARKET_ID')['win_probability'].rank(ascending=False)\n\n    # Generate our model's implied value against the best market price at the scheduled jump time\n    backtesting['MODEL_VALUE'] = backtesting['win_probability'] - 1/backtesting['BEST_AVAIL_BACK_AT_SCHEDULED_OFF']\n\n    # Generate a profit / loss column on a per runner basis without commission\n    backtesting['PROPORTIONAL_BACK_STAKE'] = np.where(backtesting['WIN_RESULT'] == 'WINNER',\n                            backtesting['win_probability'] * 100 * (backtesting['WIN_BSP'] - 1),\n                            backtesting['win_probability'] * -100)\n\n    return backtesting\n\nbacktesting = generate_backtesting_columns(backtesting)\n\n# This is a non-exhaustive list of race types from the Betfair Pricing Data which groups together different race types. \n# The greyhound race grading system is convoluted and varies by state\nRACE_TYPE_DICTIONARY = {\n            'B8':'Other',\n            'FFA':'Ungraded',\n            'Final':'Prestige',\n            'G3/4/5':'Graded',\n            'Gr':'Graded',\n            'Gr1':'Graded',\n            'Gr1/2':'Graded',\n            'Gr1/2/3':'Graded',\n            'Gr2/3':'Graded',\n            'Gr2/3/4':'Graded',\n            'Gr3':'Graded',\n            'Gr3/4':'Graded',\n            'Gr3/4/5':'Graded',\n            'Gr4':'Graded',\n            'Gr4/5':'Graded',\n            'Gr4/5/6':'Graded',\n            'Gr5':'Graded',\n            'Gr5/6':'Graded',\n            'Gr5/Mdn':'Graded',\n            'Gr6':'Graded',\n            'Gr6/7':'Graded',\n            'Gr7':'Graded',\n            'Grp1':'Prestige',\n            'Grp2':'Prestige',\n            'Grp3':'Prestige',\n            'Heat':'Heat',\n            'Inv':'Other',\n            'Juv':'Novice',\n            'Juv/Grad':'Novice',\n            'Juv/Mdn':'Novice',\n            'Listed':'Prestige',\n            'M1':'Masters',\n            'M1/2':'Masters',\n            'M1/2/3':'Masters',\n            'M1/M2':'Masters',\n            'M1/M2/M3':'Masters',\n            'M12':'Masters',\n            'M2':'Masters',\n            'M2/3':'Masters',\n            'M2/M3':'Masters',\n            'M3':'Masters',\n            'M3/4':'Masters',\n            'M4/5':'Masters',\n            'M4/6':'Masters',\n            'M4/M5':'Masters',\n            'M5':'Masters',\n            'M6':'Masters',\n            'Mdn':'Maiden',\n            'Mdn/Gr4/5':'Maiden',\n            'Mdn/Gr5':'Maiden',\n            'Mdn/Gr6':'Maiden',\n            'Mdn/Juv':'Maiden',\n            'N/G':'Ungraded',\n            'Nov':'Novice',\n            'Nov/Mdn':'Novice',\n            'Nvce':'Novice',\n            'Prov':'Other',\n            'Rest':'Win Restricted',\n            'S/E':'Other',\n            'SE':'Other',\n            'Semi':'Other',\n            'Vets':'Other',\n            }\n\n\ndef group_racetypes(backtesting):\n    '''\n    This function generates a race_type_group column using a dictionary of loose race categories\n    '''\n    # Create 'RACE_TYPE_GROUP' column by mapping values from 'race_type' column\n    backtesting['RACE_TYPE_GROUP'] = backtesting['RACE_TYPE'].map(RACE_TYPE_DICTIONARY)\n\n    # Export to csv\n    backtesting.to_csv('backtesting_results.csv',index=False)\n\n    return backtesting\n\nbacktesting = group_racetypes(backtesting)\n\n'''\nThis concludes the model generation tutorial.\nThe following code is intended for use in a live model\n'''\n</code></pre> <p>Once we've exported this to a csv file, we've conducted some basic excel analysis to segment the predictions. All figures quoted below are before commission and may not be reflective of actual profitability. These segments are intended as ideas only.</p> <p></p>"},{"location":"modelling/topazTutorial/#creating-new-ratings","title":"Creating new ratings","text":"<p>The next step will be to load up our database, and update with recent results</p> Load up the database and begin the preprocessing<pre><code>def generate_historic_data_enddate(topaz_data_all):\n    '''\n    This function finds the date to which the Topaz Historical Data has been downloaded, and then generates a datetime variable which equals the following date\n\n    '''\n    # Find the maximum value in the 'meetingDate' column\n    max_meeting_date = topaz_data_all['meetingDate'].max()\n\n    # Remove time information from the max_meeting_date and format it to '%Y-%m-%d'\n    max_meeting_date = pd.to_datetime(max_meeting_date, format='%Y-%m-%dT%H:%M:%S.%fZ').strftime('%Y-%m-%d')\n\n    # Convert max_meeting_date to datetime object\n    max_meeting_date = pd.to_datetime(max_meeting_date)\n\n    # Add one day to max_meeting_date\n    max_meeting_date_plus_one_day = max_meeting_date + timedelta(days=1)\n\n    print(\"Max meeting date plus one day:\", max_meeting_date_plus_one_day)\n\n    # Define yesterday's date\n    yesterday = datetime.today() - timedelta(days=1)\n\n    return max_meeting_date_plus_one_day, yesterday\n\ntopaz_data_all = load_topaz_data(JURISDICTION_CODES,'HISTORICAL')\n\nmax_meeting_date_plus_one_day, yesterday = generate_historic_data_enddate(topaz_data_all)\n\n# Update historical dataset up to and including yesterday's races\ncollect_topaz_data(TOPAZ_API_KEY,JURISDICTION_CODES,max_meeting_date_plus_one_day,yesterday,'HISTORICAL',1,5)\n</code></pre> <p>Following this step, we'll need to download today's upcoming races from the Topaz API and also gather market information from the Betfair API to generate today's ratings. </p> <p>An important thing to note here is that the update frequency of box information on Topaz is not known and for upcoming races, may not be reliable. We do, however, know that the Betfair Fields only appear once fields are final and so box information will be accurate. There we need to query the market clarifications field and extract the box information using a rather ugly regex script. Once we've done this, we can replace the box information from Topaz with the box information taken from the Betfair API before we process the raw Topaz data ready for application of our lovingly trained GridSearch'ed LGBM model</p> Gathering today's upcoming races from Topaz and the Betfair API<pre><code>import betfairlightweight\nfrom betfairlightweight import filters\nimport pandas as pd\nfrom datetime import timedelta\nfrom nltk.tokenize import regexp_tokenize\nimport warnings\nimport json\n\nwarnings.filterwarnings('ignore', message='The behavior of DataFrame concatenation with empty or all-NA entries is deprecated.*')\n\ndef bflw_trading():\n    '''\n    This function loads the credentials file, and passes the credentials into the betfairlightweight instance\n    '''\n    with open('credentials.json') as f:\n        cred = json.load(f)\n        username = cred['username']\n        password = cred['password']\n        app_key = cred['app_key']\n\n    # Define the betfairlightweight client\n    trading = betfairlightweight.APIClient(username, password, app_key=app_key)\n\n    return trading\n\ndef login(trading):\n    # login to the API\n    trading.login_interactive()\n\ndef greyhound_market_filter():\n    # Define the greyhound market filter\n    market_filter = filters.market_filter(\n        event_type_ids=[4339],  # For greyhound racing\n        market_countries=['AU','NZ'],  # For Australia and New Zealand\n        market_type_codes=['WIN'],# For win markets\n        venues = []  # Specify specific venues if required\n    )\n\n    return market_filter\n\ndef process_runner_books(runner_books):\n    # Define the fields required from the runner book\n    selection_ids = [runner_book.selection_id for runner_book in runner_books]\n\n    df = pd.DataFrame({\n        'selection_id': selection_ids,\n    })\n    return df\n\ndef generate_greyhound_catalogue(trading,market_filter):\n    # Load the greyhound market catalogues from the Betfair API\n    greyhound_market_catalogues = trading.betting.list_market_catalogue(\n    filter=market_filter,\n    market_projection=['RUNNER_DESCRIPTION', 'EVENT', 'MARKET_DESCRIPTION'],\n    max_results='200')\n\n    print(f\"Found {len(greyhound_market_catalogues)} markets.\")\n\n    return greyhound_market_catalogues\n\nRUNNER_DATA_COLUMNS = [\n            'market_start',\n            'venue',\n            'race_no',\n            'win_market_id',\n            'selection_id',\n            'tab_number',\n            'runner_name',\n            'boxNumber'\n            ]\n\ndef initilise_dataframe():\n    # Create the empty dataframe\n    data = pd.DataFrame(columns=RUNNER_DATA_COLUMNS)\n\n    return data\n\nPATTERN1 = r'(?&lt;=&lt;br&gt;Dog ).+?(?= starts)'\n\nPATTERN2 = r\"(?&lt;=\\bbox no. )(\\w+)\"\n\ndef process_market_clarifications(runners_df,clarifications):\n    '''\n    This function accesses the market clarifications field which explains which box the reserve runner will be starting from (if any) and parses the information using regex\n    We utilise this information rather than the Topaz API data because Betfair markets only use final field information\n\n    A clarification will look like: \"&lt;br&gt;Box changes:&lt;br&gt;Dog 9. Tralee Blaze starts from box no. 8&lt;br&gt;&lt;br&gt;Dog 6. That Other One starts from box no. 2&lt;br&gt;&lt;br&gt;\"\n    '''\n    # Define the clarifications dataframe\n    market_clarifications = pd.DataFrame(regexp_tokenize(clarifications, PATTERN1), columns = ['runner_name'])\n\n    # Remove dog name from runner_number\n    market_clarifications['tab_number'] = market_clarifications['runner_name'].str.split(r'. ').str[0]\n\n    # Extract box number from clarifications\n    market_clarifications['boxNumber'] = regexp_tokenize(clarifications, PATTERN2)\n\n    # Keep only boxNumber and rugNumber\n    market_clarifications=market_clarifications[['tab_number','boxNumber']]\n\n    # Merge the clarifications with the original dataframe\n    runners_df = pd.merge(runners_df,market_clarifications,how='left',on=['tab_number'])\n\n    # Any runners with no clarifications will start in the box that matches the rugNumber\n    runners_df['boxNumber'].fillna(runners_df['tab_number'],inplace=True)\n\n    return runners_df\n\ndef collect_greyhound_market_data(trading,greyhound_market_catalogues,data):\n    '''\n    This function will process the greyhound market catalogue to access information about the market including:\n     - Market ID\n     - Market Name\n     - Event Name\n     - Start Time\n     - Clarifications\n\n    It will then process each individual market book to gather the runner information, following by some operations to put market information into the dataframe columns including adjusting the timezone from UTC to AEST\n    Finally it will then perform some string splitting operations to generate more useful market/runner information:\n     - Track\n     - Race Number\n     - Race Type\n     - Rug Number\n     - Dog Name\n\n    These operations may be useful depending on whether the betting intention is for a specific subset of races. It is also possible to split out race distance from the market name\n    '''\n    # Initiate the for loop\n    for market_catalogue in greyhound_market_catalogues:\n\n        # Name variables for market parameters\n        market_id = market_catalogue.market_id\n        market_name = market_catalogue.market_name\n        event_name = market_catalogue.event.name\n        market_start_time = market_catalogue.description.market_time\n\n        # Try to access clarifications and replace a known string replacement to prepare it for our regex function\n        try:\n            clarifications = market_catalogue.description.clarifications.replace(\"&lt;br&gt; Dog\",\"&lt;br&gt;Dog\")\n        except AttributeError:\n            clarifications = None\n\n        # Generate our market_books list\n        market_books = trading.betting.list_market_book(market_ids=[market_id])\n\n        # Generate our runner_catalogues list\n        runner_catalogues = market_catalogue.runners\n\n        # Initiate the market_books for loop\n        for market_book in market_books:\n\n            # Call the process_runner_books function\n            runners_df = process_runner_books(market_book.runners)\n\n            # Get the runner catalogue\n            for runner in market_book.runners:\n\n                # define the runner catalogue\n                runner_catalogue = next((rd for rd in runner_catalogues if rd.selection_id == runner.selection_id), None)\n\n                # define the runner name for non-empty runner_catalogues\n                if runner_catalogue is not None:\n                    runner_name = runner_catalogue.runner_name\n                    runners_df.loc[runners_df['selection_id'] == runner.selection_id, 'runner_name'] = runner_name\n\n            # Assign market variables to the dataframe\n            runners_df['win_market_id'] = market_id\n            runners_df['marketName'] = market_name\n            runners_df['eventName'] = event_name\n            runners_df['market_start'] = market_start_time\n\n            # Adjust the timezone from UTC to AEST\n            runners_df['market_start'] = runners_df['market_start'] + timedelta(hours=10)\n\n            # Perform string split operations \n            runners_df['venue']=runners_df['eventName'].str.split(' \\(').str[0]\n            runners_df['race_no']=runners_df['marketName'].str.split(r' ').str[0]\n            runners_df['race_no']=runners_df['race_no'].str.split('R').str[1]\n            runners_df['tab_number']=runners_df['runner_name'].str.split(r'. ').str[0]\n            runners_df['runner_name']=runners_df['runner_name'].str.split('\\. ').str[1]\n\n            # Call the process_market_clarifications function. If there no reserve runners running then the boxNumber = rugNumber\n            try:\n                runners_df = process_market_clarifications(runners_df,clarifications)\n            except TypeError:\n                runners_df['boxNumber'] = runners_df['tab_number']\n\n            # concatenate the dataframes together\n            data=pd.concat([data,runners_df], sort=False)\n\n    # Keep only required columns\n    data = data[RUNNER_DATA_COLUMNS]\n    data = pd.DataFrame(data)\n\n    return data\n\ndef download_betfair_market_data():\n    '''\n    This function combines all our previously defined functions to generate our market csv from the Betfair API\n    '''\n    trading = bflw_trading()\n\n    login(trading)\n\n    market_filter = greyhound_market_filter()\n\n    greyhound_market_catalogues = generate_greyhound_catalogue(trading,market_filter)\n\n    data = initilise_dataframe()\n\n    data = collect_greyhound_market_data(trading,greyhound_market_catalogues,data)\n\n    data.to_csv('betfair_data.csv',index=False)\n\n    return data\n\nbetfair_data = download_betfair_market_data()\n\ndef delete_old_upcoming_races(codes):\n    '''\n    This function here will loop over our previously written csv files with upcoming race data and then delete them, so we can load fresh data without duplication\n    '''\n\n    # loop over all csv files\n    for code in codes:\n        # Read only the header of the CSV file\n        try:\n            with open(code+'_DATA_UPCOMING.csv', 'r') as file:\n                header = file.readline().strip().split(',')  # Read the header and split into columns\n                StateData = pd.DataFrame(columns=header)  # Create a DataFrame with the header columns\n        except FileNotFoundError:\n            continue  # Skip to the next file if the current one is not found\n\n        # Write the header back to the CSV file\n        StateData.to_csv(code+'_DATA_UPCOMING.csv', index=False)\n\ndef upcoming_topaz_data(codes,datatype,betfair_data,number_of_retries,sleep_time):\n    '''\n    This function loads our upcoming races, discards the Topaz API boxNumber and adds the boxNumber information retrieved from the Betfair API\n    '''\n    # Delete the upcoming races data we loaded previously\n    delete_old_upcoming_races(codes)\n\n    # We define this variable inside the function because it will vary with the function\n    today = pd.to_datetime(datetime.today())\n\n    # Collect today's topaz race information\n    collect_topaz_data(TOPAZ_API_KEY,JURISDICTION_CODES,today,today,datatype,number_of_retries,sleep_time)\n\n    # Load today's race information\n    todays_topaz_data = load_topaz_data(codes,datatype)\n\n    # Keep only required Betfair information\n    betfair_fields = betfair_data[['venue','race_no','tab_number','boxNumber']]\n\n    # Discard the Topaz API boxNumber information\n    todays_topaz_data.drop(columns=['boxNumber'], inplace=True)\n\n    # Merge the Betfair boxNumber information\n    todays_topaz_data = pd.merge(todays_topaz_data,betfair_fields,how='left',left_on=['track','raceNumber','rugNumber'], right_on=['venue','race_no','tab_number'])\n\n    return todays_topaz_data\n\ntodays_topaz_data = upcoming_topaz_data(JURISDICTION_CODES,'UPCOMING',betfair_data,1,5)\n\ndef load_topaz_preprocessing_data(codes,datatype,start_date):\n    '''\n    This function combines 4 of our previous functions to load our existing topaz historical data into a dataframe from our database and perform some basic cleaning and then keep only the last 12 months for preprocessing\n    '''\n    topaz_data_historical = load_topaz_data(codes,datatype)\n\n    topaz_data_historical = discard_scratched_runners_data(topaz_data_historical)\n\n    topaz_data_historical = discard_using_dates(topaz_data_historical,start_date)\n\n    topaz_data_historical = discard_unnecessary_columns(topaz_data_historical,TOPAZ_COLUMNS_TO_KEEP)\n\n    return topaz_data_historical\n\nstart_date = datetime.today() - relativedelta(years=1)\n\ntopaz_data_historical = load_topaz_preprocessing_data(JURISDICTION_CODES,'HISTORICAL',start_date)\n\ndef concatenate_data(topaz_data_historical,todays_topaz_data):\n\n    # Concatenate the last 12 months of Topaz Data with today's races\n    topaz_data_pre_processing = pd.concat([topaz_data_historical,todays_topaz_data])\n\n    return topaz_data_pre_processing\n\ntopaz_data_pre_processing = concatenate_data(topaz_data_historical,todays_topaz_data)\n\ndef forward_fill_dog_weight(df):\n    # Sort the DataFrame by dogId and meetingDate\n    df.sort_values(['dogId', 'meetingDate'], inplace=True)\n\n    # Replace zero values in weightInKg with NaN\n    df['weightInKg'].replace(0, pd.NA, inplace=True)\n\n    # Forward fill NaN values within each dogId group\n    df['weightInKg'] = df.groupby('dogId')['weightInKg'].ffill()\n\n    # Replace remaining NaN values with 0\n    df['weightInKg'].fillna(0, inplace=True)\n\n    return df\n</code></pre> <p>The final step now will be to apply all of our previously defined functions to the last 12 months of Topaz data to create our rolling window features before, finally, we can apply our trained model (in the form of a pickle file) to today's field and output our ratings ready to put in the market</p> Generate today's ratings<pre><code>def preprocess_data_for_today(topaz_data_pre_processing):\n    '''\n    This function will apply all of our previously defined functions to our dataset in order to generate all the features required for today's races\n    '''\n    topaz_data_pre_processing = forward_fill_dog_weight(topaz_data_pre_processing)\n\n    topaz_data_pre_processing = clean_track_data(topaz_data_pre_processing,TRACK_DICTIONARY)\n\n    topaz_data_pre_processing = extract_form(topaz_data_pre_processing)\n\n    topaz_data_pre_processing = correct_result_margin(topaz_data_pre_processing)\n\n    topaz_data_pre_processing = generate_dogAge(topaz_data_pre_processing)\n\n    topaz_data_pre_processing = generate_winPercentage(topaz_data_pre_processing)\n\n    topaz_data_pre_processing = generate_finishingPlaceMovement(topaz_data_pre_processing)\n\n    topaz_data_pre_processing = scale_values(topaz_data_pre_processing)\n\n    topaz_data_pre_processing = generate_runTimeNorm(topaz_data_pre_processing)\n\n    topaz_data_pre_processing = generate_adjacentVacantBox_state(topaz_data_pre_processing)\n\n    topaz_data_pre_processing = generate_boxWinPercentage(topaz_data_pre_processing)\n\n    topaz_data_pre_processing, feature_cols = generate_rollingFeatures(topaz_data_pre_processing)\n\n    feature_cols = generate_feature_list(feature_cols)\n\n    topaz_data_pre_processing, feature_cols = generate_modelling_dataset(topaz_data_pre_processing, feature_cols)\n\n    topaz_data_pre_processing = remove_races(topaz_data_pre_processing)\n\n    topaz_data_pre_processing = downcast_data(topaz_data_pre_processing)\n\n    return topaz_data_pre_processing, feature_cols\n\ntopaz_data_pre_processing, feature_cols = preprocess_data_for_today(topaz_data_pre_processing)\n\ndef todays_fields(topaz_data_pre_processing):\n    '''\n    This function removes all race data except for today's races before we load our trained model\n    '''\n    # Create our naive meetingDate\n    topaz_data_pre_processing['meetingDateNaive'] = pd.to_datetime(topaz_data_pre_processing['meetingDate'], format='%Y-%m-%dT%H:%M:%S.%fZ', utc=True).dt.tz_localize(None)\n\n    # Keep only today's races\n    todays_runners = topaz_data_pre_processing[topaz_data_pre_processing['meetingDateNaive'] &gt; datetime.today() - timedelta(days=1)].reset_index(drop=True)\n\n    # Discard the naive meetingDate\n    todays_runners.drop(columns=['meetingDateNaive'], inplace=True)\n\n    return todays_runners\n\ntodays_runners = discard_using_dates(topaz_data_pre_processing)\n\ntodays_runners.to_csv('todays_runners.csv',index=False)\n\n'''\nWe export the dataframes to csv so that it allows us to simply reload the files if our environment crashes for whatever reason\n'''\n# todays_runners = pd.read_csv('todays_runners.csv')\n# betfair_data = pd.read_csv('betfair_data.csv')\n</code></pre> <p>Here follows two blocks of code where we can either apply our GridSearch model or our suite of different ML algorithms</p> Grid Search<pre><code>''' \nThis block of code is intended for use in loading the model trained using the GridSearch and apply the model to generate today's probabilities\n'''\n\nimport joblib\n\ndef load_trained_lgbm_model():\n\n    # Load the trained model\n    model = joblib.load('best_lgbm_model.pickle')\n\n    return model\n\nmodel = load_trained_lgbm_model()\n\nMODEL_RATINGS_COLUMNS = ['state',\n                        'track',\n                        'raceNumber', \n                        'boxNumber',\n                        'rugNumber',\n                        'win_probability']\n\ndef apply_trained_model(model, todays_runners,feature_cols):\n    '''\n    This function will apply our previously trained lgbm model to our dataset to generate today's rated prices\n    '''\n\n    # Generate the feature set\n    todays_runners_features = todays_runners[feature_cols]\n\n    # Apply the trained model\n    todays_runners['win_probability'] = model.predict_proba(todays_runners_features)[:, 1]\n\n    # Normalise all probabilities\n    todays_runners['win_probability'] = todays_runners.groupby('raceId', group_keys=False)['win_probability'].apply(lambda x: x / sum(x))\n\n    # Select desired columns\n    todays_runners = todays_runners[MODEL_RATINGS_COLUMNS]\n\n    return todays_runners\n\ntodays_greyhound_ratings = apply_trained_model(model, todays_runners,feature_cols)\n\ndef join_betfair_data(todays_greyhound_ratings,betfair_data):\n    '''\n    This function will join our Betfair market and runner information with today's model ratings\n    '''\n    # Merge the dataframes\n    betfair_data = pd.merge(betfair_data,todays_greyhound_ratings,how='left',on=['track','raceNumber','rugNumber'])\n\n    # Export the data to csv\n    betfair_data.to_csv('todays_ratings.csv',index=False)\n\n    return betfair_data\n\nbetfair_data = join_betfair_data(todays_greyhound_ratings,betfair_data)\n</code></pre> Algorithm Suite<pre><code>''' \nThis block of code is intended for use in loading all the different models trained by the various algorithms to generate today's probabilities\n\n'''\n\nimport os\nimport joblib\nimport re\n\nfolder_path = 'INSERT FOLDER PATH'\n\ndef find_pickle_files(folder_path):\n\n    all_files = os.listdir(folder_path)\n    pickle_files = [file for file in all_files if file.endswith('_.pickle')]\n    file_model_mapping = {}\n\n    for pickle_file in pickle_files:\n        file_path = os.path.join(folder_path, pickle_file)\n        model = joblib.load(file_path)\n        file_model_mapping[pickle_file] = model\n\n    return file_model_mapping\n\nMODEL_RATINGS_COLUMNS = ['state',\n                         'track',\n                         'raceNumber', \n                         'boxNumber',\n                         'rugNumber']\n\ndef apply_trained_model(file_model_mapping, todays_runners, feature_cols):\n\n    todays_runners_features = todays_runners[feature_cols]\n\n    model_list = []\n\n    for filename, model in file_model_mapping.items():\n\n        todays_runners[f'win_probability_{filename}'] = model.predict_proba(todays_runners_features)[:, 1]\n\n        model_list.append(f'win_probability_{filename}')\n\n    return todays_runners, model_list\n\nfile_model_mapping = find_pickle_files(folder_path)\n\ntodays_greyhound_ratings, model_list = apply_trained_model(file_model_mapping, todays_runners, feature_cols)\n\ntodays_greyhound_ratings = todays_greyhound_ratings[MODEL_RATINGS_COLUMNS + model_list]\n\ndef join_betfair_data(todays_greyhound_ratings,betfair_data):\n    '''\n    This function will join our Betfair market and runner information with today's model ratings\n    '''\n    # Merge the dataframes\n    betfair_data = pd.merge(betfair_data,todays_greyhound_ratings,how='left',left_on=['venue','race_no','tab_number'],right_on=['track','raceNumber','rugNumber'])\n\n    for i in model_list:\n\n        betfair_data[i] = betfair_data.groupby('win_market_id',group_keys=False)[i].apply(lambda x: x / sum(x))\n\n        this_model = betfair_data.copy()\n        this_model = this_model[['market_start',\n                                 'venue',\n                                 'race_no',\n                                 'win_market_id',\n                                 'selection_id',\n                                 'tab_number',\n                                 'runner_name',\n                                 i]]\n        this_model.rename(columns={i:'probability'},inplace=True)\n        match = re.search(r'(\\w+)_(\\w+)\\.pickle', i)\n        model_name = match.group(2)\n        this_model.to_csv(model_name+'.csv',index=False)\n\njoin_betfair_data(todays_greyhound_ratings,betfair_data)\n</code></pre>"},{"location":"modelling/topazTutorial/#betting-our-rated-prices","title":"Betting our rated prices","text":"<p>Now that we've created our rated prices, let's start betting on them!</p> <p>Our (How to Automate Part III)[https://betfair-datascientists.github.io/tutorials/How_to_Automate_3/#running-our-strategy] tutorial runs through how to bet ratings from a csv file. This code can be adapted here to use our model ratings like we would the Iggy Greyhound Prediction Model ratings.</p>"},{"location":"modelling/topazTutorial/#conclusion","title":"Conclusion","text":"<p>While creating models can be fun and rewarding, actually placing bets with real money can be a bit daunting. We recommend reading through our series of articles on the (Mental Game of Wagering)[https://betfair-datascientists.github.io/mentalGame/intro/] to help with the psychological parts of wagering that can be especially challenging.</p> <p>Hopefully this tutorial has helped you to discover the world of greyhound modelling and has given you plenty of ideas for your models moving forward! While you may find that one machine learning algorithm may have slight advantages over another, as someone who is wiser than me once said, there is \"usually much more value to be obtained coming up with some killer features than spending the same time coming mucking around with every algorithm under the sun.\"</p>"},{"location":"modelling/topazTutorial/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"tutorials/How_to_Automate_1/","title":"How to Automate 1","text":"<p>I have had a go at automating models and trading bots on Betfair for the better part of a year now, although I am not successful enough to quit my full time job, I've come across a tonne of different hurdles and issues. Resources online can sometimes be scarce or hard to understand if you are not either a software engineer or have a dev background. </p> <p>So this is the first part of a multipart series of that go through all the things you need to know to reach the end goal of learning how to automate the model Bruno taught us how to build in his greyhound modelling tutorial. At the very end we will also create a script to simulate the exchange to backtest the strategies we have created so that we can optimize our strategies. We will be using the Flumine package and all the code we create will be available on github so you can in your own models and strategies.</p> <p>This series will be split into five parts which goes into:</p> <ul> <li>Part I - Understanding how Flumine works</li> <li>Part II - Automating backing or laying the 1st/2nd/.../nth favourite</li> <li>Part III - How to Automate one of Betfair's Data Science Models</li> <li>Part IV - How to Automate the model Bruno taught us how to build</li> <li>Part V - How to simulate the Exchange to backtest and optimise our strategies</li> </ul> <p>Before we dive into the Flumine package, or any code lets gain some understanding about the Betfair API. Betfair offers their API in two forms a Rest API and a Push API often call the polling and streaming API. We don't really need to know the technical differences between a Rest API and a Push API, but there are a few key differences on the Betfair API that is important to note:</p> <p>Rest API (Polling)</p> <ul> <li>Only returns a snap shot of data, everytime you want more data you must make a new request and wait for a response</li> <li>Market catalogue and all information available in market catalogue such as Runner names or Market event names are available</li> </ul> <p>Push API (Streaming)</p> <ul> <li>Only need to connect once, any updates will be sent to you as an update</li> <li>Only contains some information such as prices, Market catalogue is not available</li> </ul> <p>Because streaming gives real time pricing information it is much better than polling, but it doesn't include some key information such as the names of horses/sport teams. So, the solution is to use both streaming and polling together. </p> <p>If you are crazy and like to build everything from scratch, feel free to work out how to use them together and to build out your betting infrastructure. But if you would like to remain sane the great thing about Flumine is that it automatically combines the two together, so you get the benefit of both real time pricing and also all the information provided in the polling API.</p> <p>There is both documentation for the Betfair API and for Flumine however a quick note is that the documentation for the Betfair API is generally in camelCase e.g. selectionId or marketId whereas Flumine being a python package follows the PEP 8 style guide e.g. selection_id and market_id. Basically this just means whenever we see something on the Betfair documentation page the equivalent for Flumine will be in lower_case_with_underscores.</p> <p>Flumine is designed as a general framework that allows you to create custom strategies and operationalise them on specific markets, handling all the bet placement and market subscription.</p> <p>Using Flumine your general code structure will look like this:</p> <ul> <li>Login</li> <li>Create your strategy as a new Python Class</li> <li>Choose the markets/sports and controls for your strategy</li> <li>Adding workers</li> <li>Run your strategy</li> </ul> <p>The bulk of the coding required is a simple copy paste job, there are only a few things you need to change such as what you specify in your strategy</p> <p>This tutorial will assume that you have an API app key. If you don't, please follow the steps outlined here.</p> <p>You will also need to have streaming enabled on your API app key, to get it enabled email automation@betfair.com.au</p> <p>You can use the Flumine package with or without certificates. There have been quite a lot of discussions of how useful the security certificates are on the Betcode (formerly Betfairlightweight) slack group, but the general  consensus is that its not too useful. Considering it is an extreme hassle to create the certificates and there is no really added benefit I prefer to log in without the certificates. </p> <p>However, if I haven't dissuaded you there are detailed instructions on how to generate certificates. For a windows machine, follow the instructions outlined here. For alternate instructions for Windows, or for Mac/Linux machines, follow the instructions outlined here. You should then create a folder for your certs, perhaps named 'certs' and grab the path location.</p> <p>Besides that, the code for logging in will basically always be the same, so you can always copy and paste this! Be sure to fill in your username, password, appkeys (and the directory where your security certificates are stored if you created them).</p> <pre><code># Import libraries for logging in\nimport betfairlightweight\nfrom flumine import Flumine, clients\n\n# Credentials to login and logging in \ntrading = betfairlightweight.APIClient('username','password',app_key='appkey')\nclient = clients.BetfairClient(trading, interactive_login=True)\n\n# Login\nframework = Flumine(client=client)\n\n# Code to login when using security certificates\n# trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs')\n# client = clients.BetfairClient(trading)\n\n# framework = Flumine(client=client)\n</code></pre> <p>This is probably the only slightly difficult part and after reading through this tutorial, it should be super easy!</p> <p>Before we delve straight into how this part works, we first need to understand some basics of how Classes work in Python. If you know this already, feel free to skip to the Creating our strategy section.</p> <p>Let's create a new class in Python. This will allow us to define an object of this type later on, just like how we can create an integer, string or boolean.</p> <pre><code>class Fruit():\n    # Initialize\n        # Sets the attributes of the fruit object\n    def __init__(self, colour, taste, size, price_per_size) -&amp;gt; None:\n        self.colour = colour\n        self.taste = taste\n        self.size = size\n        self.price_per_size = price_per_size\n        pass\n</code></pre> <p>Now that we have created a new class called <code>Fruit</code>, we can now create <code>Fruit</code> objects, the same way we can define an object as a string or an integer.</p> <p>For example if we wanted to define a variable <code>x</code> as an integer with the value 6:</p> <pre><code># define a new variable 'x' as an integer with the value 6\nx = int(6)\nprint(f'the value of x is {x}, we can see x is of the type {type(x)}')\n</code></pre> <pre>\n<code>the value of x is 6, we can see x is of the type &lt;class 'int'&gt;\n</code>\n</pre> <p>Now we can do the same thing with <code>Fruit</code>:</p> <pre><code># define a new variable 'z' as fruit with the colour green, taste sweet, size 6 and price 2.99\nz = Fruit(colour = 'green',taste = 'sweet', size = 6, price_per_size = 2.99)\nprint(f'the value of z is {z}, z is of the type {type(z)}')\n</code></pre> <pre>\n<code>the value of z is &lt;__main__.Fruit object at 0x7f6fb6794df0&gt;, z is of the type &lt;class '__main__.Fruit'&gt;\n</code>\n</pre> <pre><code># Attributes of the fruit object\nprint(f'The value of z.colour is: {z.colour}')\nprint(f'The value of z.taste is: {z.taste}')\nprint(f'The value of z.size is: {z.size}')\nprint(f'The value of z.price_per_size is: {z.price_per_size}')\n</code></pre> <pre>\n<code>The value of z.colour is: green\nThe value of z.taste is: sweet\nThe value of z.size is: 6\nThe value of z.price_per_size is: 2.99\n</code>\n</pre> <p>In Python methods are functions that you can define inside a class which run when called. For example let's take a look at the upper() method for strings:</p> <pre><code>x = 'hello world'\nx\n</code></pre> <pre>\n<code>'hello world'</code>\n</pre> <pre><code># Example of a method:\nx.upper()\n</code></pre> <pre>\n<code>'HELLO WORLD'</code>\n</pre> <p>Let's add a few methods to the fruit class we created earlier to calculate the total price for us:</p> <pre><code>class Fruit():\n    # Initialize\n    def __init__(self, colour, taste, size, price_per_size) -&amp;gt; None:\n        self.colour = colour\n        self.taste = taste\n        self.size = size\n        self.price_per_size = price_per_size\n        pass\n\n    ### New part (rest is the same):\n    # Creating first method, print the total price\n    def print_total_price(self):\n        print(f'The total_price is {self.size*self.price_per_size}')\n    # Creating second method, return the second price\n    def return_total_price(self):\n        return(self.size*self.price_per_size)\n</code></pre> <p>Now that we have defined the methods, we can call them like this:</p> <pre><code># We need to define z again\nz = Fruit(colour = 'green',taste = 'sweet', size = 6, price_per_size = 2.99)\n\n# Use the methods we just created\nz.print_total_price()\nz.return_total_price()\n</code></pre> <pre>\n<code>The total_price is 17.94\n</code>\n</pre> <pre>\n<code>17.94</code>\n</pre> <p>Now that we know a bit about Classes and Methods in Python, all that's left to learn is Class inheritance.</p> <p>In Python can create a class of another class e.g. <pre><code>class Class_2(Class_1):\n    xxx\n</code></pre> Class_2 is known as the child class and Class_1 is known as the parent class</p> <p>The child class (Class_2) inherit all the attributes and methods of the parent class (Class_1)</p> <ul> <li>But the key thing for us is that the child class (Class_2) can override the methods and attributes it inherits from the parent class (Class_1)</li> </ul> <p>So, going back to our fruit example lets create a child class that inherit from the fruit class as an example:</p> <pre><code>class Avocado(Fruit):\n    ### Override the first method but not the second method\n    # Override first method\n    def print_total_price(self):\n        print(f'The total_price is {self.price_per_size}')\n</code></pre> <p>We have now created another new class, this one called avocado, it is the same as the original parent class (Fruit), but we have overridden the <code>print_total_price</code> method</p> <pre><code># create an object that is of the class avocado\na = Avocado(colour = 'green',taste = 'good idk??', size = 2, price_per_size = 1)\na\n</code></pre> <pre>\n<code>&lt;__main__.Avocado at 0x7f6fb6f60f10&gt;</code>\n</pre> <p>We can see all the attributes behave in the way we expect:</p> <pre><code># Attributes of the avocado object\nprint(f'The value of a.colour is: {a.colour}')\nprint(f'The value of a.taste is: {a.taste}')\nprint(f'The value of a.size is: {a.size}')\nprint(f'The value of a.price_per_size is: {a.price_per_size}')\n</code></pre> <pre>\n<code>The value of a.colour is: green\nThe value of a.taste is: good idk??\nThe value of a.size is: 2\nThe value of a.price_per_size is: 1\n</code>\n</pre> <p>But now when we call our two methods, <code>return_total_price</code> works in the same way as a <code>fruit</code> class would, but now <code>print_total_price</code> is different because we have overridden it:</p> <pre><code>a.print_total_price()\na.return_total_price()\n</code></pre> <pre>\n<code>The total_price is 1\n</code>\n</pre> <pre>\n<code>2</code>\n</pre> <p>Now that we know how class in heritance works, we have armed ourselves with everything we need to know how to work with Flumine so let's tie everything together.</p> <p>Bringing what we have learned about methods and class inheritance. Flumine already has a class called <code>BaseStrategy</code> that is designed to be used as a parent class. Each of the methods defined in BaseStrategy are called automatically at specific times such as when someone places a bet. The idea is to you take <code>BaseStrategy</code> as your Parent Class and then write over the methods that get automatically called with what we want our bot to do.</p> <p>Flumine essentially loops through and automatically calls the methods that have been defined, so all you need to do is override the methods, to suit your strategy.</p> <p>If we adopt the way Flumine does things to our fruit example it will look a little like this:</p> <pre><code># Example with the fruit strategy\nfruit_market = [a,z]\nfor each_fruit in fruit_market:\n    print(f'For the fruit: {each_fruit}')\n    each_fruit.print_total_price()\n</code></pre> <pre>\n<code>For the fruit: &lt;__main__.Avocado object at 0x7f6fb6f60f10&gt;\nThe total_price is 1\nFor the fruit: &lt;__main__.Fruit object at 0x7f6fb6f60280&gt;\nThe total_price is 17.94\n</code>\n</pre> <p>Let's go through an example with a simple strategy that I tested many, many times to understand the intricacies of Flumine before we move onto anything more complex. Let's attempt to lay all selections at a price of 1.01, the good thing is as long as this isn't inplay we basically never get matched using this strategy so we can use it to test a tonne of things. The other good thing about Flumine is that by default you can only have 1 trade live (waiting to be matched) per selection at any one time. So running the below code will only place one bet per selection and another will not be placed untill the first get matched.</p> <p>If we take a closer look at the documentation and source code we can get an idea of the methods available and the ones that are run automatically.</p> <p></p> <p></p> <p>Basically the way Flumine works is any code you have under <code>start</code> runs when you first hit play, then whenever there is an update to the market <code>check_market_book</code> runs and if that returns true then <code>process_market_book</code> and <code>process_orders</code> will run. An update is whenever anyone places/cancels/modifies a bet for that specific market e.g. R7 Flemmington Win. By default Flumine will run continuously without stopping (we will learn how to make it stop later on).</p> <p>For something simple like placing a single bet per selection in a race <code>check_market_book</code> and <code>process_market_book</code> are pretty much the only method we really needed to edit. So our code structure will look something like this:</p> <pre><code>def start():\n    # This is called when you first start up your strategy, generally don't have anything important here\n\ndef check_market_book():\n    # You need this to return True, otherwise process_market_book won't run, by default it will return False \n    # generally used to check if the market is open and if not, we skip that market\n\ndef process_market_book():\n    # This is where you want the bulk of the logic for your strategy\n    # Any code here will initially run once when check_market_book() returns True and also run each time anyone on on the exchange places or cancels a order this market\n    # This is where I prefer to have my bet placement logic\n</code></pre> <p>If you have something in mind that is more complicated such as needing the constantly change the price of your bets, then you can test out <code>process_orders</code>. But for now, let's have a crack at implementing our simple strategy.</p> <pre><code># Import necessary libraries\nfrom flumine import BaseStrategy \nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder, OrderStatus\nfrom flumine.markets.market import Market\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\n\n# Create a new strategy as a new class called LayStrategy, this in turn will allow us to create a new Python object later\n    # LayStrategy is a child class inheriting from a class in Flumine we imported above called BaseStrategy\nclass LayStrategy(BaseStrategy):\n    # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy\n    def start(self) -&amp;gt; None:\n        print(\"starting strategy 'LayStrategy'\")\n\n    # Prevent looking at markets that are closed\n    def check_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; bool:\n        # process_market_book only executed if this returns True\n        if market_book.status != \"CLOSED\":\n            return True\n\n    # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially\n    #  After the first initial time, process_market_book runs every single time someone places, updates or cancels a bet\n    def process_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; None:\n        for runner in market_book.runners: # Loops through each of the runners in the race\n            if runner.status == \"ACTIVE\": # If the runner is active (hasen't been scratched)\n                # Place a lay bet at a price of 1.01 with $5 volume\n                trade = Trade(\n                    market_id=market_book.market_id, # The market_id for the specific market\n                    selection_id=runner.selection_id, # The selection_id of the horse/dog/team\n                    handicap=runner.handicap, # The handicap of the horse/dog/team\n                    strategy=self, # Strategy this bet is part of: itself (LayStrategy)\n                )\n                order = trade.create_order(\n                    side=\"LAY\", order_type=LimitOrder(price=1.01, size=5.00) # Lay bet, Limit order price of 1.01 size = $5\n                )\n                market.place_order(order) # Place the order\n</code></pre> <p>You may have noticed that when I did <code>for runner in market_book.runners:</code> I knew <code>market_book</code> had <code>runners</code> as an attribute, this is because I've spent ages in Flumine, but you can find all this information in Betfair Documentation page, and I would recommend the Betting Type Definitions page. If you are reading through the docs and want to use something from <code>market_book</code> you already have that readily available, but if you want to use something from the polling API such as <code>market_catalogue</code> then Flumine has this available as an attribute under <code>market</code>. So you will need to do something like: <code>market.market_catalogue</code>. We will go through an example of this in How to Automate IV. Quick word of warning, the first few seconds after Flumine starts <code>market.market_catalogue</code> will return <code>None</code> as it hasn't requested data from the polling api yet, but give it a few seconds and it will run fine.</p> <p>Back to the above strategy, you may be thinking that in <code>LayStrategy</code> we will be placing millions of lay bets of $5 at odds of 1.01 because we place a bet whenever <code>process_market_book</code> get called, which happens anytime someone in the same market places, updates or cancels a bet. But it won't because by default there are controls in place that limits the number of bets Flumine will place. Later on we will learn how to adjust them. This means that while the Flumine is incredibly powerful it could be devastating with incorrect code.</p> <p>Now that we have created our strategy all we need to do is to choose what sports to run it on and any trading controls we may have.</p> <p>To actually turn on our strategy we need to define a new variable as a <code>LayStrategy</code> object. Going back to our Avocado example it would look like this:</p> <pre><code>fresh_avocado = Avocado(colour='green/yellow',taste = 'creamy', size= 10,price_per_size=2.99)\nfresh_avocado\n</code></pre> <pre>\n<code>&lt;__main__.Avocado at 0x7f6fb73294f0&gt;</code>\n</pre> <p>When we define fresh_avocado as an Avocado class, we need to include multiple attributes such as colour, taste, size and price_per_size. As avocado is a child class of fruit, we can take a look at fruit for what we need to include.</p> <p>We need to do the same thing for our LayStrategy. The attributes that we need to include can be found in the documentation for BaseStrategy: </p> <p></p> <p>We can see that some of the attributes have default values such as <code>max_order_exposure</code>, but others have <code>None</code>. You can play around with them, for now lets just set <code>market_filter</code> to only bet on greyhound win markets. If you ever get confused (it happens) you can take a look at the  Betting Type Definitions.</p> <p>This is actually one of the really cool things about Flumine, all you need to do is point it at a particular criteria e.g. all greyhound win markets in Australia and it will run your strategy on all those markets.</p> <pre><code>strategy = LayStrategy(\n    market_filter=streaming_market_filter(\n        event_type_ids=[\"4339\"], # Greyhounds\n        country_codes=[\"AU\"], # Australia\n        market_types=[\"WIN\"], # Win Markets\n        market_ids=['1.196189930'] # Murray Bridge R5\n    )\n)\n</code></pre> <p>And now we can add the strategy to framework and run it!</p> <pre><code>framework.add_strategy(strategy)\n</code></pre> <pre><code># Running this will place real bets!\nframework.run()\n</code></pre> <p>Sooner or later, you will run into some sort of error. It's bound to happen. But instead of spending hours scratching your head and contemplating throwing out your laptop there is a simple one-line solution:</p> <pre><code>import logging # technically two lines since you need to import the library\nlogging.basicConfig(filename = 'how_to_automate_1.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n</code></pre> <p>This is called logging and there is a two part YouTube tutorial that explains it far better than I ever could. </p> <p>Basically, the line of code above will enable logging and create a new file called 'how_to_automate_1.log' in the same folder as this Python script. By default Flumine has a tonne of logging which will now be enabled with the line above and write logs to that file. So anytime there is an error we can easily pinpoint what the error is.</p> <p>Put that line of code somewhere near the top of your script and it will save you hours wondering why your code doesn't work.</p> <p>There is one slight drawback, and that basically everything that indicates your code is running smoothly will now go into the log file instead of printing out in your terminal. I would recommend the first time you run your strategy to open the log file so you can see what Flumine is doing, it will look a little like this:</p> <p></p> <p>Let's do something stupid, so we can see how easy it is to pinpoint errors with logging. Let's do the same strategy as above, but with a price of 0.9 instead of 1.01, clearly something that isn't allowed</p> <pre><code># Import libraries for logging in\nimport betfairlightweight\nfrom flumine import Flumine, clients\nfrom flumine import BaseStrategy \nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder, OrderStatus\nfrom flumine.markets.market import Market\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\nimport logging \n\n# Credentials to login and logging in \ntrading = betfairlightweight.APIClient('username','password',app_key='appkey')\nclient = clients.BetfairClient(trading, interactive_login=True)\n\n# Login\nframework = Flumine(client=client)\n\nlogging.basicConfig(filename = 'how_to_automate_1.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n\n\nclass LayStrategy(BaseStrategy):\n    def start(self) -&amp;gt; None:\n        print(\"starting strategy 'LayStrategy'\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; bool:\n        # process_market_book only executed if this returns True\n        if market_book.status != \"CLOSED\":\n            return True\n\n    def process_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; None:\n        for runner in market_book.runners: \n            if runner.status == \"ACTIVE\": \n\n                # Place a lay bet at a price of 0.9 with $5 size\n                trade = Trade(\n                    market_id=market_book.market_id, \n                    selection_id=runner.selection_id,\n                    handicap=runner.handicap, \n                    strategy=self, \n                )\n                order = trade.create_order(\n                    side=\"LAY\", order_type=LimitOrder(price=0.9, size=5) \n                )\n                market.place_order(order) \n\nstrategy = LayStrategy(\n    market_filter=streaming_market_filter(\n        event_type_ids=[\"4339\"], \n        country_codes=[\"AU\"], \n        market_types=[\"WIN\"],\n    )\n)\n\nframework.add_strategy(strategy)\n\nframework.run()\n</code></pre> <p>If we open the log file a few seconds after running the code (you can open it using your code editor or a text editor like notepad) we can see that it shows us what the error is:</p> <p> </p> <p>(I have spent a lot of time looking through log files...)</p> <p>Now that we understand the basics of how Flumine works, this is where the fun begins! There are three more parts of this series which goes more in-depth into automating different angles, the Betfair datascience models and eventually our final goal: our own model.</p> <ul> <li>Part II - Automating backing or laying the 1st/2nd/.../nth favourite</li> <li>Part III - Automating a Betfair model</li> <li>Part IV - Automating your own model</li> <li>Part V - How to simulate the Exchange to backtest and optimise our strategies</li> </ul> <p></p> <p>Run the code from your ide by using py <code>&lt;filename&gt;</code>.py, making sure you amend the path to point to your input data.</p> <p>Download from Github </p> <pre><code># Import libraries for logging in\nimport betfairlightweight\nfrom flumine import Flumine, clients\nfrom flumine import BaseStrategy \nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder, OrderStatus\nfrom flumine.markets.market import Market\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\nimport logging \n\n# Import libraries for logging in\nimport betfairlightweight\nfrom flumine import Flumine, clients\n\n# Credentials to login and logging in \ntrading = betfairlightweight.APIClient('username','password',app_key='appkey')\nclient = clients.BetfairClient(trading, interactive_login=True)\n\n# Login\nframework = Flumine(client=client)\n\n# Code to login when using security certificates\n# trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs')\n# client = clients.BetfairClient(trading)\n\n# framework = Flumine(client=client)\n\nlogging.basicConfig(filename = 'how_to_automate_1.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n\n# Create a new strategy as a new class called LayStrategy, this in turn will allow us to create a new Python object later\n    # LayStrategy is a child class inheriting from a class in Flumine we imported above called BaseStrategy\nclass LayStrategy(BaseStrategy):\n    # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy\n    def start(self) -&amp;gt; None:\n        print(\"starting strategy 'LayStrategy'\")\n\n    # Prevent looking at markets that are closed\n    def check_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; bool:\n        # process_market_book only executed if this returns True\n        if market_book.status != \"CLOSED\":\n            return True\n\n    # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially\n    #  After the first initial time, process_market_book runs every single time someone places, updates or cancels a bet\n    def process_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; None:\n        for runner in market_book.runners: # Loops through each of the runners in the race\n            if runner.status == \"ACTIVE\": # If the runner is active (hasen't been scratched)\n                # Place a lay bet at a price of 1.01 with $5 volume\n                trade = Trade(\n                    market_id=market_book.market_id, # The market_id for the specific market\n                    selection_id=runner.selection_id, # The selection_id of the horse/dog/team\n                    handicap=runner.handicap, # The handicap of the horse/dog/team\n                    strategy=self, # Strategy this bet is part of: itself (LayStrategy)\n                )\n                order = trade.create_order(\n                    side=\"LAY\", order_type=LimitOrder(price=1.01, size=5.00) # Lay bet, Limit order price of 1.01 size = $5\n                )\n                market.place_order(order) # Place the order\n\nstrategy = LayStrategy(\n    market_filter=streaming_market_filter(\n        event_type_ids=[\"4339\"], # Greyhounds\n        country_codes=[\"AU\"], # Australia\n        market_types=[\"WIN\"], # Win Markets\n    )\n)\n\nframework.add_strategy(strategy)\n\nframework.run()\n</code></pre> <p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"tutorials/How_to_Automate_1/#how-to-automate-i-understanding-flumine","title":"How to Automate I: Understanding Flumine","text":""},{"location":"tutorials/How_to_Automate_1/#understanding-the-betfair-api","title":"Understanding the Betfair API","text":""},{"location":"tutorials/How_to_Automate_1/#general-code-structure-and-context","title":"General code structure and context","text":""},{"location":"tutorials/How_to_Automate_1/#login","title":"Login","text":""},{"location":"tutorials/How_to_Automate_1/#create-your-strategy-as-a-new-python-class","title":"Create your strategy as a new Python class","text":""},{"location":"tutorials/How_to_Automate_1/#background-info-python-object-oriented-programming","title":"Background info - Python Object Oriented Programming:","text":""},{"location":"tutorials/How_to_Automate_1/#classes-and-methods","title":"Classes and Methods","text":""},{"location":"tutorials/How_to_Automate_1/#class-inheritance","title":"Class inheritance","text":""},{"location":"tutorials/How_to_Automate_1/#creating-our-strategy-as-a-child-class-from-basestrategy","title":"Creating our strategy as a child class from BaseStrategy","text":""},{"location":"tutorials/How_to_Automate_1/#choose-the-marketssports-and-controls-for-your-strategy","title":"Choose the markets/sports and controls for your strategy","text":""},{"location":"tutorials/How_to_Automate_1/#troubleshooting-the-most-important-part-that-is-one-line-of-code","title":"Troubleshooting (the most important part that is one line of code)","text":""},{"location":"tutorials/How_to_Automate_1/#conclusion-and-next-steps","title":"Conclusion and next steps","text":""},{"location":"tutorials/How_to_Automate_1/#complete-code","title":"Complete Code","text":""},{"location":"tutorials/How_to_Automate_1/#disclaimer","title":"Disclaimer","text":""},{"location":"tutorials/How_to_Automate_2/","title":"How to Automate 2","text":"<p>This tutorial is Part two of the How to Automate series and follows on logically from the How to Automate I: Understanding Flumine tutorial we shared previously. </p> <p>Make sure you look at part one, before diving into this one, as it will help you understand the general Flumine code structure and how it works. This tutorial will delve a bit deeper and give us more confidence working with Flumine. Not only will we get a cool new strategy, but we will be moving closer to learning how to automate our own model.</p> <p>As always please reach out with feedback, suggestions or queries. Please submit a pull request if you catch any bugs or have other improvement suggestions!</p> <p>Say for example you have done some research and found there is significant published academic literature on the existence of a favourite-longshot bias. As a result, you want to automate a strategy backing all favourites or second favourites in thoroughbred racing markets. If this is the case, then this is the perfect tutorial for you! </p> <p>Taking a quick look on Google Scholar we can see there is indeed plenty of published papers on this topic (almost as if I planned it). Many of these Journals are high quality too! According to the ABDC Journal Rankings: </p> <ul> <li>Scottish Journal of Political Economy has rating of A </li> <li>The Economic Journal is A*</li> <li>Applied Economics is A*</li> </ul> <p></p> <p>However at this point I must stress to the reader that these papers are quite old and were mainly published in the early 2000s. Schwert (2003) suggests that many of the market anomalies, discovered in financial markets as a contradiction to market efficiency, disappear once they have been published in academic literature. So, although these researchers may have found the existence of a favourite-longshot bias in betting markets in the early 2000s, they likely no longer exist as the publication of their findings leads to an increase in market efficiency.</p> <p>Schwert, G.W., 2003. Anomalies and market efficiency. Handbook of the Economics of Finance, 1, pp.939-974.</p> <p>This is basically always the same</p> <pre><code># Import libraries for logging in\nimport betfairlightweight\nfrom flumine import Flumine, clients\n\n# Credentials to login and logging in \ntrading = betfairlightweight.APIClient('username','password',app_key='appkey')\nclient = clients.BetfairClient(trading, interactive_login=True)\n\n# Login\nframework = Flumine(client=client)\n\n# Code to login when using security certificates\n# trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs')\n# client = clients.BetfairClient(trading)\n\n# framework = Flumine(client=client)\n</code></pre> <p>This is where the fun begins!</p> <p>If you haven't already, read through How to Automate I: Understanding Flumine to get a grasp on what the general code structure of Flumine looks like. The basic code structure will always be the same, but we will tailor it to whatever strategy we are trying to run.</p> <p>We have a few basic requirements for this strategy:</p> <ul> <li>We want to find what horse is the favourite / second favourite / third favourite and so on</li> <li>We want to place a bet on that horse</li> <li>We want to only place a single bet on that horse and on the market in general</li> </ul> <p>Hold up, how do we decide which horse is the favourite? </p> <p>There are two things we must consider here:</p> <ol> <li>What price should we be using to determine if a horse is the favourite?<ul> <li>Back price</li> <li>Lay price</li> <li>Some sort of mid point</li> <li>Last traded price</li> <li>Some combination??</li> </ul> </li> <li>What point in time should we reference the price to determine the favourite?<ul> <li>10 mins before the jump</li> <li>5 mins before the jump</li> <li>30 secs before the jump??</li> </ul> </li> </ol> <p>These decisions can make huge differences, and it's up to you to do your own analysis and decided which is best.</p> <p>For this example, we will keep it simple and place a Back bet on the favourite, and use the last traded price, 60 seconds before the scheduled jump time</p> <p>Now that we have decided on using the last traded price for each horse, we actually need a way to find this in real time. Luckily, the Betfair documentation provides everything we need.</p> <p>In <code>Betfairlightweight</code> and <code>Flumine</code> wrapper for the Betfair API, <code>runner</code> has the attribute <code>last_price_traded</code>, so to access it we can simply call <code>runner.last_price_traded</code>. Looping through all the runners in a race we can collect the <code>last_price_traded</code> for all runners and then convert that into a dataframe:</p> <pre><code>snapshot_last_price_traded = []\nfor runner in market_book.runners:\n        snapshot_last_price_traded.append([runner.selection_id,runner.last_price_traded])\n\nsnapshot_last_price_traded = pd.DataFrame(snapshot_last_price_traded, columns=['selection_id','last_traded_price'])\n</code></pre> <p>So, we end up with a DataFrame like this: </p> <p></p> <p>Let's sort the rows by the <code>last_traded_price</code> and define a new variable called the <code>fav_selection_id</code> which is the <code>selection_id</code> of the horse we want to bet on (first favourite horse). </p> <p>To get the corresponding <code>selection_id</code> we need to select the value from the first row of the <code>selection_id</code> column. As Python starts indexing rows from 0, instead of 1, we will be selecting the 0th index to get the value of the first row. (If your strategy was backing/laying the second favourite you will need to be selecting the 1st index)</p> <pre><code>snapshot_last_price_traded = snapshot_last_price_traded.sort_values(by = ['last_traded_price'])\nfav_selection_id = snapshot_last_price_traded['selection_id'].iloc[0]\n</code></pre> <p></p> <p>We also need a way to validate bets so that we don't bet on multiple selections. This is because process_market_book will run every time anyone places or cancels a bet on that market. So we need to prevent multiple bet placements. This is usually easily done by setting <code>max_trade_count = 1</code> however, looking at the documentation we can see that this is only on a per selection basis. As the favourite horse can change over time then we may end up betting multiple times if the favourite changes. In most situations this probably isn't an issue, as you can just limit betting to a timeframe e.g., from 60 seconds before the jump to 50 seconds before the jump. However, if the market is very illiquid there may not be any bets placed or cancelled in the time frame. So, let's come up with a way that our strategy can only bet once without using the timeframe as a work around.</p> <p>According to the documentation for Flumine we can get the runner_context, which will allow us to collect info on the trades, matched and waiting to be matched. If we loop through all the runners in the market, we can turn that into a DataFrame:</p> <pre><code>runner_context = []\nfor runner in market_book.runners:\n        runner_context = self.get_runner_context(\n                    market.market_id, runner.selection_id, runner.handicap\n                )\n        snapshot_runner_context.append([runner_context.selection_id, runner_context.executable_orders, runner_context.live_trade_count, runner_context.trade_count])\nsnapshot_runner_context = pd.DataFrame(snapshot_runner_context, columns=['selection_id','executable_orders','live_trade_count','trade_count'])\n</code></pre> <p>This will return a DataFrame like this: </p> <p></p> <p>Now we can simply just check if the sum last 3 columns equal zero, to validate that no bets have been placed:</p> <pre><code>snapshot_runner_context.iloc[:,1:].sum().sum()\n</code></pre> <p>Let's put this all together and pepper in some logging so we know what's happening:</p> <pre><code># Import necessary libraries\nfrom flumine import BaseStrategy \nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder, OrderStatus\nfrom flumine.markets.market import Market\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\nimport pandas as pd\nimport numpy as np\n\n# Logging\nimport logging\nlogging.basicConfig(filename = 'how_to_automate_2.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(lineno)d:%(message)s')\n\nclass BackFavStrategy(BaseStrategy):\n\n    # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy\n    def start(self) -&amp;gt; None:\n        print(\"starting strategy 'BackFavStrategy'\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; bool:\n        # process_market_book only executed if this returns True\n        if market_book.status != \"CLOSED\":\n            return True\n\n    def process_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; None:\n\n        # Collect data on last price traded and the number of bets we have placed\n        snapshot_last_price_traded = []\n        snapshot_runner_context = []\n        for runner in market_book.runners:\n                snapshot_last_price_traded.append([runner.selection_id,runner.last_price_traded])\n                # Get runner context for each runner\n                runner_context = self.get_runner_context(\n                    market.market_id, runner.selection_id, runner.handicap\n                )\n                snapshot_runner_context.append([runner_context.selection_id, runner_context.executable_orders, runner_context.live_trade_count, runner_context.trade_count])\n\n        # Convert last price traded data to dataframe\n        snapshot_last_price_traded = pd.DataFrame(snapshot_last_price_traded, columns=['selection_id','last_traded_price'])\n        # Find the selection_id of the favourite\n        snapshot_last_price_traded = snapshot_last_price_traded.sort_values(by = ['last_traded_price'])\n        fav_selection_id = snapshot_last_price_traded['selection_id'].iloc[0]\n        logging.info(snapshot_last_price_traded) # logging\n\n        # Convert data on number of bets we have placed to a dataframe\n        snapshot_runner_context = pd.DataFrame(snapshot_runner_context, columns=['selection_id','executable_orders','live_trade_count','trade_count'])\n        logging.info(snapshot_runner_context) # logging\n\n        for runner in market_book.runners:\n            if runner.status == \"ACTIVE\" and market.seconds_to_start &amp;lt; 60 and market_book.inplay == False and runner.selection_id == fav_selection_id and snapshot_runner_context.iloc[:,1:].sum().sum() == 0:\n                trade = Trade(\n                    market_id=market_book.market_id,\n                    selection_id=runner.selection_id,\n                    handicap=runner.handicap,\n                    strategy=self,\n                )\n                order = trade.create_order(\n                    side=\"BACK\", order_type=LimitOrder(price=runner.last_price_traded, size=5)\n                )\n                market.place_order(order)\n</code></pre> <p>Now that we have our strategy ready, we can point it to a sport, and let it run. This time we will add some trading controls because we are likely to get matched. Let's specify that we are only comfortable with 1 bet at any time with a maximum exposure of $20</p> <pre><code>strategy = BackFavStrategy(\n    market_filter=streaming_market_filter(\n        event_type_ids=[\"4339\"], # Greyhounds\n        country_codes=[\"AU\"], # Australian Markets\n        market_types=[\"WIN\"], # Win Markets\n    ),\n    max_trade_count=1, # max total number of trades per runner\n    max_live_trade_count=1, # max live (with executable orders) trades per runner\n    max_selection_exposure=20, # max exposure of 20 per horse\n    max_order_exposure= 20 # Max bet sizes of $20\n)\n</code></pre> <p>Before we start running our strategy lets learn how the Flumine framework actually works. I basically glossed over this in Part I as its not entirely necessary to get something up and running, but in a more realistic sense its very useful and we now have a good strategy to test it on.</p> <p>When we log into using Flumine we define a <code>framework</code> which is a Flumine object: <pre><code>trading = betfairlightweight.APIClient('username','password',app_key='appkey')\nclient = clients.BetfairClient(trading, interactive_login=True)\nframework = Flumine(client=client)\n</code></pre></p> <p>When we run our strategies, we are actually running <code>framework</code>. You can think of <code>framework</code> as a video game character with nothing equiped, when we add different strategies its like equipping a weapon like a sword or bow. We can also add other things to help us out such as <code>LiveLoggingControl</code> (which we will learn to create a bit futher down), and an autoterminate function so we don't need to manually turn it off each day, this is like adding buffs or armour that can help our character to progress.</p> <p>We need to define the thing that we are adding onto our <code>framework</code> and then Flumine will have specific functions that allow us to equip our strategies and helper functions. For example to add a strategy we just need to do <code>add_strategy()</code>, the list of Flumine functions that you can use to add things to <code>framework</code> are available here. Once we have our character ready with strategies and help code equiped we can do <code>framework.run()</code> and that will run your <code>framework</code> with all the strategies and all the supporting supporting code attached.</p> <p></p> <pre><code>framework.add_strategy(strategy)\n</code></pre> <p>This code is a direct copy and paste from the examples and works like a charm out of the box. It runs every 60 seconds and checks if all markets starting today have been closed for at least 20 minutes. If it has then it will stop our automation.</p> <pre><code>import logging\nimport datetime\nfrom flumine.worker import BackgroundWorker\nfrom flumine.events.events import TerminationEvent\n\nlogger = logging.getLogger(__name__)\n\n\"\"\"\nWorker can be used as followed:\n    framework.add_worker(\n        BackgroundWorker(\n            framework,\n            terminate,\n            func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n            interval=60,\n            start_delay=60,\n        )\n    )\nThis will run every 60s and will terminate \nthe framework if all markets starting 'today' \nhave been closed for at least 1200s\n\"\"\"\n\n\n# Function that stops automation running at the end of the day\ndef terminate(\n    context: dict, flumine, today_only: bool = True, seconds_closed: int = 600\n) -&amp;gt; None:\n    \"\"\"terminate framework if no markets\n    live today.\n    \"\"\"\n\n    # Creates a list of all markets \n    markets = list(flumine.markets.markets.values())\n\n    # from the above list, create a list of markets that are expected to start today\n    markets_today = [\n        m\n        for m in markets\n        if m.market_start_datetime.date() == datetime.datetime.utcnow().date()\n        and (\n            m.elapsed_seconds_closed is None\n            or (m.elapsed_seconds_closed and m.elapsed_seconds_closed &amp;lt; seconds_closed)\n        )\n    ]\n\n    # counts the markets that are expected to start today\n    if today_only:\n        market_count = len(markets_today)\n    else:\n        market_count = len(markets)\n\n    # if the number of markets that are expected to start today then stop flumine \n    if market_count == 0:\n        logger.info(\"No more markets available, terminating framework\")\n        flumine.handler_queue.put(TerminationEvent(flumine))\n</code></pre> <p>Now that we have created our terminate function we have to add it to our framework</p> <pre><code># Add the auto terminate to our framework\nframework.add_worker(\n    BackgroundWorker(\n        framework,\n        terminate,\n        func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n        interval=60,\n        start_delay=60,\n    )\n)\n</code></pre> <p>Let's also create something that records our bets in a nice csv/excel file so we can review how we went later on. Although this is also called logging it will create a clean csv file called \"orders_hta_2.csv\" that looks like this:</p> <p></p> <p>This becomes super useful when we have more than one strategy so we can track how each strategy is performing (it also reads nicely into a pandas DataFrame!). This will also allow us to do some analysis such as check how often our bets get matched. This code is copied from the examples with some very slight changes.</p> <pre><code>import os\nimport csv\nimport logging\nfrom flumine.controls.loggingcontrols import LoggingControl\nfrom flumine.order.ordertype import OrderTypes\n\nlogger = logging.getLogger(__name__)\n\nFIELDNAMES = [\n    \"bet_id\",\n    \"strategy_name\",\n    \"market_id\",\n    \"selection_id\",\n    \"trade_id\",\n    \"date_time_placed\",\n    \"price\",\n    \"price_matched\",\n    \"size\",\n    \"size_matched\",\n    \"profit\",\n    \"side\",\n    \"elapsed_seconds_executable\",\n    \"order_status\",\n    \"market_note\",\n    \"trade_notes\",\n    \"order_notes\",\n]\n\n\nclass LiveLoggingControl(LoggingControl):\n    NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n    def __init__(self, *args, **kwargs):\n        super(LiveLoggingControl, self).__init__(*args, **kwargs)\n        self._setup()\n\n    # checks if the file \"orders_hta_2.csv\" already exists, if it doens't then create it\n    def _setup(self):\n        if os.path.exists(\"orders_hta_2.csv\"):\n            logging.info(\"Results file exists\")\n        else:\n            with open(\"orders_hta_2.csv\", \"w\") as m:\n                # Create orders_hta_2.csv with the first row as the FIELDNAMES we specified above\n                csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                csv_writer.writeheader()\n\n    def _process_cleared_orders_meta(self, event):\n        orders = event.event  # gives us a list of our orders for a market that has already settled\n        with open(\"orders_hta_2.csv\", \"a\") as m:  # open orders_hta_2.csv and append a new row of data (orders)\n            for order in orders:\n                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                    size = order.order_type.size\n                else:\n                    size = order.order_type.liability\n                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                    price = None\n                else:\n                    price = order.order_type.price\n                try:  # Create a dictionary of data we want to append to our csv file\n                    order_data = {\n                        \"bet_id\": order.bet_id,\n                        \"strategy_name\": order.trade.strategy,\n                        \"market_id\": order.market_id,\n                        \"selection_id\": order.selection_id,\n                        \"trade_id\": order.trade.id,\n                        \"date_time_placed\": order.responses.date_time_placed,\n                        \"price\": price,\n                        \"price_matched\": order.average_price_matched,\n                        \"size\": size,\n                        \"size_matched\": order.size_matched,\n                        \"profit\": 0 if not order.cleared_order else order.cleared_order.profit,\n                        \"side\": order.side,\n                        \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                        \"order_status\": order.status.value,\n                        \"market_note\": order.trade.market_notes,\n                        \"trade_notes\": order.trade.notes_str,\n                        \"order_notes\": order.notes_str,\n                    }\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)  # maps our dictionary to output rows\n                    csv_writer.writerow(order_data)  # append data to csv files\n                except Exception as e:\n                    logger.error(\n                        \"_process_cleared_orders_meta: %s\" % e,\n                        extra={\"order\": order, \"error\": e},\n                    )\n\n        logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n    def _process_cleared_markets(self, event):\n        cleared_markets = event.event\n        for cleared_market in cleared_markets.orders:\n            logger.info(\n                \"Cleared market\",\n                extra={\n                    \"market_id\": cleared_market.market_id,\n                    \"bet_count\": cleared_market.bet_count,\n                    \"profit\": cleared_market.profit,\n                    \"commission\": cleared_market.commission,\n                },\n            )\n</code></pre> <p>Now let's add the bet logging to our framework and run everything at once:</p> <pre><code>framework.add_logging_control(\n    LiveLoggingControl()\n)\n</code></pre> <pre><code>framework.run() \n</code></pre> <pre>\n<code>starting strategy 'BackFavStrategy'\n</code>\n</pre> <p>There are some other things you can add such as workers that run automatically in the background at set times (e.g. every 10 seconds) and are independent of market updates. There is some documentation available but to be honest I've never had to use them. If you are doing something that is a bit more intense or needs to be run at set time intervals then they are probably useful so take a look.</p> <p>We have so far done zero backtesting on this strategy, and blindly following strategies from published papers that are over 20 years old is a sure fire way to lose money. But hopefully this gives you an idea of the things you can accomplish with Flumine. If you are keen on backtesting your own betting angles, I would suggest taking a look at our tutorial which goes into depth on how to backtest Automated Betting Angles in Python using historical data.</p> <p>Now that we have a better understanding of Flumine we are getting very close to automating our own model. We still have three parts remaining in this series which will take you step by step through:</p> <ul> <li>Part III - Automating a Betfair model</li> <li>Part IV - Automating your own model</li> <li>Part V - How to simulate the Exchange to backtest and optimise our strategies</li> </ul> <p>Run the code from your ide by using py <code>&lt;filename&gt;</code>.py, making sure you amend the path to point to your input data.</p> <p>Download from Github </p> <pre><code># Import libraries for logging in\nimport betfairlightweight\nfrom flumine import Flumine, clients\nfrom flumine import BaseStrategy \nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder, OrderStatus\nfrom flumine.markets.market import Market\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\nimport pandas as pd\nimport numpy as np\nimport logging\nimport datetime\nfrom flumine.worker import BackgroundWorker\nfrom flumine.events.events import TerminationEvent\nimport os\nimport csv\nimport logging\nfrom flumine.controls.loggingcontrols import LoggingControl\nfrom flumine.order.ordertype import OrderTypes\n\nlogging.basicConfig(filename = 'how_to_automate_2.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(lineno)d:%(message)s')\n\n# Import libraries for logging in\nimport betfairlightweight\nfrom flumine import Flumine, clients\n\n# Credentials to login and logging in \ntrading = betfairlightweight.APIClient('username','password',app_key='appkey')\nclient = clients.BetfairClient(trading, interactive_login=True)\n\n# Login\nframework = Flumine(client=client)\n\n# Code to login when using security certificates\n# trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs')\n# client = clients.BetfairClient(trading)\n\n# framework = Flumine(client=client)\n\nclass BackFavStrategy(BaseStrategy):\n\n    # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy\n    def start(self) -&amp;gt; None:\n        print(\"starting strategy 'BackFavStrategy'\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; bool:\n        # process_market_book only executed if this returns True\n        if market_book.status != \"CLOSED\":\n            return True\n\n    def process_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; None:\n\n        # Collect data on last price traded and the number of bets we have placed\n        snapshot_last_price_traded = []\n        snapshot_runner_context = []\n        for runner in market_book.runners:\n                snapshot_last_price_traded.append([runner.selection_id,runner.last_price_traded])\n                # Get runner context for each runner\n                runner_context = self.get_runner_context(\n                    market.market_id, runner.selection_id, runner.handicap\n                )\n                snapshot_runner_context.append([runner_context.selection_id, runner_context.executable_orders, runner_context.live_trade_count, runner_context.trade_count])\n\n        # Convert last price traded data to dataframe\n        snapshot_last_price_traded = pd.DataFrame(snapshot_last_price_traded, columns=['selection_id','last_traded_price'])\n        # Find the selection_id of the favourite\n        snapshot_last_price_traded = snapshot_last_price_traded.sort_values(by = ['last_traded_price'])\n        fav_selection_id = snapshot_last_price_traded['selection_id'].iloc[0]\n        logging.info(snapshot_last_price_traded) # logging\n\n        # Convert data on number of bets we have placed to a dataframe\n        snapshot_runner_context = pd.DataFrame(snapshot_runner_context, columns=['selection_id','executable_orders','live_trade_count','trade_count'])\n        logging.info(snapshot_runner_context) # logging\n\n        for runner in market_book.runners:\n            if runner.status == \"ACTIVE\" and market.seconds_to_start &amp;lt; 60 and market_book.inplay == False and runner.selection_id == fav_selection_id and snapshot_runner_context.iloc[:,1:].sum().sum() == 0:\n                trade = Trade(\n                    market_id=market_book.market_id,\n                    selection_id=runner.selection_id,\n                    handicap=runner.handicap,\n                    strategy=self,\n                )\n                order = trade.create_order(\n                    side=\"BACK\", order_type=LimitOrder(price=runner.last_price_traded, size=5)\n                )\n                market.place_order(order)\n\nlogger = logging.getLogger(__name__)\n\n\"\"\"\nWorker can be used as followed:\n    framework.add_worker(\n        BackgroundWorker(\n            framework,\n            terminate,\n            func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n            interval=60,\n            start_delay=60,\n        )\n    )\nThis will run every 60s and will terminate \nthe framework if all markets starting 'today' \nhave been closed for at least 1200s\n\"\"\"\n\n# Function that stops automation running at the end of the day\ndef terminate(\n    context: dict, flumine, today_only: bool = True, seconds_closed: int = 600\n) -&amp;gt; None:\n    \"\"\"terminate framework if no markets\n    live today.\n    \"\"\"\n    markets = list(flumine.markets.markets.values())\n    markets_today = [\n        m\n        for m in markets\n        if m.market_start_datetime.date() == datetime.datetime.utcnow().date()\n        and (\n            m.elapsed_seconds_closed is None\n            or (m.elapsed_seconds_closed and m.elapsed_seconds_closed &amp;lt; seconds_closed)\n        )\n    ]\n    if today_only:\n        market_count = len(markets_today)\n    else:\n        market_count = len(markets)\n    if market_count == 0:\n        logger.info(\"No more markets available, terminating framework\")\n        flumine.handler_queue.put(TerminationEvent(flumine))\n\nFIELDNAMES = [\n    \"bet_id\",\n    \"strategy_name\",\n    \"market_id\",\n    \"selection_id\",\n    \"trade_id\",\n    \"date_time_placed\",\n    \"price\",\n    \"price_matched\",\n    \"size\",\n    \"size_matched\",\n    \"profit\",\n    \"side\",\n    \"elapsed_seconds_executable\",\n    \"order_status\",\n    \"market_note\",\n    \"trade_notes\",\n    \"order_notes\",\n]\n\nclass LiveLoggingControl(LoggingControl):\n    NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n    def __init__(self, *args, **kwargs):\n        super(LiveLoggingControl, self).__init__(*args, **kwargs)\n        self._setup()\n\n    # Changed file path and checks if the file orders_hta_2.csv already exists, if it doens't then create it\n    def _setup(self):\n        if os.path.exists(\"orders_hta_2.csv\"):\n            logging.info(\"Results file exists\")\n        else:\n            with open(\"orders_hta_2.csv\", \"w\") as m:\n                csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                csv_writer.writeheader()\n\n    def _process_cleared_orders_meta(self, event):\n        orders = event.event\n        with open(\"orders_hta_2.csv\", \"a\") as m:\n            for order in orders:\n                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                    size = order.order_type.size\n                else:\n                    size = order.order_type.liability\n                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                    price = None\n                else:\n                    price = order.order_type.price\n                try:\n                    order_data = {\n                        \"bet_id\": order.bet_id,\n                        \"strategy_name\": order.trade.strategy,\n                        \"market_id\": order.market_id,\n                        \"selection_id\": order.selection_id,\n                        \"trade_id\": order.trade.id,\n                        \"date_time_placed\": order.responses.date_time_placed,\n                        \"price\": price,\n                        \"price_matched\": order.average_price_matched,\n                        \"size\": size,\n                        \"size_matched\": order.size_matched,\n                        \"profit\": 0 if not order.cleared_order else order.cleared_order.profit,\n                        \"side\": order.side,\n                        \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                        \"order_status\": order.status.value,\n                        \"market_note\": order.trade.market_notes,\n                        \"trade_notes\": order.trade.notes_str,\n                        \"order_notes\": order.notes_str,\n                    }\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                    csv_writer.writerow(order_data)\n                except Exception as e:\n                    logger.error(\n                        \"_process_cleared_orders_meta: %s\" % e,\n                        extra={\"order\": order, \"error\": e},\n                    )\n\n        logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n    def _process_cleared_markets(self, event):\n        cleared_markets = event.event\n        for cleared_market in cleared_markets.orders:\n            logger.info(\n                \"Cleared market\",\n                extra={\n                    \"market_id\": cleared_market.market_id,\n                    \"bet_count\": cleared_market.bet_count,\n                    \"profit\": cleared_market.profit,\n                    \"commission\": cleared_market.commission,\n                },\n            )\n\nstrategy = BackFavStrategy(\n    market_filter=streaming_market_filter(\n        event_type_ids=[\"4339\"], # Greyhounds\n        country_codes=[\"AU\"], # Australian Markets\n        market_types=[\"WIN\"], # Win Markets\n    ),\n    max_trade_count=1, # max total number of trades per runner\n    max_live_trade_count=1, # max live (with executable orders) trades per runner\n    max_selection_exposure=20, # max exposure of 20 per horse\n    max_order_exposure= 20 # Max bet sizes of $20\n)\n\nframework.add_strategy(strategy)\n\n# Add the auto terminate to our framework\nframework.add_worker(\n    BackgroundWorker(\n        framework,\n        terminate,\n        func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n        interval=60,\n        start_delay=60,\n    )\n)\n\nframework.add_logging_control(\n    LiveLoggingControl()\n)\n\nframework.run() # run our framework\n</code></pre> <p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"tutorials/How_to_Automate_2/#how-to-automate-ii-backing-or-laying-the-1st2ndnth-favourite-shot","title":"How to Automate II: Backing or laying the 1st/2nd/.../nth favourite shot","text":""},{"location":"tutorials/How_to_Automate_2/#context","title":"Context","text":""},{"location":"tutorials/How_to_Automate_2/#login","title":"Login","text":""},{"location":"tutorials/How_to_Automate_2/#creating-our-strategy","title":"Creating our strategy","text":""},{"location":"tutorials/How_to_Automate_2/#formulating-our-strategy","title":"Formulating our strategy","text":""},{"location":"tutorials/How_to_Automate_2/#implementing-our-strategy","title":"Implementing our strategy","text":""},{"location":"tutorials/How_to_Automate_2/#running-our-strategy","title":"Running our strategy","text":""},{"location":"tutorials/How_to_Automate_2/#automatic-terminate","title":"Automatic Terminate","text":""},{"location":"tutorials/How_to_Automate_2/#conclusion-and-next-steps","title":"Conclusion and next steps","text":""},{"location":"tutorials/How_to_Automate_2/#complete-code","title":"Complete code","text":""},{"location":"tutorials/How_to_Automate_2/#disclaimer","title":"Disclaimer","text":""},{"location":"tutorials/How_to_Automate_3/","title":"How to Automate 3","text":"<p>This tutorial is Part three of the How to Automate series and follows on logically from the How to Automate II tutorial we shared previously. If you're still new to Flumine we suggest you take a look at part one and part two before diving into this one. The overall goal of the How to Automate series was to learn how to use Flumine to Automate our own Betting Model. So far we have covered how Flumine works, lets now put it into action with one of Betfair's own data science models which will give us a solid foundation for part four where we automate our own model. </p> <p>As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements!</p> <p>Betfair\u2019s own Data Scientists have created a few inhouse models which produce ratings that you can use as racing and sporting tips. In our previous monthly meet ups, we learnt how to Backtest these models ourselves, now we will learn how actually automate those models! We will be automating the thoroughbred ratings model and the greyhound's model, but you can easily apply this to any model and if you have a specific angle you want to automate, you can just copy &amp; paste our code and modify it to your liking!</p> <p>Let's quickly scrape the ratings that we want to automate, all of the Betfair Data Science models are found on the Betfair Hub Models page. If we follow the links to the Horse Ratings Model and the Greyhounds Ratings Model we find that URL links behind the ratings download buttons have a consistent URL pattern that is easy to scrape.</p> <p></p> <p></p> <p>You end up with a link like this for the horse ratings model:</p> <p><code>https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date=2022-03-09&amp;presenter=RatingsPresenter&amp;csv=true</code></p> <p>and one like this for the greyhounds ratings model:</p> <p><code>https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=2022-03-09&amp;presenter=RatingsPresenter&amp;csv=true</code></p> <p>We can take advantage of this consistency and use some simple python code to scrape all the ratings into a pandas dataframe by simply changing the date in the url. </p> <pre><code>import pandas as pd\n</code></pre> <pre><code># Thoroughbred model (named the kash-ratings-model)\nkash_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date='\nkash_url_2 = pd.Timestamp.now().strftime(\"%Y-%m-%d\") # todays date formatted as YYYY-mm-dd\nkash_url_3 = '&amp;amp;presenter=RatingsPresenter&amp;amp;csv=true'\n\nkash_url = kash_url_1 + kash_url_2 + kash_url_3\nkash_url\n</code></pre> <pre>\n<code>'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date=2022-02-25&amp;presenter=RatingsPresenter&amp;csv=true'</code>\n</pre> <pre><code># Greyhounds model (named the iggy-joey-model)\niggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date='\niggy_url_2 = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\niggy_url_3 = '&amp;amp;presenter=RatingsPresenter&amp;amp;csv=true'\n\niggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3\niggy_url\n</code></pre> <pre>\n<code>'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=2022-02-25&amp;presenter=RatingsPresenter&amp;csv=true'</code>\n</pre> <p>Now that we have the URL, that dynamically changes to the current date, let's scrape the data using pandas and do some quick cleaning. We only really need three variables, the <code>market_id</code> so we know what market to bet on, the <code>selection_id</code> to know which horse/dog the rating is referring to and the most important one the <code>rating</code>.</p> <pre><code># Download todays thoroughbred ratings\nkash_df = pd.read_csv(kash_url)\n\n## Data clearning\n# Rename Columns\nkash_df = kash_df.rename(columns={\"meetings.races.bfExchangeMarketId\":\"market_id\",\"meetings.races.runners.bfExchangeSelectionId\":\"selection_id\",\"meetings.races.runners.ratedPrice\":\"rating\"})\n# Only keep columns we need\nkash_df = kash_df[['market_id','selection_id','rating']]\n# Convert market_id to string\nkash_df['market_id'] = kash_df['market_id'].astype(str)\nkash_df\n</code></pre> market_id selection_id rating 0 1.195173067 4218988 210.17 1 1.195173067 28551850 8.32 2 1.195173067 37261788 6.24 3 1.195173067 20357386 5.60 4 1.195173067 20466004 8.14 ... ... ... ... 442 1.195174127 43035476 4.68 443 1.195174127 27077942 188.93 444 1.195174127 26735524 16.85 445 1.195174127 21661954 192.68 446 1.195174127 24265033 13.85 <p>447 rows \u00d7 3 columns</p> <p>Let's also set up the Dataframe so that we can easily get our Rating when given <code>market_id</code> and <code>selection_id</code>:</p> <pre><code># Set market_id and selection_id as index for easy referencing\nkash_df = kash_df.set_index(['market_id','selection_id'])\nkash_df\n\n# e.g. can reference like this: \n    # df.loc['1.195173067'].loc['4218988']\n    # to return 210.17\n</code></pre> rating market_id selection_id 1.195173067 4218988 210.17 28551850 8.32 37261788 6.24 20357386 5.60 20466004 8.14 ... ... ... 1.195174127 43035476 4.68 27077942 188.93 26735524 16.85 21661954 192.68 24265033 13.85 <p>447 rows \u00d7 1 columns</p> <p>Let's do the same thing for the greyhound model</p> <pre><code># Download todays greyhounds ratings\niggy_df = pd.read_csv(iggy_url)\n\n## Data clearning\n# Rename Columns\niggy_df = iggy_df.rename(columns={\"meetings.races.bfExchangeMarketId\":\"market_id\",\"meetings.races.runners.bfExchangeSelectionId\":\"selection_id\",\"meetings.races.runners.ratedPrice\":\"rating\"})\n# Only keep columns we need\niggy_df = iggy_df[['market_id','selection_id','rating']]\n# Convert market_id to string\niggy_df['market_id'] = iggy_df['market_id'].astype(str)\niggy_df\n\n# Set market_id and selection_id as index for easy referencing\niggy_df = iggy_df.set_index(['market_id','selection_id'])\niggy_df\n</code></pre> rating market_id selection_id 1.195241412 43066395 12.80 43066396 12.17 43066397 11.10 43066398 5.85 42256274 2.85 ... ... ... 1.195242071 42424961 7.73 42588368 18.47 35695588 7.33 42334315 7.14 38830169 41.51 <p>960 rows \u00d7 1 columns</p> <p>Now that we have all of our ratings in a nice clean DataFrame we can easily automate them in Flumine.</p> <pre><code># Import libraries for logging in\nimport betfairlightweight\nfrom flumine import Flumine, clients\n\n# Credentials to login and logging in \ntrading = betfairlightweight.APIClient('username','password',app_key='appkey')\nclient = clients.BetfairClient(trading, interactive_login=True)\n\n# Login\nframework = Flumine(client=client)\n\n# Code to login when using security certificates\n# trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs')\n# client = clients.BetfairClient(trading)\n\n# framework = Flumine(client=client)\n</code></pre> <p>This is where you can come up with exciting and innovative strategies!</p> <p>But because we are lame, let's start off with a simple strategy that checks prices 60 seconds before the jump and Backs anything greater than our ratings and Lays anything less than our ratings all with a flat stake size of $5</p> <p>If you get confused about the code structure for how Flumine and BaseStrategy works, be sure to take a check out How to Automate I as it dives right into how to use Flumine.</p> <p>Our main logic is:</p> <ul> <li>Check how far out from the jump we are</li> <li>If we are within 60 seconds from the jump check each horse's market prices<ul> <li>If best Back price &gt; model price then place a $5 Back bet</li> <li>If best Lay price &lt; model price then place a $5 Lay bet</li> </ul> </li> </ul> <pre><code># Import libraries and logging\nfrom flumine import BaseStrategy \nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder, OrderStatus\nfrom flumine.markets.market import Market\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\nimport pandas as pd\nimport logging\n\n# Will create a file called how_to_automate_3.log in our current working directory\nlogging.basicConfig(filename = 'how_to_automate_3.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n\n\n# New strategy called FlatKashModel for the Thoroughbreds model\nclass FlatKashModel(BaseStrategy):\n\n    def start(self) -&amp;gt; None:\n        print(\"starting strategy 'FlatKashModel'\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; bool:\n        # process_market_book only executed if this returns True\n        if market_book.status != \"CLOSED\":\n            return True\n\n    # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially\n    # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again\n    def process_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; None:\n        # If time is less than 1min and we haven't placed a bet yet then look at our ratings and place bet\n        if market.seconds_to_start &amp;lt; 60 and market_book.inplay == False:\n            for runner in market_book.runners:\n                # Check runner hasn't scratched and that first layer of back or lay price exists\n                if runner.status == \"ACTIVE\" and runner.ex.available_to_back[0] and runner.ex.available_to_lay[0]:\n                    # If best available to back price is &amp;gt; rated price then flat $5 bet\n                    if runner.ex.available_to_back[0]['price'] &amp;gt; kash_df.loc[market_book.market_id].loc[runner.selection_id].item():\n                        trade = Trade(\n                        market_id=market_book.market_id,\n                        selection_id=runner.selection_id,\n                        handicap=runner.handicap,\n                        strategy=self,\n                        )\n                        order = trade.create_order(\n                            side=\"BACK\", order_type=LimitOrder(price=runner.ex.available_to_back[0]['price'], size=5.00)\n                        )\n                        market.place_order(order)\n                    # If best available to lay price is &amp;lt; rated price then flat $5 lay\n                    if runner.ex.available_to_lay[0]['price'] &amp;lt; kash_df.loc[market_book.market_id].loc[runner.selection_id].item():\n                        trade = Trade(\n                        market_id=market_book.market_id,\n                        selection_id=runner.selection_id,\n                        handicap=runner.handicap,\n                        strategy=self,\n                        )\n                        order = trade.create_order(\n                            side=\"LAY\", order_type=LimitOrder(price=runner.ex.available_to_lay[0]['price'], size=5.00)\n                        )\n                        market.place_order(order)\n</code></pre> <p>Let's create the same strategy for the Greyhounds model. The only difference here is that when we reference the model price, we are referencing the DataFrame iggy_df instead of kash_df. (An alternate way we could do this if we wanted would be to combine both strategies together and just append the dataframes together, but lets just keep it nice and simple for now)</p> <pre><code># New strategy called FlatIggyModel for the Greyhound model\nclass FlatIggyModel(BaseStrategy):\n\n    def start(self) -&amp;gt; None:\n        print(\"starting strategy 'FlatIggyModel'\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; bool:\n        # process_market_book only executed if this returns True\n        if market_book.status != \"CLOSED\":\n            return True\n\n    # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially\n    # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again\n    def process_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; None:\n\n        # If time is less than 1min and we haven't placed a bet yet then look at our ratings and place bet\n        if market.seconds_to_start &amp;lt; 60 and market_book.inplay == False:\n            for runner in market_book.runners:\n                # Check runner hasn't scratched and that first layer of back or lay price exists\n                if runner.status == \"ACTIVE\" and runner.ex.available_to_back[0] and runner.ex.available_to_lay[0]:\n                    # If best available to back price is &amp;gt; rated price then flat $5 bet\n                    if runner.ex.available_to_back[0]['price'] &amp;gt; iggy_df.loc[market_book.market_id].loc[runner.selection_id].item():\n                        trade = Trade(\n                        market_id=market_book.market_id,\n                        selection_id=runner.selection_id,\n                        handicap=runner.handicap,\n                        strategy=self,\n                        )\n                        order = trade.create_order(\n                            side=\"BACK\", order_type=LimitOrder(price=runner.ex.available_to_back[0]['price'], size=5.00)\n                        )\n                        market.place_order(order)\n                    # If best available to lay price is &amp;lt; rated price then flat $5 lay\n                    if runner.ex.available_to_lay[0]['price'] &amp;lt; iggy_df.loc[market_book.market_id].loc[runner.selection_id].item():\n                        trade = Trade(\n                        market_id=market_book.market_id,\n                        selection_id=runner.selection_id,\n                        handicap=runner.handicap,\n                        strategy=self,\n                        )\n                        order = trade.create_order(\n                            side=\"LAY\", order_type=LimitOrder(price=runner.ex.available_to_lay[0]['price'], size=5.00)\n                        )\n                        market.place_order(order)\n</code></pre> <p>Now that we have created two strategies we need to add both to the frame work along with the auto-terminate and bet logging we made in How to Automate II</p> <pre><code>thoroughbreds_strategy = FlatKashModel(\n    market_filter=streaming_market_filter(\n        event_type_ids=[\"7\"], # Horse Racing\n        country_codes=[\"AU\"], # Australian Markets\n        market_types=[\"WIN\"], # Win Markets \n    ),\n    max_order_exposure= 50, # Max bet sizes of $50\n    max_trade_count=1, # Max of trade/bet attempt per selection\n    max_live_trade_count=1, # Max of 1 unmatched Bet per selection\n)\n\ngreyhounds_strategy = FlatIggyModel(\n    market_filter=streaming_market_filter(\n        event_type_ids=[\"4339\"], # Greyhound Racing\n        country_codes=[\"AU\"], # Australian Markets\n        market_types=[\"WIN\"], # Win Markets\n    ),\n    max_order_exposure= 50, # Max bet sizes of $50\n    max_trade_count=1, # Max of trade/bet attempt per selection\n    max_live_trade_count=1, # Max of 1 unmatched Bet per selection\n)\n\nframework.add_strategy(thoroughbreds_strategy) # Add horse racing strategy to our framework\nframework.add_strategy(greyhounds_strategy) # Add greyhound racing strategy to our framework\n</code></pre> <pre><code># import logging\nimport datetime\nfrom flumine.worker import BackgroundWorker\nfrom flumine.events.events import TerminationEvent\n\n# logger = logging.getLogger(__name__)\n\n\"\"\"\nWorker can be used as followed:\n    framework.add_worker(\n        BackgroundWorker(\n            framework,\n            terminate,\n            func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n            interval=60,\n            start_delay=60,\n        )\n    )\nThis will run every 60s and will terminate \nthe framework if all markets starting 'today' \nhave been closed for at least 1200s\n\"\"\"\n\n\n# Function that stops automation running at the end of the day\ndef terminate(\n    context: dict, flumine, today_only: bool = True, seconds_closed: int = 600\n) -&amp;gt; None:\n    \"\"\"terminate framework if no markets\n    live today.\n    \"\"\"\n    markets = list(flumine.markets.markets.values())\n    markets_today = [\n        m\n        for m in markets\n        if m.market_start_datetime.date() == datetime.datetime.utcnow().date()\n        and (\n            m.elapsed_seconds_closed is None\n            or (m.elapsed_seconds_closed and m.elapsed_seconds_closed &amp;lt; seconds_closed)\n        )\n    ]\n    if today_only:\n        market_count = len(markets_today)\n    else:\n        market_count = len(markets)\n    if market_count == 0:\n        # logger.info(\"No more markets available, terminating framework\")\n        flumine.handler_queue.put(TerminationEvent(flumine))\n\n# Add the stopped to our framework\nframework.add_worker(\n    BackgroundWorker(\n        framework,\n        terminate,\n        func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n        interval=60,\n        start_delay=60,\n    )\n)\n</code></pre> <pre><code>import os\nimport csv\nimport logging\nfrom flumine.controls.loggingcontrols import LoggingControl\nfrom flumine.order.ordertype import OrderTypes\n\nlogger = logging.getLogger(__name__)\n\nFIELDNAMES = [\n    \"bet_id\",\n    \"strategy_name\",\n    \"market_id\",\n    \"selection_id\",\n    \"trade_id\",\n    \"date_time_placed\",\n    \"price\",\n    \"price_matched\",\n    \"size\",\n    \"size_matched\",\n    \"profit\",\n    \"side\",\n    \"elapsed_seconds_executable\",\n    \"order_status\",\n    \"market_note\",\n    \"trade_notes\",\n    \"order_notes\",\n]\n\n\nclass LiveLoggingControl(LoggingControl):\n    NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n    def __init__(self, *args, **kwargs):\n        super(LiveLoggingControl, self).__init__(*args, **kwargs)\n        self._setup()\n\n    # Changed file path and checks if the file orders_hta_3.csv already exists, if it doens't then create it\n    def _setup(self):\n        if os.path.exists(\"orders_hta_3.csv\"):\n            logging.info(\"Results file exists\")\n        else:\n            with open(\"orders_hta_3.csv\", \"w\") as m:\n                csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                csv_writer.writeheader()\n\n    def _process_cleared_orders_meta(self, event):\n        orders = event.event\n        with open(\"orders_hta_3.csv\", \"a\") as m:\n            for order in orders:\n                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                    size = order.order_type.size\n                else:\n                    size = order.order_type.liability\n                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                    price = None\n                else:\n                    price = order.order_type.price\n                try:\n                    order_data = {\n                        \"bet_id\": order.bet_id,\n                        \"strategy_name\": order.trade.strategy,\n                        \"market_id\": order.market_id,\n                        \"selection_id\": order.selection_id,\n                        \"trade_id\": order.trade.id,\n                        \"date_time_placed\": order.responses.date_time_placed,\n                        \"price\": price,\n                        \"price_matched\": order.average_price_matched,\n                        \"size\": size,\n                        \"size_matched\": order.size_matched,\n                        \"profit\": 0 if not order.cleared_order else order.cleared_order.profit,\n                        \"side\": order.side,\n                        \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                        \"order_status\": order.status.value,\n                        \"market_note\": order.trade.market_notes,\n                        \"trade_notes\": order.trade.notes_str,\n                        \"order_notes\": order.notes_str,\n                    }\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                    csv_writer.writerow(order_data)\n                except Exception as e:\n                    logger.error(\n                        \"_process_cleared_orders_meta: %s\" % e,\n                        extra={\"order\": order, \"error\": e},\n                    )\n\n        logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n    def _process_cleared_markets(self, event):\n        cleared_markets = event.event\n        for cleared_market in cleared_markets.orders:\n            logger.info(\n                \"Cleared market\",\n                extra={\n                    \"market_id\": cleared_market.market_id,\n                    \"bet_count\": cleared_market.bet_count,\n                    \"profit\": cleared_market.profit,\n                    \"commission\": cleared_market.commission,\n                },\n            )\n\nframework.add_logging_control(\n    LiveLoggingControl()\n)\n</code></pre> <pre><code>framework.run() # run all our strategies\n</code></pre> <pre>\n<code>starting strategy 'FlatKashModel'\nstarting strategy 'FlatIggyModel'\n</code>\n</pre> <p>There we have it. We now have a bot that you can turn on at the start of a day by hitting the run all button. It will automatically scrape all the data online, place bets throughout the day and at the end of the day stop itself.</p> <p>Then on the next day you can hit run all again, without needing to update anything.</p> <p>While Betfair's own Data Science models are easy to automate you're also not likely to become rich. They are also available to everyone freely online, so any edge is likely already priced in. </p> <p>That's why for the next part of this series we will be learning:</p> <ul> <li>Part IV - How to Automate your own model </li> <li>Part V - How to simulate the Exchange to backtest and optimise our strategies</li> </ul> <p>Run the code from your ide by using py <code>&lt;filename&gt;</code>.py, making sure you amend the path to point to your input data.</p> <p>Download from Github </p> <pre><code>import pandas as pd\nfrom flumine import BaseStrategy \nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder, OrderStatus\nfrom flumine.markets.market import Market\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\nimport logging\nimport betfairlightweight\nfrom flumine import Flumine, clients\nimport datetime\nfrom flumine.worker import BackgroundWorker\nfrom flumine.events.events import TerminationEvent\nimport os\nimport csv\nfrom flumine.controls.loggingcontrols import LoggingControl\nfrom flumine.order.ordertype import OrderTypes\n\n# Will create a file called how_to_automate_3.log in our current working directory\nlogging.basicConfig(filename = 'how_to_automate_3.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n\n# Thoroughbred model (named the kash-ratings-model)\nkash_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date='\nkash_url_2 = pd.Timestamp.now().strftime(\"%Y-%m-%d\") # todays date formatted as YYYY-mm-dd\nkash_url_3 = '&amp;amp;presenter=RatingsPresenter&amp;amp;csv=true'\n\nkash_url = kash_url_1 + kash_url_2 + kash_url_3\nkash_url\n\n# Greyhounds model (named the iggy-joey-model)\niggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date='\niggy_url_2 = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\niggy_url_3 = '&amp;amp;presenter=RatingsPresenter&amp;amp;csv=true'\n\niggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3\niggy_url\n\n# Download todays thoroughbred ratings\nkash_df = pd.read_csv(kash_url)\n\n## Data clearning\n# Rename Columns\nkash_df = kash_df.rename(columns={\"meetings.races.bfExchangeMarketId\":\"market_id\",\"meetings.races.runners.bfExchangeSelectionId\":\"selection_id\",\"meetings.races.runners.ratedPrice\":\"rating\"})\n# Only keep columns we need\nkash_df = kash_df[['market_id','selection_id','rating']]\n# Convert market_id to string\nkash_df['market_id'] = kash_df['market_id'].astype(str)\nkash_df\n\n# Set market_id and selection_id as index for easy referencing\nkash_df = kash_df.set_index(['market_id','selection_id'])\nkash_df\n\n# e.g. can reference like this: \n    # df.loc['1.195173067'].loc['4218988']\n    # to return 210.17\n\n# Download todays greyhounds ratings\niggy_df = pd.read_csv(iggy_url)\n\n## Data clearning\n# Rename Columns\niggy_df = iggy_df.rename(columns={\"meetings.races.bfExchangeMarketId\":\"market_id\",\"meetings.races.runners.bfExchangeSelectionId\":\"selection_id\",\"meetings.races.runners.ratedPrice\":\"rating\"})\n# Only keep columns we need\niggy_df = iggy_df[['market_id','selection_id','rating']]\n# Convert market_id to string\niggy_df['market_id'] = iggy_df['market_id'].astype(str)\niggy_df\n\n# Set market_id and selection_id as index for easy referencing\niggy_df = iggy_df.set_index(['market_id','selection_id'])\niggy_df\n\n# Import libraries for logging in\nimport betfairlightweight\nfrom flumine import Flumine, clients\n\n# Credentials to login and logging in \ntrading = betfairlightweight.APIClient('username','password',app_key='appkey')\nclient = clients.BetfairClient(trading, interactive_login=True)\n\n# Login\nframework = Flumine(client=client)\n\n# Code to login when using security certificates\n# trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs')\n# client = clients.BetfairClient(trading)\n\n# framework = Flumine(client=client)\n\n# Will create a file called how_to_automate_3.log in our current working directory\nlogging.basicConfig(filename = 'how_to_automate_3.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n\n# New strategy called FlatKashModel for the Thoroughbreds model\nclass FlatKashModel(BaseStrategy):\n\n    def start(self) -&amp;gt; None:\n        print(\"starting strategy 'FlatKashModel'\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; bool:\n        # process_market_book only executed if this returns True\n        if market_book.status != \"CLOSED\":\n            return True\n\n    # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially\n    # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again\n    def process_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; None:\n        # If time is less than 1min and we haven't placed a bet yet then look at our ratings and place bet\n        if market.seconds_to_start &amp;lt; 60 and market_book.inplay == False:\n            for runner in market_book.runners:\n                # Check runner hasn't scratched and that first layer of back or lay price exists\n                if runner.status == \"ACTIVE\" and runner.ex.available_to_back[0] and runner.ex.available_to_lay[0]:                     \n                    # If best available to back price is &amp;gt; rated price then flat $5 bet                \n                    if runner.ex.available_to_back[0]['price'] &amp;gt; kash_df.loc[market_book.market_id].loc[runner.selection_id].item():\n                        trade = Trade(\n                        market_id=market_book.market_id,\n                        selection_id=runner.selection_id,\n                        handicap=runner.handicap,\n                        strategy=self,\n                        )\n                        order = trade.create_order(\n                            side=\"BACK\", order_type=LimitOrder(price=runner.ex.available_to_back[0]['price'], size=5.00)\n                        )\n                        market.place_order(order)\n                    # If best available to lay price is &amp;lt; rated price then flat $5 lay\n                    if runner.ex.available_to_lay[0]['price'] &amp;lt; kash_df.loc[market_book.market_id].loc[runner.selection_id].item():\n                        trade = Trade(\n                        market_id=market_book.market_id,\n                        selection_id=runner.selection_id,\n                        handicap=runner.handicap,\n                        strategy=self,\n                        )\n                        order = trade.create_order(\n                            side=\"LAY\", order_type=LimitOrder(price=runner.ex.available_to_lay[0]['price'], size=5.00)\n                        )\n                        market.place_order(order)\n\n# New strategy called FlatIggyModel for the Greyhound model\nclass FlatIggyModel(BaseStrategy):\n\n    def start(self) -&amp;gt; None:\n        print(\"starting strategy 'FlatIggyModel'\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; bool:\n        # process_market_book only executed if this returns True\n        if market_book.status != \"CLOSED\":\n            return True\n\n    # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially\n    # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again\n    def process_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; None:\n\n        # If time is less than 1min and we haven't placed a bet yet then look at our ratings and place bet\n        if market.seconds_to_start &amp;lt; 60 and market_book.inplay == False:\n            for runner in market_book.runners:\n                # Check runner hasn't scratched and that first layer of back or lay price exists\n                if runner.status == \"ACTIVE\" and runner.ex.available_to_back[0] and runner.ex.available_to_lay[0]:                \n                # If best available to back price is &amp;gt; rated price then flat $5 bet\n                    if runner.ex.available_to_back[0]['price'] &amp;gt; iggy_df.loc[market_book.market_id].loc[runner.selection_id].item():\n                        trade = Trade(\n                        market_id=market_book.market_id,\n                        selection_id=runner.selection_id,\n                        handicap=runner.handicap,\n                        strategy=self,\n                        )\n                        order = trade.create_order(\n                            side=\"BACK\", order_type=LimitOrder(price=runner.ex.available_to_back[0]['price'], size=5.00)\n                        )\n                        market.place_order(order)\n                    # If best available to lay price is &amp;lt; rated price then flat $5 lay\n                    if runner.ex.available_to_lay[0]['price'] &amp;lt; iggy_df.loc[market_book.market_id].loc[runner.selection_id].item():\n                        trade = Trade(\n                        market_id=market_book.market_id,\n                        selection_id=runner.selection_id,\n                        handicap=runner.handicap,\n                        strategy=self,\n                        )\n                        order = trade.create_order(\n                            side=\"LAY\", order_type=LimitOrder(price=runner.ex.available_to_lay[0]['price'], size=5.00)\n                        )\n                        market.place_order(order)\n\nlogger = logging.getLogger(__name__)\n\n\"\"\"\nWorker can be used as followed:\n    framework.add_worker(\n        BackgroundWorker(\n            framework,\n            terminate,\n            func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n            interval=60,\n            start_delay=60,\n        )\n    )\nThis will run every 60s and will terminate \nthe framework if all markets starting 'today' \nhave been closed for at least 1200s\n\"\"\"\n\n# Function that stops automation running at the end of the day\ndef terminate(\n    context: dict, flumine, today_only: bool = True, seconds_closed: int = 600\n) -&amp;gt; None:\n    \"\"\"terminate framework if no markets\n    live today.\n    \"\"\"\n    markets = list(flumine.markets.markets.values())\n    markets_today = [\n        m\n        for m in markets\n        if m.market_start_datetime.date() == datetime.datetime.utcnow().date()\n        and (\n            m.elapsed_seconds_closed is None\n            or (m.elapsed_seconds_closed and m.elapsed_seconds_closed &amp;lt; seconds_closed)\n        )\n    ]\n    if today_only:\n        market_count = len(markets_today)\n    else:\n        market_count = len(markets)\n    if market_count == 0:\n        # logger.info(\"No more markets available, terminating framework\")\n        flumine.handler_queue.put(TerminationEvent(flumine))\n\nlogger = logging.getLogger(__name__)\n\nFIELDNAMES = [\n    \"bet_id\",\n    \"strategy_name\",\n    \"market_id\",\n    \"selection_id\",\n    \"trade_id\",\n    \"date_time_placed\",\n    \"price\",\n    \"price_matched\",\n    \"size\",\n    \"size_matched\",\n    \"profit\",\n    \"side\",\n    \"elapsed_seconds_executable\",\n    \"order_status\",\n    \"market_note\",\n    \"trade_notes\",\n    \"order_notes\",\n]\n\nclass LiveLoggingControl(LoggingControl):\n    NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n    def __init__(self, *args, **kwargs):\n        super(LiveLoggingControl, self).__init__(*args, **kwargs)\n        self._setup()\n\n    # Changed file path and checks if the file orders_hta_2.csv already exists, if it doens't then create it\n    def _setup(self):\n        if os.path.exists(\"orders_hta_3.csv\"):\n            logging.info(\"Results file exists\")\n        else:\n            with open(\"orders_hta_3.csv\", \"w\") as m:\n                csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                csv_writer.writeheader()\n\n    def _process_cleared_orders_meta(self, event):\n        orders = event.event\n        with open(\"orders_hta_3.csv\", \"a\") as m:\n            for order in orders:\n                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                    size = order.order_type.size\n                else:\n                    size = order.order_type.liability\n                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                    price = None\n                else:\n                    price = order.order_type.price\n                try:\n                    order_data = {\n                        \"bet_id\": order.bet_id,\n                        \"strategy_name\": order.trade.strategy,\n                        \"market_id\": order.market_id,\n                        \"selection_id\": order.selection_id,\n                        \"trade_id\": order.trade.id,\n                        \"date_time_placed\": order.responses.date_time_placed,\n                        \"price\": price,\n                        \"price_matched\": order.average_price_matched,\n                        \"size\": size,\n                        \"size_matched\": order.size_matched,\n                        \"profit\": 0 if not order.cleared_order else order.cleared_order.profit,\n                        \"side\": order.side,\n                        \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                        \"order_status\": order.status.value,\n                        \"market_note\": order.trade.market_notes,\n                        \"trade_notes\": order.trade.notes_str,\n                        \"order_notes\": order.notes_str,\n                    }\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                    csv_writer.writerow(order_data)\n                except Exception as e:\n                    logger.error(\n                        \"_process_cleared_orders_meta: %s\" % e,\n                        extra={\"order\": order, \"error\": e},\n                    )\n\n        logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n    def _process_cleared_markets(self, event):\n        cleared_markets = event.event\n        for cleared_market in cleared_markets.orders:\n            logger.info(\n                \"Cleared market\",\n                extra={\n                    \"market_id\": cleared_market.market_id,\n                    \"bet_count\": cleared_market.bet_count,\n                    \"profit\": cleared_market.profit,\n                    \"commission\": cleared_market.commission,\n                },\n            )\n\nthoroughbreds_strategy = FlatKashModel(\n    market_filter=streaming_market_filter(\n        event_type_ids=[\"7\"], # Horse Racing\n        country_codes=[\"AU\"], # Australian Markets\n        market_types=[\"WIN\"], # Win Markets \n    ),\n    max_order_exposure= 50, # Max bet sizes of $50\n    max_trade_count=1, # Max of trade/bet attempt per selection\n    max_live_trade_count=1, # Max of 1 unmatched Bet per selection\n)\n\ngreyhounds_strategy = FlatIggyModel(\n    market_filter=streaming_market_filter(\n        event_type_ids=[\"4339\"], # Greyhound Racing\n        country_codes=[\"AU\"], # Australian Markets\n        market_types=[\"WIN\"], # Win Markets\n    ),\n    max_order_exposure= 50, # Max bet sizes of $50\n    max_trade_count=1, # Max of trade/bet attempt per selection\n    max_live_trade_count=1, # Max of 1 unmatched Bet per selection\n)\n\nframework.add_strategy(thoroughbreds_strategy) # Add horse racing strategy to our framework\nframework.add_strategy(greyhounds_strategy) # Add greyhound racing strategy to our framework\n\n# Add the stopped to our framework\nframework.add_worker(\n    BackgroundWorker(\n        framework,\n        terminate,\n        func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n        interval=60,\n        start_delay=60,\n    )\n)\n\nframework.add_logging_control(\n    LiveLoggingControl()\n)\n\nframework.run() # run all our strategies\n</code></pre> <p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"tutorials/How_to_Automate_3/#how-to-automate-iii-betfair-data-scientists-models","title":"How to Automate III: Betfair Data Scientists Models","text":""},{"location":"tutorials/How_to_Automate_3/#context","title":"Context","text":""},{"location":"tutorials/How_to_Automate_3/#scrape-todays-model-ratings","title":"Scrape Today's Model Ratings","text":""},{"location":"tutorials/How_to_Automate_3/#automate-todays-model-ratings","title":"Automate Todays Model Ratings","text":""},{"location":"tutorials/How_to_Automate_3/#log-into-flumine","title":"Log into Flumine","text":""},{"location":"tutorials/How_to_Automate_3/#create-our-strategy-in-flumine","title":"Create Our Strategy in Flumine","text":""},{"location":"tutorials/How_to_Automate_3/#strategy-for-thoroughbreds","title":"Strategy for Thoroughbreds","text":""},{"location":"tutorials/How_to_Automate_3/#strategy-for-greyhounds","title":"Strategy for Greyhounds","text":""},{"location":"tutorials/How_to_Automate_3/#running-our-strategy","title":"Running our strategy","text":""},{"location":"tutorials/How_to_Automate_3/#conclusion-and-next-steps","title":"Conclusion and next steps","text":""},{"location":"tutorials/How_to_Automate_3/#complete-code","title":"Complete code","text":""},{"location":"tutorials/How_to_Automate_3/#disclaimer","title":"Disclaimer","text":""},{"location":"tutorials/How_to_Automate_4/","title":"How to Automate IV: Automate your own Model","text":"<p>For this tutorial we will be automating the model that Bruno taught us how to make in the Greyhound Modelling Tutorial. This tutorial follows on logically from How to Automate III. If you haven't already, make sure you take a look at the rest of the series first those before continuing here as they cover some key concepts!</p> <p>I've actually rewritten this post multiple times and refactored/changed the code many times, if you have any suggestions for improvements, do let me know. If you have followed this tutorial before, there is an archived  versions of this tutorial are available here</p> <p>Before we start let's think about what we are trying to achieve and build out a road map, our goal is to automate the model we built earlier. I've split it up into three main steps and a prerequisite step:</p> <ol> <li>Before automating anything we need to save the model that we built in Bruno's tutorial </li> <li>Download live data </li> <li>Clean our data and create our features</li> <li>Generate predictions and automate our model</li> </ol> <p>We will create three separate python scripts, one for each step. This will drastically reduce the complexity of our code and make things way easier to understand. We will be generating our model predictions about 1min before the scheduled start of the race, as this will allow us to account for things like scratchings, reserve dogs and we can confirm the box number. As always, the complete code will be available at the bottom of this post!</p> <p>(You might think that because we split it up into three scripts you will need to click run on the first script, wait for it to finish executing before going on the next one, but we will use a way to link all the scripts together later on, so you can just hit go once!)</p> <p>We will also create a separate fourth script that generates historical predictions which we can feed into our simulator for How to Automate V</p>"},{"location":"tutorials/How_to_Automate_4/#prerequisites","title":"Prerequisites","text":"<p>Before we even do anything here we need to save the model we learnt to build in the Greyhound modelling in Python tutorial, this is super simple as we can just need to run this extra line code (which I have copied from the documentation page) at the end of the notebook to save the model:</p> <pre><code>from joblib import dump\ndump(models['LogisticRegression'], 'logistic_regression.joblib')\n</code></pre> <p>If you are following the tutorial and have cloned the github repo you won't actually need to do this, as I have saved the model in the repo and you can read it in directly. (You may have to fiddle with your working directory)</p>"},{"location":"tutorials/How_to_Automate_4/#downloading-live-data","title":"Downloading live data","text":"<p>First off the bat is downloading live data i.e. data for today's races. My plan for this part is to reuse the original code that downloads/reads-in the historic data then also download the live data and add that on. Then we can save that as a csv (two: one for dog results and once for race details).</p> <p>Downloading/reading-in the historic data is super simple as most of the code is copied from Bruno's greyhound modelling tutorial (which I've heard is copied from the original FastTrack tutorial). However, the data is stored in a monthly format which if you're not careful can lead to missing data. I've changed the code to a daily format which fixes this issue. However, this means if you followed the old tutorial you will need to need to redownload the data (from my experience the change to daily format is so worth it!).</p> <p>I also have a pretty old and slow laptop and the pandas append method is being deprecated, so let's change it to the faster and more efficient concat method:</p> Downloading/reading-in historic data (code changes highlighted)<pre><code>    import os\n    import sys\n    import pandas as pd\n    from datetime import datetime\n    from dateutil.relativedelta import relativedelta\n\n    # Allow imports from src folder\n    module_path = os.path.abspath(os.path.join('src'))\n    if module_path not in sys.path:\n        sys.path.append(module_path)\n\n    import fasttrack as ft\n    from dotenv import load_dotenv\n    load_dotenv()\n\n    # Validate FastTrack API connection\n    api_key = os.getenv('FAST_TRACK_API_KEY',)\n    client = ft.Fasttrack(api_key)\n    track_codes = client.listTracks()\n\n    # Import race data excluding NZ races\n    au_tracks_filter = list(track_codes[track_codes['state'] != 'NZ']['track_code'])\n\n    # Time window to import data\n    # First day of the month 46 months back from now\n    date_from = (datetime.today() - relativedelta(months=46)).replace(day=1).strftime('%Y-%m-%d')\n    # Download historic data up untill yesterday\n    date_to = (datetime.today() - relativedelta(days=1)).strftime('%Y-%m-%d')\n\n    # List to populate data with, convert to dataframe after fully populated\n    race_details = []\n    dog_results = []\n\n    # Download/load historic (up until yesterday) data\n    # For each day, either fetch data from API or use local CSV file if we already have downloaded it\n    for start in pd.date_range(date_from, date_to, freq='d'):\n        start_date = start.strftime(\"%Y-%m-%d\")\n        end_date = start_date\n        try:\n            filename_races = f'FT_AU_RACES_{start_date}.csv'\n            filename_dogs = f'FT_AU_DOGS_{start_date}.csv'\n\n            filepath_races = f'../data/{filename_races}'\n            filepath_dogs = f'../data/{filename_dogs}'\n\n            print(f'Loading data from {start_date} to {end_date}')\n            if os.path.isfile(filepath_races):\n                # Load local CSV file\n                day_race_details = pd.read_csv(filepath_races) \n                day_dog_results = pd.read_csv(filepath_dogs) \n            else:\n                # Fetch data from API\n                day_race_details, day_dog_results = client.getRaceResults(start_date, end_date, au_tracks_filter)\n                day_race_details.to_csv(filepath_races, index=False)\n                day_dog_results.to_csv(filepath_dogs, index=False)\n\n            # Combine daily data\n            race_details.append(day_race_details)\n            dog_results.append(day_dog_results)\n        except:\n            print(f'Could not load data from {start_date} to {end_date}')\n</code></pre> <p>Now let's download the live data from the FastTrack API:</p> <pre><code>    # Download todays data from the api\n    todays_date = pd.Timestamp.now().strftime('%Y-%m-%d')\n    todays_race_details, todays_dog_results = client.getFullFormat(todays_date)\n</code></pre> <p>The live data from the API has a few extra columns that we don't need and a few columns with different names, lets fixed that and combine the live data with historic and save it to a csv. </p> <pre><code>    # Make live API data in the same form as historic data\n    todays_race_details = todays_race_details.rename(columns={\"Date\":\"date\"})\n    todays_dog_results = todays_dog_results.rename(columns={\"RaceBox\":\"Box\"})\n    # Only keep the columns we have in the historic data\n    usecols_race_details = ['@id','RaceNum','RaceName','RaceTime','Distance','RaceGrade','Track','date']\n    usecols_dog_results = ['@id','DogName','Box','RaceId','TrainerId','TrainerName']\n    # dog_results columns not in live data ['Place','Rug','Weight','StartPrice','Handicap','Margin1', 'Margin2','PIR','Checks','Comments','SplitMargin', 'RunTime', 'Prizemoney',]\n    todays_race_details = todays_race_details[usecols_race_details]\n    todays_dog_results = todays_dog_results[usecols_dog_results]\n\n    # Now that todays data looks similar to our historic data lets add todays data to the rest of our historic data\n    race_details.append(todays_race_details)\n    dog_results.append(todays_dog_results)\n    # Convert our data into a nice DataFrame\n    race_details = pd.concat(race_details)\n    dog_results = pd.concat(dog_results)\n\n    # Save our data to csv files\n    race_details.to_csv('../data/race_details.csv', index = False)\n    dog_results.to_csv('../data/dog_results.csv', index = False)\n    # Ready for data cleaning and feature creation\n</code></pre> <p>That's all the code for part 1 (how easy was that!), we are ready to create our features now.</p>"},{"location":"tutorials/How_to_Automate_4/#data-cleaning-and-feature-creation","title":"Data cleaning and feature creation","text":"<p>The second script gets everything ready to be fed into Flumine. So we will read in the data we downloaded in the first script, clean the data, create our features and set our DataFrame to be easily referenced like in How to Automate III.</p> <p>We need to clean our data in the same way that we did when training our model. The great thing is it means we can copy and paste most of our code from Bruno's tutorial. Initially, all the code is the same, except we are going to add one extra line of code, highlighted at the bottom to save our box win percentages to a csv file. </p> <p>The reason is we can't reliably determine the Box number from FastTrack data when there are  reserve dogs. E.g. some reserve dogs will have RaceBox/Box 9 or 10 and if there is a scratching they may start from Box 1/2/3..8 etc. Therefore we can't merge box win percentages to our DataFrame until we start hitting the API in the third script.</p> Data cleaning<pre><code>    import numpy as np\n    import pandas as pd\n    # settings to display all columns\n    pd.set_option(\"display.max_columns\", None)\n    from sklearn.preprocessing import MinMaxScaler\n    import itertools\n\n    race_details = pd.read_csv('../data/race_details.csv')\n    dog_results = pd.read_csv('../data/dog_results.csv')\n\n    ## Cleanse and normalise the data\n    # Clean up the race dataset\n    race_details = race_details.rename(columns = {'@id': 'FastTrack_RaceId'})\n    race_details['Distance'] = race_details['Distance'].apply(lambda x: int(x.replace(\"m\", \"\")))\n    race_details['date_dt'] = pd.to_datetime(race_details['date'], format = '%d %b %y')\n    # Clean up the dogs results dataset\n    dog_results = dog_results.rename(columns = {'@id': 'FastTrack_DogId', 'RaceId': 'FastTrack_RaceId'})\n\n    # Combine dogs results with race attributes\n    dog_results = dog_results.merge(\n        race_details, \n        how = 'left',\n        on = 'FastTrack_RaceId'\n    )\n\n    # Convert StartPrice to probability\n    dog_results['StartPrice'] = dog_results['StartPrice'].apply(lambda x: None if x is None else float(x.replace('$', '').replace('F', '')) if isinstance(x, str) else x)\n    dog_results['StartPrice_probability'] = (1 / dog_results['StartPrice']).fillna(0)\n    dog_results['StartPrice_probability'] = dog_results.groupby('FastTrack_RaceId')['StartPrice_probability'].apply(lambda x: x / x.sum())\n\n    # Discard entries without results (scratched or did not finish)\n    dog_results = dog_results[~dog_results['Box'].isnull()]\n    dog_results['Box'] = dog_results['Box'].astype(int)\n\n    # Clean up other attributes\n    dog_results['RunTime'] = dog_results['RunTime'].astype(float)\n    dog_results['SplitMargin'] = dog_results['SplitMargin'].astype(float)\n    dog_results['Prizemoney'] = dog_results['Prizemoney'].astype(float).fillna(0)\n    dog_results['Place'] = pd.to_numeric(dog_results['Place'].apply(lambda x: x.replace(\"=\", \"\") if isinstance(x, str) else 0), errors='coerce').fillna(0)\n    dog_results['win'] = dog_results['Place'].apply(lambda x: 1 if x == 1 else 0)\n\n    # Normalise some of the raw values\n    dog_results['Prizemoney_norm'] = np.log10(dog_results['Prizemoney'] + 1) / 12\n    dog_results['Place_inv'] = (1 / dog_results['Place']).fillna(0)\n    dog_results['Place_log'] = np.log10(dog_results['Place'] + 1).fillna(0)\n    dog_results['RunSpeed'] = (dog_results['RunTime'] / dog_results['Distance']).fillna(0)\n\n    ## Generate features using raw data\n    # Calculate median winner time per track/distance\n    win_results = dog_results[dog_results['win'] == 1]\n    median_win_time = pd.DataFrame(data=win_results[win_results['RunTime'] &gt; 0].groupby(['Track', 'Distance'])['RunTime'].median()).rename(columns={\"RunTime\": \"RunTime_median\"}).reset_index()\n    median_win_split_time = pd.DataFrame(data=win_results[win_results['SplitMargin'] &gt; 0].groupby(['Track', 'Distance'])['SplitMargin'].median()).rename(columns={\"SplitMargin\": \"SplitMargin_median\"}).reset_index()\n    median_win_time.head()\n\n    # Calculate track speed index\n    median_win_time['speed_index'] = (median_win_time['RunTime_median'] / median_win_time['Distance'])\n    median_win_time['speed_index'] = MinMaxScaler().fit_transform(median_win_time[['speed_index']])\n    median_win_time.head()\n\n    # Compare dogs finish time with median winner time\n    dog_results = dog_results.merge(median_win_time, on=['Track', 'Distance'], how='left')\n    dog_results = dog_results.merge(median_win_split_time, on=['Track', 'Distance'], how='left')\n\n    # Normalise time comparison\n    dog_results['RunTime_norm'] = (dog_results['RunTime_median'] / dog_results['RunTime']).clip(0.9, 1.1)\n    dog_results['RunTime_norm'] = MinMaxScaler().fit_transform(dog_results[['RunTime_norm']])\n    dog_results['SplitMargin_norm'] = (dog_results['SplitMargin_median'] / dog_results['SplitMargin']).clip(0.9, 1.1)\n    dog_results['SplitMargin_norm'] = MinMaxScaler().fit_transform(dog_results[['SplitMargin_norm']])\n    dog_results.head()\n\n    # Calculate box winning percentage for each track/distance\n    box_win_percent = pd.DataFrame(data=dog_results.groupby(['Track', 'Distance', 'Box'])['win'].mean()).rename(columns={\"win\": \"box_win_percent\"}).reset_index()\n    # Add to dog results dataframe\n    dog_results = dog_results.merge(box_win_percent, on=['Track', 'Distance', 'Box'], how='left')\n    # Display example of barrier winning probabilities\n    print(box_win_percent.head(8))\n    box_win_percent.to_csv('../data/box_win_percentage.csv')\n</code></pre> <p>As a single greyhound can be a reserve dog for multiple races on the same day this creates a second issue for us. FastTrack will list each race a greyhound is a reserve in as a new row. For example, 'MACI REID' is a reserve dog for three different races on the 2022-09-02:</p> <p></p> <p>When we try lag our data by using <code>.shift(1)</code> like in Bruno's original code it will produce the wrong values for our features. In the above example only the first race The Gardens Race 4 (the third row) will have correct data but all the rows under it will have incorrectly calculated features. We need each of the following rows to be the same as the third row. The solution that I have come up with is a little bit complicated, but it gets the job done:</p> Solution to reserve dogsOptimised codeComplete code chunk <pre><code>    # Please submit a pull request if you have a better solution\n    temp = rolling_result.reset_index()\n    temp = temp[temp['date_dt'] == pd.Timestamp.now().normalize()]\n    temp.groupby(['FastTrack_DogId','date_dt']).first()\n    rolling_result.loc[pd.IndexSlice[:, pd.Timestamp.now().normalize()], :] = temp.groupby(['FastTrack_DogId','date_dt']).first()\n</code></pre> <p>Because my laptop is old and slow lets also make the code a little bit more efficient</p> Old inefficient code<pre><code>    # Add features to dataset\n    dataset[agg_features_cols] = rolling_result\n</code></pre> Optimised code<pre><code>    # More efficient method to add features to dataset\n    rolling_result.columns = agg_features_cols\n    dataset = pd.concat([dataset,rolling_result],axis = 1)\n</code></pre> <p>So now our feature creation code looks like this:</p> Code changes highlighted<pre><code>    # Generate rolling window features\n    dataset = dog_results.copy()\n    dataset = dataset.set_index(['FastTrack_DogId', 'date_dt']).sort_index()\n\n    # Use rolling window of 28, 91 and 365 days\n    rolling_windows = ['28D', '91D', '365D']\n    # Features to use for rolling windows calculation\n    features = ['RunTime_norm', 'SplitMargin_norm', 'Place_inv', 'Place_log', 'Prizemoney_norm']\n    # Aggregation functions to apply\n    aggregates = ['min', 'max', 'mean', 'median', 'std']\n    # Keep track of generated feature names\n    feature_cols = ['speed_index', 'box_win_percent']\n\n    for rolling_window in rolling_windows:\n            print(f'Processing rolling window {rolling_window}')\n\n            rolling_result = (\n                dataset\n                .reset_index(level=0).sort_index()\n                .groupby('FastTrack_DogId')[features]\n                .rolling(rolling_window)\n                .agg(aggregates)\n                .groupby(level=0)  # Thanks to Brett for finding this!\n                .shift(1)\n            )\n\n            # My own dodgey code to work with reserve dogs\n            temp = rolling_result.reset_index()\n            temp = temp[temp['date_dt'] == pd.Timestamp.now().normalize()]\n            temp = temp.sort_index(axis=1)\n            rolling_result.loc[pd.IndexSlice[:, pd.Timestamp.now().normalize()], :] = temp.groupby(['FastTrack_DogId','date_dt']).first()\n\n            # Generate list of rolling window feature names (eg: RunTime_norm_min_365D)\n            agg_features_cols = [f'{f}_{a}_{rolling_window}' for f, a in itertools.product(features, aggregates)]\n\n            # More efficient method to add features to dataset\n            rolling_result.columns = agg_features_cols\n            dataset = pd.concat([dataset,rolling_result],axis = 1)\n\n            # Keep track of generated feature names\n            feature_cols.extend(agg_features_cols)\n\n    # print(feature_cols) \n    # feature_cols = ['speed_index', 'box_win_percent', 'RunTime_norm_min_28D', 'RunTime_norm_max_28D', 'RunTime_norm_mean_28D', 'RunTime_norm_median_28D', 'RunTime_norm_std_28D', 'SplitMargin_norm_min_28D', 'SplitMargin_norm_max_28D', 'SplitMargin_norm_mean_28D', 'SplitMargin_norm_median_28D', 'SplitMargin_norm_std_28D', 'Place_inv_min_28D', 'Place_inv_max_28D', 'Place_inv_mean_28D', 'Place_inv_median_28D', 'Place_inv_std_28D', 'Place_log_min_28D', 'Place_log_max_28D', 'Place_log_mean_28D', 'Place_log_median_28D', 'Place_log_std_28D', 'Prizemoney_norm_min_28D', 'Prizemoney_norm_max_28D', 'Prizemoney_norm_mean_28D', 'Prizemoney_norm_median_28D', 'Prizemoney_norm_std_28D', 'RunTime_norm_min_91D', 'RunTime_norm_max_91D', 'RunTime_norm_mean_91D', 'RunTime_norm_median_91D', 'RunTime_norm_std_91D', 'SplitMargin_norm_min_91D', 'SplitMargin_norm_max_91D', 'SplitMargin_norm_mean_91D', 'SplitMargin_norm_median_91D', 'SplitMargin_norm_std_91D', 'Place_inv_min_91D', 'Place_inv_max_91D', 'Place_inv_mean_91D', 'Place_inv_median_91D', 'Place_inv_std_91D', 'Place_log_min_91D', 'Place_log_max_91D', 'Place_log_mean_91D', 'Place_log_median_91D', 'Place_log_std_91D', 'Prizemoney_norm_min_91D', 'Prizemoney_norm_max_91D', 'Prizemoney_norm_mean_91D', 'Prizemoney_norm_median_91D', 'Prizemoney_norm_std_91D', 'RunTime_norm_min_365D', 'RunTime_norm_max_365D', 'RunTime_norm_mean_365D', 'RunTime_norm_median_365D', 'RunTime_norm_std_365D', 'SplitMargin_norm_min_365D', 'SplitMargin_norm_max_365D', 'SplitMargin_norm_mean_365D', 'SplitMargin_norm_median_365D', 'SplitMargin_norm_std_365D', 'Place_inv_min_365D', 'Place_inv_max_365D', 'Place_inv_mean_365D', 'Place_inv_median_365D', 'Place_inv_std_365D', 'Place_log_min_365D', 'Place_log_max_365D', 'Place_log_mean_365D', 'Place_log_median_365D', 'Place_log_std_365D', 'Prizemoney_norm_min_365D', 'Prizemoney_norm_max_365D', 'Prizemoney_norm_mean_365D', 'Prizemoney_norm_median_365D', 'Prizemoney_norm_std_365D']\n</code></pre> <p>Following Bruno's code, we have a few lines of cleaning left that we can just copy and paste over. But afterwards we also need to do some minor formatting changes to the FastTrack names so we can match them onto the Betfair names. Betfair excludes all apostrophes and full stops in their naming convention, so we'll create a Betfair equivalent dog name on the dataset removing these characters. We also need to do this for the tracks, sometimes FastTrack will name tracks differently to Betfair e.g., Sandown Park from Betfair is known as Sandown (SAP) in the FastTrack database. I've also included a line in there to save the dataframe <code>model_df</code>, so we can use it to generate historical predictions in our fourth script.</p> <pre><code>    # Replace missing values with 0\n    dataset.fillna(0, inplace=True)\n    print(dataset.head(8))\n\n    # Only keep data after 2018-12-01\n    model_df = dataset.reset_index()\n    feature_cols = np.unique(feature_cols).tolist()\n    model_df = model_df[model_df['date_dt'] &gt;= '2018-12-01']\n\n    # This line was originally part of Bruno's tutorial, but we don't run it in this script\n    # model_df = model_df[['date_dt', 'FastTrack_RaceId', 'DogName', 'win', 'StartPrice_probability'] + feature_cols]\n\n    # Only train model off of races where each dog has a value for each feature\n    races_exclude = model_df[model_df.isnull().any(axis = 1)]['FastTrack_RaceId'].drop_duplicates()\n    model_df = model_df[~model_df['FastTrack_RaceId'].isin(races_exclude)]\n    model_df.to_csv('../data/model_df.csv')  # Save data for part_4\n\n    # Select todays data and prepare data for easy reference in flumine\n    todays_data = model_df[model_df['date_dt'] == pd.Timestamp.now().strftime('%Y-%m-%d')]\n    todays_data['DogName_bf'] = todays_data['DogName'].apply(lambda x: x.replace(\"'\", \"\").replace(\".\", \"\").replace(\"Res\", \"\").strip())\n    final_data.replace('Sandown (SAP)','Sandown Park',inplace = True)\n    final_data.replace('Murray Bridge (MBR)','Murray Bridge',inplace = True)\n    todays_data = todays_data.set_index(['DogName_bf','Track','RaceNum'])\n    print(todays_data)\n\n    todays_data.to_csv('../data/todays_data.csv', index = True)\n</code></pre> <p>Now we are done with our second script too! We have cleaned all our data and generated all but one of our features (Box) and our data is saved and ready to be fed into Flumine. We are now ready to move onto the third script and start making live predictions and betting. We have even saved some historic data for our fourth script so we can generate historic predictions.</p>"},{"location":"tutorials/How_to_Automate_4/#automating-our-predictions","title":"Automating our predictions","text":"<p>Before we can do our model predictions, we have one feature left to collect which is the box number a greyhound is racing from. Then we can do our final prediction, renormalise our probabilities so they sum to unity (100%) and start placing bets. We will also add on our Auto-terminate and bet logging code we learnt from previous tutorials.</p> <p>If you look closely at the below screenshot, you will notice Rhinestone Ash is number 9. For non-reserve dogs (number one to eight), the number corresponds to the Box number, however reserve dogs will have a number such as 9 or 10 and there are only ever eight boxes. So, if any of the original line up is scratched the reserve dogs will be placed into one of the remaining boxes. FastTrack data won't give us which Box the reserve dog will go into, in fact it will give us Box 9 (which doesn't exist). So, we need some way to find the Box number it is actually racing from. I didn't notice this issue for quite a while, but the good thing is the website gives us the info we need to adjust:</p> <p></p> <p>If you click on rules, you can see what Box it is starting from:</p> <p></p> <p>After going through the documentation again, changes to boxes are available through the API under the <code>clarifications</code> attribute of <code>marketDescription</code>. You will be able to access this within Flumine as <code>market.market_catalogue.description.clarifications</code>, but it's a bit weird. It returns box changes as a string that looks like this:</p> <p></p> <p>Originally, I had planned to leave this post as it is since, I've never worked with anything like this before and its already getting pretty long, however huge shoutout to Brett from the Betfair Quants community who provided his solution to working with box changes.</p> Brett's Solution<pre><code>    from nltk.tokenize import regexp_tokenize\n    # my_string is an example string, that you will need to get live from the api via: market.market_catalogue.description.clarifications.replace(\"&lt;br&gt; Dog\",\"&lt;br&gt;Dog\")\n    my_string = \"&lt;br&gt;Box changes:&lt;br&gt;Dog 9. Tralee Blaze starts from box no. 8&lt;br&gt;&lt;br&gt;Dog 6. That Other One starts from box no. 2&lt;br&gt;&lt;br&gt;\"\n    print(f'HTML Comment: {my_string}')\n    pattern1 = r'(?&lt;=&lt;br&gt;Dog ).+?(?= starts)'\n    pattern2 = r\"(?&lt;=\\bbox no. )(\\w+)\"\n    runners_df = pd.DataFrame (regexp_tokenize(my_string, pattern1), columns = ['runner_name'])\n    runners_df['runner_name'] = runners_df['runner_name'].astype(str)\n    # Remove dog name from runner_number\n    runners_df['runner_number'] = runners_df['runner_name'].apply(lambda x: x[:(x.find(\" \") - 1)].upper())\n    # Remove dog number from runner_name\n    runners_df['runner_name'] = runners_df['runner_name'].apply(lambda x: x[(x.find(\" \") + 1):].upper())\n    runners_df['Box'] = regexp_tokenize(my_string, pattern2)\n    runners_df\n</code></pre> <p>Running Brett's code will give us a DataFrame like this:</p> <p></p> <p>So now, if there are no changes in Boxes we can default to the FastTrack data, and if there are changes, we can replace Box in our original DataFrame (<code>todays_data</code>) with the info from <code>runners_df</code> and then we can merge our <code>box_win_percentage</code> feature across.</p> <pre><code>    # Replace any old Box info in our original dataframe with data available in runners_df\n    runners_df = runners_df.set_index('runner_name')\n    todays_data.loc[(runners_df.index[runners_df.index.isin(dog_names)],track,race_number),'Box'] = runners_df.loc[runners_df.index.isin(dog_names),'Box'].to_list()\n    # Merge box_win_percentage back on:\n    todays_data = todays_data.drop(columns = 'box_win_percentage', axis = 1)\n    todays_data = todays_data.reset_index().merge(box_win_percent, on = ['Track', 'Distance','Box'], how = 'left').set_index(['DogName_bf','Track','RaceNum'])\n</code></pre> <p>Since we are now editing our todays_data dataframe inside our Flumine strategy we will also need to convert todays_data to a global variable which is a simple one liner:</p> <pre><code>   global todays_data\n</code></pre> <p>Now that we have all our features collected, we can predict our probabilities, renormalise and convert them to ratings</p> <pre><code>    # Generate probabilities using Bruno's model\n    todays_data.loc[(dog_names,track,race_number),'prob_LogisticRegression'] = brunos_model.predict_proba(todays_data.loc[(dog_names,track,race_number)][feature_cols])[:,1]\n    # renomalise probabilities\n    probabilities = todays_data.loc[dog_names,track,race_number]['prob_LogisticRegression']\n    todays_data.loc[(dog_names,track,race_number),'renormalised_prob'] = probabilities/probabilities.sum()\n    # convert probabilities to ratings\n    todays_data.loc[(dog_names,track,race_number),'rating'] = 1/todays_data.loc[dog_names,track,race_number]['renormalised_prob']\n</code></pre> <p>Now that we have our data nicely set up. We can reference our probabilities by getting the DogName, Track and RaceNum from the Betfair polling API and after that the rest is the same as How to Automate III.</p> <p>I also wanted to call out one gotcha that, Brett found that is almost impossible to find unless you are keeping a close eye on your logs. Sometimes the polling API and streaming API doesn't match up when there are scratchings, so we need to check if it does:</p> <pre><code>    # Check the polling API and streaming API matches up (sometimes it doesn't)\n    if runner_cata.selection_id == runner.selection_id:\n</code></pre> <p>Okay, so I've just walked through different parts of code that I am introducing, lets bring it all together now.</p> <p>Import our libraries, logging in and reading in our model and data:</p> <pre><code>from flumine import Flumine, clients\nfrom flumine import BaseStrategy \nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder\nfrom flumine.markets.market import Market\nimport betfairlightweight\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\nimport re\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport logging\nfrom nltk.tokenize import regexp_tokenize\nfrom joblib import load\n\n# Credentials to login and logging in \ntrading = betfairlightweight.APIClient('Username','Password',app_key='Appkey')\nclient = clients.BetfairClient(trading, interactive_login=True)\n\n# Login\nframework = Flumine(client=client)\n\n# Code to login when using security certificates\n# trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs')\n# client = clients.BetfairClient(trading)\n# framework = Flumine(client=client)\n\n# read in model and DataFrames\ntodays_data = pd.read_csv(r'D:\\FastTrack_data\\todays_data.csv', index_col=['DogName_bf','Track','RaceNum'])\nbox_win_percentages = pd.read_csv(r'D:\\FastTrack_data\\box_win_percentage.csv')\nbrunos_model = load(r'C:\\Users\\Ivan\\Documents\\GitHub\\How-to-Automate-Public-Version\\how_to_automate_IV\\logistic_regression.joblib')\nfeature_cols = ['speed_index', 'box_win_percent', 'RunTime_norm_min_28D', 'RunTime_norm_max_28D', 'RunTime_norm_mean_28D', 'RunTime_norm_median_28D', 'RunTime_norm_std_28D', 'SplitMargin_norm_min_28D', 'SplitMargin_norm_max_28D', 'SplitMargin_norm_mean_28D', 'SplitMargin_norm_median_28D', 'SplitMargin_norm_std_28D', 'Place_inv_min_28D', 'Place_inv_max_28D', 'Place_inv_mean_28D', 'Place_inv_median_28D', 'Place_inv_std_28D', 'Place_log_min_28D', 'Place_log_max_28D', 'Place_log_mean_28D', 'Place_log_median_28D', 'Place_log_std_28D', 'Prizemoney_norm_min_28D', 'Prizemoney_norm_max_28D', 'Prizemoney_norm_mean_28D', 'Prizemoney_norm_median_28D', 'Prizemoney_norm_std_28D', 'RunTime_norm_min_91D', 'RunTime_norm_max_91D', 'RunTime_norm_mean_91D', 'RunTime_norm_median_91D', 'RunTime_norm_std_91D', 'SplitMargin_norm_min_91D', 'SplitMargin_norm_max_91D', 'SplitMargin_norm_mean_91D', 'SplitMargin_norm_median_91D', 'SplitMargin_norm_std_91D', 'Place_inv_min_91D', 'Place_inv_max_91D', 'Place_inv_mean_91D', 'Place_inv_median_91D', 'Place_inv_std_91D', 'Place_log_min_91D', 'Place_log_max_91D', 'Place_log_mean_91D', 'Place_log_median_91D', 'Place_log_std_91D', 'Prizemoney_norm_min_91D', 'Prizemoney_norm_max_91D', 'Prizemoney_norm_mean_91D', 'Prizemoney_norm_median_91D', 'Prizemoney_norm_std_91D', 'RunTime_norm_min_365D', 'RunTime_norm_max_365D', 'RunTime_norm_mean_365D', 'RunTime_norm_median_365D', 'RunTime_norm_std_365D', 'SplitMargin_norm_min_365D', 'SplitMargin_norm_max_365D', 'SplitMargin_norm_mean_365D', 'SplitMargin_norm_median_365D', 'SplitMargin_norm_std_365D', 'Place_inv_min_365D', 'Place_inv_max_365D', 'Place_inv_mean_365D', 'Place_inv_median_365D', 'Place_inv_std_365D', 'Place_log_min_365D', 'Place_log_max_365D', 'Place_log_mean_365D', 'Place_log_median_365D', 'Place_log_std_365D', 'Prizemoney_norm_min_365D', 'Prizemoney_norm_max_365D', 'Prizemoney_norm_mean_365D', 'Prizemoney_norm_median_365D', 'Prizemoney_norm_std_365D']\nlogging.basicConfig(filename = 'how_to_automate_4.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n</code></pre> <p>Creating our Flumine Strategy:</p> <pre><code>    class FlatBetting(BaseStrategy):\n        def start(self) -&gt; None:\n            print(\"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\")\n\n        def check_market_book(self, market: Market, market_book: MarketBook) -&gt; bool:\n            if market_book.status != \"CLOSED\":\n                return True\n\n        def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:\n            # Convert dataframe to a global variable\n            global todays_data\n\n            # At the 60 second mark:\n            if market.seconds_to_start &lt; 60 and market_book.inplay == False:\n                # get the list of dog_names, name of the track/venue and race_number/RaceNum from Betfair Polling API\n                dog_names = []\n                track = market.market_catalogue.event.venue\n                race_number = market.market_catalogue.market_name.split(' ',1)[0]  # comes out as R1/R2/R3 .. etc\n                race_number = re.sub(\"[^0-9]\", \"\", race_number)  # only keep the numbers \n                race_number = int(race_number)\n                for runner_cata in market.market_catalogue.runners:\n                    dog_name = runner_cata.runner_name.split(' ',1)[1].upper()\n                    dog_names.append(dog_name)\n\n                # Check if there are box changes, if there are then use Brett's code\n                if market.market_catalogue.description.clarifications != None:\n                    # Brett's code to get Box changes:\n                    my_string = market.market_catalogue.description.clarifications.replace(\"&lt;br&gt; Dog\",\"&lt;br&gt;Dog\")\n                    pattern1 = r'(?&lt;=&lt;br&gt;Dog ).+?(?= starts)'\n                    pattern2 = r\"(?&lt;=\\bbox no. )(\\w+)\"\n                    runners_df = pd.DataFrame (regexp_tokenize(my_string, pattern1), columns = ['runner_name'])\n                    runners_df['runner_name'] = runners_df['runner_name'].astype(str)\n                    # Remove dog name from runner_number\n                    runners_df['runner_number'] = runners_df['runner_name'].apply(lambda x: x[:(x.find(\" \") - 1)].upper())\n                    # Remove dog number from runner_name\n                    runners_df['runner_name'] = runners_df['runner_name'].apply(lambda x: x[(x.find(\" \") + 1):].upper())\n                    runners_df['Box'] = regexp_tokenize(my_string, pattern2)\n\n                    # Replace any old Box info in our original dataframe with data available in runners_df\n                    runners_df = runners_df.set_index('runner_name')\n                    todays_data.loc[(runners_df.index[runners_df.index.isin(dog_names)],track,race_number),'Box'] = runners_df.loc[runners_df.index.isin(dog_names),'Box'].to_list()\n                    # Merge box_win_percentage back on:\n                    todays_data = todays_data.drop(columns = 'box_win_percentage', axis = 1)\n                    todays_data = todays_data.reset_index().merge(box_win_percent, on = ['Track', 'Distance','Box'], how = 'left').set_index(['DogName_bf','Track','RaceNum'])\n\n                # Generate probabilities using Bruno's model\n                todays_data.loc[(dog_names,track,race_number),'prob_LogisticRegression'] = brunos_model.predict_proba(todays_data.loc[(dog_names,track,race_number)][feature_cols])[:,1]\n                # renomalise probabilities\n                probabilities = todays_data.loc[dog_names,track,race_number]['prob_LogisticRegression']\n                todays_data.loc[(dog_names,track,race_number),'renormalised_prob'] = probabilities/probabilities.sum()\n                # convert probaiblities to ratings\n                todays_data.loc[(dog_names,track,race_number),'rating'] = 1/todays_data.loc[dog_names,track,race_number]['renormalised_prob']\n\n                # Use both the polling api (market.catalogue) and the streaming api at once:\n                for runner_cata, runner in zip(market.market_catalogue.runners, market_book.runners):\n                    # Check the polling api and streaming api matches up (sometimes it doesn't)\n                    if runner_cata.selection_id == runner.selection_id:\n                        # Get the dog_name from polling api then reference our data for our model rating\n                        dog_name = runner_cata.runner_name.split(' ',1)[1].upper()\n\n                        # Rest is the same as How to Automate III\n                        model_price = todays_data.loc[dog_name,track,race_number]['rating']\n                        ### If you have an issue such as:\n                            # Unknown error The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n                            # Then do model_price = todays_data.loc[dog_name,track,race_number]['rating'].item()\n\n                        # Log info before placing bets\n                        logging.info(f'dog_name: {dog_name}')\n                        logging.info(f'model_price: {model_price}')\n                        logging.info(f'market_id: {market_book.market_id}')\n                        logging.info(f'selection_id: {runner.selection_id}')\n\n                        # If best available to back price is &gt; rated price then flat $5 back\n                        if runner.status == \"ACTIVE\" and runner.ex.available_to_back[0]['price'] &gt; model_price:\n                            trade = Trade(\n                            market_id=market_book.market_id,\n                            selection_id=runner.selection_id,\n                            handicap=runner.handicap,\n                            strategy=self,\n                            )\n                            order = trade.create_order(\n                                side=\"BACK\", order_type=LimitOrder(price=runner.ex.available_to_back[0]['price'], size=5.00)\n                            )\n                            market.place_order(order)\n                        # If best available to lay price is &lt; rated price then flat $5 lay\n                        if runner.status == \"ACTIVE\" and runner.ex.available_to_lay[0]['price'] &lt; model_price:\n                            trade = Trade(\n                            market_id=market_book.market_id,\n                            selection_id=runner.selection_id,\n                            handicap=runner.handicap,\n                            strategy=self,\n                            )\n                            order = trade.create_order(\n                                side=\"LAY\", order_type=LimitOrder(price=runner.ex.available_to_lay[0]['price'], size=5.00)\n                            )\n                            market.place_order(order)\n</code></pre> <p>As the model we have built is a greyhound model for Australian racing let's point our strategy to Australian greyhound win markets</p> <pre><code>    greyhounds_strategy = FlatBetting(\n        market_filter=streaming_market_filter(\n            event_type_ids=[\"4339\"], # Greyhounds markets\n            country_codes=[\"AU\"], # Australian markets\n            market_types=[\"WIN\"], # Win markets\n        ),\n        max_order_exposure= 50, # Max exposure per order = 50\n        max_trade_count=1, # Max 1 trade per selection\n        max_live_trade_count=1, # Max 1 unmatched trade per selection\n    )\n\n    framework.add_strategy(greyhounds_strategy)\n</code></pre> <p>And add our auto-terminate and bet logging from the previous tutorials:</p> <pre><code>    # import logging\n    import datetime\n    from flumine.worker import BackgroundWorker\n    from flumine.events.events import TerminationEvent\n\n    # logger = logging.getLogger(__name__)\n\n    \"\"\"\n    Worker can be used as followed:\n        framework.add_worker(\n            BackgroundWorker(\n                framework,\n                terminate,\n                func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n                interval=60,\n                start_delay=60,\n            )\n        )\n    This will run every 60s and will terminate \n    the framework if all markets starting 'today' \n    have been closed for at least 1200s\n    \"\"\"\n\n\n    # Function that stops automation running at the end of the day\n    def terminate(\n        context: dict, flumine, today_only: bool = True, seconds_closed: int = 600\n    ) -&gt; None:\n        \"\"\"terminate framework if no markets\n        live today.\n        \"\"\"\n        markets = list(flumine.markets.markets.values())\n        markets_today = [\n            m\n            for m in markets\n            if m.market_start_datetime.date() == datetime.datetime.utcnow().date()\n            and (\n                m.elapsed_seconds_closed is None\n                or (m.elapsed_seconds_closed and m.elapsed_seconds_closed &lt; seconds_closed)\n            )\n        ]\n        if today_only:\n            market_count = len(markets_today)\n        else:\n            market_count = len(markets)\n        if market_count == 0:\n            # logger.info(\"No more markets available, terminating framework\")\n            flumine.handler_queue.put(TerminationEvent(flumine))\n\n    # Add the stopped to our framework\n    framework.add_worker(\n        BackgroundWorker(\n            framework,\n            terminate,\n            func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n            interval=60,\n            start_delay=60,\n        )\n    )\n\nimport os\nimport csv\nimport logging\nfrom flumine.controls.loggingcontrols import LoggingControl\nfrom flumine.order.ordertype import OrderTypes\n\nlogger = logging.getLogger(__name__)\n\nFIELDNAMES = [\n    \"bet_id\",\n    \"strategy_name\",\n    \"market_id\",\n    \"selection_id\",\n    \"trade_id\",\n    \"date_time_placed\",\n    \"price\",\n    \"price_matched\",\n    \"size\",\n    \"size_matched\",\n    \"profit\",\n    \"side\",\n    \"elapsed_seconds_executable\",\n    \"order_status\",\n    \"market_note\",\n    \"trade_notes\",\n    \"order_notes\",\n]\n\n\nclass LiveLoggingControl(LoggingControl):\n    NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n    def __init__(self, *args, **kwargs):\n        super(LiveLoggingControl, self).__init__(*args, **kwargs)\n        self._setup()\n\n    # Changed file path and checks if the file orders_hta_4.csv already exists, if it doens't then create it\n    def _setup(self):\n        if os.path.exists(\"orders_hta_4.csv\"):\n            logging.info(\"Results file exists\")\n        else:\n            with open(\"orders_hta_4.csv\", \"w\") as m:\n                csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                csv_writer.writeheader()\n\n    def _process_cleared_orders_meta(self, event):\n        orders = event.event\n        with open(\"orders_hta_4.csv\", \"a\") as m:\n            for order in orders:\n                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                    size = order.order_type.size\n                else:\n                    size = order.order_type.liability\n                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                    price = None\n                else:\n                    price = order.order_type.price\n                try:\n                    order_data = {\n                        \"bet_id\": order.bet_id,\n                        \"strategy_name\": order.trade.strategy,\n                        \"market_id\": order.market_id,\n                        \"selection_id\": order.selection_id,\n                        \"trade_id\": order.trade.id,\n                        \"date_time_placed\": order.responses.date_time_placed,\n                        \"price\": price,\n                        \"price_matched\": order.average_price_matched,\n                        \"size\": size,\n                        \"size_matched\": order.size_matched,\n                        \"profit\": 0 if not order.cleared_order else order.cleared_order.profit,\n                        \"side\": order.side,\n                        \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                        \"order_status\": order.status.value,\n                        \"market_note\": order.trade.market_notes,\n                        \"trade_notes\": order.trade.notes_str,\n                        \"order_notes\": order.notes_str,\n                    }\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                    csv_writer.writerow(order_data)\n                except Exception as e:\n                    logger.error(\n                        \"_process_cleared_orders_meta: %s\" % e,\n                        extra={\"order\": order, \"error\": e},\n                    )\n\n        logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n    def _process_cleared_markets(self, event):\n        cleared_markets = event.event\n        for cleared_market in cleared_markets.orders:\n            logger.info(\n                \"Cleared market\",\n                extra={\n                    \"market_id\": cleared_market.market_id,\n                    \"bet_count\": cleared_market.bet_count,\n                    \"profit\": cleared_market.profit,\n                    \"commission\": cleared_market.commission,\n                },\n            )\n\nframework.add_logging_control(\n    LiveLoggingControl()\n)\n</code></pre> <pre><code>framework.run()\n</code></pre> <p>And we are all done. Lets GOOOO!!! Once you run this script, 1 minute from the scheduled start of the race it will creating our predictions and start placing $5 bets. </p> <p>--</p>"},{"location":"tutorials/How_to_Automate_4/#generating-historical-predictions","title":"Generating historical predictions","text":"<p>Feeding our predictions through the simulator is entirely optional, but, in my opinion it is where the real sauce is made. The idea is that if we are testing our model live, we can also use the simulator to test what would happen if we tested different staking methodologies, market timings and bet placement to optimise our model. This way you can have a model but test out different strategies to optimise model performance. The thing is, I have had a play with the simulator already and we can't simulate market_catalogue unless you have recorded it yourself (which is what I'll be using to get market_id and selection_id to place live bets). The simulator we will use later on will only take your ratings, market_id and selection_id, so we need our data in a similar format to what we had in How to automate III. In other words, since we don't have market_catalogue in the simulator, we need another way to get the market_id and selection_id.</p> <p>My hacky work around is to generate the probabilities like normal, using basically the same code as above, and since the data is historical its actually even easier as we don't need to deal with reserve dogs and scratching's. Once we have our model predictions, we cheat and get the market_id and selection_id from the Betfair datascience greyhound model by merging on DogName and date. We can take the code we wrote in How to automate III that downloads the greyhound ratings and convert that into a function that downloads the ratings for a date range. This is what the complete code looks like:</p> <pre><code>    import pandas as pd\n    from joblib import load\n\n    model_df = pd.read_csv('../data/model_df.csv')\n    feature_cols = ['speed_index', 'box_win_percent', 'RunTime_norm_min_28D', 'RunTime_norm_max_28D', 'RunTime_norm_mean_28D', 'RunTime_norm_median_28D', 'RunTime_norm_std_28D', 'SplitMargin_norm_min_28D', 'SplitMargin_norm_max_28D', 'SplitMargin_norm_mean_28D', 'SplitMargin_norm_median_28D', 'SplitMargin_norm_std_28D', 'Place_inv_min_28D', 'Place_inv_max_28D', 'Place_inv_mean_28D', 'Place_inv_median_28D', 'Place_inv_std_28D', 'Place_log_min_28D', 'Place_log_max_28D', 'Place_log_mean_28D', 'Place_log_median_28D', 'Place_log_std_28D', 'Prizemoney_norm_min_28D', 'Prizemoney_norm_max_28D', 'Prizemoney_norm_mean_28D', 'Prizemoney_norm_median_28D', 'Prizemoney_norm_std_28D', 'RunTime_norm_min_91D', 'RunTime_norm_max_91D', 'RunTime_norm_mean_91D', 'RunTime_norm_median_91D', 'RunTime_norm_std_91D', 'SplitMargin_norm_min_91D', 'SplitMargin_norm_max_91D', 'SplitMargin_norm_mean_91D', 'SplitMargin_norm_median_91D', 'SplitMargin_norm_std_91D', 'Place_inv_min_91D', 'Place_inv_max_91D', 'Place_inv_mean_91D', 'Place_inv_median_91D', 'Place_inv_std_91D', 'Place_log_min_91D', 'Place_log_max_91D', 'Place_log_mean_91D', 'Place_log_median_91D', 'Place_log_std_91D', 'Prizemoney_norm_min_91D', 'Prizemoney_norm_max_91D', 'Prizemoney_norm_mean_91D', 'Prizemoney_norm_median_91D', 'Prizemoney_norm_std_91D', 'RunTime_norm_min_365D', 'RunTime_norm_max_365D', 'RunTime_norm_mean_365D', 'RunTime_norm_median_365D', 'RunTime_norm_std_365D', 'SplitMargin_norm_min_365D', 'SplitMargin_norm_max_365D', 'SplitMargin_norm_mean_365D', 'SplitMargin_norm_median_365D', 'SplitMargin_norm_std_365D', 'Place_inv_min_365D', 'Place_inv_max_365D', 'Place_inv_mean_365D', 'Place_inv_median_365D', 'Place_inv_std_365D', 'Place_log_min_365D', 'Place_log_max_365D', 'Place_log_mean_365D', 'Place_log_median_365D', 'Place_log_std_365D', 'Prizemoney_norm_min_365D', 'Prizemoney_norm_max_365D', 'Prizemoney_norm_mean_365D', 'Prizemoney_norm_median_365D', 'Prizemoney_norm_std_365D']\n    brunos_model = load('../data/logistic_regression.joblib')\n\n    # Generate predictions like normal\n    # Range of dates that we want to simulate later '2022-03-01' to '2022-04-01'\n    model_df['date_dt'] = pd.to_datetime(model_df['date_dt'])\n    todays_data = model_df[(model_df['date_dt'] &gt;= pd.Timestamp('2022-03-01').strftime('%Y-%m-%d')) &amp; (model_df['date_dt'] &lt; pd.Timestamp('2022-04-01').strftime('%Y-%m-%d'))]\n    dog_win_probabilities = brunos_model.predict_proba(todays_data[feature_cols])[:,1]\n    todays_data['prob_LogisticRegression'] = dog_win_probabilities\n    todays_data['renormalise_prob'] = todays_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / x.sum())\n    todays_data['rating'] = 1/todays_data['renormalise_prob']\n    todays_data = todays_data.sort_values(by = 'date_dt')\n    todays_data\n\n    def download_iggy_ratings(date):\n        \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame.\n\n        Args:\n            date (datetime): the date we want to download the ratings for\n        \"\"\"\n        iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date='\n        iggy_url_2 = date.strftime(\"%Y-%m-%d\")\n        iggy_url_3 = '&amp;presenter=RatingsPresenter&amp;csv=true'\n        iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3\n\n        # Download todays greyhounds ratings\n        iggy_df = pd.read_csv(iggy_url)\n\n        # Data clearning\n        iggy_df = iggy_df.rename(\n        columns={\n            \"meetings.races.bfExchangeMarketId\":\"market_id\",\n            \"meetings.races.runners.bfExchangeSelectionId\":\"selection_id\",\n            \"meetings.races.runners.ratedPrice\":\"rating\",\n            \"meetings.races.number\":\"RaceNum\",\n            \"meetings.name\":\"Track\",\n            \"meetings.races.runners.name\":\"DogName\"\n            }\n        )\n        # iggy_df = iggy_df[['market_id','selection_id','rating']]\n        iggy_df['market_id'] = iggy_df['market_id'].astype(str)\n        iggy_df['date_dt'] = date\n\n        # Set market_id and selection_id as index for easy referencing\n        # iggy_df = iggy_df.set_index(['market_id','selection_id'])\n        return(iggy_df)\n\n    # Download historical ratings over a time period and convert into a big DataFrame.\n    back_test_period = pd.date_range(start='2022-03-01', end='2022-04-01')\n    frames = [download_iggy_ratings(day) for day in back_test_period]\n    iggy_df = pd.concat(frames)\n    iggy_df\n\n    # format DogNames to merge\n    todays_data['DogName'] = todays_data['DogName'].apply(lambda x: x.replace(\"'\", \"\").replace(\".\", \"\").replace(\"Res\", \"\").strip())\n    iggy_df['DogName'] = iggy_df['DogName'].str.upper()\n    # Merge\n    backtest = iggy_df[['market_id','selection_id','DogName','date_dt']].merge(todays_data[['rating','DogName','date_dt']], how = 'inner', on = ['DogName','date_dt'])\n    backtest\n\n    # Save predictions for if we want to backtest/simulate it later\n    backtest.to_csv('../data/backtest.csv', index=False) # Csv format\n</code></pre>"},{"location":"tutorials/How_to_Automate_4/#conclusions-and-next-steps","title":"Conclusions and next steps","text":"<p>Boom! We now have a set of automated script that will download all the data we need in the morning, generates a set of predictions, place flat stakes bets, logs all bets and switches itself off at the end of the day. (If you look in the github repo you will find a script that links all three scripts together so all we need to do is hit play once in the morning!)</p> <p>We have now written code automation code for three different strategies, however we haven't actually backtested any of our strategies or models yet. So for the final part of the How to Automate series we will be writing code to How to simulate the Exchange to backtest and optimise our strategies. Make sure not to miss it as this is where I believe the sauce is made (not that I have made significant sauce).</p>"},{"location":"tutorials/How_to_Automate_4/#complete-code","title":"Complete code","text":"<p>Run the code from your ide by using py <code>&lt;filename&gt;</code>.py, making sure you amend the path to point to your input data.</p> Part 1Part 2Part 3Part 4 <p>Download from Github</p> <pre><code>    import os\n    import sys\n    import pandas as pd\n    from datetime import datetime\n    from dateutil.relativedelta import relativedelta\n\n    # Allow imports from src folder\n    module_path = os.path.abspath(os.path.join('src'))\n    if module_path not in sys.path:\n        sys.path.append(module_path)\n\n    import fasttrack as ft\n    from dotenv import load_dotenv\n    load_dotenv()\n\n    # Validate FastTrack API connection\n    api_key = os.getenv('FAST_TRACK_API_KEY',)\n    client = ft.Fasttrack(api_key)\n    track_codes = client.listTracks()\n\n    # Import race data excluding NZ races\n    au_tracks_filter = list(track_codes[track_codes['state'] != 'NZ']['track_code'])\n\n    # Time window to import data\n    # First day of the month 46 months back from now\n    date_from = (datetime.today() - relativedelta(months=46)).replace(day=1).strftime('%Y-%m-%d')\n    # Download historic data up untill yesterday\n    date_to = (datetime.today() - relativedelta(days=1)).strftime('%Y-%m-%d')\n\n    # List to populate data with, convert to dataframe after fully populated\n    race_details = []\n    dog_results = []\n\n    # Download/load historic (up until yesterday) data\n    # For each day, either fetch data from API or use local CSV file if we already have downloaded it\n    for start in pd.date_range(date_from, date_to, freq='d'):\n        start_date = start.strftime(\"%Y-%m-%d\")\n        end_date = start_date\n        try:\n            filename_races = f'FT_AU_RACES_{start_date}.csv'\n            filename_dogs = f'FT_AU_DOGS_{start_date}.csv'\n\n            filepath_races = f'../data/{filename_races}'\n            filepath_dogs = f'../data/{filename_dogs}'\n\n            print(f'Loading data from {start_date} to {end_date}')\n            if os.path.isfile(filepath_races):\n                # Load local CSV file\n                day_race_details = pd.read_csv(filepath_races) \n                day_dog_results = pd.read_csv(filepath_dogs) \n            else:\n                # Fetch data from API\n                day_race_details, day_dog_results = client.getRaceResults(start_date, end_date, au_tracks_filter)\n                day_race_details.to_csv(filepath_races, index=False)\n                day_dog_results.to_csv(filepath_dogs, index=False)\n\n            # Combine daily data\n            race_details.append(day_race_details)\n            dog_results.append(day_dog_results)\n        except:\n            print(f'Could not load data from {start_date} to {end_date}')\n\n    # Download todays data from the api\n    todays_date = pd.Timestamp.now().strftime('%Y-%m-%d')\n    todays_race_details, todays_dog_results = client.getFullFormat(todays_date)\n\n    # Make live API data in the same form as historic data\n    todays_race_details = todays_race_details.rename(columns={\"Date\":\"date\"})\n    todays_dog_results = todays_dog_results.rename(columns={\"RaceBox\":\"Box\"})\n    # Only keep the columns we have in the historic data\n    usecols_race_details = ['@id','RaceNum','RaceName','RaceTime','Distance','RaceGrade','Track','date']\n    usecols_dog_results = ['@id','DogName','Box','RaceId','TrainerId','TrainerName']\n    # dog_results columns not in live data ['Place','Rug','Weight','StartPrice','Handicap','Margin1', 'Margin2','PIR','Checks','Comments','SplitMargin', 'RunTime', 'Prizemoney',]\n    todays_race_details = todays_race_details[usecols_race_details]\n    todays_dog_results = todays_dog_results[usecols_dog_results]\n\n    # Now that todays data looks similar to our historic data lets add todays data to the rest of our historic data\n    race_details.append(todays_race_details)\n    dog_results.append(todays_dog_results)\n    # Convert our data into a nice DataFrame\n    race_details = pd.concat(race_details)\n    dog_results = pd.concat(dog_results)\n\n    # Save our data to csv files\n    race_details.to_csv('../data/race_details.csv', index = False)\n    dog_results.to_csv('../data/dog_results.csv', index = False)\n    # Ready for data cleaning and feature creation\n</code></pre> <p>Download from Github</p> <pre><code>    import numpy as np\n    import pandas as pd\n    # settings to display all columns\n    pd.set_option(\"display.max_columns\", None)\n    from sklearn.preprocessing import MinMaxScaler\n    import itertools\n\n    race_details = pd.read_csv('../data/race_details.csv')\n    dog_results = pd.read_csv('../data/dog_results.csv')\n\n    ## Cleanse and normalise the data\n    # Clean up the race dataset\n    race_details = race_details.rename(columns = {'@id': 'FastTrack_RaceId'})\n    race_details['Distance'] = race_details['Distance'].apply(lambda x: int(x.replace(\"m\", \"\")))\n    race_details['date_dt'] = pd.to_datetime(race_details['date'], format = '%d %b %y')\n    # Clean up the dogs results dataset\n    dog_results = dog_results.rename(columns = {'@id': 'FastTrack_DogId', 'RaceId': 'FastTrack_RaceId'})\n\n    # Combine dogs results with race attributes\n    dog_results = dog_results.merge(\n        race_details, \n        how = 'left',\n        on = 'FastTrack_RaceId'\n    )\n\n    # Convert StartPrice to probability\n    dog_results['StartPrice'] = dog_results['StartPrice'].apply(lambda x: None if x is None else float(x.replace('$', '').replace('F', '')) if isinstance(x, str) else x)\n    dog_results['StartPrice_probability'] = (1 / dog_results['StartPrice']).fillna(0)\n    dog_results['StartPrice_probability'] = dog_results.groupby('FastTrack_RaceId')['StartPrice_probability'].apply(lambda x: x / x.sum())\n\n    # Discard entries without results (scratched or did not finish)\n    dog_results = dog_results[~dog_results['Box'].isnull()]\n    dog_results['Box'] = dog_results['Box'].astype(int)\n\n    # Clean up other attributes\n    dog_results['RunTime'] = dog_results['RunTime'].astype(float)\n    dog_results['SplitMargin'] = dog_results['SplitMargin'].astype(float)\n    dog_results['Prizemoney'] = dog_results['Prizemoney'].astype(float).fillna(0)\n    dog_results['Place'] = pd.to_numeric(dog_results['Place'].apply(lambda x: x.replace(\"=\", \"\") if isinstance(x, str) else 0), errors='coerce').fillna(0)\n    dog_results['win'] = dog_results['Place'].apply(lambda x: 1 if x == 1 else 0)\n\n    # Normalise some of the raw values\n    dog_results['Prizemoney_norm'] = np.log10(dog_results['Prizemoney'] + 1) / 12\n    dog_results['Place_inv'] = (1 / dog_results['Place']).fillna(0)\n    dog_results['Place_log'] = np.log10(dog_results['Place'] + 1).fillna(0)\n    dog_results['RunSpeed'] = (dog_results['RunTime'] / dog_results['Distance']).fillna(0)\n\n    ## Generate features using raw data\n    # Calculate median winner time per track/distance\n    win_results = dog_results[dog_results['win'] == 1]\n    median_win_time = pd.DataFrame(data=win_results[win_results['RunTime'] &gt; 0].groupby(['Track', 'Distance'])['RunTime'].median()).rename(columns={\"RunTime\": \"RunTime_median\"}).reset_index()\n    median_win_split_time = pd.DataFrame(data=win_results[win_results['SplitMargin'] &gt; 0].groupby(['Track', 'Distance'])['SplitMargin'].median()).rename(columns={\"SplitMargin\": \"SplitMargin_median\"}).reset_index()\n    median_win_time.head()\n\n    # Calculate track speed index\n    median_win_time['speed_index'] = (median_win_time['RunTime_median'] / median_win_time['Distance'])\n    median_win_time['speed_index'] = MinMaxScaler().fit_transform(median_win_time[['speed_index']])\n    median_win_time.head()\n\n    # Compare dogs finish time with median winner time\n    dog_results = dog_results.merge(median_win_time, on=['Track', 'Distance'], how='left')\n    dog_results = dog_results.merge(median_win_split_time, on=['Track', 'Distance'], how='left')\n\n    # Normalise time comparison\n    dog_results['RunTime_norm'] = (dog_results['RunTime_median'] / dog_results['RunTime']).clip(0.9, 1.1)\n    dog_results['RunTime_norm'] = MinMaxScaler().fit_transform(dog_results[['RunTime_norm']])\n    dog_results['SplitMargin_norm'] = (dog_results['SplitMargin_median'] / dog_results['SplitMargin']).clip(0.9, 1.1)\n    dog_results['SplitMargin_norm'] = MinMaxScaler().fit_transform(dog_results[['SplitMargin_norm']])\n    dog_results.head()\n\n    # Calculate box winning percentage for each track/distance\n    box_win_percent = pd.DataFrame(data=dog_results.groupby(['Track', 'Distance', 'Box'])['win'].mean()).rename(columns={\"win\": \"box_win_percent\"}).reset_index()\n    # Add to dog results dataframe\n    dog_results = dog_results.merge(box_win_percent, on=['Track', 'Distance', 'Box'], how='left')\n    # Display example of barrier winning probabilities\n    print(box_win_percent.head(8))\n    box_win_percent.to_csv('../data/box_win_percentage.csv')\n\n    # Generate rolling window features\n    dataset = dog_results.copy()\n    dataset = dataset.set_index(['FastTrack_DogId', 'date_dt']).sort_index()\n\n    # Use rolling window of 28, 91 and 365 days\n    rolling_windows = ['28D', '91D', '365D']\n    # Features to use for rolling windows calculation\n    features = ['RunTime_norm', 'SplitMargin_norm', 'Place_inv', 'Place_log', 'Prizemoney_norm']\n    # Aggregation functions to apply\n    aggregates = ['min', 'max', 'mean', 'median', 'std']\n    # Keep track of generated feature names\n    feature_cols = ['speed_index', 'box_win_percent']\n\n    for rolling_window in rolling_windows:\n            print(f'Processing rolling window {rolling_window}')\n\n            rolling_result = (\n                dataset\n                .reset_index(level=0).sort_index()\n                .groupby('FastTrack_DogId')[features]\n                .rolling(rolling_window)\n                .agg(aggregates)\n                .groupby(level=0)  # Thanks to Brett for finding this!\n                .shift(1)\n            )\n\n            # My own dodgey code to work with reserve dogs\n            temp = rolling_result.reset_index()\n            temp = temp[temp['date_dt'] == pd.Timestamp.now().normalize()]\n            temp = temp.sort_index(axis=1)\n            rolling_result.loc[pd.IndexSlice[:, pd.Timestamp.now().normalize()], :] = temp.groupby(['FastTrack_DogId','date_dt']).first()\n\n            # Generate list of rolling window feature names (eg: RunTime_norm_min_365D)\n            agg_features_cols = [f'{f}_{a}_{rolling_window}' for f, a in itertools.product(features, aggregates)]\n\n            # More efficient method to add features to dataset\n            rolling_result.columns = agg_features_cols\n            dataset = pd.concat([dataset,rolling_result],axis = 1)\n\n            # Keep track of generated feature names\n            feature_cols.extend(agg_features_cols)\n\n    # print(feature_cols) \n    # feature_cols = ['speed_index', 'box_win_percent', 'RunTime_norm_min_28D', 'RunTime_norm_max_28D', 'RunTime_norm_mean_28D', 'RunTime_norm_median_28D', 'RunTime_norm_std_28D', 'SplitMargin_norm_min_28D', 'SplitMargin_norm_max_28D', 'SplitMargin_norm_mean_28D', 'SplitMargin_norm_median_28D', 'SplitMargin_norm_std_28D', 'Place_inv_min_28D', 'Place_inv_max_28D', 'Place_inv_mean_28D', 'Place_inv_median_28D', 'Place_inv_std_28D', 'Place_log_min_28D', 'Place_log_max_28D', 'Place_log_mean_28D', 'Place_log_median_28D', 'Place_log_std_28D', 'Prizemoney_norm_min_28D', 'Prizemoney_norm_max_28D', 'Prizemoney_norm_mean_28D', 'Prizemoney_norm_median_28D', 'Prizemoney_norm_std_28D', 'RunTime_norm_min_91D', 'RunTime_norm_max_91D', 'RunTime_norm_mean_91D', 'RunTime_norm_median_91D', 'RunTime_norm_std_91D', 'SplitMargin_norm_min_91D', 'SplitMargin_norm_max_91D', 'SplitMargin_norm_mean_91D', 'SplitMargin_norm_median_91D', 'SplitMargin_norm_std_91D', 'Place_inv_min_91D', 'Place_inv_max_91D', 'Place_inv_mean_91D', 'Place_inv_median_91D', 'Place_inv_std_91D', 'Place_log_min_91D', 'Place_log_max_91D', 'Place_log_mean_91D', 'Place_log_median_91D', 'Place_log_std_91D', 'Prizemoney_norm_min_91D', 'Prizemoney_norm_max_91D', 'Prizemoney_norm_mean_91D', 'Prizemoney_norm_median_91D', 'Prizemoney_norm_std_91D', 'RunTime_norm_min_365D', 'RunTime_norm_max_365D', 'RunTime_norm_mean_365D', 'RunTime_norm_median_365D', 'RunTime_norm_std_365D', 'SplitMargin_norm_min_365D', 'SplitMargin_norm_max_365D', 'SplitMargin_norm_mean_365D', 'SplitMargin_norm_median_365D', 'SplitMargin_norm_std_365D', 'Place_inv_min_365D', 'Place_inv_max_365D', 'Place_inv_mean_365D', 'Place_inv_median_365D', 'Place_inv_std_365D', 'Place_log_min_365D', 'Place_log_max_365D', 'Place_log_mean_365D', 'Place_log_median_365D', 'Place_log_std_365D', 'Prizemoney_norm_min_365D', 'Prizemoney_norm_max_365D', 'Prizemoney_norm_mean_365D', 'Prizemoney_norm_median_365D', 'Prizemoney_norm_std_365D']\n\n    # Replace missing values with 0\n    dataset.fillna(0, inplace=True)\n    print(dataset.head(8))\n\n    # Only keep data after 2018-12-01\n    model_df = dataset.reset_index()\n    feature_cols = np.unique(feature_cols).tolist()\n    model_df = model_df[model_df['date_dt'] &gt;= '2018-12-01']\n\n    # This line was originally part of Bruno's tutorial, but we don't run it in this script\n    # model_df = model_df[['date_dt', 'FastTrack_RaceId', 'DogName', 'win', 'StartPrice_probability'] + feature_cols]\n\n    # Only train model off of races where each dog has a value for each feature\n    races_exclude = model_df[model_df.isnull().any(axis = 1)]['FastTrack_RaceId'].drop_duplicates()\n    model_df = model_df[~model_df['FastTrack_RaceId'].isin(races_exclude)]\n    model_df.to_csv('../data/model_df.csv')  # Save data for part_4\n\n    # Select todays data and prepare data for easy reference in flumine\n    todays_data = model_df[model_df['date_dt'] == pd.Timestamp.now().strftime('%Y-%m-%d')]\n    todays_data['DogName_bf'] = todays_data['DogName'].apply(lambda x: x.replace(\"'\", \"\").replace(\".\", \"\").replace(\"Res\", \"\").strip())\n    final_data.replace('Sandown (SAP)','Sandown Park',inplace = True)\n    final_data.replace('Murray Bridge (MBR)','Murray Bridge',inplace = True)\n    todays_data = todays_data.set_index(['DogName_bf','Track','RaceNum'])\n    print(todays_data)\n\n    todays_data.to_csv('../data/todays_data.csv', index = True)\n</code></pre> <p>Download from Github</p> <pre><code>    # Import libraries for logging in\n    import betfairlightweight\n    from flumine import Flumine, clients\n\n    # Credentials to login and logging in \n    trading = betfairlightweight.APIClient('Username','Password',app_key='Appkey')\n    client = clients.BetfairClient(trading, interactive_login=True)\n\n    # Login\n    framework = Flumine(client=client)\n\n    # Code to login when using security certificates\n    # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs')\n    # client = clients.BetfairClient(trading)\n\n    # framework = Flumine(client=client)\n\n    # Import libraries and logging\n    from flumine import BaseStrategy \n    from flumine.order.trade import Trade\n    from flumine.order.order import LimitOrder\n    from flumine.markets.market import Market\n    from betfairlightweight.filters import streaming_market_filter\n    from betfairlightweight.resources import MarketBook\n    import re\n    import pandas as pd\n    import numpy as np\n    import datetime\n    import logging\n    from nltk.tokenize import regexp_tokenize\n    from joblib import load\n\n    # read in model and DataFrames\n    todays_data = pd.read_csv(r'D:\\FastTrack_data\\todays_data.csv', index_col=['DogName_bf','Track','RaceNum'])\n    box_win_percentages = pd.read_csv(r'D:\\FastTrack_data\\box_win_percentage.csv')\n    brunos_model = load(r'C:\\Users\\Ivan\\Documents\\GitHub\\How-to-Automate-Public-Version\\how_to_automate_IV\\logistic_regression.joblib')\n    feature_cols = ['speed_index', 'box_win_percent', 'RunTime_norm_min_28D', 'RunTime_norm_max_28D', 'RunTime_norm_mean_28D', 'RunTime_norm_median_28D', 'RunTime_norm_std_28D', 'SplitMargin_norm_min_28D', 'SplitMargin_norm_max_28D', 'SplitMargin_norm_mean_28D', 'SplitMargin_norm_median_28D', 'SplitMargin_norm_std_28D', 'Place_inv_min_28D', 'Place_inv_max_28D', 'Place_inv_mean_28D', 'Place_inv_median_28D', 'Place_inv_std_28D', 'Place_log_min_28D', 'Place_log_max_28D', 'Place_log_mean_28D', 'Place_log_median_28D', 'Place_log_std_28D', 'Prizemoney_norm_min_28D', 'Prizemoney_norm_max_28D', 'Prizemoney_norm_mean_28D', 'Prizemoney_norm_median_28D', 'Prizemoney_norm_std_28D', 'RunTime_norm_min_91D', 'RunTime_norm_max_91D', 'RunTime_norm_mean_91D', 'RunTime_norm_median_91D', 'RunTime_norm_std_91D', 'SplitMargin_norm_min_91D', 'SplitMargin_norm_max_91D', 'SplitMargin_norm_mean_91D', 'SplitMargin_norm_median_91D', 'SplitMargin_norm_std_91D', 'Place_inv_min_91D', 'Place_inv_max_91D', 'Place_inv_mean_91D', 'Place_inv_median_91D', 'Place_inv_std_91D', 'Place_log_min_91D', 'Place_log_max_91D', 'Place_log_mean_91D', 'Place_log_median_91D', 'Place_log_std_91D', 'Prizemoney_norm_min_91D', 'Prizemoney_norm_max_91D', 'Prizemoney_norm_mean_91D', 'Prizemoney_norm_median_91D', 'Prizemoney_norm_std_91D', 'RunTime_norm_min_365D', 'RunTime_norm_max_365D', 'RunTime_norm_mean_365D', 'RunTime_norm_median_365D', 'RunTime_norm_std_365D', 'SplitMargin_norm_min_365D', 'SplitMargin_norm_max_365D', 'SplitMargin_norm_mean_365D', 'SplitMargin_norm_median_365D', 'SplitMargin_norm_std_365D', 'Place_inv_min_365D', 'Place_inv_max_365D', 'Place_inv_mean_365D', 'Place_inv_median_365D', 'Place_inv_std_365D', 'Place_log_min_365D', 'Place_log_max_365D', 'Place_log_mean_365D', 'Place_log_median_365D', 'Place_log_std_365D', 'Prizemoney_norm_min_365D', 'Prizemoney_norm_max_365D', 'Prizemoney_norm_mean_365D', 'Prizemoney_norm_median_365D', 'Prizemoney_norm_std_365D']\n    logging.basicConfig(filename = 'how_to_automate_4.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n\n    class FlatBetting(BaseStrategy):\n        def start(self) -&gt; None:\n            print(\"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\")\n\n        def check_market_book(self, market: Market, market_book: MarketBook) -&gt; bool:\n            if market_book.status != \"CLOSED\":\n                return True\n\n        def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:\n            # Convert dataframe to a global variable\n            global todays_data\n\n            # At the 60 second mark:\n            if market.seconds_to_start &lt; 60 and market_book.inplay == False:\n                # get the list of dog_names, name of the track/venue and race_number/RaceNum from Betfair Polling API\n                dog_names = []\n                track = market.market_catalogue.event.venue\n                race_number = market.market_catalogue.market_name.split(' ',1)[0]  # comes out as R1/R2/R3 .. etc\n                race_number = re.sub(\"[^0-9]\", \"\", race_number)  # only keep the numbers \n                race_number = int(race_number)\n                for runner_cata in market.market_catalogue.runners:\n                    dog_name = runner_cata.runner_name.split(' ',1)[1].upper()\n                    dog_names.append(dog_name)\n\n                # Check if there are box changes, if there are then use Brett's code\n                if market.market_catalogue.description.clarifications != None:\n                    # Brett's code to get Box changes:\n                    my_string = market.market_catalogue.description.clarifications.replace(\"&lt;br&gt; Dog\",\"&lt;br&gt;Dog\")\n                    pattern1 = r'(?&lt;=&lt;br&gt;Dog ).+?(?= starts)'\n                    pattern2 = r\"(?&lt;=\\bbox no. )(\\w+)\"\n                    runners_df = pd.DataFrame (regexp_tokenize(my_string, pattern1), columns = ['runner_name'])\n                    runners_df['runner_name'] = runners_df['runner_name'].astype(str)\n                    # Remove dog name from runner_number\n                    runners_df['runner_number'] = runners_df['runner_name'].apply(lambda x: x[:(x.find(\" \") - 1)].upper())\n                    # Remove dog number from runner_name\n                    runners_df['runner_name'] = runners_df['runner_name'].apply(lambda x: x[(x.find(\" \") + 1):].upper())\n                    runners_df['Box'] = regexp_tokenize(my_string, pattern2)\n\n                    # Replace any old Box info in our original dataframe with data available in runners_df\n                    runners_df = runners_df.set_index('runner_name')\n                    todays_data.loc[(runners_df.index[runners_df.index.isin(dog_names)],track,race_number),'Box'] = runners_df.loc[runners_df.index.isin(dog_names),'Box'].to_list()\n                    # Merge box_win_percentage back on:\n                    todays_data = todays_data.drop(columns = 'box_win_percentage', axis = 1)\n                    todays_data = todays_data.reset_index().merge(box_win_percent, on = ['Track', 'Distance','Box'], how = 'left').set_index(['DogName_bf','Track','RaceNum'])\n\n                # Generate probabilities using Bruno's model\n                todays_data.loc[(dog_names,track,race_number),'prob_LogisticRegression'] = brunos_model.predict_proba(todays_data.loc[(dog_names,track,race_number)][feature_cols])[:,1]\n                # renomalise probabilities\n                probabilities = todays_data.loc[dog_names,track,race_number]['prob_LogisticRegression']\n                todays_data.loc[(dog_names,track,race_number),'renormalised_prob'] = probabilities/probabilities.sum()\n                # convert probaiblities to ratings\n                todays_data.loc[(dog_names,track,race_number),'rating'] = 1/todays_data.loc[dog_names,track,race_number]['renormalised_prob']\n\n                # Use both the polling api (market.catalogue) and the streaming api at once:\n                for runner_cata, runner in zip(market.market_catalogue.runners, market_book.runners):\n                    # Check the polling api and streaming api matches up (sometimes it doesn't)\n                    if runner_cata.selection_id == runner.selection_id:\n                        # Get the dog_name from polling api then reference our data for our model rating\n                        dog_name = runner_cata.runner_name.split(' ',1)[1].upper()\n\n                        # Rest is the same as How to Automate III\n                        model_price = todays_data.loc[dog_name,track,race_number]['rating']\n                        ### If you have an issue such as:\n                            # Unknown error The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n                            # Then do model_price = todays_data.loc[dog_name,track,race_number]['rating'].item()\n\n                        # Log info before placing bets\n                        logging.info(f'dog_name: {dog_name}')\n                        logging.info(f'model_price: {model_price}')\n                        logging.info(f'market_id: {market_book.market_id}')\n                        logging.info(f'selection_id: {runner.selection_id}')\n\n                        # If best available to back price is &gt; rated price then flat $5 back\n                        if runner.status == \"ACTIVE\" and runner.ex.available_to_back[0]['price'] &gt; model_price:\n                            trade = Trade(\n                            market_id=market_book.market_id,\n                            selection_id=runner.selection_id,\n                            handicap=runner.handicap,\n                            strategy=self,\n                            )\n                            order = trade.create_order(\n                                side=\"BACK\", order_type=LimitOrder(price=runner.ex.available_to_back[0]['price'], size=5.00)\n                            )\n                            market.place_order(order)\n                        # If best available to lay price is &lt; rated price then flat $5 lay\n                        if runner.status == \"ACTIVE\" and runner.ex.available_to_lay[0]['price'] &lt; model_price:\n                            trade = Trade(\n                            market_id=market_book.market_id,\n                            selection_id=runner.selection_id,\n                            handicap=runner.handicap,\n                            strategy=self,\n                            )\n                            order = trade.create_order(\n                                side=\"LAY\", order_type=LimitOrder(price=runner.ex.available_to_lay[0]['price'], size=5.00)\n                            )\n                            market.place_order(order)\n\n    greyhounds_strategy = FlatBetting(\n        market_filter=streaming_market_filter(\n            event_type_ids=[\"4339\"], # Greyhounds markets\n            country_codes=[\"AU\"], # Australian markets\n            market_types=[\"WIN\"], # Win markets\n        ),\n        max_order_exposure= 50, # Max exposure per order = 50\n        max_trade_count=1, # Max 1 trade per selection\n        max_live_trade_count=1, # Max 1 unmatched trade per selection\n    )\n\n    framework.add_strategy(greyhounds_strategy)\n\n    # import logging\n    import datetime\n    from flumine.worker import BackgroundWorker\n    from flumine.events.events import TerminationEvent\n\n    # logger = logging.getLogger(__name__)\n\n    \"\"\"\n    Worker can be used as followed:\n        framework.add_worker(\n            BackgroundWorker(\n                framework,\n                terminate,\n                func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n                interval=60,\n                start_delay=60,\n            )\n        )\n    This will run every 60s and will terminate \n    the framework if all markets starting 'today' \n    have been closed for at least 1200s\n    \"\"\"\n\n\n    # Function that stops automation running at the end of the day\n    def terminate(\n        context: dict, flumine, today_only: bool = True, seconds_closed: int = 600\n    ) -&gt; None:\n        \"\"\"terminate framework if no markets\n        live today.\n        \"\"\"\n        markets = list(flumine.markets.markets.values())\n        markets_today = [\n            m\n            for m in markets\n            if m.market_start_datetime.date() == datetime.datetime.utcnow().date()\n            and (\n                m.elapsed_seconds_closed is None\n                or (m.elapsed_seconds_closed and m.elapsed_seconds_closed &lt; seconds_closed)\n            )\n        ]\n        if today_only:\n            market_count = len(markets_today)\n        else:\n            market_count = len(markets)\n        if market_count == 0:\n            # logger.info(\"No more markets available, terminating framework\")\n            flumine.handler_queue.put(TerminationEvent(flumine))\n\n    # Add the stopped to our framework\n    framework.add_worker(\n        BackgroundWorker(\n            framework,\n            terminate,\n            func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n            interval=60,\n            start_delay=60,\n        )\n    )\n\n    import os\n    import csv\n    import logging\n    from flumine.controls.loggingcontrols import LoggingControl\n    from flumine.order.ordertype import OrderTypes\n\n    logger = logging.getLogger(__name__)\n\n    FIELDNAMES = [\n        \"bet_id\",\n        \"strategy_name\",\n        \"market_id\",\n        \"selection_id\",\n        \"trade_id\",\n        \"date_time_placed\",\n        \"price\",\n        \"price_matched\",\n        \"size\",\n        \"size_matched\",\n        \"profit\",\n        \"side\",\n        \"elapsed_seconds_executable\",\n        \"order_status\",\n        \"market_note\",\n        \"trade_notes\",\n        \"order_notes\",\n    ]\n\n    class LiveLoggingControl(LoggingControl):\n        NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n        def __init__(self, *args, **kwargs):\n            super(LiveLoggingControl, self).__init__(*args, **kwargs)\n            self._setup()\n\n        # Changed file path and checks if the file orders_hta_4.csv already exists, if it doens't then create it\n        def _setup(self):\n            if os.path.exists(\"orders_hta_4.csv\"):\n                logging.info(\"Results file exists\")\n            else:\n                with open(\"orders_hta_4.csv\", \"w\") as m:\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                    csv_writer.writeheader()\n\n        def _process_cleared_orders_meta(self, event):\n            orders = event.event\n            with open(\"orders_hta_4.csv\", \"a\") as m:\n                for order in orders:\n                    if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                        size = order.order_type.size\n                    else:\n                        size = order.order_type.liability\n                    if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                        price = None\n                    else:\n                        price = order.order_type.price\n                    try:\n                        order_data = {\n                            \"bet_id\": order.bet_id,\n                            \"strategy_name\": order.trade.strategy,\n                            \"market_id\": order.market_id,\n                            \"selection_id\": order.selection_id,\n                            \"trade_id\": order.trade.id,\n                            \"date_time_placed\": order.responses.date_time_placed,\n                            \"price\": price,\n                            \"price_matched\": order.average_price_matched,\n                            \"size\": size,\n                            \"size_matched\": order.size_matched,\n                            \"profit\": 0 if not order.cleared_order else order.cleared_order.profit,\n                            \"side\": order.side,\n                            \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                            \"order_status\": order.status.value,\n                            \"market_note\": order.trade.market_notes,\n                            \"trade_notes\": order.trade.notes_str,\n                            \"order_notes\": order.notes_str,\n                        }\n                        csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                        csv_writer.writerow(order_data)\n                    except Exception as e:\n                        logger.error(\n                            \"_process_cleared_orders_meta: %s\" % e,\n                            extra={\"order\": order, \"error\": e},\n                        )\n\n            logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n        def _process_cleared_markets(self, event):\n            cleared_markets = event.event\n            for cleared_market in cleared_markets.orders:\n                logger.info(\n                    \"Cleared market\",\n                    extra={\n                        \"market_id\": cleared_market.market_id,\n                        \"bet_count\": cleared_market.bet_count,\n                        \"profit\": cleared_market.profit,\n                        \"commission\": cleared_market.commission,\n                    },\n                )\n\n    framework.add_logging_control(\n        LiveLoggingControl()\n    )\n\n    framework.run()\n</code></pre> <p>Download from Github</p> <pre><code>    import pandas as pd\n    from joblib import load\n\n    model_df = pd.read_csv('../data/model_df.csv')\n    feature_cols = ['speed_index', 'box_win_percent', 'RunTime_norm_min_28D', 'RunTime_norm_max_28D', 'RunTime_norm_mean_28D', 'RunTime_norm_median_28D', 'RunTime_norm_std_28D', 'SplitMargin_norm_min_28D', 'SplitMargin_norm_max_28D', 'SplitMargin_norm_mean_28D', 'SplitMargin_norm_median_28D', 'SplitMargin_norm_std_28D', 'Place_inv_min_28D', 'Place_inv_max_28D', 'Place_inv_mean_28D', 'Place_inv_median_28D', 'Place_inv_std_28D', 'Place_log_min_28D', 'Place_log_max_28D', 'Place_log_mean_28D', 'Place_log_median_28D', 'Place_log_std_28D', 'Prizemoney_norm_min_28D', 'Prizemoney_norm_max_28D', 'Prizemoney_norm_mean_28D', 'Prizemoney_norm_median_28D', 'Prizemoney_norm_std_28D', 'RunTime_norm_min_91D', 'RunTime_norm_max_91D', 'RunTime_norm_mean_91D', 'RunTime_norm_median_91D', 'RunTime_norm_std_91D', 'SplitMargin_norm_min_91D', 'SplitMargin_norm_max_91D', 'SplitMargin_norm_mean_91D', 'SplitMargin_norm_median_91D', 'SplitMargin_norm_std_91D', 'Place_inv_min_91D', 'Place_inv_max_91D', 'Place_inv_mean_91D', 'Place_inv_median_91D', 'Place_inv_std_91D', 'Place_log_min_91D', 'Place_log_max_91D', 'Place_log_mean_91D', 'Place_log_median_91D', 'Place_log_std_91D', 'Prizemoney_norm_min_91D', 'Prizemoney_norm_max_91D', 'Prizemoney_norm_mean_91D', 'Prizemoney_norm_median_91D', 'Prizemoney_norm_std_91D', 'RunTime_norm_min_365D', 'RunTime_norm_max_365D', 'RunTime_norm_mean_365D', 'RunTime_norm_median_365D', 'RunTime_norm_std_365D', 'SplitMargin_norm_min_365D', 'SplitMargin_norm_max_365D', 'SplitMargin_norm_mean_365D', 'SplitMargin_norm_median_365D', 'SplitMargin_norm_std_365D', 'Place_inv_min_365D', 'Place_inv_max_365D', 'Place_inv_mean_365D', 'Place_inv_median_365D', 'Place_inv_std_365D', 'Place_log_min_365D', 'Place_log_max_365D', 'Place_log_mean_365D', 'Place_log_median_365D', 'Place_log_std_365D', 'Prizemoney_norm_min_365D', 'Prizemoney_norm_max_365D', 'Prizemoney_norm_mean_365D', 'Prizemoney_norm_median_365D', 'Prizemoney_norm_std_365D']\n    brunos_model = load('../data/logistic_regression.joblib')\n\n    # Generate predictions like normal\n    # Range of dates that we want to simulate later '2022-03-01' to '2022-04-01'\n    model_df['date_dt'] = pd.to_datetime(model_df['date_dt'])\n    todays_data = model_df[(model_df['date_dt'] &gt;= pd.Timestamp('2022-03-01').strftime('%Y-%m-%d')) &amp; (model_df['date_dt'] &lt; pd.Timestamp('2022-04-01').strftime('%Y-%m-%d'))]\n    dog_win_probabilities = brunos_model.predict_proba(todays_data[feature_cols])[:,1]\n    todays_data['prob_LogisticRegression'] = dog_win_probabilities\n    todays_data['renormalise_prob'] = todays_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / x.sum())\n    todays_data['rating'] = 1/todays_data['renormalise_prob']\n    todays_data = todays_data.sort_values(by = 'date_dt')\n    todays_data\n\n    def download_iggy_ratings(date):\n        \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame.\n\n        Args:\n            date (datetime): the date we want to download the ratings for\n        \"\"\"\n        iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date='\n        iggy_url_2 = date.strftime(\"%Y-%m-%d\")\n        iggy_url_3 = '&amp;presenter=RatingsPresenter&amp;csv=true'\n        iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3\n\n        # Download todays greyhounds ratings\n        iggy_df = pd.read_csv(iggy_url)\n\n        # Data clearning\n        iggy_df = iggy_df.rename(\n        columns={\n            \"meetings.races.bfExchangeMarketId\":\"market_id\",\n            \"meetings.races.runners.bfExchangeSelectionId\":\"selection_id\",\n            \"meetings.races.runners.ratedPrice\":\"rating\",\n            \"meetings.races.number\":\"RaceNum\",\n            \"meetings.name\":\"Track\",\n            \"meetings.races.runners.name\":\"DogName\"\n            }\n        )\n        # iggy_df = iggy_df[['market_id','selection_id','rating']]\n        iggy_df['market_id'] = iggy_df['market_id'].astype(str)\n        iggy_df['date_dt'] = date\n\n        # Set market_id and selection_id as index for easy referencing\n        # iggy_df = iggy_df.set_index(['market_id','selection_id'])\n        return(iggy_df)\n\n    # Download historical ratings over a time period and convert into a big DataFrame.\n    back_test_period = pd.date_range(start='2022-03-01', end='2022-04-01')\n    frames = [download_iggy_ratings(day) for day in back_test_period]\n    iggy_df = pd.concat(frames)\n    iggy_df\n\n    # format DogNames to merge\n    todays_data['DogName'] = todays_data['DogName'].apply(lambda x: x.replace(\"'\", \"\").replace(\".\", \"\").replace(\"Res\", \"\").strip())\n    iggy_df['DogName'] = iggy_df['DogName'].str.upper()\n    # Merge\n    backtest = iggy_df[['market_id','selection_id','DogName','date_dt']].merge(todays_data[['rating','DogName','date_dt']], how = 'inner', on = ['DogName','date_dt'])\n    backtest\n\n    # Save predictions for if we want to backtest/simulate it later\n    backtest.to_csv('../data/backtest.csv', index=False) # Csv format\n</code></pre>"},{"location":"tutorials/How_to_Automate_4/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"tutorials/How_to_Automate_4_archived/","title":"How to Automate 4 archived","text":"<p>This is an archived version of How to Automate 4 the latest version is available here</p> <p>For this tutorial we will be automating the model that Bruno taught us how to make in the Greyhound Modelling Tutorial. This tutorial follows on logically from How to Automate III. If you haven't already, make sure you take a look at the rest of the series first those before continuing here as they cover some key concepts!</p> <p>To generate our predictions, we have two options: we can generate our predictions using the same notebook used to train our model then read those predictions into this notebook, or we can save the model and read that model into this notebook.</p> <p>For this tutorial we have chosen to save the model, as it becomes a bit less confusing and easier to manage, although there are some pieces of code we may have to write twice (copy and paste). So first we will need to run the code from the tutorial and then save the model. This is super as simple we can just copy and paste the complete code provided at the end of the tutorial or download from Github. Then we can just run this extra line code (which I have copied from the documentation page) at the end of the notebook to save the model. </p> <pre><code>from joblib import dump\ndump(models['LogisticRegression'], 'logistic_regression.joblib')\n</code></pre> <p>Now that the file is saved, let's read it into this note book:</p> <pre><code>from joblib import load\n\nbrunos_model = load('logistic_regression.joblib')\nbrunos_model\n</code></pre> <pre>\n<code>LogisticRegression(n_jobs=-1, solver='saga')</code>\n</pre> <p>Now that we have the model loaded in, we need the data, to generate our predictions for today's races!</p> <pre><code># Import libraries required to download today's races\nimport os\nimport sys\n\n# Allow imports from src folder\nmodule_path = os.path.abspath(os.path.join('../src'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nfrom dateutil import tz\nfrom pandas.tseries.offsets import MonthEnd\nfrom sklearn.preprocessing import MinMaxScaler\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nfrom nltk.tokenize import regexp_tokenize\n\n# settings to display all columns\npd.set_option(\"display.max_columns\", None)\n\nimport fasttrack as ft\n\nfrom dotenv import load_dotenv\nload_dotenv()\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code># Validate FastTrack API connection\napi_key = os.getenv('FAST_TRACK_API_KEY',)\nclient = ft.Fasttrack(api_key)\ntrack_codes = client.listTracks()\n</code></pre> <pre>\n<code>Valid Security Key\n</code>\n</pre> <pre><code># Import race data excluding NZ races\nau_tracks_filter = list(track_codes[track_codes['state'] != 'NZ']['track_code'])\n\n# Time window to import data\n# First day of the month 46 months back from now\ndate_from = (datetime.today() - relativedelta(months=46)).replace(day=1).strftime('%Y-%m-%d')\n# First day of previous month\ndate_to = (datetime.today() - relativedelta(months=1)).replace(day=1).strftime('%Y-%m-%d')\n\n# Dataframes to populate data with\nrace_details = pd.DataFrame()\ndog_results = pd.DataFrame()\n\n# For each month, either fetch data from API or use local CSV file if we already have downloaded it\nfor start in pd.date_range(date_from, date_to, freq='MS'):\n    start_date = start.strftime(\"%Y-%m-%d\")\n    end_date = (start + MonthEnd(1)).strftime(\"%Y-%m-%d\")\n    try:\n        filename_races = f'FT_AU_RACES_{start_date}.csv'\n        filename_dogs = f'FT_AU_DOGS_{start_date}.csv'\n\n        filepath_races = f'../data/{filename_races}'\n        filepath_dogs = f'../data/{filename_dogs}'\n\n        print(f'Loading data from {start_date} to {end_date}')\n        if os.path.isfile(filepath_races):\n            # Load local CSV file\n            month_race_details = pd.read_csv(filepath_races) \n            month_dog_results = pd.read_csv(filepath_dogs) \n        else:\n            # Fetch data from API\n            month_race_details, month_dog_results = client.getRaceResults(start_date, end_date, au_tracks_filter)\n            month_race_details.to_csv(filepath_races, index=False)\n            month_dog_results.to_csv(filepath_dogs, index=False)\n\n        # Combine monthly data\n        race_details = race_details.append(month_race_details, ignore_index=True)\n        dog_results = dog_results.append(month_dog_results, ignore_index=True)\n    except:\n        print(f'Could not load data from {start_date} to {end_date}')\n</code></pre> <pre>\n<code>Loading data from 2018-09-01 to 2018-09-30\nLoading data from 2018-10-01 to 2018-10-31\nLoading data from 2018-11-01 to 2018-11-30\nLoading data from 2018-12-01 to 2018-12-31\nLoading data from 2019-01-01 to 2019-01-31\nLoading data from 2019-02-01 to 2019-02-28\nLoading data from 2019-03-01 to 2019-03-31\nLoading data from 2019-04-01 to 2019-04-30\nLoading data from 2019-05-01 to 2019-05-31\nLoading data from 2019-06-01 to 2019-06-30\nLoading data from 2019-07-01 to 2019-07-31\nLoading data from 2019-08-01 to 2019-08-31\nLoading data from 2019-09-01 to 2019-09-30\nLoading data from 2019-10-01 to 2019-10-31\nLoading data from 2019-11-01 to 2019-11-30\nLoading data from 2019-12-01 to 2019-12-31\nLoading data from 2020-01-01 to 2020-01-31\nLoading data from 2020-02-01 to 2020-02-29\nLoading data from 2020-03-01 to 2020-03-31\nLoading data from 2020-04-01 to 2020-04-30\nLoading data from 2020-05-01 to 2020-05-31\nLoading data from 2020-06-01 to 2020-06-30\nLoading data from 2020-07-01 to 2020-07-31\nLoading data from 2020-08-01 to 2020-08-31\nLoading data from 2020-09-01 to 2020-09-30\nLoading data from 2020-10-01 to 2020-10-31\nLoading data from 2020-11-01 to 2020-11-30\nLoading data from 2020-12-01 to 2020-12-31\nLoading data from 2021-01-01 to 2021-01-31\nLoading data from 2021-02-01 to 2021-02-28\nLoading data from 2021-03-01 to 2021-03-31\nLoading data from 2021-04-01 to 2021-04-30\nLoading data from 2021-05-01 to 2021-05-31\nLoading data from 2021-06-01 to 2021-06-30\nLoading data from 2021-07-01 to 2021-07-31\nLoading data from 2021-08-01 to 2021-08-31\nLoading data from 2021-09-01 to 2021-09-30\nLoading data from 2021-10-01 to 2021-10-31\nLoading data from 2021-11-01 to 2021-11-30\nLoading data from 2021-12-01 to 2021-12-31\nLoading data from 2022-01-01 to 2022-01-31\n</code>\n</pre> <pre>\n<code>c:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n</code>\n</pre> <pre>\n<code>Loading data from 2022-02-01 to 2022-02-28\nLoading data from 2022-03-01 to 2022-03-31\nLoading data from 2022-04-01 to 2022-04-30\nLoading data from 2022-05-01 to 2022-05-31\nLoading data from 2022-06-01 to 2022-06-30\n</code>\n</pre> <p>This piece of code we copied and pasted from the Greyhound Modelling Tutorial is fantastic! It has downloaded/read-in a ton of historic data! There is an issue though! We don't have the data for today's races, and also for any races that has occurred this month. This is because the code above only downloaded data up until the end of last month. </p> <p>For example, if we are in the middle of June, then any races in the first two weeks of June won't be downloaded by the chunk of code above. An issue is that if we download it now, when tomorrow rolls around it won't include the extra races that have finished today. </p> <p>So, the simple but inefficient solution is that every single day we redownload all the races that have already concluded this month. (Ideally you have some sort of database set up or you store and download your data in a daily format instead of the monthly format)</p> <pre><code>race_details.tail()\n</code></pre> @id RaceNum RaceName RaceTime Distance RaceGrade Track date 88510 792243395 6 SKY RACING (N/P) STAKE 01:27PM 300m Restricted Win Murray Bridge (MBS) 07 Jun 22 88511 792243396 7 KURT DONSBERG PHOTOGRAPHY MIXED STAKE 01:44PM 300m Mixed 4/5 Murray Bridge (MBS) 07 Jun 22 88512 792243397 8 GREYHOUNDS AS PETS 02:04PM 300m Grade 5 Final Murray Bridge (MBS) 07 Jun 22 88513 792243398 9 @THEDOGSSA (N/P) STAKE 02:19PM 300m Restricted Win Murray Bridge (MBS) 07 Jun 22 88514 792243399 10 FOLLOW THEDOGSSA ON TWITTER (N/P) STAKE 02:39PM 300m Restricted Win Murray Bridge (MBS) 07 Jun 22 <pre><code>current_month_start_date = pd.Timestamp.now().replace(day=1).strftime(\"%Y-%m-%d\")\ncurrent_month_end_date = (pd.Timestamp.now().replace(day=1)+ MonthEnd(1))\ncurrent_month_end_date = (current_month_end_date - pd.Timedelta('1 day')).strftime(\"%Y-%m-%d\")\n\nprint(f'Start date: {current_month_start_date}')\nprint(f'End Date: {current_month_end_date}')\n</code></pre> <pre>\n<code>Start date: 2022-07-01\nEnd Date: 2022-07-30\n</code>\n</pre> <pre><code># Download data for races that have concluded this current month up untill today\n# Start and end dates for current month\ncurrent_month_start_date = pd.Timestamp.now().replace(day=1).strftime(\"%Y-%m-%d\")\ncurrent_month_end_date = (pd.Timestamp.now().replace(day=1)+ MonthEnd(1))\ncurrent_month_end_date = (current_month_end_date - pd.Timedelta('1 day')).strftime(\"%Y-%m-%d\")\n\n# Files names \nfilename_races = f'FT_AU_RACES_{current_month_start_date}.csv'\nfilename_dogs = f'FT_AU_DOGS_{current_month_start_date}.csv'\n# Where to store files locally\nfilepath_races = f'../data/{filename_races}'\nfilepath_dogs = f'../data/{filename_dogs}'\n\n# Fetch data from API\nmonth_race_details, month_dog_results = client.getRaceResults(current_month_start_date, current_month_end_date, au_tracks_filter)\n\n# Save the files locally and replace any out of date fields\nmonth_race_details.to_csv(filepath_races, index=False)\nmonth_dog_results.to_csv(filepath_dogs, index=False)\n</code></pre> <pre>\n<code>Getting meets for each date ..\n</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:14&lt;00:00,  2.01it/s]\n</code>\n</pre> <pre>\n<code>Getting historic results details ..\n</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162/162 [01:30&lt;00:00,  1.80it/s]\n</code>\n</pre> <pre><code>dog_results\n</code></pre> @id Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney RaceId TrainerId TrainerName 0 114215500 1 DR. MURPHY 7.0 10 29.7 $4.10 NaN 4.24 NaN Q/111 0 NaN 4.70 22.84 NaN 356387352 107925 W McMahon 1 131737955 2 MOLLY SPOLLY 8.0 8 27.3 $2.20F NaN 4.24 4.24 M/222 0 NaN 4.72 23.14 NaN 356387352 199516 K Leviston 2 204414097 3 ASTON NARITA 2.0 2 29.2 $4.50 NaN 4.94 0.70 M/343 2 NaN 4.88 23.19 NaN 356387352 101224 K Gorman 3 126744995 4 ONI 6.0 6 25.0 $15.50 NaN 5.70 0.76 S/674 0 NaN 4.95 23.24 NaN 356387352 107925 W McMahon 4 120958941 5 DARCON FLASH 1.0 1 29.3 $31.00 NaN 6.54 0.84 M/765 8 NaN 4.96 23.30 NaN 356387352 125087 R Conway ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 749514 415996416 1 WEBLEC WHIRL 2.0 2 27.6 $4.20 NaN 0.10 NaN 0 0 NaN 4.38 16.75 510.0 792243399 76598 N Loechel 749515 557002281 2 UP THERE BILLY 1.0 1 32.7 $1.80F NaN 0.10 0.14 NaN 0 NaN 4.43 16.76 175.0 792243399 327728 J Trengove 749516 529022935 3 WEBLEC FLAME 6.0 6 31.8 $5.50 NaN 4.25 4.00 NaN 0 NaN 4.48 17.04 140.0 792243399 76598 N Loechel 749517 383604709 4 STRAIGHT BLAZE 8.0 8 34.7 $23.00 NaN 5.00 0.86 NaN 0 NaN 4.61 17.10 115.0 792243399 123529 D Johnstone 749518 529022943 5 WEBLEC MIST 4.0 4 27.6 $7.00 NaN 15.00 10.14 0 0 VINJ(21) 4.67 17.81 0.0 792243399 76598 N Loechel <p>749519 rows \u00d7 19 columns</p> <pre><code># This is super important I have spent literally hours before I found out this was causing errors\ndog_results['@id'] = pd.to_numeric(dog_results['@id'])\n</code></pre> <pre><code># Append the extra data to our data frames \nrace_details = race_details.append(month_race_details, ignore_index=True)\ndog_results = dog_results.append(month_dog_results, ignore_index=True)\n</code></pre> <p>What we are really interested in are races that are scheduled for today as we want to use our model to predict their ratings. So, let's write some code we can run in the morning that will download the data for the day:</p> <pre><code># Download the data for todays races\ntodays_date = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\ntodays_races, todays_dogs = client.getFullFormat(dt= todays_date, tracks = au_tracks_filter)\n\ndisplay(todays_races.head(1), todays_dogs.head(1))\n</code></pre> <pre>\n<code>Getting meets for each date ..\n</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.89it/s]\n</code>\n</pre> <pre>\n<code>Getting dog lineups ..\n</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:13&lt;00:00,  1.14s/it]\n</code>\n</pre> @id RaceNum RaceName RaceTime RaceTimeDateUTC Distance RaceGrade PrizeMoney1 PrizeMoney2 PrizeMoney3 PrizeMoney4 PrizeMoney5 PrizeMoney6 PrizeMoney7 PrizeMoney8 GOBIS Hurdle Handicap TAB GradeCode VICGREYS RaceComment Track Date Quali TipsComments_Bet TipsComments_Tips 0 801896110 1 GPP LASER3300 06:44PM 04 Jul 22 08:44AM 385m Maiden $1600 $460 $230 $115 None None None None None None None TRI/QUIN R/D EXACTA PICK4 M None \"KASUMI BERRY (5) is a well bred type and her ... Shepparton 04 Jul 22 None Box Quinella 1,2,4,7 ($10 for 166.67%) 4, 7, 1, 2 @id RaceBox DogName BestTime DogHandicap Odds Rating Speed DogComment StartsTOT StartsTTD Suburb Owner Colour Sex Whelped DogGrade DogGOBIS DogPRIZE AgedPrizeMoney Form DamId DamName SireId SireName TrainerId TrainerName RaceId 0 536196758 1 HAVE A SHIRAZ FSH None $4.60 0 None Dam produced the highly talented Flaming rush Starts 0-0-0-0 Trk/Dst 0-0-0-0 Heathcote Paul Ellis BK B 03 Dec 19 M N 0 0 [None, None, None, None, None] 1550070039 Pepper Shiraz -710494 Barcia Bale 117228 Jason Formosa 801896110 <pre><code># It seems that the todays_races dataframe doesn't have the date column, so let's add that on\ntodays_races['date'] = pd.Timestamp.now().strftime('%d %b %y')\ntodays_races.head(1)\n</code></pre> @id RaceNum RaceName RaceTime RaceTimeDateUTC Distance RaceGrade PrizeMoney1 PrizeMoney2 PrizeMoney3 PrizeMoney4 PrizeMoney5 PrizeMoney6 PrizeMoney7 PrizeMoney8 GOBIS Hurdle Handicap TAB GradeCode VICGREYS RaceComment Track Date Quali TipsComments_Bet TipsComments_Tips date 0 801896110 1 GPP LASER3300 06:44PM 04 Jul 22 08:44AM 385m Maiden $1600 $460 $230 $115 None None None None None None None TRI/QUIN R/D EXACTA PICK4 M None \"KASUMI BERRY (5) is a well bred type and her ... Shepparton 04 Jul 22 None Box Quinella 1,2,4,7 ($10 for 166.67%) 4, 7, 1, 2 04 Jul 22 <pre><code># It also seems that in todays_dogs dataframe Box is labeled as RaceBox instead, so let's rename it\n# We can also see that there are some specific dogs that have \"Res.\" as a suffix of their name, i.e. they are reserve dogs,\n# We will treat this later\ntodays_dogs = todays_dogs.rename(columns={\"RaceBox\":\"Box\"})\ntodays_dogs.tail(3)\n</code></pre> @id Box DogName BestTime DogHandicap Odds Rating Speed DogComment StartsTOT StartsTTD Suburb Owner Colour Sex Whelped DogGrade DogGOBIS DogPRIZE AgedPrizeMoney Form DamId DamName SireId SireName TrainerId TrainerName RaceId 1061 400500428 5 TEQUILA TALKING 22.63 None None 100 66.425 None Starts 58-11-7-12 Trk/Dst 28-6-5-5 Wolffdene Fives Alive Synd D Wolff,N Brauer BE D 19 Oct 18 4 N 28855 None [{'Place': '2nd', 'FormBox': '5', 'Weight': '3... 255840075 Sivamet -737547 Hostile 93322 Michael Brauer 801490825 1062 525622257 7 REFERRAL FSTD None None 81 61.343 None Starts 51-4-7-6 Trk/Dst 0-0-0-0 Laidley Heights Bad Decisions Synd P O'Reilly,A Pearce,D Henery BD D 22 Oct 19 5 N 13945 None [{'Place': '5th', 'FormBox': '7', 'Weight': '3... 257880044 Lovelace 792880037 Sh Avatar 313314 Andrew Pearce 801490825 1063 566347962 8 STARDUST DREAMS NBT None None 92 61.271 None Starts 22-3-4-2 Trk/Dst 2-0-1-0 Park Ridge Kerri-Lyn Harkness BK D 07 Mar 20 5 N 8240 None [{'Place': '2nd', 'FormBox': '8', 'Weight': '3... 118703516 Ellie Belles 141317074 My Redeemer 127311 Stephen Woods 801490825 <pre><code># Appending todays data to this months data\nmonth_dog_results = pd.concat([month_dog_results,todays_dogs],join='outer')[month_dog_results.columns]\nmonth_race_details = pd.concat([month_race_details,todays_races],join='outer')[month_race_details.columns]\n\n# Appending this months data to the rest of our historical data\nrace_details = race_details.append(month_race_details, ignore_index=True)\ndog_results = dog_results.append(month_dog_results, ignore_index=True)\n</code></pre> <p>Originally I thought that since we now that we have all the data we can easily copy and paste the code used in the greyhound modelling tutorial to clean our data and create the features. </p> <p>But after staring at weird predictions and spending hours trying to work out why some things weren't working I realised that for the most part we can copy and paste code, but when working with the live data we do need to make a few changes. I'll point them out when we get to it, but the main things that tripped me up is the data types the FastTrack API gives and that we need a system to work around reserve dogs</p> <pre><code>race_details\n</code></pre> @id RaceNum RaceName RaceTime Distance RaceGrade Track date 0 356387352 1 RUTTER'S BUTCHERY &amp; POULTRY 05:29PM 395m Mixed 6/7 Traralgon 01 Sep 18 1 356387359 2 TAB - WE LOVE A BET 05:47PM 395m Grade 5 Traralgon 01 Sep 18 2 356387358 3 HALEY CONCRETING 06:05PM 395m Grade 5 Traralgon 01 Sep 18 3 356387355 4 R.W &amp; A.R INGLIS ELECTRICIANS 06:29PM 395m Free For All Traralgon 01 Sep 18 4 356387363 5 PRINTMAC 06:45PM 525m Grade 5 Traralgon 01 Sep 18 ... ... ... ... ... ... ... ... ... 89359 801490821 5 SENNACHIE @ STUD - STEVE WHITE 08:13PM 520m Grade 5 Heat Albion Park 04 Jul 22 89360 801490822 6 ORSON ALLEN @ METICULOUS LODGE 08:35PM 520m Grade 5 Heat Albion Park 04 Jul 22 89361 801490823 7 SKY RACING 08:53PM 600m Mixed 4/5 Albion Park 04 Jul 22 89362 801490824 8 BORGBET TIPPING SERVICE 09:15PM 520m Mixed 3/4 Albion Park 04 Jul 22 89363 801490825 9 TIGGERLONG TONK @ STUD 09:37PM 395m Mixed 4/5 Albion Park 04 Jul 22 <p>89364 rows \u00d7 8 columns</p> <p>The first thing that tripped me up was when <code>FastTrack_DogId</code> for live data was in a string format, and because everything looks like it works, it took ages to find this error. So, let's make sure we deal with it here using:</p> <pre><code>dog_results['FastTrack_DogId'] = pd.to_numeric(dog_results['FastTrack_DogId'])\n</code></pre> <pre><code>## Cleanse and normalise the data\n# Clean up the race dataset\nrace_details = race_details.rename(columns = {'@id': 'FastTrack_RaceId'})\nrace_details['Distance'] = race_details['Distance'].apply(lambda x: int(x.replace(\"m\", \"\")))\nrace_details['date_dt'] = pd.to_datetime(race_details['date'], format = '%d %b %y')\n# Clean up the dogs results dataset\ndog_results = dog_results.rename(columns = {'@id': 'FastTrack_DogId', 'RaceId': 'FastTrack_RaceId'})\n\n# New line of code (rest of this code chunk is copied from bruno's code)\ndog_results['FastTrack_DogId'] = pd.to_numeric(dog_results['FastTrack_DogId'])\n\n# Combine dogs results with race attributes\ndog_results = dog_results.merge(\n    race_details, \n    how = 'left',\n    on = 'FastTrack_RaceId'\n)\n\n# Convert StartPrice to probability\ndog_results['StartPrice'] = dog_results['StartPrice'].apply(lambda x: None if x is None else float(x.replace('$', '').replace('F', '')) if isinstance(x, str) else x)\ndog_results['StartPrice_probability'] = (1 / dog_results['StartPrice']).fillna(0)\ndog_results['StartPrice_probability'] = dog_results.groupby('FastTrack_RaceId')['StartPrice_probability'].apply(lambda x: x / x.sum())\n\n# Discard entries without results (scratched or did not finish)\ndog_results = dog_results[~dog_results['Box'].isnull()]\ndog_results['Box'] = dog_results['Box'].astype(int)\n\n# Clean up other attributes\ndog_results['RunTime'] = dog_results['RunTime'].astype(float)\ndog_results['SplitMargin'] = dog_results['SplitMargin'].astype(float)\ndog_results['Prizemoney'] = dog_results['Prizemoney'].astype(float).fillna(0)\ndog_results['Place'] = pd.to_numeric(dog_results['Place'].apply(lambda x: x.replace(\"=\", \"\") if isinstance(x, str) else 0), errors='coerce').fillna(0)\ndog_results['win'] = dog_results['Place'].apply(lambda x: 1 if x == 1 else 0)\n\n# Normalise some of the raw values\ndog_results['Prizemoney_norm'] = np.log10(dog_results['Prizemoney'] + 1) / 12\ndog_results['Place_inv'] = (1 / dog_results['Place']).fillna(0)\ndog_results['Place_log'] = np.log10(dog_results['Place'] + 1).fillna(0)\ndog_results['RunSpeed'] = (dog_results['RunTime'] / dog_results['Distance']).fillna(0)\n</code></pre> <pre><code>## Generate features using raw data\n# Calculate median winner time per track/distance\nwin_results = dog_results[dog_results['win'] == 1]\nmedian_win_time = pd.DataFrame(data=win_results[win_results['RunTime'] &amp;gt; 0].groupby(['Track', 'Distance'])['RunTime'].median()).rename(columns={\"RunTime\": \"RunTime_median\"}).reset_index()\nmedian_win_split_time = pd.DataFrame(data=win_results[win_results['SplitMargin'] &amp;gt; 0].groupby(['Track', 'Distance'])['SplitMargin'].median()).rename(columns={\"SplitMargin\": \"SplitMargin_median\"}).reset_index()\nmedian_win_time.head()\n\n# Calculate track speed index\nmedian_win_time['speed_index'] = (median_win_time['RunTime_median'] / median_win_time['Distance'])\nmedian_win_time['speed_index'] = MinMaxScaler().fit_transform(median_win_time[['speed_index']])\nmedian_win_time.head()\n\n# Compare dogs finish time with median winner time\ndog_results = dog_results.merge(median_win_time, on=['Track', 'Distance'], how='left')\ndog_results = dog_results.merge(median_win_split_time, on=['Track', 'Distance'], how='left')\n\n# Normalise time comparison\ndog_results['RunTime_norm'] = (dog_results['RunTime_median'] / dog_results['RunTime']).clip(0.9, 1.1)\ndog_results['RunTime_norm'] = MinMaxScaler().fit_transform(dog_results[['RunTime_norm']])\ndog_results['SplitMargin_norm'] = (dog_results['SplitMargin_median'] / dog_results['SplitMargin']).clip(0.9, 1.1)\ndog_results['SplitMargin_norm'] = MinMaxScaler().fit_transform(dog_results[['SplitMargin_norm']])\ndog_results.head()\n\n# Calculate box winning percentage for each track/distance\nbox_win_percent = pd.DataFrame(data=dog_results.groupby(['Track', 'Distance', 'Box'])['win'].mean()).rename(columns={\"win\": \"box_win_percent\"}).reset_index()\n# Add to dog results dataframe\ndog_results = dog_results.merge(box_win_percent, on=['Track', 'Distance', 'Box'], how='left')\n# Display example of barrier winning probabilities\nprint(box_win_percent.head(8))\n</code></pre> <pre>\n<code>         Track  Distance  Box  box_win_percent\n0  Albion Park       331    1         0.198089\n1  Albion Park       331    2         0.152116\n2  Albion Park       331    3         0.127354\n3  Albion Park       331    4         0.126605\n4  Albion Park       331    5         0.111058\n5  Albion Park       331    6         0.109304\n6  Albion Park       331    7         0.105310\n7  Albion Park       331    8         0.115146\n</code>\n</pre> <p>The second thing that we need to add is related to reserve dogs, and this took me ages to come to this solution, but if you have a better one, please submit a pull request.</p> <p>Basically, a single greyhound can be a reserve dog for multiple races on the same day. They each appear as a new row in our data frame. For example, 'MACI REID' is a reserve dog for three different races on the 2022-09-02:</p> <p></p> <p>When we try lag our data by using <code>.shift(1)</code> like in Bruno's original code it will produce the wrong values for our features. In the above example only the first race The Gardens Race 4 (the third row) will have correct data but all the rows under it will have incorrectly calculated features. We need each of the following rows to be the same as the third row. The solution that I have come up with is a little bit complicated, but it gets the job done:</p> <pre><code># Please submit a pull request if you have a better solution\ntemp = rolling_result.reset_index()\ntemp = temp[temp['date_dt'] == pd.Timestamp.now().normalize()]\ntemp.groupby(['FastTrack_DogId','date_dt']).first()\nrolling_result.loc[pd.IndexSlice[:, pd.Timestamp.now().normalize()], :] = temp.groupby(['FastTrack_DogId','date_dt']).first()\n</code></pre> <p>Basically, for each greyhound we can just take the first row of data (which is correct) and set the rest of today's races to have the same value</p> <pre><code># Generate rolling window features\ndataset = dog_results.copy()\ndataset = dataset.set_index(['FastTrack_DogId', 'date_dt']).sort_index()\n\n# Use rolling window of 28, 91 and 365 days\nrolling_windows = ['28D', '91D', '365D']\n# Features to use for rolling windows calculation\nfeatures = ['RunTime_norm', 'SplitMargin_norm', 'Place_inv', 'Place_log', 'Prizemoney_norm']\n# Aggregation functions to apply\naggregates = ['min', 'max', 'mean', 'median', 'std']\n# Keep track of generated feature names\nfeature_cols = ['speed_index', 'box_win_percent']\n\nfor rolling_window in rolling_windows:\n        print(f'Processing rolling window {rolling_window}')\n\n        rolling_result = (\n            dataset\n            .reset_index(level=0).sort_index()\n            .groupby('FastTrack_DogId')[features]\n            .rolling(rolling_window)\n            .agg(aggregates)\n            .groupby(level=0)  # Thanks to Brett for finding this!\n            .shift(1)\n        )\n\n        # My own dodgey code to work with reserve dogs\n        temp = rolling_result.reset_index()\n        temp = temp[temp['date_dt'] == pd.Timestamp.now().normalize()]\n        temp.groupby(['FastTrack_DogId','date_dt']).first()\n        rolling_result.loc[pd.IndexSlice[:, pd.Timestamp.now().normalize()], :] = temp.groupby(['FastTrack_DogId','date_dt']).first()\n\n        # Generate list of rolling window feature names (eg: RunTime_norm_min_365D)\n        agg_features_cols = [f'{f}_{a}_{rolling_window}' for f, a in itertools.product(features, aggregates)]\n        # Add features to dataset\n        dataset[agg_features_cols] = rolling_result\n        # Keep track of generated feature names\n        feature_cols.extend(agg_features_cols)\n</code></pre> <pre>\n<code>Processing rolling window 28D\n</code>\n</pre> <pre>\n<code>c:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\pandas\\core\\generic.py:4150: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n</code>\n</pre> <pre>\n<code>Processing rolling window 91D\n</code>\n</pre> <pre>\n<code>c:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\pandas\\core\\generic.py:4150: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n</code>\n</pre> <pre>\n<code>Processing rolling window 365D\n</code>\n</pre> <pre>\n<code>c:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\pandas\\core\\generic.py:4150: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n</code>\n</pre> <pre><code># Replace missing values with 0\ndataset.fillna(0, inplace=True)\ndisplay(dataset.head(8))\n\n# Only keep data after 2018-12-01\nmodel_df = dataset.reset_index()\nfeature_cols = np.unique(feature_cols).tolist()\nmodel_df = model_df[model_df['date_dt'] &amp;gt;= '2018-12-01']\n\n# This line was originally part of Bruno's tutorial, but we don't run it in this script\n# model_df = model_df[['date_dt', 'FastTrack_RaceId', 'DogName', 'win', 'StartPrice_probability'] + feature_cols]\n\n# Only train model off of races where each dog has a value for each feature\nraces_exclude = model_df[model_df.isnull().any(axis = 1)]['FastTrack_RaceId'].drop_duplicates()\nmodel_df = model_df[~model_df['FastTrack_RaceId'].isin(races_exclude)]\n</code></pre> Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceNum RaceName RaceTime Distance RaceGrade Track date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D FastTrack_DogId date_dt -2143477291 2018-09-02 7.0 YOU TELAM ANFY 1 1 31.2 5.2 0.0 7.0 0.57 8 0 8.0 7.48 19.85 0.0 354469749 8462 A Bunney 12 BRISGREYS.COM 09:01PM 331 GRADE 5  PATHWAY NON-PENALTY Albion Park 02 Sep 18 0.161184 0 0.0 0.142857 0.903090 0.059970 19.17 0.600092 7.18 0.328715 0.299465 0.198089 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.0 0.0 0.0 0.0 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.0 0.0 0.0 0.0 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.00000 0.000000 0.000000 0.000000 0.000000 0.0 0.0 0.0 0.0 0.0 2018-09-16 4.0 YOU TELAM ANFY 1 1 31.0 61.0 0.0 10.0 5.0 8 0 8.0 7.43 19.75 0.0 360928569 8462 A Bunney 11 SKY RACING 05:24PM 331 Grade 5 Albion Park 16 Sep 18 0.013847 0 0.0 0.250000 0.698970 0.059668 19.17 0.600092 7.18 0.353165 0.331763 0.198089 0.328715 0.328715 0.328715 0.328715 0.000000 0.299465 0.299465 0.299465 0.299465 0.000000 0.142857 0.142857 0.142857 0.142857 0.000000 0.903090 0.903090 0.903090 0.903090 0.000000 0.0 0.0 0.0 0.0 0.0 0.328715 0.328715 0.328715 0.328715 0.000000 0.299465 0.299465 0.299465 0.299465 0.000000 0.142857 0.142857 0.142857 0.142857 0.000000 0.903090 0.903090 0.903090 0.903090 0.000000 0.0 0.0 0.0 0.0 0.0 0.328715 0.328715 0.328715 0.328715 0.000000 0.299465 0.299465 0.299465 0.299465 0.000000 0.142857 0.142857 0.142857 0.142857 0.000000 0.90309 0.903090 0.903090 0.903090 0.000000 0.0 0.0 0.0 0.0 0.0 2018-10-07 7.0 YOU TELAM ANFY 1 1 30.5 71.0 0.0 8.25 4.0 7 0 7.0 7.42 19.71 0.0 367774713 8462 A Bunney 11 SKY RACING 08:27PM 331 Grade 5 Albion Park 07 Oct 18 0.011604 0 0.0 0.142857 0.903090 0.059547 19.17 0.600092 7.18 0.363014 0.338275 0.198089 0.328715 0.353165 0.340940 0.340940 0.017288 0.299465 0.331763 0.315614 0.315614 0.022838 0.142857 0.250000 0.196429 0.196429 0.075761 0.698970 0.903090 0.801030 0.801030 0.144335 0.0 0.0 0.0 0.0 0.0 0.328715 0.353165 0.340940 0.340940 0.017288 0.299465 0.331763 0.315614 0.315614 0.022838 0.142857 0.250000 0.196429 0.196429 0.075761 0.698970 0.903090 0.801030 0.801030 0.144335 0.0 0.0 0.0 0.0 0.0 0.328715 0.353165 0.340940 0.340940 0.017288 0.299465 0.331763 0.315614 0.315614 0.022838 0.142857 0.250000 0.196429 0.196429 0.075761 0.69897 0.903090 0.801030 0.801030 0.144335 0.0 0.0 0.0 0.0 0.0 2018-10-21 5.0 YOU TELAM ANFY 7 9 29.8 26.0 0.0 5.25 0.29 6 0 6.0 7.46 19.85 0.0 370420123 8462 A Bunney 12 ZILLMERE SPORTS 08:55PM 331 GRADE 5  PATHWAY NON-PENALTY Albion Park 21 Oct 18 0.032235 0 0.0 0.200000 0.778151 0.059970 19.17 0.600092 7.18 0.328715 0.312332 0.105310 0.353165 0.363014 0.358089 0.358089 0.006964 0.331763 0.338275 0.335019 0.335019 0.004605 0.142857 0.250000 0.196429 0.196429 0.075761 0.698970 0.903090 0.801030 0.801030 0.144335 0.0 0.0 0.0 0.0 0.0 0.328715 0.363014 0.348298 0.353165 0.017659 0.299465 0.338275 0.323168 0.331763 0.020784 0.142857 0.250000 0.178571 0.142857 0.061859 0.698970 0.903090 0.835050 0.903090 0.117849 0.0 0.0 0.0 0.0 0.0 0.328715 0.363014 0.348298 0.353165 0.017659 0.299465 0.338275 0.323168 0.331763 0.020784 0.142857 0.250000 0.178571 0.142857 0.061859 0.69897 0.903090 0.835050 0.903090 0.117849 0.0 0.0 0.0 0.0 0.0 2018-11-18 5.0 YOU TELAM ANFY 1 1 30.2 41.0 0.0 6.25 3.14 6 0 6.0 7.43 19.86 0.0 378695693 8462 A Bunney 10 ZILLMERE SPORTS 08:16PM 331 Grade 5 Albion Park 18 Nov 18 0.020215 0 0.0 0.200000 0.778151 0.060000 19.17 0.600092 7.18 0.326284 0.331763 0.198089 0.328715 0.363014 0.345865 0.345865 0.024253 0.312332 0.338275 0.325304 0.325304 0.018344 0.142857 0.200000 0.171429 0.171429 0.040406 0.778151 0.903090 0.840621 0.840621 0.088345 0.0 0.0 0.0 0.0 0.0 0.328715 0.363014 0.343402 0.340940 0.017429 0.299465 0.338275 0.320459 0.322048 0.017814 0.142857 0.250000 0.183929 0.171429 0.051632 0.698970 0.903090 0.820825 0.840621 0.100341 0.0 0.0 0.0 0.0 0.0 0.328715 0.363014 0.343402 0.340940 0.017429 0.299465 0.338275 0.320459 0.322048 0.017814 0.142857 0.250000 0.183929 0.171429 0.051632 0.69897 0.903090 0.820825 0.840621 0.100341 0.0 0.0 0.0 0.0 0.0 2019-06-23 5.0 YOU TELAM ANFY 1 1 29.6 51.0 0.0 5.75 0.86 8 0 8.0 7.44 19.66 0.0 445056131 8462 A Bunney 9 GREYHOUND ADOPTION PROGRAM 07:54PM 331 Grade 5 Albion Park 23 Jun 19 0.016271 0 0.0 0.200000 0.778151 0.059396 19.17 0.600092 7.18 0.375381 0.325269 0.198089 0.326284 0.326284 0.326284 0.326284 0.000000 0.331763 0.331763 0.331763 0.331763 0.000000 0.200000 0.200000 0.200000 0.200000 0.000000 0.778151 0.778151 0.778151 0.778151 0.000000 0.0 0.0 0.0 0.0 0.0 0.326284 0.363014 0.339979 0.328715 0.016924 0.299465 0.338275 0.322720 0.331763 0.016234 0.142857 0.250000 0.187143 0.200000 0.045288 0.698970 0.903090 0.812290 0.778151 0.088969 0.0 0.0 0.0 0.0 0.0 0.326284 0.363014 0.339979 0.328715 0.016924 0.299465 0.338275 0.322720 0.331763 0.016234 0.142857 0.250000 0.187143 0.200000 0.045288 0.69897 0.903090 0.812290 0.778151 0.088969 0.0 0.0 0.0 0.0 0.0 2019-06-30 8.0 YOU TELAM ANFY 4 10 29.5 51.0 0.0 17.25 2.0 7 0 7.0 11.38 24.16 0.0 448789428 8462 A Bunney 7 SKY RACING 07:45PM 395 Open Albion Park 30 Jun 19 0.015995 0 0.0 0.125000 0.954243 0.061165 22.85 0.588509 10.58 0.228891 0.148506 0.130206 0.375381 0.375381 0.375381 0.375381 0.000000 0.325269 0.325269 0.325269 0.325269 0.000000 0.200000 0.200000 0.200000 0.200000 0.000000 0.778151 0.778151 0.778151 0.778151 0.000000 0.0 0.0 0.0 0.0 0.0 0.375381 0.375381 0.375381 0.375381 0.000000 0.325269 0.325269 0.325269 0.325269 0.000000 0.200000 0.200000 0.200000 0.200000 0.000000 0.778151 0.778151 0.778151 0.778151 0.000000 0.0 0.0 0.0 0.0 0.0 0.326284 0.375381 0.345879 0.340940 0.020929 0.299465 0.338275 0.323145 0.328516 0.014558 0.142857 0.250000 0.189286 0.200000 0.040846 0.69897 0.903090 0.806601 0.778151 0.080787 0.0 0.0 0.0 0.0 0.0 2019-08-25 6.0 YOU TELAM ANFY 4 4 29.5 14.0 0.0 5.0 0.57 3 0 3.0 7.33 19.72 0.0 465432748 8462 A Bunney 9 FABREGAS @ METICULOUS LODGE 07:40PM 331 Masters Grade 5 Albion Park 25 Aug 19 0.058684 0 0.0 0.166667 0.845098 0.059577 19.17 0.600092 7.18 0.360548 0.397681 0.126605 0.228891 0.375381 0.302136 0.302136 0.103585 0.148506 0.325269 0.236887 0.236887 0.124990 0.125000 0.200000 0.162500 0.162500 0.053033 0.778151 0.954243 0.866197 0.866197 0.124515 0.0 0.0 0.0 0.0 0.0 0.228891 0.375381 0.302136 0.302136 0.103585 0.148506 0.325269 0.236887 0.236887 0.124990 0.125000 0.200000 0.162500 0.162500 0.053033 0.778151 0.954243 0.866197 0.866197 0.124515 0.0 0.0 0.0 0.0 0.0 0.228891 0.375381 0.329166 0.328715 0.048169 0.148506 0.338275 0.298196 0.325269 0.067332 0.125000 0.250000 0.180102 0.200000 0.044505 0.69897 0.954243 0.827692 0.778151 0.092481 0.0 0.0 0.0 0.0 0.0 <p>Now this is the part that gets a bit hairy, so I am going to split it up into two parts. The good thing is that the coding will remain relatively simple.</p> <p>The two things that I want to do is place live bets and save our predictions so that we can use them in a simulator we will create in the Part V.</p> <p>Let's save our historical ratings for our simulator first as its quick and straight forward and then move on to placing live bets:</p> <p>Feeding our predictions through the simulator is entirely optional, but, in my opinion it is where the real sauce is made. The idea is that if we are testing our model live, we can also use the simulator to test what would happen if we tested different staking methodologies, market timings and bet placement to optimise our model. This way you can have a model but test out different strategies to optimise model performance. The thing is, I have had a play with the simulator already and we can't simulate <code>market_catalogue</code> unless you have recorded it yourself (which is what I'll be using to get <code>market_id</code> and <code>selection_id</code> to place live bets). The simulator we will use later on will only take your ratings, market_id and selection_id, so we need our data in a similar format to what we had in How to automate III. In other words, since we don't have  <code>market_catalogue</code> in the simulator, we need another way to get the market_id and selection_id.</p> <p>My hacky work around is to generate the probabilities like normal (since the data is historical), we don't need to deal with reserve dogs and scratching's, then get the <code>market_id</code> and <code>selection_id</code> from the Betfair datascience greyhound model by merging on DogName and date. We can take the code we wrote in How to automate III that downloads the greyhound ratings and convert that into a function that downloads the ratings for a date range.</p> <pre><code># Generate predictions like normal\n# Range of dates that we want to simulate later '2022-03-01' to '2022-04-01'\ntodays_data = model_df[(model_df['date_dt'] &amp;gt;= pd.Timestamp('2022-03-01').strftime('%Y-%m-%d')) &amp;amp; (model_df['date_dt'] &amp;lt; pd.Timestamp('2022-04-01').strftime('%Y-%m-%d'))]\ndog_win_probabilities = brunos_model.predict_proba(todays_data[feature_cols])[:,1]\ntodays_data['prob_LogisticRegression'] = dog_win_probabilities\ntodays_data['renormalise_prob'] = todays_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / x.sum())\ntodays_data['rating'] = 1/todays_data['renormalise_prob']\ntodays_data = todays_data.sort_values(by = 'date_dt')\ntodays_data\n</code></pre> <pre>\n<code>C:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/3121846001.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  todays_data['prob_LogisticRegression'] = dog_win_probabilities\nC:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/3121846001.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  todays_data['renormalise_prob'] = todays_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / x.sum())\nC:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/3121846001.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  todays_data['rating'] = 1/todays_data['renormalise_prob']\n</code>\n</pre> FastTrack_DogId date_dt Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceNum RaceName RaceTime Distance RaceGrade Track date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D prob_LogisticRegression renormalise_prob rating 526490 523685389 2022-03-01 1.0 JOSEPH RUMBLE 8 8 30.1 2.5 0.0 8.25 0 0 0 0 0.00 21.83 1365.0 764579619 91264 B Belford 1 TAB 06:55PM 380 Novice Non Penalty Townsville 01 Mar 22 0.318829 1 0.261288 1.000000 0.301030 0.057447 22.10 0.641825 7.56 0.561842 0.000000 0.123070 0.514985 0.514985 0.514985 0.514985 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 1.00 1.000000 1.000000 0.000000 0.301030 0.301030 0.301030 0.301030 0.000000 0.264578 0.264578 2.645776e-01 0.264578 0.000000 0.386805 0.514985 0.450895 0.450895 0.090637 0.482301 0.482301 0.482301 0.482301 0.000000 0.500000 1.00 0.750000 0.750000 0.353553 0.30103 0.477121 0.389076 0.389076 0.124515 0.238161 0.264578 0.251369 0.251369 0.018679 0.386805 0.514985 0.450895 0.450895 0.090637 0.482301 0.482301 0.482301 0.482301 0.000000 0.500000 1.00 0.750000 0.750000 0.353553 0.30103 0.477121 0.389076 0.389076 0.124515 0.238161 0.264578 0.251369 0.251369 0.018679 0.360558 0.321076 3.114527 494469 482776246 2022-03-01 1.0 BLAZING NENNA 4 4 25.7 2.3 0.0 3.31 0 Q/11 0 0 10.27 23.30 0.0 764625202 115912 M Delbridge 6 CHS GROUP HT3 05:37PM 410 Grade 5 Heat Horsham 01 Mar 22 0.365634 1 0.000000 1.000000 0.301030 0.056829 23.46 0.480327 10.39 0.534335 0.558423 0.131261 0.500000 0.513187 0.504396 0.500000 0.007613 0.464830 0.633333 0.570108 0.612161 0.091786 0.250000 1.00 0.500000 0.250000 0.433013 0.301030 0.698970 0.566323 0.698970 0.229751 0.000000 0.000000 1.628327e-15 0.000000 0.000000 0.307377 0.609439 0.508348 0.524719 0.074167 0.373358 0.651515 0.543480 0.584034 0.093516 0.142857 1.00 0.535714 0.500000 0.324194 0.30103 0.903090 0.528325 0.477121 0.198314 0.000000 0.000000 0.000000 0.000000 0.000000 0.233840 0.609439 0.493044 0.512184 0.074069 0.000000 0.651515 0.515072 0.539757 0.120687 0.125000 1.00 0.443328 0.333333 0.314640 0.30103 0.954243 0.607971 0.602060 0.219526 0.000000 0.000000 0.000000 0.000000 0.000000 0.204496 0.145077 6.892877 583640 578899991 2022-03-01 4.0 RIVER RAGING 2 2 30.3 13.8 0.0 3.71 1.13 M/3 0 0 6.89 20.36 0.0 764592641 283109 L Dalziel 1 FOLLOW @GRV_NEWS ON TWITTER 11:08AM 350 Maiden Healesville 01 Mar 22 0.061390 0 0.000000 0.250000 0.698970 0.058171 19.56 0.250778 6.64 0.303536 0.318578 0.138427 0.312992 0.312992 0.312992 0.312992 0.000000 0.346715 0.346715 0.346715 0.346715 0.000000 0.250000 0.25 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.000000 0.000000 0.000000e+00 0.000000 0.000000 0.241288 0.312992 0.285038 0.303536 0.033489 0.251704 0.346715 0.290635 0.290765 0.038193 0.142857 0.25 0.201905 0.200000 0.048369 0.69897 0.903090 0.784856 0.778151 0.090009 0.000000 0.000000 0.000000 0.000000 0.000000 0.241288 0.312992 0.285038 0.303536 0.033489 0.251704 0.346715 0.290635 0.290765 0.038193 0.142857 0.25 0.201905 0.200000 0.048369 0.69897 0.903090 0.784856 0.778151 0.090009 0.000000 0.000000 0.000000 0.000000 0.000000 0.065977 0.102443 9.761504 385698 419530408 2022-03-01 5.0 FREENEY 8 8 25.1 14.0 0.0 8.25 1.14 0 0 0 0.00 22.47 20.0 764579628 63422 R Lound 10 BURDEKIN VET CLINIC 09:55PM 380 Grade 5 Townsville 01 Mar 22 0.058903 0 0.110185 0.200000 0.778151 0.059132 22.10 0.641825 7.56 0.417668 0.000000 0.123070 0.389381 0.406750 0.398065 0.398065 0.012282 0.519920 0.519920 0.519920 0.519920 0.000000 0.125000 0.25 0.187500 0.187500 0.088388 0.698970 0.954243 0.826606 0.826606 0.180505 0.110185 0.161208 1.356966e-01 0.135697 0.036079 0.378587 0.520445 0.438208 0.435239 0.050906 0.519920 0.519920 0.519920 0.519920 0.000000 0.125000 1.00 0.398810 0.250000 0.304154 0.30103 0.954243 0.636079 0.698970 0.229354 0.086783 0.256326 0.171119 0.161208 0.059824 0.277345 0.547967 0.448106 0.459606 0.067232 0.486807 0.519920 0.509978 0.516591 0.015762 0.125000 1.00 0.437302 0.333333 0.287424 0.30103 0.954243 0.595411 0.602060 0.199176 0.000000 0.256326 0.161721 0.181581 0.080195 0.148800 0.136846 7.307467 453065 451768903 2022-03-01 5.0 ENCOURAGING 2 2 22.1 15.0 0.0 6.0 0.29 455 0 455 10.35 22.57 0.0 764579685 70111 B Young 12 GOSSIE TIGERS GOOD TIMES 10:39PM 388 Non Graded Gosford 01 Mar 22 0.058204 0 0.000000 0.200000 0.778151 0.058170 22.39 0.564085 10.19 0.460124 0.422705 0.166667 0.462217 0.582400 0.498498 0.474688 0.056299 0.413211 0.605769 0.539241 0.568992 0.086316 0.200000 0.50 0.300000 0.250000 0.135401 0.000000 0.778151 0.530643 0.698970 0.317165 0.000000 0.227091 4.541824e-02 0.000000 0.101558 0.435567 0.582400 0.510280 0.506062 0.043547 0.335526 0.605769 0.507137 0.513845 0.080824 0.200000 1.00 0.559375 0.500000 0.321914 0.00000 0.778151 0.476169 0.477121 0.204293 0.000000 0.269225 0.170899 0.227091 0.115477 0.435567 0.620208 0.519683 0.508949 0.050995 0.335526 0.676056 0.528287 0.534247 0.085502 0.200000 1.00 0.592857 0.500000 0.310625 0.00000 0.778151 0.460377 0.477121 0.185631 0.000000 0.269225 0.187400 0.227091 0.105954 0.310750 0.189764 5.269698 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 269364 328634961 2022-03-31 6.0 LUNARAY 7 7 28.8 18.8 0.0 6.96 0.24 M/766 0 0 6.99 25.92 0.0 771803534 117062 V Mileto 6 SHEPPARTON WORKWEAR &amp; SAFETY 01:19PM 450 Grade 5 T3 Shepparton 31 Mar 22 0.044487 0 0.000000 0.166667 0.845098 0.057600 25.55 0.404304 6.70 0.428627 0.292561 0.118601 0.370377 0.461165 0.415771 0.415771 0.064197 0.320144 0.341040 0.330592 0.330592 0.014776 0.200000 0.25 0.225000 0.225000 0.035355 0.698970 0.778151 0.738561 0.738561 0.055990 0.000000 0.000000 2.720046e-15 0.000000 0.000000 0.282853 0.461165 0.399299 0.415352 0.060173 0.000000 0.397661 0.280607 0.320144 0.129212 0.125000 1.00 0.379762 0.250000 0.297826 0.30103 0.954243 0.644364 0.698970 0.211158 0.000000 0.159040 0.022720 0.000000 0.060112 0.280719 0.529528 0.410459 0.414407 0.058641 0.000000 0.441003 0.297743 0.312132 0.111641 0.125000 1.00 0.324033 0.250000 0.245639 0.30103 0.954243 0.687225 0.698970 0.181101 0.000000 0.177795 0.010526 0.000000 0.041488 0.075047 0.097817 10.223167 538082 534921234 2022-03-31 6.0 QUINNLEY BALE 6 6 26.7 18.0 0.0 7.5 1.14 56 0 56 5.47 17.83 0.0 771810563 98781 S Rhodes 12 WATCH LIVE ON SPORTSBET 10:46PM 297 Grade 5 Dapto 31 Mar 22 0.046607 0 0.000000 0.166667 0.845098 0.060034 17.19 0.593790 5.37 0.320527 0.408592 0.120301 0.288301 0.347716 0.318008 0.318008 0.042013 0.303220 0.390710 0.346965 0.346965 0.061865 0.142857 0.50 0.321429 0.321429 0.252538 0.477121 0.903090 0.690106 0.690106 0.301205 0.000000 0.221975 1.109875e-01 0.110988 0.156960 0.288301 0.471082 0.384562 0.389432 0.082219 0.303220 0.426606 0.368411 0.371909 0.052814 0.142857 1.00 0.473214 0.375000 0.381742 0.30103 0.903090 0.595053 0.588046 0.262071 0.000000 0.264698 0.121668 0.110988 0.141569 0.236247 0.500000 0.387665 0.390897 0.075799 0.000000 0.481447 0.342572 0.353107 0.117160 0.125000 1.00 0.374454 0.250000 0.278973 0.00000 0.954243 0.605624 0.650515 0.270858 0.000000 0.264698 0.053752 0.000000 0.100704 0.068536 0.065307 15.312345 363059 403640676 2022-03-31 3.0 SAINT CHARLOTTE 5 5 27.7 18.0 0.0 4.5 0.14 0 0 0 10.87 23.59 140.0 771539517 67189 W Wilson 8 EXCHANGE PRINTERS (N/P) STAKE 01:52PM 400 Restricted Win Mount Gambier 31 Mar 22 0.041828 0 0.179102 0.333333 0.602060 0.058975 23.39 0.696399 10.98 0.457609 0.550598 0.147799 0.401509 0.534438 0.467974 0.467974 0.093995 0.328496 0.527473 0.427984 0.427984 0.140698 0.125000 1.00 0.562500 0.562500 0.618718 0.301030 0.954243 0.627636 0.627636 0.461891 0.000000 0.225702 1.128509e-01 0.112851 0.159595 0.401509 0.534438 0.475213 0.486146 0.041384 0.328496 0.564576 0.467640 0.504558 0.078033 0.125000 1.00 0.440833 0.333333 0.320552 0.30103 0.954243 0.603688 0.602060 0.220083 0.000000 0.225702 0.135590 0.179102 0.095348 0.257933 0.534438 0.457113 0.477655 0.064917 0.212446 0.564576 0.443305 0.445950 0.089104 0.125000 1.00 0.358408 0.250000 0.274086 0.30103 0.954243 0.660966 0.698970 0.195529 0.000000 0.225702 0.106690 0.172038 0.098393 0.155050 0.100326 9.967472 362095 403093108 2022-03-31 3.0 SILVER SANDALS 5 5 27.0 10.0 0.0 2.5 2.29 42 0 42 5.58 31.67 800.0 771539501 87148 S Lawrance 2 SKY RACING 06:40PM 520 Masters Grade 5 Ipswich 31 Mar 22 0.083134 0 0.241969 0.333333 0.602060 0.060904 30.79 0.823159 5.42 0.361067 0.356631 0.095370 0.435748 0.439595 0.437671 0.437671 0.002720 0.354196 0.368046 0.361121 0.361121 0.009793 0.333333 1.00 0.666667 0.666667 0.471405 0.301030 0.602060 0.451545 0.451545 0.212860 0.207730 0.275374 2.415521e-01 0.241552 0.047832 0.330867 0.507902 0.426973 0.435748 0.052271 0.198046 0.460029 0.340578 0.340419 0.073412 0.166667 1.00 0.498485 0.333333 0.340462 0.30103 0.845098 0.560166 0.602060 0.203530 0.110185 0.275374 0.199906 0.207730 0.064087 0.297445 0.507902 0.398206 0.402791 0.056453 0.156690 0.460029 0.325128 0.317851 0.074465 0.125000 1.00 0.328665 0.225000 0.260181 0.30103 0.954243 0.694669 0.738561 0.197928 0.000000 0.275374 0.127103 0.138606 0.097759 0.081817 0.110114 9.081527 565804 556974861 2022-03-31 2.0 ORSON LAURIE 3 3 30.4 3.1 0.0 0.75 0.86 112 0 112 8.32 23.04 530.0 771810580 125472 E Harris 4 RIVERINA STOCKFEEDS 08:04PM 411 Free For All Casino 31 Mar 22 0.275736 0 0.227091 0.500000 0.477121 0.056058 23.37 0.418680 8.62 0.571615 0.680288 0.062500 0.351105 0.583040 0.453627 0.445632 0.092450 0.487805 0.651134 0.562563 0.574442 0.064323 0.142857 1.00 0.362698 0.183333 0.339607 0.000000 0.903090 0.592798 0.778151 0.343479 0.000000 0.267033 7.058912e-02 0.000000 0.121104 0.351105 0.588161 0.473661 0.483607 0.085113 0.465422 0.664141 0.554578 0.574442 0.067472 0.142857 1.00 0.543492 0.500000 0.401886 0.00000 0.903090 0.545322 0.477121 0.298264 0.000000 0.275104 0.142790 0.167586 0.124768 0.351105 0.588161 0.479065 0.494281 0.085020 0.465422 0.664141 0.557285 0.574442 0.065012 0.142857 1.00 0.572024 0.500000 0.404685 0.00000 0.903090 0.530952 0.477121 0.294808 0.000000 0.275104 0.149961 0.224986 0.124372 0.150460 0.123089 8.124181 <p>26438 rows \u00d7 117 columns</p> <pre><code>def download_iggy_ratings(date):\n    \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame.\n\n    Args:\n        date (datetime): the date we want to download the ratings for\n    \"\"\"\n    iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date='\n    iggy_url_2 = date.strftime(\"%Y-%m-%d\")\n    iggy_url_3 = '&amp;amp;presenter=RatingsPresenter&amp;amp;csv=true'\n    iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3\n\n    # Download todays greyhounds ratings\n    iggy_df = pd.read_csv(iggy_url)\n\n    # Data clearning\n    iggy_df = iggy_df.rename(\n    columns={\n        \"meetings.races.bfExchangeMarketId\":\"market_id\",\n        \"meetings.races.runners.bfExchangeSelectionId\":\"selection_id\",\n        \"meetings.races.runners.ratedPrice\":\"rating\",\n        \"meetings.races.number\":\"RaceNum\",\n        \"meetings.name\":\"Track\",\n        \"meetings.races.runners.name\":\"DogName\"\n        }\n    )\n    # iggy_df = iggy_df[['market_id','selection_id','rating']]\n    iggy_df['market_id'] = iggy_df['market_id'].astype(str)\n    iggy_df['date_dt'] = date\n\n    # Set market_id and selection_id as index for easy referencing\n    # iggy_df = iggy_df.set_index(['market_id','selection_id'])\n    return(iggy_df)\n\n# Download historical ratings over a time period and convert into a big DataFrame.\nback_test_period = pd.date_range(start='2022-03-01', end='2022-04-01')\nframes = [download_iggy_ratings(day) for day in back_test_period]\niggy_df = pd.concat(frames)\niggy_df\n</code></pre> Track meetings.bfExchangeEventId meetings.races.name RaceNum market_id meetings.races.comment selection_id meetings.races.runners.number DogName rating date_dt 0 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 43154446 1 Magic Rogue 11.07 2022-03-01 1 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 43154447 2 Youre Off 6.10 2022-03-01 2 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 42352031 3 Castle Town 6.60 2022-03-01 3 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 43154448 4 Buckle Up Aumond 8.02 2022-03-01 4 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 42413752 5 Was That Then 17.14 2022-03-01 ... ... ... ... ... ... ... ... ... ... ... ... 906 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 27692794 3 Mercator Closer 24.49 2022-04-01 907 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 36057267 4 Kooringa Lucy 2.61 2022-04-01 908 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 25540155 6 Shanjo Prince 213.53 2022-04-01 909 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 39788790 7 Lots Of Chatter 2.10 2022-04-01 910 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 30681833 8 Zipping Brady 18.25 2022-04-01 <p>27346 rows \u00d7 11 columns</p> <pre><code># format DogNames to merge\ntodays_data['DogName'] = todays_data['DogName'].apply(lambda x: x.replace(\"'\", \"\").replace(\".\", \"\").replace(\"Res\", \"\").strip())\niggy_df['DogName'] = iggy_df['DogName'].str.upper()\n# Merge\nbacktest = iggy_df[['market_id','selection_id','DogName','date_dt']].merge(todays_data[['rating','DogName','date_dt']], how = 'inner', on = ['DogName','date_dt'])\nbacktest\n</code></pre> market_id selection_id DogName date_dt rating 0 1.195395419 43154446 MAGIC ROGUE 2022-03-01 8.137725 1 1.195395419 43154447 YOURE OFF 2022-03-01 6.051752 2 1.195395419 42352031 CASTLE TOWN 2022-03-01 9.768546 3 1.195395419 43154448 BUCKLE UP AUMOND 2022-03-01 13.656089 4 1.195395419 42413752 WAS THAT THEN 2022-03-01 11.941057 ... ... ... ... ... ... 25540 1.196921144 39309141 RANAKO BALE 2022-03-31 3.975601 25541 1.196921144 39348645 NEVAEH BALE 2022-03-31 3.917458 25542 1.196921144 26870111 INGA MIA 2022-03-31 7.459386 25543 1.196921144 42472271 WINNIE COASTER 2022-03-31 7.046953 25544 1.196921144 40022831 ASTON HEBE 2022-03-31 4.603341 <p>25545 rows \u00d7 5 columns</p> <pre><code># Save predictions for if we want to backtest/simulate it later\nbacktest.to_csv('backtest.csv', index=False) # Csv format\n# backtest.to_pickle('backtest.pkl') # pickle format (faster, but can't open in excel)\n</code></pre> <p>Perfect, with our hacky solution we have managed to merge around a months' worth of data relatively quickly and saved it in a csv format. With all the merging it seems we have only lost around 1000 - 2000 rows of data out of 27,000 rows of data, which seems only a small price to pay.</p> <p>Placing live bets is pretty simple but we have one issue. FastTrack Data alone is unable to tell us how many greyhounds will run in the race. For example, this race later today (2022-07-04) has 8 runners + 2 reserves:</p> <pre><code>todays_data[todays_data['FastTrack_RaceId'] == '798906744']\n</code></pre> FastTrack_DogId date_dt Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceName RaceTime Distance RaceGrade date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D prob_LogisticRegression DogName_bf Track RaceNum YOU SEE LINA Cannington 1 530411826 2022-07-04 0.0 YOU SEE LINA 1 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 10408 Michael McLennan FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.167785 0.363135 0.419607 0.395672 0.404274 0.029202 0.395833 0.432534 0.410499 0.403130 0.019428 0.200000 0.500000 0.300000 0.200000 0.173205 0.477121 0.778151 0.677808 0.778151 0.173800 0.0 0.213623 7.120781e-02 0.000000 0.123336 0.184971 0.419607 0.324576 0.346645 0.092382 0.220812 0.432534 0.338647 0.353089 0.084555 0.166667 0.500000 0.316667 0.266667 0.153116 0.477121 0.845098 0.659617 0.690106 0.162743 0.0 0.213623 0.101436 0.092505 0.111554 0.179269 0.419607 0.303822 0.326906 0.075138 0.133803 0.432534 0.315318 0.310345 0.082022 0.125000 0.500000 0.247937 0.200000 0.121737 0.477121 0.954243 0.743299 0.778151 0.156196 0.0 0.213623 0.085118 0.000000 0.095461 0.115534 BELLA LINA Cannington 1 547605028 2022-07-04 0.0 BELLA LINA 5 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 83951 Rodney Noden FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.111111 0.228705 0.307236 0.273491 0.284534 0.040413 0.220812 0.285592 0.245073 0.228814 0.035318 0.166667 0.200000 0.188889 0.200000 0.019245 0.778151 0.845098 0.800467 0.778151 0.038652 0.0 0.000000 3.700743e-16 0.000000 0.000000 0.158046 0.307236 0.246575 0.264844 0.050600 0.029221 0.285592 0.203061 0.228814 0.091884 0.142857 0.200000 0.187075 0.200000 0.023119 0.778151 0.903090 0.805563 0.778151 0.049718 0.0 0.000000 0.000000 0.000000 0.000000 0.084276 0.307236 0.225040 0.234235 0.061784 0.029221 0.366864 0.240440 0.236842 0.088850 0.125000 0.250000 0.174702 0.166667 0.034400 0.698970 0.954243 0.835377 0.845098 0.073324 0.0 0.142298 0.008894 0.000000 0.035574 0.031221 PENNY KEEPING Cannington 1 561780971 2022-07-04 0.0 PENNY KEEPING 8 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 68481 Bradley Cook FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.100671 0.076736 0.356220 0.228759 0.253321 0.141352 0.424645 0.513538 0.474906 0.486535 0.045573 0.125000 0.500000 0.250000 0.125000 0.216506 0.477121 0.954243 0.795202 0.954243 0.275466 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.076736 0.356220 0.236342 0.256207 0.116406 0.424645 0.513538 0.467291 0.465490 0.040207 0.125000 0.500000 0.223214 0.133929 0.184716 0.477121 0.954243 0.822174 0.928666 0.231296 0.0 0.000000 0.000000 0.000000 0.000000 0.076736 0.356220 0.236342 0.256207 0.116406 0.424645 0.513538 0.467291 0.465490 0.040207 0.125000 0.500000 0.223214 0.133929 0.184716 0.477121 0.954243 0.822174 0.928666 0.231296 0.0 0.000000 0.000000 0.000000 0.000000 0.049673 WHAT A PHOENIX Cannington 1 603189486 2022-07-04 0.0 WHAT A PHOENIX 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 10408 Michael McLennan FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.108392 0.155537 0.318460 0.227087 0.207265 0.083251 0.000000 0.338235 0.150847 0.114306 0.172053 0.125000 0.250000 0.166667 0.125000 0.072169 0.698970 0.954243 0.869152 0.954243 0.147382 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.155537 0.318460 0.227087 0.207265 0.083251 0.000000 0.338235 0.150847 0.114306 0.172053 0.125000 0.250000 0.166667 0.125000 0.072169 0.698970 0.954243 0.869152 0.954243 0.147382 0.0 0.000000 0.000000 0.000000 0.000000 0.155537 0.318460 0.227087 0.207265 0.083251 0.000000 0.338235 0.150847 0.114306 0.172053 0.125000 0.250000 0.166667 0.125000 0.072169 0.698970 0.954243 0.869152 0.954243 0.147382 0.0 0.000000 0.000000 0.000000 0.000000 0.040679 WHAT A QUIZ Cannington 1 603189487 2022-07-04 0.0 WHAT A QUIZ 2 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 72510 Barry McPherson FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.141892 0.233563 0.233563 0.233563 0.233563 0.000000 0.302920 0.302920 0.302920 0.302920 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.233563 0.233563 0.233563 0.233563 0.000000 0.302920 0.302920 0.302920 0.302920 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.233563 0.233563 0.233563 0.233563 0.000000 0.302920 0.302920 0.302920 0.302920 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.043975 WHAT A SHAKER Cannington 1 603189986 2022-07-04 0.0 WHAT A SHAKER 4 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 72510 Barry McPherson FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.125000 0.282892 0.282892 0.282892 0.282892 0.000000 0.268116 0.268116 0.268116 0.268116 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 0.602060 0.602060 0.000000 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.282892 0.282892 0.282892 0.282892 0.000000 0.268116 0.268116 0.268116 0.268116 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 0.602060 0.602060 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.282892 0.282892 0.282892 0.282892 0.000000 0.268116 0.268116 0.268116 0.268116 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 0.602060 0.602060 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.059174 WIZARDS LEGEND Cannington 1 614056673 2022-07-04 0.0 WIZARD'S LEGEND Res. 10 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 127397 Colin Bainbridge FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.000000 0.307944 0.341758 0.326417 0.327983 0.015946 0.206724 0.288937 0.259703 0.271576 0.036365 0.166667 0.250000 0.204167 0.200000 0.034359 0.698970 0.845098 0.775093 0.778151 0.059761 0.0 0.151629 3.790717e-02 0.000000 0.075814 0.089468 0.341758 0.269683 0.313202 0.093934 0.103960 0.377622 0.240904 0.244173 0.081242 0.125000 0.250000 0.194792 0.200000 0.042477 0.698970 0.954243 0.797104 0.778151 0.084209 0.0 0.151629 0.037164 0.000000 0.068832 0.089468 0.341758 0.275533 0.303754 0.077507 0.103960 0.377622 0.242702 0.244173 0.071625 0.125000 0.333333 0.204266 0.200000 0.058218 0.602060 0.954243 0.785504 0.778151 0.099650 0.0 0.151629 0.037412 0.000000 0.067696 0.019388 WIZARDS DRAMA Cannington 1 614057677 2022-07-04 0.0 WIZARD'S DRAMA Res. 9 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 127397 Colin Bainbridge FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.000000 0.225948 0.344591 0.271951 0.245316 0.063648 0.150000 0.269231 0.221376 0.244898 0.063000 0.142857 0.500000 0.280952 0.200000 0.191840 0.477121 0.903090 0.719454 0.778151 0.218967 0.0 0.209986 6.999522e-02 0.000000 0.121235 0.086870 0.344591 0.234575 0.238391 0.084399 0.088816 0.269231 0.188587 0.189288 0.067980 0.142857 0.500000 0.229762 0.171429 0.139270 0.477121 0.903090 0.777252 0.840621 0.169536 0.0 0.209986 0.059278 0.000000 0.094057 0.036656 0.344591 0.200202 0.228706 0.100907 0.069444 0.269231 0.170954 0.162215 0.071007 0.125000 0.500000 0.229613 0.171429 0.130210 0.477121 0.954243 0.777477 0.840621 0.171435 0.0 0.209986 0.044459 0.000000 0.084096 0.014393 DASHING ONYX Cannington 1 626191408 2022-07-04 0.0 DASHING ONYX 6 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 67839 Graeme Hall FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.144876 0.262045 0.359113 0.314231 0.321535 0.048944 0.244898 0.377622 0.319292 0.335355 0.067805 0.166667 0.333333 0.277778 0.333333 0.096225 0.602060 0.845098 0.683073 0.602060 0.140318 0.0 0.185009 1.233393e-01 0.185009 0.106815 0.229498 0.359113 0.293047 0.291790 0.058241 0.244898 0.377622 0.318715 0.326170 0.055374 0.142857 0.333333 0.244048 0.250000 0.103555 0.602060 0.903090 0.738077 0.723579 0.158833 0.0 0.185009 0.092505 0.092505 0.106815 0.229498 0.359113 0.293047 0.291790 0.058241 0.244898 0.377622 0.318715 0.326170 0.055374 0.142857 0.333333 0.244048 0.250000 0.103555 0.602060 0.903090 0.738077 0.723579 0.158833 0.0 0.185009 0.092505 0.092505 0.106815 0.096959 WINTER RAIN Cannington 1 637972981 2022-07-04 0.0 WINTER RAIN 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 27464 Jennifer Thompson FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.101754 0.239673 0.371004 0.305338 0.305338 0.092865 0.133803 0.350640 0.242221 0.242221 0.153327 0.166667 0.500000 0.333333 0.333333 0.235702 0.477121 0.845098 0.661110 0.661110 0.260199 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.239673 0.371004 0.305338 0.305338 0.092865 0.133803 0.350640 0.242221 0.242221 0.153327 0.166667 0.500000 0.333333 0.333333 0.235702 0.477121 0.845098 0.661110 0.661110 0.260199 0.0 0.000000 0.000000 0.000000 0.000000 0.239673 0.371004 0.305338 0.305338 0.092865 0.133803 0.350640 0.242221 0.242221 0.153327 0.166667 0.500000 0.333333 0.333333 0.235702 0.477121 0.845098 0.661110 0.661110 0.260199 0.0 0.000000 0.000000 0.000000 0.000000 0.101402 <p>If we predict probabilities and renormalise now, we will calculate incorrect probabilities.</p> <p>I've spent a really long time thinking about this and testing different methods that didn't work or weren't optimal. The best solution (and least complicated) that I have come up with is to predict probabilities on the FastTrack data first. Then a few minutes before the jump when all the lineups have been confirmed we use <code>market_catalogue</code> from the Betfair API to merge our predicted probabilities, merging on <code>DogName</code>,<code>Track</code> and <code>RaceNum</code>. If we merge on these three fields, it will bypass any issues with reserve dogs and scratchings. Then we can renormalise probabilities live within Flumine.</p> <pre><code># Select todays data \ntodays_data = model_df[model_df['date_dt'] == pd.Timestamp.now().strftime('%Y-%m-%d')]\n\n# Generate runner win predictions\ndog_win_probabilities = brunos_model.predict_proba(todays_data[feature_cols])[:,1]\ntodays_data['prob_LogisticRegression'] = dog_win_probabilities\n\n# We no longer renomralise probability in this chunk of code, do it in Flumine instead\n# todays_data['renormalise_prob'] = todays_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / x.sum())\n# todays_data['rating'] = 1/todays_data['renormalise_prob']\n# todays_data = todays_data.sort_values(by = 'date_dt')\n\ntodays_data\n</code></pre> <pre>\n<code>C:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/2638603781.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  todays_data['prob_LogisticRegression'] = dog_win_probabilities\n</code>\n</pre> FastTrack_DogId date_dt Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceNum RaceName RaceTime Distance RaceGrade Track date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D prob_LogisticRegression 44514 148673258 2022-07-04 0.0 SPEEDY MARINA 5 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455740 65928 Dawn Lee 5 LADBROKES BLENDED BETS 1-3 WIN 04:37PM 307 Grade 5 Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.136986 0.317232 0.317232 0.317232 0.317232 0.000000 0.173267 0.173267 0.173267 0.173267 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 6.020600e-01 0.602060 0.000000 0.212109 0.212109 2.121089e-01 0.212109 0.000000e+00 0.317232 0.332373 0.324803 0.324803 0.010706 0.173267 0.210579 0.191923 0.191923 0.026383 0.166667 0.333333 0.250000 0.250000 0.117851 0.602060 0.845098 7.235790e-01 0.723579 0.171854 0.000000 0.212109 0.106054 0.106054 0.149984 0.236982 0.371585 0.306603 0.317232 0.052095 0.173267 0.406600 0.242140 0.210579 0.096894 0.125000 0.333333 0.186905 0.166667 0.083715 0.602060 0.954243 8.299177e-01 0.845098 0.135269 0.000000 0.212109 0.042422 0.000000 0.094858 0.073607 54463 161977365 2022-07-04 0.0 FILTHY PHANTOM 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801448232 110385 Tony Hinrichsen 5 GIDDY-UP (N/P) STAKE 07:27PM 342 Masters Angle Park 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 19.675 0.533632 7.730 0.0 0.0 0.124590 0.386907 0.455919 0.427165 0.420839 0.028492 0.314004 0.500000 0.408769 0.394027 0.082256 0.166667 0.333333 0.220000 0.200000 0.064979 0.602060 0.845098 7.563224e-01 0.778151 0.090977 0.000000 0.182760 3.655208e-02 0.000000 8.173293e-02 0.386907 0.572783 0.449667 0.436026 0.053935 0.210921 0.551665 0.403088 0.394027 0.092070 0.142857 1.000000 0.291209 0.200000 0.235613 0.000000 0.903090 6.692431e-01 0.778151 0.257193 0.000000 0.249855 0.057150 0.000000 0.095131 0.277002 0.572783 0.459854 0.456897 0.059514 0.210921 0.616496 0.443040 0.443820 0.089192 0.142857 1.000000 0.464354 0.333333 0.320517 0.000000 0.903090 5.716763e-01 0.602060 0.227821 0.000000 0.249855 0.121936 0.180363 0.106520 0.128265 77950 196384049 2022-07-04 0.0 HOUND 'EM DOWN 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801490787 313281 Steven Winstanley 3 ZIPPING GARTH @ STUD 0-2 WIN 07:36PM 565 Mixed Maiden and Grade Five Maitland 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 31.875 0.342029 13.795 0.0 0.0 0.200000 0.233442 0.277637 0.261405 0.273136 0.024321 0.218310 0.316690 0.270554 0.276662 0.049474 0.142857 0.200000 0.161905 0.142857 0.032991 0.778151 0.903090 8.614437e-01 0.903090 0.072133 0.000000 0.000000 4.440892e-16 0.000000 0.000000e+00 0.194132 0.355912 0.279655 0.277637 0.046001 0.218310 0.316690 0.271078 0.275180 0.032574 0.125000 0.200000 0.158862 0.142857 0.026641 0.778151 0.954243 8.681223e-01 0.903090 0.060784 0.000000 0.000000 0.000000 0.000000 0.000000 0.194132 0.462627 0.328446 0.316396 0.054098 0.218310 0.346154 0.292413 0.296715 0.036166 0.125000 0.333333 0.187245 0.166667 0.054481 0.000000 0.954243 7.520173e-01 0.845098 0.239572 0.000000 0.212109 0.014373 0.000000 0.050118 0.070298 121171 230053393 2022-07-04 0.0 CAWBOURNE CROSS 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801448232 104699 Lisa Rasmussen 5 GIDDY-UP (N/P) STAKE 07:27PM 342 Masters Angle Park 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 19.675 0.533632 7.730 0.0 0.0 0.169811 0.336395 0.413836 0.386878 0.410402 0.043753 0.500000 0.500000 0.500000 0.500000 0.000000 0.142857 0.250000 0.186508 0.166667 0.056260 0.698970 0.903090 8.157193e-01 0.845098 0.105184 0.000000 0.000000 3.700743e-16 0.000000 2.533726e-17 0.336395 0.491121 0.420107 0.412119 0.059452 0.401747 0.506477 0.480510 0.500000 0.044239 0.142857 0.500000 0.273810 0.250000 0.129975 0.477121 0.903090 7.042182e-01 0.698970 0.155860 0.000000 0.188140 0.058566 0.000000 0.091070 0.280126 0.505631 0.403460 0.405048 0.064662 0.329857 0.610664 0.472084 0.500000 0.079724 0.142857 0.500000 0.269048 0.225000 0.125458 0.477121 0.903090 7.115827e-01 0.738561 0.144209 0.000000 0.205324 0.060368 0.000000 0.090871 0.179535 142478 243599770 2022-07-04 0.0 SKAIKRU 1 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455741 83214 Robert Sonter 6 BATHURST RSL CLUB 04:59PM 450 Grade 5 Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 26.000 0.576406 15.450 0.0 0.0 0.159574 0.392354 0.392354 0.392354 0.392354 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 6.989700e-01 0.698970 0.000000 0.000000 0.000000 1.998401e-15 0.000000 0.000000e+00 0.293318 0.392354 0.329286 0.302186 0.054798 0.238956 0.238956 0.238956 0.238956 0.000000 0.142857 0.333333 0.242063 0.250000 0.095486 0.602060 0.903090 7.347067e-01 0.698970 0.153664 0.000000 0.167027 0.055676 0.000000 0.096433 0.285293 0.392354 0.317943 0.303341 0.036066 0.162722 0.258454 0.223927 0.237266 0.042031 0.125000 0.333333 0.202551 0.200000 0.070985 0.602060 0.954243 7.942519e-01 0.778151 0.120113 0.000000 0.167027 0.023861 0.000000 0.063130 0.092948 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 599603 673011364 2022-07-04 0.0 FERAL AGENT 8 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455737 96421 Derek Kerr 2 ZIPPING GARTH @ STUD MAIDEN 03:28PM 307 Maiden Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.177215 0.409480 0.425999 0.418593 0.420299 0.008391 0.411700 0.431973 0.421836 0.421836 0.014335 0.250000 1.000000 0.527778 0.333333 0.411074 0.301030 0.698970 5.340200e-01 0.602060 0.207512 0.000000 0.231573 1.478939e-01 0.212109 1.284491e-01 0.409480 0.425999 0.418593 0.420299 0.008391 0.411700 0.431973 0.421836 0.421836 0.014335 0.250000 1.000000 0.527778 0.333333 0.411074 0.301030 0.698970 5.340200e-01 0.602060 0.207512 0.000000 0.231573 0.147894 0.212109 0.128449 0.409480 0.425999 0.418593 0.420299 0.008391 0.411700 0.431973 0.421836 0.421836 0.014335 0.250000 1.000000 0.527778 0.333333 0.411074 0.301030 0.698970 5.340200e-01 0.602060 0.207512 0.000000 0.231573 0.147894 0.212109 0.128449 0.194287 599956 694776805 2022-07-04 0.0 WENDY MAREE 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801490818 307271 Brian Baker 2 GARRARD'S HORSE AND HOUND 07:11PM 520 Novice Non Penalty Albion Park 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 30.220 0.634509 5.630 0.0 0.0 0.136417 0.325934 0.410627 0.368281 0.368281 0.059887 0.168325 0.336770 0.252547 0.252547 0.119108 0.200000 0.250000 0.225000 0.225000 0.035355 0.698970 0.778151 7.385606e-01 0.738561 0.055990 0.110185 0.193690 1.519376e-01 0.151938 5.904714e-02 0.325934 0.410627 0.366778 0.363772 0.042426 0.168325 0.344322 0.283139 0.336770 0.099504 0.200000 0.250000 0.216667 0.200000 0.028868 0.698970 0.778151 7.517575e-01 0.778151 0.045715 0.110185 0.193690 0.138020 0.110185 0.048212 0.305980 0.410627 0.355981 0.361146 0.036561 0.168325 0.344322 0.274549 0.279411 0.067105 0.142857 0.333333 0.221032 0.200000 0.064636 0.602060 0.903090 7.564290e-01 0.778151 0.100056 0.110185 0.218690 0.142187 0.110185 0.050203 0.123793 600100 707214702 2022-07-04 0.0 WARDEN JODIE Res. 10 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455737 75264 Sam Simonetta 2 ZIPPING GARTH @ STUD MAIDEN 03:28PM 307 Maiden Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.000000 0.146202 0.146202 0.146202 0.146202 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.125000 0.125000 0.125000 0.125000 0.000000 0.000000 0.954243 1.908485e-01 0.000000 0.426750 0.000000 0.000000 0.000000e+00 0.000000 0.000000e+00 0.146202 0.146202 0.146202 0.146202 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.125000 0.125000 0.125000 0.125000 0.000000 0.000000 0.954243 1.908485e-01 0.000000 0.426750 0.000000 0.000000 0.000000 0.000000 0.000000 0.146202 0.146202 0.146202 0.146202 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.125000 0.125000 0.125000 0.125000 0.000000 0.000000 0.954243 1.908485e-01 0.000000 0.426750 0.000000 0.000000 0.000000 0.000000 0.000000 0.014319 600105 707215693 2022-07-04 0.0 MYSTERY ANNE 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455737 75264 Sam Simonetta 2 ZIPPING GARTH @ STUD MAIDEN 03:28PM 307 Maiden Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.125874 0.331265 0.331265 0.331265 0.331265 0.000000 0.473776 0.473776 0.473776 0.473776 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 6.989700e-01 0.698970 0.000000 0.000000 0.000000 0.000000e+00 0.000000 0.000000e+00 0.331265 0.331265 0.331265 0.331265 0.000000 0.473776 0.473776 0.473776 0.473776 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 6.989700e-01 0.698970 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.331265 0.331265 0.331265 0.331265 0.000000 0.473776 0.473776 0.473776 0.473776 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 6.989700e-01 0.698970 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.090248 600106 707215696 2022-07-04 0.0 BROKEN PROMISES 2 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455736 75264 Sam Simonetta 1 WELCOME GBOTA MAIDEN 03:07PM 450 Maiden Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 26.000 0.576406 15.450 0.0 0.0 0.139785 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 2.708944e-14 0.000000 0.000000 0.000000 0.000000 0.000000e+00 0.000000 0.000000e+00 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 2.642331e-14 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 2.842171e-14 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.060767 <p>1704 rows \u00d7 115 columns</p> <p>Before we merge, let's do some minor formatting changes to the FastTrack names so we can match onto the Betfair names. Betfair excludes all apostrophes and full stops in their naming convention, so we'll create a Betfair equivalent dog name on the dataset removing these characters. We also need to do this for the tracks, sometimes FastTrack will name tracks differently to Betfair e.g., Sandown Park from Betfair is known as Sandown (SAP) in the FastTrack database.</p> <pre><code># Prepare data for easy reference in flumine\ntodays_data['DogName_bf'] = todays_data['DogName'].apply(lambda x: x.replace(\"'\", \"\").replace(\".\", \"\").replace(\"Res\", \"\").strip())\ntodays_data.replace({'Sandown (SAP)': 'Sandown Park'}, regex=True, inplace=True)\ntodays_data = todays_data.set_index(['DogName_bf','Track','RaceNum'])\ntodays_data.head()\n</code></pre> <pre>\n<code>C:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/90992895.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  todays_data['DogName_bf'] = todays_data['DogName'].apply(lambda x: x.replace(\"'\", \"\").replace(\".\", \"\").replace(\"Res\", \"\").strip())\n</code>\n</pre> FastTrack_DogId date_dt Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceName RaceTime Distance RaceGrade date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D prob_LogisticRegression DogName_bf Track RaceNum SPEEDY MARINA Bathurst 5 148673258 2022-07-04 0.0 SPEEDY MARINA 5 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455740 65928 Dawn Lee LADBROKES BLENDED BETS 1-3 WIN 04:37PM 307 Grade 5 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.136986 0.317232 0.317232 0.317232 0.317232 0.000000 0.173267 0.173267 0.173267 0.173267 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 0.602060 0.602060 0.000000 0.212109 0.212109 2.121089e-01 0.212109 0.000000e+00 0.317232 0.332373 0.324803 0.324803 0.010706 0.173267 0.210579 0.191923 0.191923 0.026383 0.166667 0.333333 0.250000 0.250000 0.117851 0.602060 0.845098 0.723579 0.723579 0.171854 0.0 0.212109 0.106054 0.106054 0.149984 0.236982 0.371585 0.306603 0.317232 0.052095 0.173267 0.406600 0.242140 0.210579 0.096894 0.125000 0.333333 0.186905 0.166667 0.083715 0.602060 0.954243 0.829918 0.845098 0.135269 0.0 0.212109 0.042422 0.000000 0.094858 0.073607 FILTHY PHANTOM Angle Park 5 161977365 2022-07-04 0.0 FILTHY PHANTOM 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801448232 110385 Tony Hinrichsen GIDDY-UP (N/P) STAKE 07:27PM 342 Masters 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 19.675 0.533632 7.730 0.0 0.0 0.124590 0.386907 0.455919 0.427165 0.420839 0.028492 0.314004 0.500000 0.408769 0.394027 0.082256 0.166667 0.333333 0.220000 0.200000 0.064979 0.602060 0.845098 0.756322 0.778151 0.090977 0.000000 0.182760 3.655208e-02 0.000000 8.173293e-02 0.386907 0.572783 0.449667 0.436026 0.053935 0.210921 0.551665 0.403088 0.394027 0.092070 0.142857 1.000000 0.291209 0.200000 0.235613 0.000000 0.903090 0.669243 0.778151 0.257193 0.0 0.249855 0.057150 0.000000 0.095131 0.277002 0.572783 0.459854 0.456897 0.059514 0.210921 0.616496 0.443040 0.443820 0.089192 0.142857 1.000000 0.464354 0.333333 0.320517 0.000000 0.903090 0.571676 0.602060 0.227821 0.0 0.249855 0.121936 0.180363 0.106520 0.128265 HOUND EM DOWN Maitland 3 196384049 2022-07-04 0.0 HOUND 'EM DOWN 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801490787 313281 Steven Winstanley ZIPPING GARTH @ STUD 0-2 WIN 07:36PM 565 Mixed Maiden and Grade Five 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 31.875 0.342029 13.795 0.0 0.0 0.200000 0.233442 0.277637 0.261405 0.273136 0.024321 0.218310 0.316690 0.270554 0.276662 0.049474 0.142857 0.200000 0.161905 0.142857 0.032991 0.778151 0.903090 0.861444 0.903090 0.072133 0.000000 0.000000 4.440892e-16 0.000000 0.000000e+00 0.194132 0.355912 0.279655 0.277637 0.046001 0.218310 0.316690 0.271078 0.275180 0.032574 0.125000 0.200000 0.158862 0.142857 0.026641 0.778151 0.954243 0.868122 0.903090 0.060784 0.0 0.000000 0.000000 0.000000 0.000000 0.194132 0.462627 0.328446 0.316396 0.054098 0.218310 0.346154 0.292413 0.296715 0.036166 0.125000 0.333333 0.187245 0.166667 0.054481 0.000000 0.954243 0.752017 0.845098 0.239572 0.0 0.212109 0.014373 0.000000 0.050118 0.070298 CAWBOURNE CROSS Angle Park 5 230053393 2022-07-04 0.0 CAWBOURNE CROSS 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801448232 104699 Lisa Rasmussen GIDDY-UP (N/P) STAKE 07:27PM 342 Masters 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 19.675 0.533632 7.730 0.0 0.0 0.169811 0.336395 0.413836 0.386878 0.410402 0.043753 0.500000 0.500000 0.500000 0.500000 0.000000 0.142857 0.250000 0.186508 0.166667 0.056260 0.698970 0.903090 0.815719 0.845098 0.105184 0.000000 0.000000 3.700743e-16 0.000000 2.533726e-17 0.336395 0.491121 0.420107 0.412119 0.059452 0.401747 0.506477 0.480510 0.500000 0.044239 0.142857 0.500000 0.273810 0.250000 0.129975 0.477121 0.903090 0.704218 0.698970 0.155860 0.0 0.188140 0.058566 0.000000 0.091070 0.280126 0.505631 0.403460 0.405048 0.064662 0.329857 0.610664 0.472084 0.500000 0.079724 0.142857 0.500000 0.269048 0.225000 0.125458 0.477121 0.903090 0.711583 0.738561 0.144209 0.0 0.205324 0.060368 0.000000 0.090871 0.179535 SKAIKRU Bathurst 6 243599770 2022-07-04 0.0 SKAIKRU 1 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455741 83214 Robert Sonter BATHURST RSL CLUB 04:59PM 450 Grade 5 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 26.000 0.576406 15.450 0.0 0.0 0.159574 0.392354 0.392354 0.392354 0.392354 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.000000 0.000000 1.998401e-15 0.000000 0.000000e+00 0.293318 0.392354 0.329286 0.302186 0.054798 0.238956 0.238956 0.238956 0.238956 0.000000 0.142857 0.333333 0.242063 0.250000 0.095486 0.602060 0.903090 0.734707 0.698970 0.153664 0.0 0.167027 0.055676 0.000000 0.096433 0.285293 0.392354 0.317943 0.303341 0.036066 0.162722 0.258454 0.223927 0.237266 0.042031 0.125000 0.333333 0.202551 0.200000 0.070985 0.602060 0.954243 0.794252 0.778151 0.120113 0.0 0.167027 0.023861 0.000000 0.063130 0.092948 <p>If you look closely at the data frame above you might notice that for reserve dogs, they will have a Box number of 9 or 10. There is only ever a max of 8 greyhounds per race therefore we will need to adjust it somehow. I didn't notice this issue for quite a while, but the good thing is the website gives us the info we need to adjust: </p> <p></p> <p>We can see that Rhinestone Ash is a reserve dog and has the number 9, if you click on rules, you can see what Box it is starting from: </p> <p></p> <p>The problem is, my webscraping is pretty poor, and it would take significant time for me to learn it. But after going through the documentation again, changes to boxes are actually available through the API under the <code>clarifications</code> attribute of <code>marketDescription</code>. You will be able to access this within Flumine as <code>market.market_catalogue.description.clarifications</code>, but it's a bit weird. It returns box changes as a string that looks like this: </p> <p>Originally I had planned to leave this article as it is since, I've never worked with anything like this before and its already getting pretty long, however huge shoutout to Betfair Quants community and especially Brett who provided his solution to working with box changes. </p> <pre><code>from nltk.tokenize import regexp_tokenize\n# my_string is an example string, that you will need to get live from the api via: market.market_catalogue.description.clarifications.replace(\"&lt;br/&gt; Dog\",\"&lt;br/&gt;Dog\")\nmy_string = \"&lt;br/&gt;Box changes:&lt;br/&gt;Dog 9. Tralee Blaze starts from box no. 8&lt;br/&gt;&lt;br/&gt;Dog 6. That Other One starts from box no. 2&lt;br/&gt;&lt;br/&gt;\"\nprint(f'HTML Comment: {my_string}')\npattern1 = r'(?&amp;lt;=&lt;br/&gt;Dog ).+?(?= starts)'\npattern2 = r\"(?&amp;lt;=\\bbox no. )(\\w+)\"\nrunners_df = pd.DataFrame (regexp_tokenize(my_string, pattern1), columns = ['runner_name'])\nrunners_df['runner_name'] = runners_df['runner_name'].astype(str)\n# Remove dog name from runner_number\nrunners_df['runner_number'] = runners_df['runner_name'].apply(lambda x: x[:(x.find(\" \") - 1)].upper())\n# Remove dog number from runner_name\nrunners_df['runner_name'] = runners_df['runner_name'].apply(lambda x: x[(x.find(\" \") + 1):].upper())\nrunners_df['Box'] = regexp_tokenize(my_string, pattern2)\nrunners_df\n</code></pre> <pre>\n<code>HTML Comment: &lt;br&gt;Box changes:&lt;br&gt;Dog 9. Tralee Blaze starts from box no. 8&lt;br&gt;&lt;br&gt;Dog 6. That Other One starts from box no. 2&lt;br&gt;&lt;br&gt;\n</code>\n</pre> runner_name runner_number Box 0 TRALEE BLAZE 9 8 1 THAT OTHER ONE 6 2 <p>Brett's solution is amazing, there is only one problem, currently our code is structured so that we generate our predictions in the morning well before the race starts. To implement the above fix, we need to generate our predictions just before the race starts to incorporate the Box information. </p> <p>This means we need to write a little bit more code to make it happen, but we are almost there. </p> <p>So now my plan to update the old data and generate probabilities just before the race. So now just before the jump my code structure will look like this:</p> <ul> <li>pull any data on box changes from the Betfair API</li> <li>convert the box change data into a dataframe named <code>runners_df</code> using the Brett's code</li> <li>in my original dataframe named <code>todays_data</code> replace any Box data with <code>runners_df</code> data, otherwise leave it untouched</li> <li>then merge the <code>box_win_percent</code> dataframe back onto the <code>todays_data</code> dataframe </li> <li>now we can predict probabilities again and then renormalise them</li> </ul> <p>It may sound a little complicated but as we already have Brett's code there is only a few extra lines of code we need to write. This is what we will add into our Flumine strategy along with Brett's code:</p> <pre><code># Running Brett's code gives us a nice dataframe named runners_df that we can work with\n# Replace any old Box info in our original dataframe with data available in \nrunners_df = runners_df.set_index('runner_name')\ntodays_data.loc[(runners_df.index[runners_df.index.isin(dog_names)],track,race_number),'Box'] = runners_df.loc[runners_df.index.isin(dog_names),'Box'].to_list()\n# Merge box_win_percent data onto todays_data\ntodays_data = todays_data.merge(box_win_percent, on=['Track', 'Distance', 'Box'], how='left')\n# Merge box_win_percentage back on:\ntodays_data = todays_data.drop(columns = 'box_win_percentage', axis = 1)\ntodays_data = todays_data.merge(box_win_percent, on = ['Track', 'Distance','Box'], how = 'left')\n\n# Generate probabilities using Bruno's model\ntodays_data.loc[(dog_names,track,race_number),'prob_LogisticRegression'] = brunos_model.predict_proba(todays_data.loc[(dog_names,track,race_number)][feature_cols])[:,1]\n# renomalise probabilities\nprobabilities = todays_data.loc[dog_names,track,race_number]['prob_LogisticRegression']\ntodays_data.loc[(dog_names,track,race_number),'renormalised_prob'] = probabilities/probabilities.sum()\n# convert probaiblities to ratings\ntodays_data.loc[(dog_names,track,race_number),'rating'] = 1/todays_data.loc[dog_names,track,race_number]['renormalised_prob']\n</code></pre> <p>Now everything is done, and we can finally move onto placing our bets</p> <p>Now that we have our data nicely set up. We can reference our probabilities by getting the DogName, Track and RaceNum from the Betfair polling API and then renormalised probabilities to calculate ratings with only a few lines of code. Then the rest is the same as How to Automate III</p> <pre><code># Import libraries for logging in\nimport betfairlightweight\nfrom flumine import Flumine, clients\n\n# Credentials to login and logging in \ntrading = betfairlightweight.APIClient('username','password',app_key='appkey')\nclient = clients.BetfairClient(trading, interactive_login=True)\n\n# Login\nframework = Flumine(client=client)\n\n# Code to login when using security certificates\n# trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs')\n# client = clients.BetfairClient(trading)\n\n# framework = Flumine(client=client)\n</code></pre> <pre><code># Import libraries and logging\nfrom flumine import BaseStrategy \nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder\nfrom flumine.markets.market import Market\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\nimport re\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport logging\nlogging.basicConfig(filename = 'how_to_automate_4.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n</code></pre> <p>Let's create a new class for our strategy called FlatBetting that finds the best available to back and lay price 60 seconds before the jump. If any of those prices have value, we will place a flat bet for $5 at those prices. This code is almost the same as How to Automate III</p> <p>Since we are now editing our <code>todays_data</code> dataframe inside our Flumine strategy we will also need to convert <code>todays_data</code> to a global variable which is a simple one liner: <pre><code>global todays_data\n</code></pre></p> <p>I also wanted to call out one gotcha that, Brett found that is almost impossible to find unless you are keeping a close eye on your logs. Sometimes the polling API and streaming API doesn't match up when there are scratchings, so we need to check if it does: <pre><code># Check the polling API and streaming API matches up (sometimes it doesn't)\nif runner_cata.selection_id == runner.selection_id:\n</code></pre></p> <pre><code>class FlatBetting(BaseStrategy):\n    def start(self) -&amp;gt; None:\n        print(\"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; bool:\n        if market_book.status != \"CLOSED\":\n            return True\n\n    def process_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; None:\n        # Convert dataframe to a global variable\n        global todays_data\n\n        # At the 60 second mark:\n        if market.seconds_to_start &amp;lt; 60 and market_book.inplay == False:\n            # get the list of dog_names, name of the track/venue and race_number/RaceNum from Betfair Polling API\n            dog_names = []\n            track = market.market_catalogue.event.venue\n            race_number = market.market_catalogue.market_name.split(' ',1)[0]  # comes out as R1/R2/R3 .. etc\n            race_number = re.sub(\"[^0-9]\", \"\", race_number)  # only keep the numbers \n            for runner_cata in market.market_catalogue.runners:\n                dog_name = runner_cata.runner_name.split(' ',1)[1].upper()\n                dog_names.append(dog_name)\n\n            # Check if there are box changes, if there are then use Brett's code\n            if market.market_catalogue.description.clarifications != None:\n                # Brett's code to get Box changes:\n                my_string = market.market_catalogue.description.clarifications.replace(\"&lt;br/&gt; Dog\",\"&lt;br/&gt;Dog\")\n                pattern1 = r'(?&amp;lt;=&lt;br/&gt;Dog ).+?(?= starts)'\n                pattern2 = r\"(?&amp;lt;=\\bbox no. )(\\w+)\"\n                runners_df = pd.DataFrame (regexp_tokenize(my_string, pattern1), columns = ['runner_name'])\n                runners_df['runner_name'] = runners_df['runner_name'].astype(str)\n                # Remove dog name from runner_number\n                runners_df['runner_number'] = runners_df['runner_name'].apply(lambda x: x[:(x.find(\" \") - 1)].upper())\n                # Remove dog number from runner_name\n                runners_df['runner_name'] = runners_df['runner_name'].apply(lambda x: x[(x.find(\" \") + 1):].upper())\n                runners_df['Box'] = regexp_tokenize(my_string, pattern2)\n\n                # Replace any old Box info in our original dataframe with data available in runners_df\n                runners_df = runners_df.set_index('runner_name')\n                todays_data.loc[(runners_df.index[runners_df.index.isin(dog_names)],track,race_number),'Box'] = runners_df.loc[runners_df.index.isin(dog_names),'Box'].to_list()\n                # Merge box_win_percentage back on:\n                todays_data = todays_data.drop(columns = 'box_win_percentage', axis = 1)\n                todays_data = todays_data.reset_index().merge(box_win_percent, on = ['Track', 'Distance','Box'], how = 'left').set_index(['DogName_bf','Track','RaceNum'])\n\n            # Generate probabilities using Bruno's model\n            todays_data.loc[(dog_names,track,race_number),'prob_LogisticRegression'] = brunos_model.predict_proba(todays_data.loc[(dog_names,track,race_number)][feature_cols])[:,1]\n            # renomalise probabilities\n            probabilities = todays_data.loc[dog_names,track,race_number]['prob_LogisticRegression']\n            todays_data.loc[(dog_names,track,race_number),'renormalised_prob'] = probabilities/probabilities.sum()\n            # convert probaiblities to ratings\n            todays_data.loc[(dog_names,track,race_number),'rating'] = 1/todays_data.loc[dog_names,track,race_number]['renormalised_prob']\n\n            # Use both the polling api (market.catalogue) and the streaming api at once:\n            for runner_cata, runner in zip(market.market_catalogue.runners, market_book.runners):\n                # Check the polling api and streaming api matches up (sometimes it doesn't)\n                if runner_cata.selection_id == runner.selection_id:\n                    # Get the dog_name from polling api then reference our data for our model rating\n                    dog_name = runner_cata.runner_name.split(' ',1)[1].upper()\n\n                    # Rest is the same as How to Automate III\n                    model_price = todays_data.loc[dog_name,track,race_number]['rating']\n                    ### If you have an issue such as:\n                        # Unknown error The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n                        # Then do model_price = todays_data.loc[dog_name,track,race_number]['rating'].item()\n\n                    # Log info before placing bets\n                    logging.info(f'dog_name: {dog_name}')\n                    logging.info(f'model_price: {model_price}')\n                    logging.info(f'market_id: {market_book.market_id}')\n                    logging.info(f'selection_id: {runner.selection_id}')\n\n                    # If best available to back price is &amp;gt; rated price then flat $5 back\n                    if runner.status == \"ACTIVE\" and runner.ex.available_to_back[0]['price'] &amp;gt; model_price:\n                        trade = Trade(\n                        market_id=market_book.market_id,\n                        selection_id=runner.selection_id,\n                        handicap=runner.handicap,\n                        strategy=self,\n                        )\n                        order = trade.create_order(\n                            side=\"BACK\", order_type=LimitOrder(price=runner.ex.available_to_back[0]['price'], size=5.00)\n                        )\n                        market.place_order(order)\n                    # If best available to lay price is &amp;lt; rated price then flat $5 lay\n                    if runner.status == \"ACTIVE\" and runner.ex.available_to_lay[0]['price'] &amp;lt; model_price:\n                        trade = Trade(\n                        market_id=market_book.market_id,\n                        selection_id=runner.selection_id,\n                        handicap=runner.handicap,\n                        strategy=self,\n                        )\n                        order = trade.create_order(\n                            side=\"LAY\", order_type=LimitOrder(price=runner.ex.available_to_lay[0]['price'], size=5.00)\n                        )\n                        market.place_order(order)\n</code></pre> <p>As the model we have built is a greyhound model for Australian racing let's point our strategy to Australian greyhound win markets</p> <pre><code>greyhounds_strategy = FlatBetting(\n    market_filter=streaming_market_filter(\n        event_type_ids=[\"4339\"], # Greyhounds markets\n        country_codes=[\"AU\"], # Australian markets\n        market_types=[\"WIN\"], # Win markets\n    ),\n    max_order_exposure= 50, # Max exposure per order = 50\n    max_trade_count=1, # Max 1 trade per selection\n    max_live_trade_count=1, # Max 1 unmatched trade per selection\n)\n\nframework.add_strategy(greyhounds_strategy)\n</code></pre> <p>And add our auto-terminate and bet logging from the previous tutorials:</p> <pre><code># import logging\nimport datetime\nfrom flumine.worker import BackgroundWorker\nfrom flumine.events.events import TerminationEvent\n\n# logger = logging.getLogger(__name__)\n\n\"\"\"\nWorker can be used as followed:\n    framework.add_worker(\n        BackgroundWorker(\n            framework,\n            terminate,\n            func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n            interval=60,\n            start_delay=60,\n        )\n    )\nThis will run every 60s and will terminate \nthe framework if all markets starting 'today' \nhave been closed for at least 1200s\n\"\"\"\n\n\n# Function that stops automation running at the end of the day\ndef terminate(\n    context: dict, flumine, today_only: bool = True, seconds_closed: int = 600\n) -&amp;gt; None:\n    \"\"\"terminate framework if no markets\n    live today.\n    \"\"\"\n    markets = list(flumine.markets.markets.values())\n    markets_today = [\n        m\n        for m in markets\n        if m.market_start_datetime.date() == datetime.datetime.utcnow().date()\n        and (\n            m.elapsed_seconds_closed is None\n            or (m.elapsed_seconds_closed and m.elapsed_seconds_closed &amp;lt; seconds_closed)\n        )\n    ]\n    if today_only:\n        market_count = len(markets_today)\n    else:\n        market_count = len(markets)\n    if market_count == 0:\n        # logger.info(\"No more markets available, terminating framework\")\n        flumine.handler_queue.put(TerminationEvent(flumine))\n\n# Add the stopped to our framework\nframework.add_worker(\n    BackgroundWorker(\n        framework,\n        terminate,\n        func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n        interval=60,\n        start_delay=60,\n    )\n)\n</code></pre> <pre><code>import os\nimport csv\nimport logging\nfrom flumine.controls.loggingcontrols import LoggingControl\nfrom flumine.order.ordertype import OrderTypes\n\nlogger = logging.getLogger(__name__)\n\nFIELDNAMES = [\n    \"bet_id\",\n    \"strategy_name\",\n    \"market_id\",\n    \"selection_id\",\n    \"trade_id\",\n    \"date_time_placed\",\n    \"price\",\n    \"price_matched\",\n    \"size\",\n    \"size_matched\",\n    \"profit\",\n    \"side\",\n    \"elapsed_seconds_executable\",\n    \"order_status\",\n    \"market_note\",\n    \"trade_notes\",\n    \"order_notes\",\n]\n\n\nclass LiveLoggingControl(LoggingControl):\n    NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n    def __init__(self, *args, **kwargs):\n        super(LiveLoggingControl, self).__init__(*args, **kwargs)\n        self._setup()\n\n    # Changed file path and checks if the file orders_hta_4.csv already exists, if it doens't then create it\n    def _setup(self):\n        if os.path.exists(\"orders_hta_4.csv\"):\n            logging.info(\"Results file exists\")\n        else:\n            with open(\"orders_hta_4.csv\", \"w\") as m:\n                csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                csv_writer.writeheader()\n\n    def _process_cleared_orders_meta(self, event):\n        orders = event.event\n        with open(\"orders_hta_4.csv\", \"a\") as m:\n            for order in orders:\n                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                    size = order.order_type.size\n                else:\n                    size = order.order_type.liability\n                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                    price = None\n                else:\n                    price = order.order_type.price\n                try:\n                    order_data = {\n                        \"bet_id\": order.bet_id,\n                        \"strategy_name\": order.trade.strategy,\n                        \"market_id\": order.market_id,\n                        \"selection_id\": order.selection_id,\n                        \"trade_id\": order.trade.id,\n                        \"date_time_placed\": order.responses.date_time_placed,\n                        \"price\": price,\n                        \"price_matched\": order.average_price_matched,\n                        \"size\": size,\n                        \"size_matched\": order.size_matched,\n                        \"profit\": 0 if not order.cleared_order else order.cleared_order.profit,\n                        \"side\": order.side,\n                        \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                        \"order_status\": order.status.value,\n                        \"market_note\": order.trade.market_notes,\n                        \"trade_notes\": order.trade.notes_str,\n                        \"order_notes\": order.notes_str,\n                    }\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                    csv_writer.writerow(order_data)\n                except Exception as e:\n                    logger.error(\n                        \"_process_cleared_orders_meta: %s\" % e,\n                        extra={\"order\": order, \"error\": e},\n                    )\n\n        logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n    def _process_cleared_markets(self, event):\n        cleared_markets = event.event\n        for cleared_market in cleared_markets.orders:\n            logger.info(\n                \"Cleared market\",\n                extra={\n                    \"market_id\": cleared_market.market_id,\n                    \"bet_count\": cleared_market.bet_count,\n                    \"profit\": cleared_market.profit,\n                    \"commission\": cleared_market.commission,\n                },\n            )\n\nframework.add_logging_control(\n    LiveLoggingControl()\n)\n</code></pre> <pre><code>framework.run()\n</code></pre> <p>Boom! We now have an automated script that will downloads all the data we need in the morning, generates a set of predictions, place flat stakes bets, logs all bets and switches itself off at the end of the day. All we need to do is hit play in the morning!</p> <p>We have now written code automation code for three different strategies, however we haven't actually backtested any of our strategies or models yet. So for the final part of the How to Automate series we will be writing code to How to simulate the Exchange to backtest and optimise our strategies. Make sure not to miss it as this is where I believe the sauce is made (not that I have made significant sauce).</p> <p>Run the code from your ide by using py <code>&lt;filename&gt;</code>.py, making sure you amend the path to point to your input data.</p> <p>Download from Github </p> <pre><code>from joblib import load\nimport os\nimport sys\n\n# Allow imports from src folder\nmodule_path = os.path.abspath(os.path.join('../src'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nfrom dateutil import tz\nfrom pandas.tseries.offsets import MonthEnd\nfrom sklearn.preprocessing import MinMaxScaler\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nfrom nltk.tokenize import regexp_tokenize\n\n# settings to display all columns\npd.set_option(\"display.max_columns\", None)\n\nimport fasttrack as ft\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Import libraries for logging in\nimport betfairlightweight\nfrom flumine import Flumine, clients\n# Import libraries and logging\nfrom flumine import BaseStrategy \nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder\nfrom flumine.markets.market import Market\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\nimport re\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport logging\nlogging.basicConfig(filename = 'how_to_automate_4.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n\n# import logging\nfrom flumine.worker import BackgroundWorker\nfrom flumine.events.events import TerminationEvent\n\nimport csv\nfrom flumine.controls.loggingcontrols import LoggingControl\nfrom flumine.order.ordertype import OrderTypes\n\nlogger = logging.getLogger(__name__)\n\nbrunos_model = load('logistic_regression.joblib')\nbrunos_model\n\n# Validate FastTrack API connection\napi_key = os.getenv('FAST_TRACK_API_KEY',)\nclient = ft.Fasttrack(api_key)\ntrack_codes = client.listTracks()\n\n# Import race data excluding NZ races\nau_tracks_filter = list(track_codes[track_codes['state'] != 'NZ']['track_code'])\n\n# Time window to import data\n# First day of the month 46 months back from now\ndate_from = (datetime.today() - relativedelta(months=46)).replace(day=1).strftime('%Y-%m-%d')\n# First day of previous month\ndate_to = (datetime.today() - relativedelta(months=1)).replace(day=1).strftime('%Y-%m-%d')\n\n# Dataframes to populate data with\nrace_details = pd.DataFrame()\ndog_results = pd.DataFrame()\n\n# For each month, either fetch data from API or use local CSV file if we already have downloaded it\nfor start in pd.date_range(date_from, date_to, freq='MS'):\n    start_date = start.strftime(\"%Y-%m-%d\")\n    end_date = (start + MonthEnd(1)).strftime(\"%Y-%m-%d\")\n    try:\n        filename_races = f'FT_AU_RACES_{start_date}.csv'\n        filename_dogs = f'FT_AU_DOGS_{start_date}.csv'\n\n        filepath_races = f'../data/{filename_races}'\n        filepath_dogs = f'../data/{filename_dogs}'\n\n        print(f'Loading data from {start_date} to {end_date}')\n        if os.path.isfile(filepath_races):\n            # Load local CSV file\n            month_race_details = pd.read_csv(filepath_races) \n            month_dog_results = pd.read_csv(filepath_dogs) \n        else:\n            # Fetch data from API\n            month_race_details, month_dog_results = client.getRaceResults(start_date, end_date, au_tracks_filter)\n            month_race_details.to_csv(filepath_races, index=False)\n            month_dog_results.to_csv(filepath_dogs, index=False)\n\n        # Combine monthly data\n        race_details = race_details.append(month_race_details, ignore_index=True)\n        dog_results = dog_results.append(month_dog_results, ignore_index=True)\n    except:\n        print(f'Could not load data from {start_date} to {end_date}')\n\nrace_details.tail()\n\ncurrent_month_start_date = pd.Timestamp.now().replace(day=1).strftime(\"%Y-%m-%d\")\ncurrent_month_end_date = (pd.Timestamp.now().replace(day=1)+ MonthEnd(1))\ncurrent_month_end_date = (current_month_end_date - pd.Timedelta('1 day')).strftime(\"%Y-%m-%d\")\n\nprint(f'Start date: {current_month_start_date}')\nprint(f'End Date: {current_month_end_date}')\n\n# Download data for races that have concluded this current month up untill today\n# Start and end dates for current month\ncurrent_month_start_date = pd.Timestamp.now().replace(day=1).strftime(\"%Y-%m-%d\")\ncurrent_month_end_date = (pd.Timestamp.now().replace(day=1)+ MonthEnd(1))\ncurrent_month_end_date = (current_month_end_date - pd.Timedelta('1 day')).strftime(\"%Y-%m-%d\")\n\n# Files names \nfilename_races = f'FT_AU_RACES_{current_month_start_date}.csv'\nfilename_dogs = f'FT_AU_DOGS_{current_month_start_date}.csv'\n# Where to store files locally\nfilepath_races = f'../data/{filename_races}'\nfilepath_dogs = f'../data/{filename_dogs}'\n\n# Fetch data from API\nmonth_race_details, month_dog_results = client.getRaceResults(current_month_start_date, current_month_end_date, au_tracks_filter)\n\n# Save the files locally and replace any out of date fields\nmonth_race_details.to_csv(filepath_races, index=False)\nmonth_dog_results.to_csv(filepath_dogs, index=False)\n\ndog_results\n\n# This is super important I have spent literally hours before I found out this was causing errors\ndog_results['@id'] = pd.to_numeric(dog_results['@id'])\n\n# Append the extra data to our data frames \nrace_details = race_details.append(month_race_details, ignore_index=True)\ndog_results = dog_results.append(month_dog_results, ignore_index=True)\n\n# Download the data for todays races\ntodays_date = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\ntodays_races, todays_dogs = client.getFullFormat(dt= todays_date, tracks = au_tracks_filter)\n\n# display is for ipython notebooks only\n# display(todays_races.head(1), todays_dogs.head(1))\n\n# It seems that the todays_races dataframe doesn't have the date column, so let's add that on\ntodays_races['date'] = pd.Timestamp.now().strftime('%d %b %y')\ntodays_races.head(1)\n\n# It also seems that in todays_dogs dataframe Box is labeled as RaceBox instead, so let's rename it\n# We can also see that there are some specific dogs that have \"Res.\" as a suffix of their name, i.e. they are reserve dogs,\n# We will treat this later\ntodays_dogs = todays_dogs.rename(columns={\"RaceBox\":\"Box\"})\ntodays_dogs.tail(3)\n\n# Appending todays data to this months data\nmonth_dog_results = pd.concat([month_dog_results,todays_dogs],join='outer')[month_dog_results.columns]\nmonth_race_details = pd.concat([month_race_details,todays_races],join='outer')[month_race_details.columns]\n\n# Appending this months data to the rest of our historical data\nrace_details = race_details.append(month_race_details, ignore_index=True)\ndog_results = dog_results.append(month_dog_results, ignore_index=True)\n\nrace_details\n\n## Cleanse and normalise the data\n# Clean up the race dataset\nrace_details = race_details.rename(columns = {'@id': 'FastTrack_RaceId'})\nrace_details['Distance'] = race_details['Distance'].apply(lambda x: int(x.replace(\"m\", \"\")))\nrace_details['date_dt'] = pd.to_datetime(race_details['date'], format = '%d %b %y')\n# Clean up the dogs results dataset\ndog_results = dog_results.rename(columns = {'@id': 'FastTrack_DogId', 'RaceId': 'FastTrack_RaceId'})\n\n# New line of code (rest of this code chunk is copied from bruno's code)\ndog_results['FastTrack_DogId'] = pd.to_numeric(dog_results['FastTrack_DogId'])\n\n# Combine dogs results with race attributes\ndog_results = dog_results.merge(\n    race_details, \n    how = 'left',\n    on = 'FastTrack_RaceId'\n)\n\n# Convert StartPrice to probability\ndog_results['StartPrice'] = dog_results['StartPrice'].apply(lambda x: None if x is None else float(x.replace('$', '').replace('F', '')) if isinstance(x, str) else x)\ndog_results['StartPrice_probability'] = (1 / dog_results['StartPrice']).fillna(0)\ndog_results['StartPrice_probability'] = dog_results.groupby('FastTrack_RaceId')['StartPrice_probability'].apply(lambda x: x / x.sum())\n\n# Discard entries without results (scratched or did not finish)\ndog_results = dog_results[~dog_results['Box'].isnull()]\ndog_results['Box'] = dog_results['Box'].astype(int)\n\n# Clean up other attributes\ndog_results['RunTime'] = dog_results['RunTime'].astype(float)\ndog_results['SplitMargin'] = dog_results['SplitMargin'].astype(float)\ndog_results['Prizemoney'] = dog_results['Prizemoney'].astype(float).fillna(0)\ndog_results['Place'] = pd.to_numeric(dog_results['Place'].apply(lambda x: x.replace(\"=\", \"\") if isinstance(x, str) else 0), errors='coerce').fillna(0)\ndog_results['win'] = dog_results['Place'].apply(lambda x: 1 if x == 1 else 0)\n\n# Normalise some of the raw values\ndog_results['Prizemoney_norm'] = np.log10(dog_results['Prizemoney'] + 1) / 12\ndog_results['Place_inv'] = (1 / dog_results['Place']).fillna(0)\ndog_results['Place_log'] = np.log10(dog_results['Place'] + 1).fillna(0)\ndog_results['RunSpeed'] = (dog_results['RunTime'] / dog_results['Distance']).fillna(0)\n\n## Generate features using raw data\n# Calculate median winner time per track/distance\nwin_results = dog_results[dog_results['win'] == 1]\nmedian_win_time = pd.DataFrame(data=win_results[win_results['RunTime'] &amp;gt; 0].groupby(['Track', 'Distance'])['RunTime'].median()).rename(columns={\"RunTime\": \"RunTime_median\"}).reset_index()\nmedian_win_split_time = pd.DataFrame(data=win_results[win_results['SplitMargin'] &amp;gt; 0].groupby(['Track', 'Distance'])['SplitMargin'].median()).rename(columns={\"SplitMargin\": \"SplitMargin_median\"}).reset_index()\nmedian_win_time.head()\n\n# Calculate track speed index\nmedian_win_time['speed_index'] = (median_win_time['RunTime_median'] / median_win_time['Distance'])\nmedian_win_time['speed_index'] = MinMaxScaler().fit_transform(median_win_time[['speed_index']])\nmedian_win_time.head()\n\n# Compare dogs finish time with median winner time\ndog_results = dog_results.merge(median_win_time, on=['Track', 'Distance'], how='left')\ndog_results = dog_results.merge(median_win_split_time, on=['Track', 'Distance'], how='left')\n\n# Normalise time comparison\ndog_results['RunTime_norm'] = (dog_results['RunTime_median'] / dog_results['RunTime']).clip(0.9, 1.1)\ndog_results['RunTime_norm'] = MinMaxScaler().fit_transform(dog_results[['RunTime_norm']])\ndog_results['SplitMargin_norm'] = (dog_results['SplitMargin_median'] / dog_results['SplitMargin']).clip(0.9, 1.1)\ndog_results['SplitMargin_norm'] = MinMaxScaler().fit_transform(dog_results[['SplitMargin_norm']])\ndog_results.head()\n\n# Calculate box winning percentage for each track/distance\nbox_win_percent = pd.DataFrame(data=dog_results.groupby(['Track', 'Distance', 'Box'])['win'].mean()).rename(columns={\"win\": \"box_win_percent\"}).reset_index()\n# Add to dog results dataframe\ndog_results = dog_results.merge(box_win_percent, on=['Track', 'Distance', 'Box'], how='left')\n# Display example of barrier winning probabilities\nprint(box_win_percent.head(8))\n\ndog_results[dog_results['FastTrack_DogId'] == 592253143].tail()[['date_dt','Place','DogName','RaceNum','Track','Distance','win','Prizemoney_norm','Place_inv','Place_log']]\n\n# Generate rolling window features\ndataset = dog_results.copy()\ndataset = dataset.set_index(['FastTrack_DogId', 'date_dt']).sort_index()\n\n# Use rolling window of 28, 91 and 365 days\nrolling_windows = ['28D', '91D', '365D']\n# Features to use for rolling windows calculation\nfeatures = ['RunTime_norm', 'SplitMargin_norm', 'Place_inv', 'Place_log', 'Prizemoney_norm']\n# Aggregation functions to apply\naggregates = ['min', 'max', 'mean', 'median', 'std']\n# Keep track of generated feature names\nfeature_cols = ['speed_index', 'box_win_percent']\n\nfor rolling_window in rolling_windows:\n        print(f'Processing rolling window {rolling_window}')\n\n        rolling_result = (\n            dataset\n            .reset_index(level=0).sort_index()\n            .groupby('FastTrack_DogId')[features]\n            .rolling(rolling_window)\n            .agg(aggregates)\n            .groupby(level=0)  # Thanks to Brett for finding this!\n            .shift(1)\n        )\n\n        # My own dodgey code to work with reserve dogs\n        temp = rolling_result.reset_index()\n        temp = temp[temp['date_dt'] == pd.Timestamp.now().normalize()]\n        temp.groupby(['FastTrack_DogId','date_dt']).first()\n        rolling_result.loc[pd.IndexSlice[:, pd.Timestamp.now().normalize()], :] = temp.groupby(['FastTrack_DogId','date_dt']).first()\n\n        # Generate list of rolling window feature names (eg: RunTime_norm_min_365D)\n        agg_features_cols = [f'{f}_{a}_{rolling_window}' for f, a in itertools.product(features, aggregates)]\n        # Add features to dataset\n        dataset[agg_features_cols] = rolling_result\n        # Keep track of generated feature names\n        feature_cols.extend(agg_features_cols)\n\n# Replace missing values with 0\ndataset.fillna(0, inplace=True)\n# display(dataset.head(8))  # display is only for ipython notebooks\n\n# Only keep data after 2018-12-01\nmodel_df = dataset.reset_index()\nfeature_cols = np.unique(feature_cols).tolist()\nmodel_df = model_df[model_df['date_dt'] &amp;gt;= '2018-12-01']\n\n# This line was originally part of Bruno's tutorial, but we don't run it in this script\n# model_df = model_df[['date_dt', 'FastTrack_RaceId', 'DogName', 'win', 'StartPrice_probability'] + feature_cols]\n\n# Only train model off of races where each dog has a value for each feature\nraces_exclude = model_df[model_df.isnull().any(axis = 1)]['FastTrack_RaceId'].drop_duplicates()\nmodel_df = model_df[~model_df['FastTrack_RaceId'].isin(races_exclude)]\n\n# Generate predictions like normal\n# Range of dates that we want to simulate later '2022-03-01' to '2022-04-01'\ntodays_data = model_df[(model_df['date_dt'] &amp;gt;= pd.Timestamp('2022-03-01').strftime('%Y-%m-%d')) &amp;amp; (model_df['date_dt'] &amp;lt; pd.Timestamp('2022-04-01').strftime('%Y-%m-%d'))]\ndog_win_probabilities = brunos_model.predict_proba(todays_data[feature_cols])[:,1]\ntodays_data['prob_LogisticRegression'] = dog_win_probabilities\ntodays_data['renormalise_prob'] = todays_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / x.sum())\ntodays_data['rating'] = 1/todays_data['renormalise_prob']\ntodays_data = todays_data.sort_values(by = 'date_dt')\ntodays_data\n\ndef download_iggy_ratings(date):\n    \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame.\n\n    Args:\n        date (datetime): the date we want to download the ratings for\n    \"\"\"\n    iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date='\n    iggy_url_2 = date.strftime(\"%Y-%m-%d\")\n    iggy_url_3 = '&amp;amp;presenter=RatingsPresenter&amp;amp;csv=true'\n    iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3\n\n    # Download todays greyhounds ratings\n    iggy_df = pd.read_csv(iggy_url)\n\n    # Data clearning\n    iggy_df = iggy_df.rename(\n    columns={\n        \"meetings.races.bfExchangeMarketId\":\"market_id\",\n        \"meetings.races.runners.bfExchangeSelectionId\":\"selection_id\",\n        \"meetings.races.runners.ratedPrice\":\"rating\",\n        \"meetings.races.number\":\"RaceNum\",\n        \"meetings.name\":\"Track\",\n        \"meetings.races.runners.name\":\"DogName\"\n        }\n    )\n    # iggy_df = iggy_df[['market_id','selection_id','rating']]\n    iggy_df['market_id'] = iggy_df['market_id'].astype(str)\n    iggy_df['date_dt'] = date\n\n    # Set market_id and selection_id as index for easy referencing\n    # iggy_df = iggy_df.set_index(['market_id','selection_id'])\n    return(iggy_df)\n\n# Download historical ratings over a time period and convert into a big DataFrame.\nback_test_period = pd.date_range(start='2022-03-01', end='2022-04-01')\nframes = [download_iggy_ratings(day) for day in back_test_period]\niggy_df = pd.concat(frames)\niggy_df\n\n# format DogNames to merge\ntodays_data['DogName'] = todays_data['DogName'].apply(lambda x: x.replace(\"'\", \"\").replace(\".\", \"\").replace(\"Res\", \"\").strip())\niggy_df['DogName'] = iggy_df['DogName'].str.upper()\n# Merge\nbacktest = iggy_df[['market_id','selection_id','DogName','date_dt']].merge(todays_data[['rating','DogName','date_dt']], how = 'inner', on = ['DogName','date_dt'])\nbacktest\n\n# Save predictions for if we want to backtest/simulate it later\nbacktest.to_csv('backtest.csv', index=False) # Csv format\n# backtest.to_pickle('backtest.pkl') # pickle format (faster, but can't open in excel)\n\ntodays_data[todays_data['FastTrack_RaceId'] == '798906744']\n\n# Select todays data \ntodays_data = model_df[model_df['date_dt'] == pd.Timestamp.now().strftime('%Y-%m-%d')]\n\n# Generate runner win predictions\ndog_win_probabilities = brunos_model.predict_proba(todays_data[feature_cols])[:,1]\ntodays_data['prob_LogisticRegression'] = dog_win_probabilities\n\n# We no longer renomralise probability in this chunk of code, do it in Flumine instead\n# todays_data['renormalise_prob'] = todays_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / x.sum())\n# todays_data['rating'] = 1/todays_data['renormalise_prob']\n# todays_data = todays_data.sort_values(by = 'date_dt')\n\ntodays_data\n\n# Prepare data for easy reference in flumine\ntodays_data['DogName_bf'] = todays_data['DogName'].apply(lambda x: x.replace(\"'\", \"\").replace(\".\", \"\").replace(\"Res\", \"\").strip())\ntodays_data.replace({'Sandown (SAP)': 'Sandown Park'}, regex=True, inplace=True)\ntodays_data = todays_data.set_index(['DogName_bf','Track','RaceNum'])\ntodays_data.head()\n\n# Credentials to login and logging in \ntrading = betfairlightweight.APIClient('username','password',app_key='appkey')\nclient = clients.BetfairClient(trading, interactive_login=True)\n\n# Login\nframework = Flumine(client=client)\n\n# Code to login when using security certificates\n# trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs')\n# client = clients.BetfairClient(trading)\n\n# framework = Flumine(client=client)\n\nclass FlatBetting(BaseStrategy):\n    def start(self) -&amp;gt; None:\n        print(\"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; bool:\n        if market_book.status != \"CLOSED\":\n            return True\n\n    def process_market_book(self, market: Market, market_book: MarketBook) -&amp;gt; None:\n        # Convert dataframe to a global variable\n        global todays_data\n\n        # At the 60 second mark:\n        if market.seconds_to_start &amp;lt; 60 and market_book.inplay == False:\n            # get the list of dog_names, name of the track/venue and race_number/RaceNum from Betfair Polling API\n            dog_names = []\n            track = market.market_catalogue.event.venue\n            race_number = market.market_catalogue.market_name.split(' ',1)[0]  # comes out as R1/R2/R3 .. etc\n            race_number = re.sub(\"[^0-9]\", \"\", race_number)  # only keep the numbers \n            for runner_cata in market.market_catalogue.runners:\n                dog_name = runner_cata.runner_name.split(' ',1)[1].upper()\n                dog_names.append(dog_name)\n\n            # Check if there are box changes, if there are then use Brett's code\n            if market.market_catalogue.description.clarifications != None:\n                # Brett's code to get Box changes:\n                my_string = market.market_catalogue.description.clarifications.replace(\"&lt;br/&gt; Dog\",\"&lt;br/&gt;Dog\")\n                pattern1 = r'(?&amp;lt;=&lt;br/&gt;Dog ).+?(?= starts)'\n                pattern2 = r\"(?&amp;lt;=\\bbox no. )(\\w+)\"\n                runners_df = pd.DataFrame (regexp_tokenize(my_string, pattern1), columns = ['runner_name'])\n                runners_df['runner_name'] = runners_df['runner_name'].astype(str)\n                # Remove dog name from runner_number\n                runners_df['runner_number'] = runners_df['runner_name'].apply(lambda x: x[:(x.find(\" \") - 1)].upper())\n                # Remove dog number from runner_name\n                runners_df['runner_name'] = runners_df['runner_name'].apply(lambda x: x[(x.find(\" \") + 1):].upper())\n                runners_df['Box'] = regexp_tokenize(my_string, pattern2)\n\n                # Replace any old Box info in our original dataframe with data available in runners_df\n                runners_df = runners_df.set_index('runner_name')\n                todays_data.loc[(runners_df.index[runners_df.index.isin(dog_names)],track,race_number),'Box'] = runners_df.loc[runners_df.index.isin(dog_names),'Box'].to_list()\n                # Merge box_win_percentage back on:\n                todays_data = todays_data.drop(columns = 'box_win_percentage', axis = 1)\n                todays_data = todays_data.reset_index().merge(box_win_percent, on = ['Track', 'Distance','Box'], how = 'left').set_index(['DogName_bf','Track','RaceNum'])\n\n            # Generate probabilities using Bruno's model\n            todays_data.loc[(dog_names,track,race_number),'prob_LogisticRegression'] = brunos_model.predict_proba(todays_data.loc[(dog_names,track,race_number)][feature_cols])[:,1]\n            # renomalise probabilities\n            probabilities = todays_data.loc[dog_names,track,race_number]['prob_LogisticRegression']\n            todays_data.loc[(dog_names,track,race_number),'renormalised_prob'] = probabilities/probabilities.sum()\n            # convert probaiblities to ratings\n            todays_data.loc[(dog_names,track,race_number),'rating'] = 1/todays_data.loc[dog_names,track,race_number]['renormalised_prob']\n\n            # Use both the polling api (market.catalogue) and the streaming api at once:\n            for runner_cata, runner in zip(market.market_catalogue.runners, market_book.runners):\n                # Check the polling api and streaming api matches up (sometimes it doesn't)\n                if runner_cata.selection_id == runner.selection_id:\n                    # Get the dog_name from polling api then reference our data for our model rating\n                    dog_name = runner_cata.runner_name.split(' ',1)[1].upper()\n\n                    # Rest is the same as How to Automate III\n                    model_price = todays_data.loc[dog_name,track,race_number]['rating']\n                    ### If you have an issue such as:\n                        # Unknown error The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n                        # Then do model_price = todays_data.loc[dog_name,track,race_number]['rating'].item()\n\n                    # Log info before placing bets\n                    logging.info(f'dog_name: {dog_name}')\n                    logging.info(f'model_price: {model_price}')\n                    logging.info(f'market_id: {market_book.market_id}')\n                    logging.info(f'selection_id: {runner.selection_id}')\n\n                    # If best available to back price is &amp;gt; rated price then flat $5 back\n                    if runner.status == \"ACTIVE\" and runner.ex.available_to_back[0]['price'] &amp;gt; model_price:\n                        trade = Trade(\n                        market_id=market_book.market_id,\n                        selection_id=runner.selection_id,\n                        handicap=runner.handicap,\n                        strategy=self,\n                        )\n                        order = trade.create_order(\n                            side=\"BACK\", order_type=LimitOrder(price=runner.ex.available_to_back[0]['price'], size=5.00)\n                        )\n                        market.place_order(order)\n                    # If best available to lay price is &amp;lt; rated price then flat $5 lay\n                    if runner.status == \"ACTIVE\" and runner.ex.available_to_lay[0]['price'] &amp;lt; model_price:\n                        trade = Trade(\n                        market_id=market_book.market_id,\n                        selection_id=runner.selection_id,\n                        handicap=runner.handicap,\n                        strategy=self,\n                        )\n                        order = trade.create_order(\n                            side=\"LAY\", order_type=LimitOrder(price=runner.ex.available_to_lay[0]['price'], size=5.00)\n                        )\n                        market.place_order(order)\n\ngreyhounds_strategy = FlatBetting(\n    market_filter=streaming_market_filter(\n        event_type_ids=[\"4339\"], # Greyhounds markets\n        country_codes=[\"AU\"], # Australian markets\n        market_types=[\"WIN\"], # Win markets\n    ),\n    max_order_exposure= 50, # Max exposure per order = 50\n    max_trade_count=1, # Max 1 trade per selection\n    max_live_trade_count=1, # Max 1 unmatched trade per selection\n)\n\nframework.add_strategy(greyhounds_strategy)\n\n# logger = logging.getLogger(__name__)\n\n\"\"\"\nWorker can be used as followed:\n    framework.add_worker(\n        BackgroundWorker(\n            framework,\n            terminate,\n            func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n            interval=60,\n            start_delay=60,\n        )\n    )\nThis will run every 60s and will terminate \nthe framework if all markets starting 'today' \nhave been closed for at least 1200s\n\"\"\"\n\n\n# Function that stops automation running at the end of the day\ndef terminate(\n    context: dict, flumine, today_only: bool = True, seconds_closed: int = 600\n) -&amp;gt; None:\n    \"\"\"terminate framework if no markets\n    live today.\n    \"\"\"\n    markets = list(flumine.markets.markets.values())\n    markets_today = [\n        m\n        for m in markets\n        if m.market_start_datetime.date() == datetime.datetime.utcnow().date()\n        and (\n            m.elapsed_seconds_closed is None\n            or (m.elapsed_seconds_closed and m.elapsed_seconds_closed &amp;lt; seconds_closed)\n        )\n    ]\n    if today_only:\n        market_count = len(markets_today)\n    else:\n        market_count = len(markets)\n    if market_count == 0:\n        # logger.info(\"No more markets available, terminating framework\")\n        flumine.handler_queue.put(TerminationEvent(flumine))\n\n# Add the stopped to our framework\nframework.add_worker(\n    BackgroundWorker(\n        framework,\n        terminate,\n        func_kwargs={\"today_only\": True, \"seconds_closed\": 1200},\n        interval=60,\n        start_delay=60,\n    )\n)\n\nlogger = logging.getLogger(__name__)\n\nFIELDNAMES = [\n    \"bet_id\",\n    \"strategy_name\",\n    \"market_id\",\n    \"selection_id\",\n    \"trade_id\",\n    \"date_time_placed\",\n    \"price\",\n    \"price_matched\",\n    \"size\",\n    \"size_matched\",\n    \"profit\",\n    \"side\",\n    \"elapsed_seconds_executable\",\n    \"order_status\",\n    \"market_note\",\n    \"trade_notes\",\n    \"order_notes\",\n]\n\n\nclass LiveLoggingControl(LoggingControl):\n    NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n    def __init__(self, *args, **kwargs):\n        super(LiveLoggingControl, self).__init__(*args, **kwargs)\n        self._setup()\n\n    # Changed file path and checks if the file orders_hta_4.csv already exists, if it doens't then create it\n    def _setup(self):\n        if os.path.exists(\"orders_hta_4.csv\"):\n            logging.info(\"Results file exists\")\n        else:\n            with open(\"orders_hta_4.csv\", \"w\") as m:\n                csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                csv_writer.writeheader()\n\n    def _process_cleared_orders_meta(self, event):\n        orders = event.event\n        with open(\"orders_hta_4.csv\", \"a\") as m:\n            for order in orders:\n                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                    size = order.order_type.size\n                else:\n                    size = order.order_type.liability\n                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                    price = None\n                else:\n                    price = order.order_type.price\n                try:\n                    order_data = {\n                        \"bet_id\": order.bet_id,\n                        \"strategy_name\": order.trade.strategy,\n                        \"market_id\": order.market_id,\n                        \"selection_id\": order.selection_id,\n                        \"trade_id\": order.trade.id,\n                        \"date_time_placed\": order.responses.date_time_placed,\n                        \"price\": price,\n                        \"price_matched\": order.average_price_matched,\n                        \"size\": size,\n                        \"size_matched\": order.size_matched,\n                        \"profit\": 0 if not order.cleared_order else order.cleared_order.profit,\n                        \"side\": order.side,\n                        \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                        \"order_status\": order.status.value,\n                        \"market_note\": order.trade.market_notes,\n                        \"trade_notes\": order.trade.notes_str,\n                        \"order_notes\": order.notes_str,\n                    }\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                    csv_writer.writerow(order_data)\n                except Exception as e:\n                    logger.error(\n                        \"_process_cleared_orders_meta: %s\" % e,\n                        extra={\"order\": order, \"error\": e},\n                    )\n\n        logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n    def _process_cleared_markets(self, event):\n        cleared_markets = event.event\n        for cleared_market in cleared_markets.orders:\n            logger.info(\n                \"Cleared market\",\n                extra={\n                    \"market_id\": cleared_market.market_id,\n                    \"bet_count\": cleared_market.bet_count,\n                    \"profit\": cleared_market.profit,\n                    \"commission\": cleared_market.commission,\n                },\n            )\n\nframework.add_logging_control(\n    LiveLoggingControl()\n)\n\nframework.run()\n</code></pre> <p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"tutorials/How_to_Automate_4_archived/#how-to-automate-iv-automate-your-own-model","title":"How to Automate IV: Automate your own Model","text":""},{"location":"tutorials/How_to_Automate_4_archived/#saving-and-loading-in-our-model","title":"Saving and loading in our model","text":""},{"location":"tutorials/How_to_Automate_4_archived/#generating-predictions-for-today","title":"Generating predictions for today","text":""},{"location":"tutorials/How_to_Automate_4_archived/#cleaning-our-data-and-feature-creation","title":"Cleaning our data and feature creation","text":""},{"location":"tutorials/How_to_Automate_4_archived/#generate-predictions","title":"Generate predictions","text":""},{"location":"tutorials/How_to_Automate_4_archived/#getting-data-ready-for-our-simulator","title":"Getting data ready for our simulator","text":""},{"location":"tutorials/How_to_Automate_4_archived/#getting-data-ready-for-placing-live-bets","title":"Getting data ready for placing live bets","text":""},{"location":"tutorials/How_to_Automate_4_archived/#automating-our-predictions","title":"Automating our predictions","text":""},{"location":"tutorials/How_to_Automate_4_archived/#conclusion-and-next-steps","title":"Conclusion and next steps","text":""},{"location":"tutorials/How_to_Automate_4_archived/#complete-code","title":"Complete code","text":""},{"location":"tutorials/How_to_Automate_4_archived/#disclaimer","title":"Disclaimer","text":""},{"location":"tutorials/How_to_Automate_5/","title":"How to Automate 5","text":"<p>Before you start</p> <p>This tutorial follows on from our How to Automate series which stepped through how to create a trading bot for a few different strategies. Make sure you take a look through them first before you start here.</p> <p>This is the final part of the How to Automate series (for a while at least). In my previous posts we have created a few different strategies, but I haven't actually backtested or simulated any of them yet. How will do we even know if they have any edge? </p> <p>Today we test those strategies by running simulations and try to optimise performance.</p> <p>But how do we test strategies?</p> <p>One method is to follow the steps shown in this fantastic article: Backtesting wagering models with Betfair JSON stream data. But if you already have access to Betfair historic data or you have reccorded it yourself, you are not making full use of your data. This is because the above method will take all the amazing data thats has been collected and only extract a sliver of data from a few time points.</p> <p>Flumine is amazing because it can make full use of the entire dataset to simulate the market from when the market is first created to settlement. Instead of looking at the prices at 3 minutes before the race and assuming we get matched we can for example simulate placing a back bet hours before the race starts and replay exactly what happened in that market second by second to see if we would have gotten matched between the time we placed the bet and when the market settles. This is really cool because we might have a really awesome model that is close to being profitable but not quite and we want to optimise it.</p> <p>This will give us the most realistic back testing available and let us test if we are getting matched for the volume and price we want and if we have any edge at all.</p> <p>Something that is important to note is that although this is the most realistic backtest you can probably get, it is not 100% accurate. This is because we are simply replaying a market with our orders being added in, we cannot take into account how other market participants react. If we place a huge order e.g. $1000 or more we will likely trigger other peoples bots the market will likely move against us.</p> <p>But either way, we can still try some really cool things such as testing different time points to place bets without needing to re-extract data each time, change staking methodology or placing bets a few ticks away from the best available prices and hoping it gets matched.</p>"},{"location":"tutorials/How_to_Automate_5/#set-up","title":"Set up","text":"<p>Before we get started, although Jupyter Notebook/lab is a quants' favourite tool we need to use a different IDE such as VS Code for our simulation code (feel free to try it out, it didn't work for me and I read a note somewhere about it in the docs, but can't find it anymore). All code files are made available on github.</p> <p>I am going to use the March 2022 Greyhound Pro data and I've provided a sample of that data in the github repo which you can use to follow along, but if your an Australian and New Zealand customer make sure to shoot an email to data@betfair.com.au. </p> <p>Simulation mode in Flumine requires your data to be structured a certain way. So, if you have purchased data you will need it to be extracted formatted so that each market file is within a single file, instead or having files within files within files (default).</p> <p>You can do it manually, which will take an unimaginable amount of time, but I've written a simple script that will do it for you. But you just need to do few things before you run the script. </p> <ul> <li>Take your data that has the .tar extension, mine was 2022_03_MarGreyhoundsPro.tar and extract it using winrar/7zip etc this will create a file named 2022_03_MarGreyhoundsPro</li> <li>make sure 2022_03_MarGreyhoundsPro is stored in the same location as the data extractor script</li> <li>create a new empty folder that you want the extracted data to be outputted to, I created output_2022_03_MarGreyhoundsPro</li> <li>then run the script</li> </ul> Extracts and formats the content of .tar files<pre><code># Extracts all the bzip2 files (.bz2) \n# contained within the specified output_folder and any sub-folders\n# and writes them to a file with their market_id as their file name\n# This will take around 10 mins to run for one month of Pro Greyhound data\nimport glob\nimport bz2\nimport shutil\n\n# Folder containing bz2 files or any subfolders with bz2 files\ninput_folder = '2022_04_AprGreyhoundsPro'  # change to what you have named your folder e.g. 'sample_monthly_data'\n# Folder to write our extracted bz2 files to, this folder needs to already be created\noutput_folder = 'output_2022_04_AprGreyhoundsPro'  # change to what you have named your folder e.g. 'sample_monthly_data_output'\n\n# Returns a list of paths to bz2 files within the input folder and any sub folders\nfiles = glob.iglob(f'{input_folder}/**/**/**/**/**/*.bz2', recursive = False)\n\n# Extracts each bz2 file and write it to the output folder\nfor path in files:\n    market_id = path[-15:-4]\n    print(path, market_id)\n    with bz2.BZ2File(path) as fr, open(f'{output_folder}/{market_id}',\"wb\") as fw:\n        shutil.copyfileobj(fr,fw)\n</code></pre> <p>Now we are all set up lets run our sim!</p>"},{"location":"tutorials/How_to_Automate_5/#how-the-sims-work","title":"How the sims work","text":"<p>Flumine is pretty cool, by default it hooks up to the Betfair API and it will run our strategy on live markets. When we set it to simulation mode we can hook it up to the historic data instead. The historic data is basically photos of the exhange up to every 50ms, in simulation mode Flumine essentially quickly scans through each picture sequentially essentially replaying the market. Just like how you would <code>add_strategy()</code> to Flumine to add a strategy that runs live, you can do the same thing in simulation mode and it will place into the simulated markets it creates.</p> <p>The coolest thing is, it is super easy to change it to simulation mode:</p> Setting Flumine to simulation Mode<pre><code># Set Flumine to simulation mode\nclient = clients.SimulatedClient()\nframework = FlumineSimulation(client=client)\n</code></pre> <p>and instead of pointing it to markets you want to run your strategy on, you point it to your historic data files instead (as it is quite slow I would also suggest only replaying a subsection of the historic files, you can change that with the listner_kwargs), then just run it as you would any other strategy in Flumine:</p> Pointing the simulation to the historical files<pre><code># Searches for all betfair data files within the folder sample_monthly_data_output\ndata_folder = 'sample_monthly_data_output'\ndata_files = os.listdir(data_folder,)\ndata_files = [f'{data_folder}/{path}' for path in data_files]\n\nstrategy = BackFavStrategy(\n    # market_filter selects what portion of the historic data we simulate our strategy on\n    # markets selects the list of betfair historic data files\n    # market_types specifies the type of markets\n    # listener_kwargs specifies the time period we simulate for each market\n    market_filter={\n        \"markets\": data_files,  \n        'market_types':['WIN'],\n        \"listener_kwargs\": {\"inplay\": False, \"seconds_to_start\": 80},  \n        },\n    max_order_exposure=1000,\n    max_selection_exposure=1000,\n    max_live_trade_count=1,\n    max_trade_count=1,\n)\n# Run our strategy on the simulated market\nframework.add_strategy(strategy)\nframework.run()\n</code></pre>"},{"location":"tutorials/How_to_Automate_5/#running-sims-how-to-automate-ii","title":"Running Sims: How to Automate II","text":"<p>First off the bat is simulating the strategy we created in How to Automate II. </p> <p>Its actually pretty easy to simulate using Flumine especially if your strategy doesn't require outside data. In fact almost all our code we previously made can just be copied accross. We just need to set Flumine to simulation mode and point it to our data files instead of at the Betfair API, which is only a few lines of code and once you read it, its pretty self explanatory.</p> <p>One thing we must remember to do is to add the bet logging code we made in How to Automate II so we can analyse how our strategy went afterwards. I've copied both the changes you need to make and also the complete code, give that bad boy a spin, and it will create a csv file as a log of all bets placed. </p> <p>A months worth of data will take ages to run (like 8 hours on my slow laptop), but the sample data should only take around 10 mins (we will go into speeding up the sims later).</p> Changes you need to makeComplete Code, changes are at the bottom <pre><code># Searches for all betfair data files within the folder sample_monthly_data_output\ndata_folder = 'sample_monthly_data_output'\ndata_files = os.listdir(data_folder,)\ndata_files = [f'{data_folder}/{path}' for path in data_files]\n\n# Set Flumine to simulation mode\nclient = clients.SimulatedClient()\nframework = FlumineSimulation(client=client)\n\n# Set parameters for our strategy\nstrategy = BackFavStrategy(\n    # market_filter selects what portion of the historic data we simulate our strategy on\n    # markets selects the list of betfair historic data files\n    # market_types specifies the type of markets\n    # listener_kwargs specifies the time period we simulate for each market\n    market_filter={\n        \"markets\": data_files,  \n        'market_types':['WIN'],\n        \"listener_kwargs\": {\"inplay\": False, \"seconds_to_start\": 80},  \n        },\n    max_order_exposure=1000,\n    max_selection_exposure=1000,\n    max_live_trade_count=1,\n    max_trade_count=1,\n)\n# Run our strategy on the simulated market\nframework.add_strategy(strategy)\nframework.add_logging_control(\n    BacktestLoggingControl()\n)\nframework.run()\n</code></pre> <pre><code># Import libraries\nimport glob\nimport os\nimport time\nimport logging\nimport csv\nimport pandas as pd\nimport json\nimport math\nfrom pythonjsonlogger import jsonlogger\nfrom flumine import FlumineSimulation, BaseStrategy, utils, clients\nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder, OrderStatus\nfrom flumine.order.ordertype import OrderTypes\nfrom flumine.markets.market import Market\nfrom flumine.controls.loggingcontrols import LoggingControl\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\nfrom pythonjsonlogger import jsonlogger\nfrom concurrent import futuresimport glob\n\n# Logging\nlogger = logging.getLogger()\ncustom_format = \"%(asctime) %(levelname) %(message)\"\nlog_handler = logging.StreamHandler()\nformatter = jsonlogger.JsonFormatter(custom_format)\nformatter.converter = time.gmtime\nlog_handler.setFormatter(formatter)\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.INFO)  # Set to logging.CRITICAL to speed up simulation\n\nclass BackFavStrategy(BaseStrategy):\n\n    # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy\n    def start(self) -&gt; None:\n        print(\"starting strategy 'BackFavStrategy'\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&gt; bool:\n        # process_market_book only executed if this returns True\n        if market_book.status != \"CLOSED\":\n            return True\n\n    def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:\n\n        # Collect data on last price traded and the number of bets we have placed\n        snapshot_last_price_traded = []\n        snapshot_runner_context = []\n        for runner in market_book.runners:\n                snapshot_last_price_traded.append([runner.selection_id,runner.last_price_traded])\n                # Get runner context for each runner\n                runner_context = self.get_runner_context(\n                    market.market_id, runner.selection_id, runner.handicap\n                )\n                snapshot_runner_context.append([runner_context.selection_id, runner_context.executable_orders, runner_context.live_trade_count, runner_context.trade_count])\n\n        # Convert last price traded data to dataframe\n        snapshot_last_price_traded = pd.DataFrame(snapshot_last_price_traded, columns=['selection_id','last_traded_price'])\n        # Find the selection_id of the favourite\n        snapshot_last_price_traded = snapshot_last_price_traded.sort_values(by = ['last_traded_price'])\n        fav_selection_id = snapshot_last_price_traded['selection_id'].iloc[0]\n        logging.info(snapshot_last_price_traded) # logging\n\n        # Convert data on number of bets we have placed to a dataframe\n        snapshot_runner_context = pd.DataFrame(snapshot_runner_context, columns=['selection_id','executable_orders','live_trade_count','trade_count'])\n        logging.info(snapshot_runner_context) # logging\n\n        for runner in market_book.runners:\n            if runner.status == \"ACTIVE\" and market.seconds_to_start &lt; 60 and market_book.inplay == False and runner.selection_id == fav_selection_id and snapshot_runner_context.iloc[:,1:].sum().sum() == 0:\n                trade = Trade(\n                    market_id=market_book.market_id,\n                    selection_id=runner.selection_id,\n                    handicap=runner.handicap,\n                    strategy=self,\n                )\n                order = trade.create_order(\n                    side=\"BACK\", order_type=LimitOrder(price=runner.last_price_traded, size=5)\n                )\n                market.place_order(order)\n\n# Fields we want to log in our simulations\nFIELDNAMES = [\n    \"bet_id\",\n    \"strategy_name\",\n    \"market_id\",\n    \"selection_id\",\n    \"trade_id\",\n    \"date_time_placed\",\n    \"price\",\n    \"price_matched\",\n    \"size\",\n    \"size_matched\",\n    \"profit\",\n    \"side\",\n    \"elapsed_seconds_executable\",\n    \"order_status\",\n    \"market_note\",\n    \"trade_notes\",\n    \"order_notes\",\n]\n\n# Log results from simulation into csv file named sim_hta_2.csv\n# If the csv file doesn't exist then it is created, otherwise we append results to the csv file\nclass BacktestLoggingControl(LoggingControl):\n    NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n    def __init__(self, *args, **kwargs):\n        super(BacktestLoggingControl, self).__init__(*args, **kwargs)\n        self._setup()\n\n    def _setup(self):\n        if os.path.exists(\"sim_hta_2.csv\"):\n            logging.info(\"Results file exists\")\n        else:\n            with open(\"sim_hta_2.csv\", \"w\") as m:\n                csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                csv_writer.writeheader()\n\n    def _process_cleared_orders_meta(self, event):\n        orders = event.event\n        with open(\"sim_hta_2.csv\", \"a\") as m:\n            for order in orders:\n                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                    size = order.order_type.size\n                else:\n                    size = order.order_type.liability\n                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                    price = None\n                else:\n                    price = order.order_type.price\n                try:\n                    order_data = {\n                        \"bet_id\": order.bet_id,\n                        \"strategy_name\": order.trade.strategy,\n                        \"market_id\": order.market_id,\n                        \"selection_id\": order.selection_id,\n                        \"trade_id\": order.trade.id,\n                        \"date_time_placed\": order.responses.date_time_placed,\n                        \"price\": price,\n                        \"price_matched\": order.average_price_matched,\n                        \"size\": size,\n                        \"size_matched\": order.size_matched,\n                        \"profit\": order.simulated.profit,\n                        \"side\": order.side,\n                        \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                        \"order_status\": order.status.value,\n                        \"market_note\": order.trade.market_notes,\n                        \"trade_notes\": order.trade.notes_str,\n                        \"order_notes\": order.notes_str,\n                    }\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                    csv_writer.writerow(order_data)\n                except Exception as e:\n                    logger.error(\n                        \"_process_cleared_orders_meta: %s\" % e,\n                        extra={\"order\": order, \"error\": e},\n                    )\n\n        logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n    def _process_cleared_markets(self, event):\n        cleared_markets = event.event\n        for cleared_market in cleared_markets.orders:\n            logger.info(\n                \"Cleared market\",\n                extra={\n                    \"market_id\": cleared_market.market_id,\n                    \"bet_count\": cleared_market.bet_count,\n                    \"profit\": cleared_market.profit,\n                    \"commission\": cleared_market.commission,\n                },\n            )\n\n# Searches for all betfair data files within the folder sample_monthly_data_output\ndata_folder = 'sample_monthly_data_output'\ndata_files = os.listdir(data_folder,)\ndata_files = [f'{data_folder}/{path}' for path in data_files]\n\n# Set Flumine to simulation mode\nclient = clients.SimulatedClient()\nframework = FlumineSimulation(client=client)\n\n# Set parameters for our strategy\nstrategy = BackFavStrategy(\n    # market_filter selects what portion of the historic data we simulate our strategy on\n    # markets selects the list of betfair historic data files\n    # market_types specifies the type of markets\n    # listener_kwargs specifies the time period we simulate for each market\n    market_filter={\n        \"markets\": data_files,  \n        'market_types':['WIN'],\n        \"listener_kwargs\": {\"inplay\": False, \"seconds_to_start\": 80},  \n        },\n    max_order_exposure=1000,\n    max_selection_exposure=1000,\n    max_live_trade_count=1,\n    max_trade_count=1,\n)\n# Run our strategy on the simulated market\nframework.add_strategy(strategy)\nframework.add_logging_control(\n    BacktestLoggingControl()\n)\nframework.run()\n</code></pre>"},{"location":"tutorials/How_to_Automate_5/#running-sims-how-to-automate-iii","title":"Running Sims: How to Automate III","text":"<p>Okay, so we got the first one running pretty easily, a little too easily (a few lines of code and no major issues or hacky work arounds), lets test out a strategy that requires external data. In How to Automate III we automated the betfair data scientists model, lets now simulate performance. I'm going to do just the greyhound model, 'Iggy', at the moment, but the code is basically the same for the thoroughbred model, 'Kash'.</p> <p>Because we didn't save any of our ratings in How to Automate III we will need to redownload it now. And instead or redownloading just one days worth of data lets test out a whole month at a time. Lets reuse the function we created in How to Automate IV that we used a hacky work around that downloads the ratings for a range of dates:</p> Download a whole month of Iggy ratings and convert it to a DataFrame<pre><code>def download_iggy_ratings(date):\n    \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame.\n\n    Args:\n        date (datetime): the date we want to download the ratings for\n    \"\"\"\n    iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date='\n    iggy_url_2 = date.strftime(\"%Y-%m-%d\")\n    iggy_url_3 = '&amp;presenter=RatingsPresenter&amp;csv=true'\n    iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3\n\n    # Download todays greyhounds ratings\n    iggy_df = pd.read_csv(iggy_url)\n\n    # Data clearning\n    iggy_df = iggy_df.rename(columns={\"meetings.races.bfExchangeMarketId\":\"market_id\",\"meetings.races.runners.bfExchangeSelectionId\":\"selection_id\",\"meetings.races.runners.ratedPrice\":\"rating\"})\n    iggy_df = iggy_df[['market_id','selection_id','rating']]\n    iggy_df['market_id'] = iggy_df['market_id'].astype(str)\n\n    # Set market_id and selection_id as index for easy referencing\n    iggy_df = iggy_df.set_index(['market_id','selection_id'])\n    return(iggy_df)\n\n# Download historical ratings over a time period and convert into a big DataFrame.\nback_test_period = pd.date_range(start='2022/02/27', end='2022/03/05')\nframes = [download_iggy_ratings(day) for day in back_test_period]\niggy_df = pd.concat(frames)\nprint(iggy_df)\n</code></pre> <p>Now that we have downloaded a whole month of Iggy ratings to simulate it is crazy easy to simulate. We do the same thing we did when simulating How to Automate II: copy and paste the original code, and set Flumine into simulation mode pointing it to the historic data instead of the Betfair API.</p> Changes made to the original How to Automate III codeComplete Code <pre><code>def download_iggy_ratings(date):\n    \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame.\n\n    Args:\n        date (datetime): the date we want to download the ratings for\n    \"\"\"\n    iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date='\n    iggy_url_2 = date.strftime(\"%Y-%m-%d\")\n    iggy_url_3 = '&amp;presenter=RatingsPresenter&amp;csv=true'\n    iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3\n\n    # Download todays greyhounds ratings\n    iggy_df = pd.read_csv(iggy_url)\n\n    # Data clearning\n    iggy_df = iggy_df.rename(columns={\"meetings.races.bfExchangeMarketId\":\"market_id\",\"meetings.races.runners.bfExchangeSelectionId\":\"selection_id\",\"meetings.races.runners.ratedPrice\":\"rating\"})\n    iggy_df = iggy_df[['market_id','selection_id','rating']]\n    iggy_df['market_id'] = iggy_df['market_id'].astype(str)\n\n    # Set market_id and selection_id as index for easy referencing\n    iggy_df = iggy_df.set_index(['market_id','selection_id'])\n    return(iggy_df)\n\n# Download historical ratings over a time period and convert into a big DataFrame.\nback_test_period = pd.date_range(start='2022/02/27', end='2022/03/05')\nframes = [download_iggy_ratings(day) for day in back_test_period]\niggy_df = pd.concat(frames)\nprint(iggy_df)\n\n# Searches for all betfair data files within the folder sample_monthly_data_output\ndata_folder = 'sample_monthly_data_output'\ndata_files = os.listdir(data_folder,)\ndata_files = [f'{data_folder}/{path}' for path in data_files]\n\n# Set Flumine to simulation mode\nclient = clients.SimulatedClient()\nframework = FlumineSimulation(client=client)\n\n# Set parameters for our strategy\nstrategy = FlatIggyModel(\n    market_filter={\n        \"markets\": data_files,  \n        'market_types':['WIN'],\n        \"listener_kwargs\": {\"inplay\": False, \"seconds_to_start\": 80},  \n        },\n    max_order_exposure=1000,\n    max_selection_exposure=1000,\n    max_live_trade_count=1,\n    max_trade_count=1,\n)\n# Run our strategy on the simulated market\nframework.add_strategy(strategy)\nframework.add_logging_control(\n    BacktestLoggingControl()\n)\nframework.run()\n</code></pre> <pre><code># Import libraries\nimport glob\nimport os\nimport time\nimport logging\nimport csv\nimport pandas as pd\nfrom pythonjsonlogger import jsonlogger\nfrom flumine import FlumineSimulation, BaseStrategy, utils, clients\nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder, OrderStatus\nfrom flumine.order.ordertype import OrderTypes\nfrom flumine.markets.market import Market\nfrom flumine.controls.loggingcontrols import LoggingControl\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\n\n# Logging\nlogger = logging.getLogger()\ncustom_format = \"%(asctime) %(levelname) %(message)\"\nlog_handler = logging.StreamHandler()\nformatter = jsonlogger.JsonFormatter(custom_format)\nformatter.converter = time.gmtime\nlog_handler.setFormatter(formatter)\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.INFO)  # Set to logging.CRITICAL to speed up simulation\n\ndef download_iggy_ratings(date):\n    \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame.\n\n    Args:\n        date (datetime): the date we want to download the ratings for\n    \"\"\"\n    iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date='\n    iggy_url_2 = date.strftime(\"%Y-%m-%d\")\n    iggy_url_3 = '&amp;presenter=RatingsPresenter&amp;csv=true'\n    iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3\n\n    # Download todays greyhounds ratings\n    iggy_df = pd.read_csv(iggy_url)\n\n    # Data clearning\n    iggy_df = iggy_df.rename(columns={\"meetings.races.bfExchangeMarketId\":\"market_id\",\"meetings.races.runners.bfExchangeSelectionId\":\"selection_id\",\"meetings.races.runners.ratedPrice\":\"rating\"})\n    iggy_df = iggy_df[['market_id','selection_id','rating']]\n    iggy_df['market_id'] = iggy_df['market_id'].astype(str)\n\n    # Set market_id and selection_id as index for easy referencing\n    iggy_df = iggy_df.set_index(['market_id','selection_id'])\n    return(iggy_df)\n\n# Download historical ratings over a time period and convert into a big DataFrame.\nback_test_period = pd.date_range(start='2022/02/27', end='2022/03/05')\nframes = [download_iggy_ratings(day) for day in back_test_period]\niggy_df = pd.concat(frames)\nprint(iggy_df)\n\n# Create strategy, this is the exact same strategy shown in How to Automate III\nclass FlatIggyModel(BaseStrategy):\n    def start(self) -&gt; None:\n        print(\"starting strategy 'FlatIggyModel'\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&gt; bool:\n        if market_book.status != \"CLOSED\":\n            return True\n\n    def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:\n        if market.seconds_to_start &lt; 60 and market_book.inplay == False:\n            for runner in market_book.runners:\n                if runner.status == \"ACTIVE\" and runner.ex.available_to_back[0]['price'] &gt; iggy_df.loc[market_book.market_id].loc[runner.selection_id].item():\n                    trade = Trade(\n                    market_id=market_book.market_id,\n                    selection_id=runner.selection_id,\n                    handicap=runner.handicap,\n                    strategy=self,\n                    )\n                    order = trade.create_order(\n                        side=\"BACK\", order_type=LimitOrder(price=runner.ex.available_to_back[0]['price'], size=5.00)\n                    )\n                    market.place_order(order)\n                if runner.status == \"ACTIVE\" and runner.ex.available_to_lay[0]['price'] &lt; iggy_df.loc[market_book.market_id].loc[runner.selection_id].item():\n                    trade = Trade(\n                    market_id=market_book.market_id,\n                    selection_id=runner.selection_id,\n                    handicap=runner.handicap,\n                    strategy=self,\n                    )\n                    order = trade.create_order(\n                        side=\"LAY\", order_type=LimitOrder(price=runner.ex.available_to_lay[0]['price'], size=5.00)\n                    )\n                    market.place_order(order)\n\n# Fields we want to log in our simulations\nFIELDNAMES = [\n    \"bet_id\",\n    \"strategy_name\",\n    \"market_id\",\n    \"selection_id\",\n    \"trade_id\",\n    \"date_time_placed\",\n    \"price\",\n    \"price_matched\",\n    \"size\",\n    \"size_matched\",\n    \"profit\",\n    \"side\",\n    \"elapsed_seconds_executable\",\n    \"order_status\",\n    \"market_note\",\n    \"trade_notes\",\n    \"order_notes\",\n]\n\n# Log results from simulation into csv file named sim_hta_3.csv\n# If the csv file doesn't exist then it is created, otherwise we append results to the csv file\nclass BacktestLoggingControl(LoggingControl):\n    NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n    def __init__(self, *args, **kwargs):\n        super(BacktestLoggingControl, self).__init__(*args, **kwargs)\n        self._setup()\n\n    def _setup(self):\n        if os.path.exists(\"sim_hta_3.csv\"):\n            logging.info(\"Results file exists\")\n        else:\n            with open(\"sim_hta_3.csv\", \"w\") as m:\n                csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                csv_writer.writeheader()\n\n    def _process_cleared_orders_meta(self, event):\n        orders = event.event\n        with open(\"sim_hta_3.csv\", \"a\") as m:\n            for order in orders:\n                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                    size = order.order_type.size\n                else:\n                    size = order.order_type.liability\n                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                    price = None\n                else:\n                    price = order.order_type.price\n                try:\n                    order_data = {\n                        \"bet_id\": order.bet_id,\n                        \"strategy_name\": order.trade.strategy,\n                        \"market_id\": order.market_id,\n                        \"selection_id\": order.selection_id,\n                        \"trade_id\": order.trade.id,\n                        \"date_time_placed\": order.responses.date_time_placed,\n                        \"price\": price,\n                        \"price_matched\": order.average_price_matched,\n                        \"size\": size,\n                        \"size_matched\": order.size_matched,\n                        \"profit\": order.simulated.profit,\n                        \"side\": order.side,\n                        \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                        \"order_status\": order.status.value,\n                        \"market_note\": order.trade.market_notes,\n                        \"trade_notes\": order.trade.notes_str,\n                        \"order_notes\": order.notes_str,\n                    }\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                    csv_writer.writerow(order_data)\n                except Exception as e:\n                    logger.error(\n                        \"_process_cleared_orders_meta: %s\" % e,\n                        extra={\"order\": order, \"error\": e},\n                    )\n\n        logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n    def _process_cleared_markets(self, event):\n        cleared_markets = event.event\n        for cleared_market in cleared_markets.orders:\n            logger.info(\n                \"Cleared market\",\n                extra={\n                    \"market_id\": cleared_market.market_id,\n                    \"bet_count\": cleared_market.bet_count,\n                    \"profit\": cleared_market.profit,\n                    \"commission\": cleared_market.commission,\n                },\n            )\n\n# Searches for all betfair data files within the folder sample_monthly_data_output\ndata_folder = 'sample_monthly_data_output'\ndata_files = os.listdir(data_folder,)\ndata_files = [f'{data_folder}/{path}' for path in data_files]\n\n# Set Flumine to simulation mode\nclient = clients.SimulatedClient()\nframework = FlumineSimulation(client=client)\n\n# Set parameters for our strategy\nstrategy = FlatIggyModel(\n    market_filter={\n        \"markets\": data_files,  \n        'market_types':['WIN'],\n        \"listener_kwargs\": {\"inplay\": False, \"seconds_to_start\": 80},  \n        },\n    max_order_exposure=1000,\n    max_selection_exposure=1000,\n    max_live_trade_count=1,\n    max_trade_count=1,\n)\n# Run our strategy on the simulated market\nframework.add_strategy(strategy)\nframework.add_logging_control(\n    BacktestLoggingControl()\n)\nframework.run()\n</code></pre>"},{"location":"tutorials/How_to_Automate_5/#simulating-how-to-automate-iv","title":"Simulating How to Automate IV","text":"<p>Because we coded How to Automate IV with simulating in mind (I didn't originally and had to recode it a few times), its easy for us to simulate the performance of our model. As we saved our model ratings to a csv, reading it in now actually makes the code simpler then what we created for placing live bets. This is because the issues we had working around reserve dogs and matching with the Betfair API has been taken care of (there are no reserve dogs to work around in the historic data). In fact thanks to my hacky work around in How to Automate IV the data is also in the same format as How to Automate III so we can basically use almost the exact same code we used to simulate How to Automate III. </p> <p>The only real differences from simulating How to Automate III and How to Automate IV is that we need to have the csv file of predictions already, read that in, and change any naming conventions that might be different.</p> Read in model predictions and format dataframe for easy referenceComplete code, almost the same as simulating How to Automate III <pre><code># Read in predictions from hta_4\ntodays_data = pd.read_csv('backtest.csv', dtype = ({\"market_id\":str}))\ntodays_data = todays_data.set_index(['market_id','selection_id'])\n</code></pre> <pre><code># Import libraries\nimport glob\nimport os\nimport time\nimport logging\nimport csv\nimport pandas as pd\nimport json\nfrom pythonjsonlogger import jsonlogger\nfrom flumine import FlumineSimulation, BaseStrategy, utils, clients\nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder, OrderStatus\nfrom flumine.order.ordertype import OrderTypes\nfrom flumine.markets.market import Market\nfrom flumine.controls.loggingcontrols import LoggingControl\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\nfrom dateutil import tz\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nfrom betfairlightweight.resources import MarketCatalogue\nfrom flumine.markets.middleware import Middleware\n\n# Logging\nlogger = logging.getLogger()\ncustom_format = \"%(asctime) %(levelname) %(message)\"\nlog_handler = logging.StreamHandler()\nformatter = jsonlogger.JsonFormatter(custom_format)\nformatter.converter = time.gmtime\nlog_handler.setFormatter(formatter)\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.INFO)  # Set to logging.CRITICAL to speed up simulation\n\n# Read in predictions from hta_4\ntodays_data = pd.read_csv('backtest.csv', dtype = ({\"market_id\":str}))\ntodays_data = todays_data.set_index(['market_id','selection_id'])\n\n### New implementation\nclass FlatBetting(BaseStrategy):\n    def start(self) -&gt; None:\n        print(\"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&gt; bool:\n        if market_book.status != \"CLOSED\":\n            return True\n\n    def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:\n\n        # At the 60 second mark:\n        if market.seconds_to_start &lt; 60 and market_book.inplay == False:\n\n            # Can't simulate polling API\n            # Only use streaming API:\n            for runner in market_book.runners:\n                model_price = todays_data.loc[market.market_id].loc[runner.selection_id]['rating']\n                # If best available to back price is &gt; rated price then flat $5 back\n                if runner.status == \"ACTIVE\" and runner.ex.available_to_back[0]['price'] &gt; model_price:\n                    trade = Trade(\n                    market_id=market_book.market_id,\n                    selection_id=runner.selection_id,\n                    handicap=runner.handicap,\n                    strategy=self,\n                    )\n                    order = trade.create_order(\n                        side=\"BACK\", order_type=LimitOrder(price=runner.ex.available_to_back[0]['price'], size=5.00)\n                    )\n                    market.place_order(order)\n                # If best available to lay price is &lt; rated price then flat $5 lay\n                if runner.status == \"ACTIVE\" and runner.ex.available_to_lay[0]['price'] &lt; model_price:\n                    trade = Trade(\n                    market_id=market_book.market_id,\n                    selection_id=runner.selection_id,\n                    handicap=runner.handicap,\n                    strategy=self,\n                    )\n                    order = trade.create_order(\n                        side=\"LAY\", order_type=LimitOrder(price=runner.ex.available_to_lay[0]['price'], size=5.00)\n                    )\n                    market.place_order(order)\n\n# Fields we want to log in our simulations\nFIELDNAMES = [\n    \"bet_id\",\n    \"strategy_name\",\n    \"market_id\",\n    \"selection_id\",\n    \"trade_id\",\n    \"date_time_placed\",\n    \"price\",\n    \"price_matched\",\n    \"size\",\n    \"size_matched\",\n    \"profit\",\n    \"side\",\n    \"elapsed_seconds_executable\",\n    \"order_status\",\n    \"market_note\",\n    \"trade_notes\",\n    \"order_notes\",\n]\n\n# Log results from simulation into csv file named sim_hta_4.csv\n# If the csv file doesn't exist then it is created, otherwise we append results to the csv file\nclass BacktestLoggingControl(LoggingControl):\n    NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n    def __init__(self, *args, **kwargs):\n        super(BacktestLoggingControl, self).__init__(*args, **kwargs)\n        self._setup()\n\n    def _setup(self):\n        if os.path.exists(\"sim_hta_4.csv\"):\n            logging.info(\"Results file exists\")\n        else:\n            with open(\"sim_hta_4.csv\", \"w\") as m:\n                csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                csv_writer.writeheader()\n\n    def _process_cleared_orders_meta(self, event):\n        orders = event.event\n        with open(\"sim_hta_4.csv\", \"a\") as m:\n            for order in orders:\n                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                    size = order.order_type.size\n                else:\n                    size = order.order_type.liability\n                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                    price = None\n                else:\n                    price = order.order_type.price\n                try:\n                    order_data = {\n                        \"bet_id\": order.bet_id,\n                        \"strategy_name\": order.trade.strategy,\n                        \"market_id\": order.market_id,\n                        \"selection_id\": order.selection_id,\n                        \"trade_id\": order.trade.id,\n                        \"date_time_placed\": order.responses.date_time_placed,\n                        \"price\": price,\n                        \"price_matched\": order.average_price_matched,\n                        \"size\": size,\n                        \"size_matched\": order.size_matched,\n                        \"profit\": order.simulated.profit,\n                        \"side\": order.side,\n                        \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                        \"order_status\": order.status.value,\n                        \"market_note\": order.trade.market_notes,\n                        \"trade_notes\": order.trade.notes_str,\n                        \"order_notes\": order.notes_str,\n                    }\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                    csv_writer.writerow(order_data)\n                except Exception as e:\n                    logger.error(\n                        \"_process_cleared_orders_meta: %s\" % e,\n                        extra={\"order\": order, \"error\": e},\n                    )\n\n        logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n    def _process_cleared_markets(self, event):\n        cleared_markets = event.event\n        for cleared_market in cleared_markets.orders:\n            logger.info(\n                \"Cleared market\",\n                extra={\n                    \"market_id\": cleared_market.market_id,\n                    \"bet_count\": cleared_market.bet_count,\n                    \"profit\": cleared_market.profit,\n                    \"commission\": cleared_market.commission,\n                },\n            )\n\n# Searches for all betfair data files within the folder sample_monthly_data_output\ndata_folder = 'sample_monthly_data_output'\ndata_files = os.listdir(data_folder,)\ndata_files = [f'{data_folder}/{path}' for path in data_files]\n\n# Set Flumine to simulation mode\nclient = clients.SimulatedClient()\nframework = FlumineSimulation(client=client)\n\n# Set parameters for our strategy\nstrategy = FlatBetting(\n    market_filter={\n        \"markets\": data_files,  \n        'market_types':['WIN'],\n        \"listener_kwargs\": {\"inplay\": False, \"seconds_to_start\": 80},  \n        },\n    max_order_exposure=1000,\n    max_selection_exposure=1000,\n    max_live_trade_count=1,\n    max_trade_count=1,\n)\n# Run our strategy on the simulated market\nframework.add_strategy(strategy)\nframework.add_logging_control(\n    BacktestLoggingControl()\n)\nframework.run()\n</code></pre>"},{"location":"tutorials/How_to_Automate_5/#gotta-go-fast","title":"Gotta go fast","text":"<p>Now that we have everything working, if you have tried any of the simulations you may notice its pretty slow. I definitely have, especially for larger files such as on 1 months worth of data (probably took me around 8 hours of just running the code in the background). The good thing is we can speed it up, the bad thing is, its via multiprocessing which I have never touched before. But turns out its not too bad. </p> <p>You really only need to wrap your Flumine client into a function:</p> <pre><code>def run_process(markets):\n    \"\"\"Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data.\n\n    Args:\n        markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv\n    \"\"\"    \n    # Set Flumine to simulation mode\n    client = clients.SimulatedClient()\n    framework = FlumineSimulation(client=client)\n\n    # Set parameters for our strategy\n    strategy = FlatBetting(\n        market_filter={\n            \"markets\": markets,  \n            'market_types':['WIN'],\n            \"listener_kwargs\": {\"inplay\": False, \"seconds_to_start\": 80},  \n            },\n        max_order_exposure=1000,\n        max_selection_exposure=1000,\n        max_live_trade_count=1,\n        max_trade_count=1,\n    )\n    # Run our strategy on the simulated market\n    framework.add_strategy(strategy)\n    framework.add_logging_control(\n        BacktestLoggingControl()\n    )\n    framework.run()\n\n# Searches for all betfair data files within the folder sample_monthly_data_output\ndata_folder = 'sample_monthly_data_output'\ndata_files = os.listdir(data_folder,)\ndata_files = [f'{data_folder}/{path}' for path in data_files]\n</code></pre> <p>and then split the files to run on muliple processors. You can copy the code, which is what I did, and it works without a hitch. </p> <pre><code># Multi processing\nif __name__ == \"__main__\":\n    all_markets = data_files  # All the markets we want to simulate\n    processes = os.cpu_count()  # Returns the number of CPUs in the system.\n    markets_per_process = 8   # 8 is optimal as it prevents data leakage.\n\n    _process_jobs = []\n    with futures.ProcessPoolExecutor(max_workers=processes) as p:\n        # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have.\n        chunk = min(\n            markets_per_process, math.ceil(len(all_markets) / processes)\n        )\n        # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs\n        for m in (utils.chunks(all_markets, chunk)):\n            _process_jobs.append(\n                p.submit(\n                    run_process,\n                    markets=m,\n                )\n            )\n        for job in futures.as_completed(_process_jobs):\n            job.result()  # wait for result\n</code></pre> <p>Essentially it takes all the historical markets you have e.g. 1000, and splits it into 8 chunks. Then we run our strategy on all 8 chunks simultaniously. And this gives serious speed improvements. The complete code for all three simulations using multi processing are available at the end of this post. </p> <p>Future versions of Flumine are using the new BetfairData library to speed up simulations which once fully implemented should also give some serious speed benefits. </p>"},{"location":"tutorials/How_to_Automate_5/#analysing-and-optimising-our-results","title":"Analysing and optimising our results","text":"<p>Now that we have all our speedy simulation code lets look at our results and see if we found anything good.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\n# Read data\nresults = pd.read_csv('sim_hta_4.csv', parse_dates = ['date_time_placed'], dtype = {'market_id':str})\n# calculate and display cumulative pnl\nresults = results.sort_values(by = ['date_time_placed'])\nresults['cum_profit'] = results['profit'].cumsum()\npx.line(results, 'date_time_placed', 'cum_profit').show()\n</code></pre> <p></p> <p>Before commissions the model is profitable, which is awesome as I didn't think that would be the case. Bruno has mentioned to me that the model in the tutorial was quite \"basic\" and not profitable, but it seems we got super lucky with a few long shots getting up in March. Lets incorporate commissions into our results and see if it remains profitable:</p> <pre><code>gross_profit = pd.DataFrame(results.groupby(['market_id'])['profit'].sum())\n# 7% commission rate on greyhounds, commissions calculated on profit at a market level\ncalc_comms = lambda gross_profit: np.where(gross_profit&gt;0, gross_profit*(1-0.07), gross_profit)\ngross_profit['net_pnl'] = calc_comms(gross_profit['profit'])\ngross_profit['cum_npl'] = gross_profit['net_pnl'].cumsum()\npx.line(gross_profit, gross_profit.index, 'cum_npl').show()\n</code></pre> <p></p> <p>We are close, infact we were up a bit at the start but it seems after taking into account commissions we are no longer profitable and end the month down around $800. Lets try two different things to see if we can optimse our strategy: a different staking methodology and also a different time we start placing our bets.</p> <p>My theory is that because we are crossing the spread and taking whatever prices are available we are probably losing a bit of our edge there. If we bet when markets are more liquid then we will may lose less. But as markets become more liquid they also tend to become more efficient so lets it could work against us. Nonetheless lets test it out:</p> Placing bets at 30 seconds instead of 60 secondsResults Before CommissionsResults After Commissions <pre><code>def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:\n    # At the 60 second mark:\n    if market.seconds_to_start &lt; 30 and market_book.inplay == False:\n        # Can't simulate polling API\n        # Only use streaming API:\n        for runner in market_book.runners:\n</code></pre> <p></p> <p></p> <p>So the results seem pretty similar to before. After commisions we are down around $700 so we seem to be doing slighlty better.</p> <p>Lets try a different staking method instead, this time I have opted for a proportional staking strategy going for a fixed $10 profit on back bets and a fixed $10 liability on lay bets. There is an excellent post analysing different staking methods and I would encourage everyone to take a look at it. Lets see how our simulation went:</p> Proportional Staking instead of Flat StakingResults Before CommissionsResults After Commissions <pre><code>if market.seconds_to_start &lt; 60 and market_book.inplay == False:\n    # Can't simulate polling API\n    # Only use streaming API:\n    for runner in market_book.runners:\n        model_price = todays_data.loc[market.market_id].loc[runner.selection_id]['rating']\n        # If best available to back price is &gt; rated price then proportional back stake\n        if runner.status == \"ACTIVE\" and runner.ex.available_to_back[0]['price'] &gt; model_price:\n            trade = Trade(\n            market_id=market_book.market_id,\n            selection_id=runner.selection_id,\n            handicap=runner.handicap,\n            strategy=self,\n            )\n            order = trade.create_order(\n                side=\"BACK\", order_type=LimitOrder(price=runner.ex.available_to_back[0]['price'], size=round(10/(runner.ex.available_to_back[0]['price']-1),2))\n            )\n            market.place_order(order)\n        # If best available to lay price is &lt; rated price then proportional lay stake\n        if runner.status == \"ACTIVE\" and runner.ex.available_to_lay[0]['price'] &lt; model_price:\n            trade = Trade(\n            market_id=market_book.market_id,\n            selection_id=runner.selection_id,\n            handicap=runner.handicap,\n            strategy=self,\n            )\n            order = trade.create_order(\n                side=\"LAY\", order_type=LimitOrder(price=runner.ex.available_to_lay[0]['price'], size=round(10/(runner.ex.available_to_lay[0]['price']-1),2))\n            )\n            market.place_order(order)\n</code></pre> <p></p> <p></p> <p>I'm pretty surprised, I really did not expect to be profitable after commissions. We are slightly profitable at the end but we spent a significant amount of time in the negatives during the month. Without testing it out further with more historic data I going to put this down as variance for now. And I'll hand it over to you.</p>"},{"location":"tutorials/How_to_Automate_5/#conclusion-and-next-steps","title":"Conclusion and next steps","text":"<p>While we have tested our strategy and optimised it so far, I mearly tried one month of data and only three different variations of our strategy (most of which are unprofitable). Hopefully these posts help you think about what is possible when automating your strategy and how to optimise your strategy. </p> <p>There are plenty of other things to look at when optimising your strategy such as different staking methodologies or being more selective with your bets based on the track or state. The natural next step based on my above results would be to test out proportional staking at 30 seconds and to use a longer backtesting period.</p> <p>We need more data to draw a good conclusion about long term results, I have definitely found some strategies that fluke one month, but are long term losers using this method.</p>"},{"location":"tutorials/How_to_Automate_5/#complete-code","title":"Complete Code","text":"Multiprocess How to Automate IIMultiprocess How to Automate IIIMultiprocess How to Automate IV <p>Download from Github</p> <pre><code># Import libraries\nimport glob\nimport os\nimport time\nimport logging\nimport csv\nimport math\nfrom pythonjsonlogger import jsonlogger\nfrom concurrent import futures\nfrom flumine import FlumineSimulation, clients, utils\nfrom flumine.controls.loggingcontrols import LoggingControl\nfrom flumine.order.ordertype import OrderTypes\n\nfrom flumine import BaseStrategy \nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder, OrderStatus\nfrom flumine.markets.market import Market\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\n\nimport pandas as pd\nimport numpy as np\nimport logging\n\n# Logging\nlogger = logging.getLogger()\ncustom_format = \"%(asctime) %(levelname) %(message)\"\nlog_handler = logging.StreamHandler()\nformatter = jsonlogger.JsonFormatter(custom_format)\nformatter.converter = time.gmtime\nlog_handler.setFormatter(formatter)\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.INFO)  # Set to logging.CRITICAL to speed up simulation\n\n# Create a new strategy as a new class called BackFavStrategy, this in turn will allow us to create a new Python object later\n    # BackFavStrategy is a child class inhereting from a predefined class in Flumine we imported above called BaseStrategy\nclass BackFavStrategy(BaseStrategy):\n    # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy\n    def start(self) -&gt; None:\n        # We will want to change what is printed with we have multiple strategies\n        print(\"starting strategy 'BackFavStrategy'\")\n\n    # Defines what happens when we first look at a market\n    # This method will prevent looking at markets that are closed\n    def check_market_book(self, market: Market, market_book: MarketBook) -&gt; bool:\n        # process_market_book only executed if this returns True\n        if market_book.status != \"CLOSED\":\n            return True\n\n    # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially\n    # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again\n    def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:\n\n        # Find last traded price as a dataframe\n        snapshot_last_price_traded = []\n        snapshot_runner_context = []\n        for runner in market_book.runners:\n                snapshot_last_price_traded.append([runner.selection_id,runner.last_price_traded])\n                # Get runner context for each runner\n                runner_context = self.get_runner_context(\n                    market.market_id, runner.selection_id, runner.handicap\n                )\n                snapshot_runner_context.append([runner_context.selection_id, runner_context.executable_orders, runner_context.live_trade_count, runner_context.trade_count])\n\n        snapshot_last_price_traded = pd.DataFrame(snapshot_last_price_traded, columns=['selection_id','last_traded_price'])\n        snapshot_last_price_traded = snapshot_last_price_traded.sort_values(by = ['last_traded_price'])\n        fav_selection_id = snapshot_last_price_traded['selection_id'].iloc[0]\n\n        snapshot_runner_context = pd.DataFrame(snapshot_runner_context, columns=['selection_id','executable_orders','live_trade_count','trade_count'])\n\n        for runner in market_book.runners:\n            if runner.status == \"ACTIVE\" and market.seconds_to_start &lt; 60 and market_book.inplay == False and runner.selection_id == fav_selection_id and snapshot_runner_context.iloc[:,1:].sum().sum() == 0:\n                trade = Trade(\n                    market_id=market_book.market_id,\n                    selection_id=runner.selection_id,\n                    handicap=runner.handicap,\n                    strategy=self,\n                )\n                order = trade.create_order(\n                    side=\"BACK\", order_type=LimitOrder(price=runner.last_price_traded, size=5)\n                )\n                market.place_order(order)\n\n\n# Fields we want to log in our simulations\nFIELDNAMES = [\n    \"bet_id\",\n    \"strategy_name\",\n    \"market_id\",\n    \"selection_id\",\n    \"trade_id\",\n    \"date_time_placed\",\n    \"price\",\n    \"price_matched\",\n    \"size\",\n    \"size_matched\",\n    \"profit\",\n    \"side\",\n    \"elapsed_seconds_executable\",\n    \"order_status\",\n    \"market_note\",\n    \"trade_notes\",\n    \"order_notes\",\n]\n\n# Log results from simulation into csv file named sim_hta_2.csv\n# If the csv file doesn't exist then it is created, otherwise we append results to the csv file\nclass BacktestLoggingControl(LoggingControl):\n    NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n    def __init__(self, *args, **kwargs):\n        super(BacktestLoggingControl, self).__init__(*args, **kwargs)\n        self._setup()\n\n    def _setup(self):\n        if os.path.exists(\"sim_hta_2.csv\"):\n            logging.info(\"Results file exists\")\n        else:\n            with open(\"sim_hta_2.csv\", \"w\") as m:\n                csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                csv_writer.writeheader()\n\n    def _process_cleared_orders_meta(self, event):\n        orders = event.event\n        with open(\"sim_hta_2.csv\", \"a\") as m:\n            for order in orders:\n                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                    size = order.order_type.size\n                else:\n                    size = order.order_type.liability\n                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                    price = None\n                else:\n                    price = order.order_type.price\n                try:\n                    order_data = {\n                        \"bet_id\": order.bet_id,\n                        \"strategy_name\": order.trade.strategy,\n                        \"market_id\": order.market_id,\n                        \"selection_id\": order.selection_id,\n                        \"trade_id\": order.trade.id,\n                        \"date_time_placed\": order.responses.date_time_placed,\n                        \"price\": price,\n                        \"price_matched\": order.average_price_matched,\n                        \"size\": size,\n                        \"size_matched\": order.size_matched,\n                        \"profit\": order.simulated.profit,\n                        \"side\": order.side,\n                        \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                        \"order_status\": order.status.value,\n                        \"market_note\": order.trade.market_notes,\n                        \"trade_notes\": order.trade.notes_str,\n                        \"order_notes\": order.notes_str,\n                    }\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                    csv_writer.writerow(order_data)\n                except Exception as e:\n                    logger.error(\n                        \"_process_cleared_orders_meta: %s\" % e,\n                        extra={\"order\": order, \"error\": e},\n                    )\n\n        logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n    def _process_cleared_markets(self, event):\n        cleared_markets = event.event\n        for cleared_market in cleared_markets.orders:\n            logger.info(\n                \"Cleared market\",\n                extra={\n                    \"market_id\": cleared_market.market_id,\n                    \"bet_count\": cleared_market.bet_count,\n                    \"profit\": cleared_market.profit,\n                    \"commission\": cleared_market.commission,\n                },\n            )\n\n# Searches for all betfair data files within the folder sample_monthly_data_output\ndata_folder = 'sample_monthly_data_output'\ndata_files = os.listdir(data_folder,)\ndata_files = [f'{data_folder}/{path}' for path in data_files]\n\ndef run_process(markets):\n    \"\"\"Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data.\n\n    Args:\n        markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv\n    \"\"\"    \n    # Set Flumine to simulation mode\n    client = clients.SimulatedClient()\n    framework = FlumineSimulation(client=client)    \n\n    # Set parameters for our strategy\n    strategy = BackFavStrategy(\n        # market_filter selects what portion of the historic data we simulate our strategy on\n        # markets selects the list of betfair historic data files\n        # market_types specifies the type of markets\n        # listener_kwargs specifies the time period we simulate for each market\n        market_filter={\n            \"markets\": markets,  \n            'market_types':['WIN'],\n            \"listener_kwargs\": {\"inplay\": False, \"seconds_to_start\": 80},  \n            },\n        max_order_exposure=1000,\n        max_selection_exposure=1000,\n        max_live_trade_count=1,\n        max_trade_count=1,\n    )\n    # Run our strategy on the simulated market\n    framework.add_strategy(strategy)\n    framework.add_logging_control(\n        BacktestLoggingControl()\n    )\n    framework.run()\n\n# Multi processing\nif __name__ == \"__main__\":\n    all_markets = data_files  # All the markets we want to simulate\n    processes = os.cpu_count()  # Returns the number of CPUs in the system.\n    markets_per_process = 8   # 8 is optimal as it prevents data leakage.\n\n    _process_jobs = []\n    with futures.ProcessPoolExecutor(max_workers=processes) as p:\n        # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have.\n        chunk = min(\n            markets_per_process, math.ceil(len(all_markets) / processes)\n        )\n        # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs\n        for m in (utils.chunks(all_markets, chunk)):\n            _process_jobs.append(\n                p.submit(\n                    run_process,\n                    markets=m,\n                )\n            )\n        for job in futures.as_completed(_process_jobs):\n            job.result()  # wait for result\n</code></pre> <p>Download from Github</p> <pre><code># Import libraries\nimport glob\nimport os\nimport time\nimport logging\nimport csv\nimport math\nimport pandas as pd\nfrom pythonjsonlogger import jsonlogger\nfrom concurrent import futures\nfrom flumine import FlumineSimulation, BaseStrategy, utils, clients\nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder, OrderStatus\nfrom flumine.order.ordertype import OrderTypes\nfrom flumine.markets.market import Market\nfrom flumine.controls.loggingcontrols import LoggingControl\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\n\n# Logging\nlogger = logging.getLogger()\ncustom_format = \"%(asctime) %(levelname) %(message)\"\nlog_handler = logging.StreamHandler()\nformatter = jsonlogger.JsonFormatter(custom_format)\nformatter.converter = time.gmtime\nlog_handler.setFormatter(formatter)\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.INFO)  # Set to logging.CRITICAL to speed up simulation\n\ndef download_iggy_ratings(date):\n    \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame.\n\n    Args:\n        date (datetime): the date we want to download the ratings for\n    \"\"\"\n    iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date='\n    iggy_url_2 = date.strftime(\"%Y-%m-%d\")\n    iggy_url_3 = '&amp;presenter=RatingsPresenter&amp;csv=true'\n    iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3\n\n    # Download todays greyhounds ratings\n    iggy_df = pd.read_csv(iggy_url)\n\n    # Data clearning\n    iggy_df = iggy_df.rename(columns={\"meetings.races.bfExchangeMarketId\":\"market_id\",\"meetings.races.runners.bfExchangeSelectionId\":\"selection_id\",\"meetings.races.runners.ratedPrice\":\"rating\"})\n    iggy_df = iggy_df[['market_id','selection_id','rating']]\n    iggy_df['market_id'] = iggy_df['market_id'].astype(str)\n\n    # Set market_id and selection_id as index for easy referencing\n    iggy_df = iggy_df.set_index(['market_id','selection_id'])\n    return(iggy_df)\n\nback_test_period = pd.date_range(start='2022/02/27', end='2022/03/05')\nframes = [download_iggy_ratings(day) for day in back_test_period]\niggy_df = pd.concat(frames)\nprint(iggy_df)\n\n# Create strategy, this is the exact same strategy shown in How to Automate III\nclass FlatIggyModel(BaseStrategy):\n    def start(self) -&gt; None:\n        print(\"starting strategy 'FlatIggyModel'\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&gt; bool:\n        if market_book.status != \"CLOSED\":\n            return True\n\n    def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:\n        if market.seconds_to_start &lt; 60 and market_book.inplay == False:\n            for runner in market_book.runners:\n                if runner.status == \"ACTIVE\" and runner.ex.available_to_back[0]['price'] &gt; iggy_df.loc[market_book.market_id].loc[runner.selection_id].item():\n                    trade = Trade(\n                    market_id=market_book.market_id,\n                    selection_id=runner.selection_id,\n                    handicap=runner.handicap,\n                    strategy=self,\n                    )\n                    order = trade.create_order(\n                        side=\"BACK\", order_type=LimitOrder(price=runner.ex.available_to_back[0]['price'], size=5.00)\n                    )\n                    market.place_order(order)\n                if runner.status == \"ACTIVE\" and runner.ex.available_to_lay[0]['price'] &lt; iggy_df.loc[market_book.market_id].loc[runner.selection_id].item():\n                    trade = Trade(\n                    market_id=market_book.market_id,\n                    selection_id=runner.selection_id,\n                    handicap=runner.handicap,\n                    strategy=self,\n                    )\n                    order = trade.create_order(\n                        side=\"LAY\", order_type=LimitOrder(price=runner.ex.available_to_lay[0]['price'], size=5.00)\n                    )\n                    market.place_order(order)\n\n# Fields we want to log in our simulations\nFIELDNAMES = [\n    \"bet_id\",\n    \"strategy_name\",\n    \"market_id\",\n    \"selection_id\",\n    \"trade_id\",\n    \"date_time_placed\",\n    \"price\",\n    \"price_matched\",\n    \"size\",\n    \"size_matched\",\n    \"profit\",\n    \"side\",\n    \"elapsed_seconds_executable\",\n    \"order_status\",\n    \"market_note\",\n    \"trade_notes\",\n    \"order_notes\",\n]\n\n# Log results from simulation into csv file named sim_hta_3.csv\n# If the csv file doesn't exist then it is created, otherwise we append results to the csv file\nclass BacktestLoggingControl(LoggingControl):\n    NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n    def __init__(self, *args, **kwargs):\n        super(BacktestLoggingControl, self).__init__(*args, **kwargs)\n        self._setup()\n\n    def _setup(self):\n        if os.path.exists(\"sim_hta_3.csv\"):\n            logging.info(\"Results file exists\")\n        else:\n            with open(\"sim_hta_3.csv\", \"w\") as m:\n                csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                csv_writer.writeheader()\n\n    def _process_cleared_orders_meta(self, event):\n        orders = event.event\n        with open(\"sim_hta_3.csv\", \"a\") as m:\n            for order in orders:\n                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                    size = order.order_type.size\n                else:\n                    size = order.order_type.liability\n                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                    price = None\n                else:\n                    price = order.order_type.price\n                try:\n                    order_data = {\n                        \"bet_id\": order.bet_id,\n                        \"strategy_name\": order.trade.strategy,\n                        \"market_id\": order.market_id,\n                        \"selection_id\": order.selection_id,\n                        \"trade_id\": order.trade.id,\n                        \"date_time_placed\": order.responses.date_time_placed,\n                        \"price\": price,\n                        \"price_matched\": order.average_price_matched,\n                        \"size\": size,\n                        \"size_matched\": order.size_matched,\n                        \"profit\": order.simulated.profit,\n                        \"side\": order.side,\n                        \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                        \"order_status\": order.status.value,\n                        \"market_note\": order.trade.market_notes,\n                        \"trade_notes\": order.trade.notes_str,\n                        \"order_notes\": order.notes_str,\n                    }\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                    csv_writer.writerow(order_data)\n                except Exception as e:\n                    logger.error(\n                        \"_process_cleared_orders_meta: %s\" % e,\n                        extra={\"order\": order, \"error\": e},\n                    )\n\n        logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n    def _process_cleared_markets(self, event):\n        cleared_markets = event.event\n        for cleared_market in cleared_markets.orders:\n            logger.info(\n                \"Cleared market\",\n                extra={\n                    \"market_id\": cleared_market.market_id,\n                    \"bet_count\": cleared_market.bet_count,\n                    \"profit\": cleared_market.profit,\n                    \"commission\": cleared_market.commission,\n                },\n            )\n\n# Searches for all betfair data files within the folder sample_monthly_data_output\ndata_folder = 'sample_monthly_data_output'\ndata_files = os.listdir(data_folder,)\ndata_files = [f'{data_folder}/{path}' for path in data_files]\n\ndef run_process(markets):\n    \"\"\"Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data.\n\n    Args:\n        markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv\n    \"\"\"    \n    # Set Flumine to simulation mode\n    client = clients.SimulatedClient()\n    framework = FlumineSimulation(client=client)\n\n    # Set parameters for our strategy\n    strategy = FlatIggyModel(\n        market_filter={\n            \"markets\": markets,  \n            'market_types':['WIN'],\n            \"listener_kwargs\": {\"inplay\": False, \"seconds_to_start\": 80},  \n            },\n        max_order_exposure=1000,\n        max_selection_exposure=1000,\n        max_live_trade_count=1,\n        max_trade_count=1,\n    )\n    # Run our strategy on the simulated market\n    framework.add_strategy(strategy)\n    framework.add_logging_control(\n        BacktestLoggingControl()\n    )\n    framework.run()\n\n# Multi processing\nif __name__ == \"__main__\":\n    all_markets = data_files  # All the markets we want to simulate\n    processes = os.cpu_count()  # Returns the number of CPUs in the system.\n    markets_per_process = 8   # 8 is optimal as it prevents data leakage.\n\n    _process_jobs = []\n    with futures.ProcessPoolExecutor(max_workers=processes) as p:\n        # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have.\n        chunk = min(\n            markets_per_process, math.ceil(len(all_markets) / processes)\n        )\n        # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs\n        for m in (utils.chunks(all_markets, chunk)):\n            _process_jobs.append(\n                p.submit(\n                    run_process,\n                    markets=m,\n                )\n            )\n        for job in futures.as_completed(_process_jobs):\n            job.result()  # wait for result\n</code></pre> <p>Download from Github</p> <pre><code># Import libraries\nimport glob\nimport os\nimport time\nimport logging\nimport csv\nimport pandas as pd\nimport json\nimport math\nfrom pythonjsonlogger import jsonlogger\nfrom flumine import FlumineSimulation, BaseStrategy, utils, clients\nfrom flumine.order.trade import Trade\nfrom flumine.order.order import LimitOrder, OrderStatus\nfrom flumine.order.ordertype import OrderTypes\nfrom flumine.markets.market import Market\nfrom flumine.controls.loggingcontrols import LoggingControl\nfrom betfairlightweight.filters import streaming_market_filter\nfrom betfairlightweight.resources import MarketBook\nfrom pythonjsonlogger import jsonlogger\nfrom concurrent import futures\n\n# Logging\nlogger = logging.getLogger()\ncustom_format = \"%(asctime) %(levelname) %(message)\"\nlog_handler = logging.StreamHandler()\nformatter = jsonlogger.JsonFormatter(custom_format)\nformatter.converter = time.gmtime\nlog_handler.setFormatter(formatter)\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.INFO)  # Set to logging.CRITICAL to speed up simulation\n\n# Read in predictions from hta_4\ntodays_data = pd.read_csv('backtest.csv', dtype = ({\"market_id\":str}))\ntodays_data = todays_data.set_index(['market_id','selection_id'])\n\n### New implementation\nclass FlatBetting(BaseStrategy):\n    def start(self) -&gt; None:\n        print(\"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\")\n\n    def check_market_book(self, market: Market, market_book: MarketBook) -&gt; bool:\n        if market_book.status != \"CLOSED\":\n            return True\n\n    def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:\n\n        # At the 60 second mark:\n        if market.seconds_to_start &lt; 60 and market_book.inplay == False:\n\n            # Can't simulate polling API\n            # Only use streaming API:\n            for runner in market_book.runners:\n                model_price = todays_data.loc[market.market_id].loc[runner.selection_id]['rating']\n                # If best available to back price is &gt; rated price then flat $5 back\n                if runner.status == \"ACTIVE\" and runner.ex.available_to_back[0]['price'] &gt; model_price:\n                    trade = Trade(\n                    market_id=market_book.market_id,\n                    selection_id=runner.selection_id,\n                    handicap=runner.handicap,\n                    strategy=self,\n                    )\n                    order = trade.create_order(\n                        side=\"BACK\", order_type=LimitOrder(price=runner.ex.available_to_back[0]['price'], size=5.00)\n                    )\n                    market.place_order(order)\n                # If best available to lay price is &lt; rated price then flat $5 lay\n                if runner.status == \"ACTIVE\" and runner.ex.available_to_lay[0]['price'] &lt; model_price:\n                    trade = Trade(\n                    market_id=market_book.market_id,\n                    selection_id=runner.selection_id,\n                    handicap=runner.handicap,\n                    strategy=self,\n                    )\n                    order = trade.create_order(\n                        side=\"LAY\", order_type=LimitOrder(price=runner.ex.available_to_lay[0]['price'], size=5.00)\n                    )\n                    market.place_order(order)\n\n# Fields we want to log in our simulations\nFIELDNAMES = [\n    \"bet_id\",\n    \"strategy_name\",\n    \"market_id\",\n    \"selection_id\",\n    \"trade_id\",\n    \"date_time_placed\",\n    \"price\",\n    \"price_matched\",\n    \"size\",\n    \"size_matched\",\n    \"profit\",\n    \"side\",\n    \"elapsed_seconds_executable\",\n    \"order_status\",\n    \"market_note\",\n    \"trade_notes\",\n    \"order_notes\",\n]\n\n# Log results from simulation into csv file named sim_hta_4.csv\n# If the csv file doesn't exist then it is created, otherwise we append results to the csv file\nclass BacktestLoggingControl(LoggingControl):\n    NAME = \"BACKTEST_LOGGING_CONTROL\"\n\n    def __init__(self, *args, **kwargs):\n        super(BacktestLoggingControl, self).__init__(*args, **kwargs)\n        self._setup()\n\n    def _setup(self):\n        if os.path.exists(\"sim_hta_4.csv\"):\n            logging.info(\"Results file exists\")\n        else:\n            with open(\"sim_hta_4.csv\", \"w\") as m:\n                csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                csv_writer.writeheader()\n\n    def _process_cleared_orders_meta(self, event):\n        orders = event.event\n        with open(\"sim_hta_4.csv\", \"a\") as m:\n            for order in orders:\n                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:\n                    size = order.order_type.size\n                else:\n                    size = order.order_type.liability\n                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:\n                    price = None\n                else:\n                    price = order.order_type.price\n                try:\n                    order_data = {\n                        \"bet_id\": order.bet_id,\n                        \"strategy_name\": order.trade.strategy,\n                        \"market_id\": order.market_id,\n                        \"selection_id\": order.selection_id,\n                        \"trade_id\": order.trade.id,\n                        \"date_time_placed\": order.responses.date_time_placed,\n                        \"price\": price,\n                        \"price_matched\": order.average_price_matched,\n                        \"size\": size,\n                        \"size_matched\": order.size_matched,\n                        \"profit\": order.simulated.profit,\n                        \"side\": order.side,\n                        \"elapsed_seconds_executable\": order.elapsed_seconds_executable,\n                        \"order_status\": order.status.value,\n                        \"market_note\": order.trade.market_notes,\n                        \"trade_notes\": order.trade.notes_str,\n                        \"order_notes\": order.notes_str,\n                    }\n                    csv_writer = csv.DictWriter(m, delimiter=\",\", fieldnames=FIELDNAMES)\n                    csv_writer.writerow(order_data)\n                except Exception as e:\n                    logger.error(\n                        \"_process_cleared_orders_meta: %s\" % e,\n                        extra={\"order\": order, \"error\": e},\n                    )\n\n        logger.info(\"Orders updated\", extra={\"order_count\": len(orders)})\n\n    def _process_cleared_markets(self, event):\n        cleared_markets = event.event\n        for cleared_market in cleared_markets.orders:\n            logger.info(\n                \"Cleared market\",\n                extra={\n                    \"market_id\": cleared_market.market_id,\n                    \"bet_count\": cleared_market.bet_count,\n                    \"profit\": cleared_market.profit,\n                    \"commission\": cleared_market.commission,\n                },\n            )\n\n# Searches for all betfair data files within the folder sample_monthly_data_output\ndata_folder = 'sample_monthly_data_output'\ndata_files = os.listdir(data_folder,)\ndata_files = [f'{data_folder}/{path}' for path in data_files]\n\ndef run_process(markets):\n    \"\"\"Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data.\n\n    Args:\n        markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv\n    \"\"\"    \n    # Set Flumine to simulation mode\n    client = clients.SimulatedClient()\n    framework = FlumineSimulation(client=client)\n\n    # Set parameters for our strategy\n    strategy = FlatBetting(\n        market_filter={\n            \"markets\": markets,  \n            'market_types':['WIN'],\n            \"listener_kwargs\": {\"inplay\": False, \"seconds_to_start\": 80},  \n            },\n        max_order_exposure=1000,\n        max_selection_exposure=1000,\n        max_live_trade_count=1,\n        max_trade_count=1,\n    )\n    # Run our strategy on the simulated market\n    framework.add_strategy(strategy)\n    framework.add_logging_control(\n        BacktestLoggingControl()\n    )\n    framework.run()\n\n# Multi processing\nif __name__ == \"__main__\":\n    all_markets = data_files  # All the markets we want to simulate\n    processes = os.cpu_count()  # Returns the number of CPUs in the system.\n    markets_per_process = 8   # 8 is optimal as it prevents data leakage.\n\n    _process_jobs = []\n    with futures.ProcessPoolExecutor(max_workers=processes) as p:\n        # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have.\n        chunk = min(\n            markets_per_process, math.ceil(len(all_markets) / processes)\n        )\n        # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs\n        for m in (utils.chunks(all_markets, chunk)):\n            _process_jobs.append(\n                p.submit(\n                    run_process,\n                    markets=m,\n                )\n            )\n        for job in futures.as_completed(_process_jobs):\n            job.result()  # wait for result\n</code></pre>"},{"location":"tutorials/How_to_Automate_5/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"tutorials/analysingAndPredictingBSP/","title":"Wisdom of the crowd? Analysing & understanding BSP","text":"<p>This tutorial was written by Tom Bishop and was originally published on Github. It is shared here with his permission.</p> <p>This tutorial follows on logically from the Market Movements tutorial we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at that tutorial before diving into this one.</p> <p>As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements!</p> <p>This article was written more than 2 years ago and some packages used here will have changed since the article was written. Continue at your peril</p> <pre><code>import pandas as pd\nimport numpy as np\nimport requests\nimport os\nimport re\nimport csv\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport math\nimport logging\nimport yaml\nimport csv\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\nimport ast\n\nfrom datetime import date, timedelta\nfrom unittest.mock import patch\nfrom typing import List, Set, Dict, Tuple, Optional\nfrom itertools import zip_longest\nimport betfairlightweight\nfrom betfairlightweight import StreamListener\nfrom betfairlightweight.resources.bettingresources import (\n    PriceSize,\n    MarketBook\n)\n</code></pre> <pre><code># General Utility Functions\n# _________________________________\n\ndef split_anz_horse_market_name(market_name: str) -&amp;gt; (str, str, str):\n    parts = market_name.split(' ')\n    race_no = parts[0] # return example R6\n    race_len = parts[1] # return example 1400m\n    race_type = parts[2].lower() # return example grp1, trot, pace\n    return (race_no, race_len, race_type)\n\n\ndef load_markets(file_paths):\n    for file_path in file_paths:\n        print(file_path)\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n\n    return None\n\ndef pull_ladder(availableLadder, n = 5):\n        out = {}\n        price = []\n        volume = []\n        if len(availableLadder) == 0:\n            return(out)        \n        else:\n            for rung in availableLadder[0:n]:\n                price.append(rung.price)\n                volume.append(rung.size)\n\n            out[\"p\"] = price\n            out[\"v\"] = volume\n            return(out)\n\ndef filter_market(market: MarketBook) -&amp;gt; bool: \n\n    d = market.market_definition\n\n    return (d.country_code == 'AU' \n        and d.market_type == 'WIN' \n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n</code></pre> <pre><code>def final_market_book(s):\n    with patch(\"builtins.open\", lambda f, _: f):\n        gen = s.get_generator()\n        for market_books in gen():\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    return(None)\n            for market_book in market_books:\n                last_market_book = market_book\n        return(last_market_book)\n\ndef parse_final_selection_meta(dir, out_file):\n\n    with open(out_file, \"w+\") as output:\n\n        output.write(\"market_id,selection_id,venue,market_time,selection_name,win,bsp\\n\")\n\n        for file_obj in load_markets(dir):\n\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            last_market_book = final_market_book(stream)\n            if last_market_book is None:\n                continue \n\n            # Extract Info ++++++++++++++++++++++++++++++++++\n            runnerMeta = [\n                {\n                    'selection_id': r.selection_id,\n                    'selection_name': next((rd.name for rd in last_market_book.market_definition.runners if rd.selection_id == r.selection_id), None),\n                    'selection_status': r.status,\n                    'win': np.where(r.status == \"WINNER\", 1, 0),\n                    'sp': r.sp.actual_sp\n                }\n                for r in last_market_book.runners \n            ]\n\n            # Return Info ++++++++++++++++++++++++++++++++++\n            for runnerMeta in runnerMeta:\n                if runnerMeta['selection_status'] != 'REMOVED':\n                    output.write(\n                        \"{},{},{},{},{},{},{}\\n\".format(\n                            str(last_market_book.market_id),\n                            runnerMeta['selection_id'],\n                            last_market_book.market_definition.venue,\n                            last_market_book.market_definition.market_time,\n                            runnerMeta['selection_name'],\n                            runnerMeta['win'],\n                            runnerMeta['sp']\n                        )\n                    )\n</code></pre> <pre><code>selection_meta = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\"\nstream_files = glob.glob(\"[PATH TO STREAM FILES]*.tar\")\n# trading = betfairlightweight.APIClient(\"username\", \"password\")\n# listener = StreamListener(max_latency=None)\n\nprint(\"__ Parsing Selection Metadata ___ \")\n# parse_final_selection_meta(stream_files, selection_meta)\n</code></pre> <pre>\n<code>__ Parsing Selection Metadata ___ \n</code>\n</pre> <pre><code>def loop_preplay_prices(s, o):\n\n    with patch(\"builtins.open\", lambda f, _: f):\n\n        gen = s.get_generator()\n\n        marketID = None\n        tradeVols = None\n        time = None\n        last_book_recorded = False\n        prev_book = None\n\n        for market_books in gen():\n\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    break\n\n            for market_book in market_books:\n\n                # Time Step Management ++++++++++++++++++++++++++++++++++\n\n                if marketID is None:\n                    # No market initialised\n                    marketID = market_book.market_id\n                    time =  market_book.publish_time\n                elif market_book.inplay and last_book_recorded:\n                    break\n                else:\n\n                    seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n\n                    if seconds_to_start &amp;gt; 120:\n                        # Too early before off to start logging prices\n                        prev_book = market_book\n                        continue\n                    else:\n\n                        # Update data at different time steps depending on seconds to off\n                        wait = 10\n\n                        # New Market\n                        if market_book.market_id != marketID:\n                            last_book_recorded = False\n                            marketID = market_book.market_id\n                            time =  market_book.publish_time\n                            continue\n                        # (wait) seconds elapsed since last write\n                        elif (market_book.publish_time - time).total_seconds() &amp;gt; wait:\n                            time = market_book.publish_time\n                        # if current marketbook is inplay want to record the previous market book as it's the last preplay marketbook\n                        elif market_book.inplay:\n                            last_book_recorded = True\n                            market_book = prev_book\n                        # fewer than (wait) seconds elapsed continue to next loop\n                        else:\n                            prev_book = market_book\n                            continue\n\n                # Execute Data Logging ++++++++++++++++++++++++++++++++++\n                for runner in market_book.runners:\n\n                    try:\n                        atb_ladder = pull_ladder(runner.ex.available_to_back, n = 5)\n                        atl_ladder = pull_ladder(runner.ex.available_to_lay, n = 5)\n                    except:\n                        atb_ladder = {}\n                        atl_ladder = {}\n\n                    limitTradedVol = sum([rung.size for rung in runner.ex.traded_volume])\n\n                    o.writerow(\n                        (\n                            market_book.market_id,\n                            runner.selection_id,\n                            market_book.publish_time,\n                            int(limitTradedVol),\n                            # SP Fields\n                            runner.sp.near_price,\n                            runner.sp.far_price,\n                            int(sum([ps.size for ps in runner.sp.back_stake_taken])),\n                            int(sum([ps.size for ps in runner.sp.lay_liability_taken])),\n                            # Limit bets available\n                            str(atb_ladder).replace(' ',''), \n                            str(atl_ladder).replace(' ','')\n                        )\n                    )\n\n                prev_book = market_book\n\ndef parse_preplay_prices(dir, out_file):\n\n    with open(out_file, \"w+\") as output:\n\n        writer = csv.writer(\n            output, \n            delimiter=',',\n            lineterminator='\\r\\n',\n            quoting=csv.QUOTE_ALL\n        )\n        writer.writerow((\"market_id\",\"selection_id\",\"time\",\"traded_volume\",\"near_price\",\"far_price\",\"bsp_back_pool_stake\",\"bsp_lay_pool_liability\",\"atb_ladder\",'atl_ladder'))\n\n        for file_obj in load_markets(dir):\n\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            loop_preplay_prices(stream, writer)\n</code></pre> <pre><code>price = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\"\nstream_files = glob.glob(\"[PATH TO STREAM FILES]*.tar\")\n# trading = betfairlightweight.APIClient(\"username\", \"password\")\n# listener = StreamListener(max_latency=None)\n\nprint(\"__ Parsing Selection Prices ___ \")\n# parse_final_selection_meta(stream_files, price)\n</code></pre> <pre>\n<code>__ Parsing Selection Prices ___ \n</code>\n</pre> <pre><code>selection = pd.read_csv(\"[PATH TO YOUR SELECTION METADATA FILE]\", dtype={'market_id': object, 'selection_id': object}, parse_dates = ['market_time'])\n\nselection.head(3)\n</code></pre> market_id selection_id venue market_time selection_name win bsp 0 1.179845158 23493550 Cowra 2021-03-01 04:15:00 1. Larmour 0 6.20 1 1.179845158 16374800 Cowra 2021-03-01 04:15:00 3. Careering Away 1 3.60 2 1.179845158 19740699 Cowra 2021-03-01 04:15:00 4. Bells N Bows 0 6.62 <p>Now let's load the prices file. We'll apply some extra logic to parse the ladder columns into dictionaries and also remove the first odds record per group as it's the first record as the market was instantiated.</p> <pre><code>prices = pd.read_csv(\n    \"[PATH TO YOUR PRICES FILE]\", \n    quoting=csv.QUOTE_ALL,\n    dtype={'market_id': 'string', 'selection_id': 'string', 'atb_ladder': 'string', 'atl_ladder': 'string'},\n    parse_dates=['time']\n)\n\n# Parse ladder columns\nprices['atb_ladder'] = [ast.literal_eval(x) for x in prices['atb_ladder']]\nprices['atl_ladder'] = [ast.literal_eval(x) for x in prices['atl_ladder']]\n\n# Drop the first row within each group\nprices = prices.drop(prices.groupby(['market_id', 'selection_id'],as_index=False).nth(0).index)\n\nprices.head(3)\n</code></pre> market_id selection_id time traded_volume near_price far_price bsp_back_pool_stake bsp_lay_pool_liability atb_ladder atl_ladder 21 1.179845158 23493550 2021-03-01 04:13:00.058 2465 5.93 3.17 113 238 {'p': [6.2, 6, 5.9, 5.8, 5.7], 'v': [14.63, 13... {'p': [6.6, 6.8, 7, 7.2, 7.4], 'v': [22.86, 12... 22 1.179845158 16374800 2021-03-01 04:13:00.058 5046 3.35 1.70 449 300 {'p': [3.65, 3.6, 3.55, 3.5, 3.45], 'v': [0.45... {'p': [3.75, 3.8, 3.9, 4.1, 4.3], 'v': [5.14, ... 23 1.179845158 19740699 2021-03-01 04:13:00.058 1978 6.39 3.25 154 251 {'p': [6, 5.9, 5.8, 5.7, 5.6], 'v': [4.71, 89.... {'p': [6.4, 6.6, 6.8, 7, 7.2], 'v': [30.24, 2.... <pre><code>f'The shape of the prices data file is {prices.shape[0]} rows and {prices.shape[1]} columns'\n</code></pre> <pre>\n<code>'The shape of the prices data file is 3937805 rows and 10 columns'</code>\n</pre> <pre><code># Let's have a look at the prices datafile for a distinct market and selection\nprices.query('market_id == \"1.183995724\" and selection_id == \"22832649\"')\n</code></pre> market_id selection_id time traded_volume near_price far_price bsp_back_pool_stake bsp_lay_pool_liability atb_ladder atl_ladder 2762714 1.183995724 22832649 2021-06-01 01:38:00.062 1894 8.60 7.69 27 184 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [31.23, 37... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [63.98, 14... 2762724 1.183995724 22832649 2021-06-01 01:38:10.158 2082 8.60 7.69 27 184 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [25.54, 49... {'p': [8.2, 8.4, 8.6, 8.8, 9], 'v': [37.63, 68... 2762734 1.183995724 22832649 2021-06-01 01:38:20.159 2094 8.00 7.69 27 184 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [94.52, 49... {'p': [8.2, 8.4, 8.6, 8.8, 9], 'v': [37.63, 56... 2762744 1.183995724 22832649 2021-06-01 01:38:30.182 2229 8.00 7.69 27 190 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [62.88, 49... {'p': [8.2, 8.4, 8.6, 8.8, 9], 'v': [22.99, 56... 2762754 1.183995724 22832649 2021-06-01 01:38:40.221 2240 8.00 7.69 136 205 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [13.92, 12... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [53.28, 41... 2762764 1.183995724 22832649 2021-06-01 01:38:50.923 2294 8.00 7.69 136 205 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [129.4, 53... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [67.76, 41... 2762774 1.183995724 22832649 2021-06-01 01:39:00.955 2297 8.00 7.69 137 214 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [26.93, 13... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [53.28, 41... 2762784 1.183995724 22832649 2021-06-01 01:39:10.962 2417 8.00 7.69 137 229 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [29.2, 146... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [53.26, 23... 2762794 1.183995724 22832649 2021-06-01 01:39:20.966 2677 8.18 2.68 212 243 {'p': [8.4, 8.2, 8, 7.8, 7.6], 'v': [19.2, 89.... {'p': [8.6, 8.8, 9, 9.2, 9.4], 'v': [69.9, 16.... 2762804 1.183995724 22832649 2021-06-01 01:39:30.971 2795 8.18 2.68 212 243 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [79.45, 82... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [59.89, 65... 2762814 1.183995724 22832649 2021-06-01 01:39:41.018 3039 8.18 2.68 245 324 {'p': [8.4, 8.2, 8, 7.8, 7.6], 'v': [37.01, 12... {'p': [8.6, 8.8, 9, 9.2, 9.4], 'v': [73.69, 37... 2762824 1.183995724 22832649 2021-06-01 01:39:51.119 3290 8.18 2.68 324 459 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [63.46, 94... {'p': [8.6, 8.8, 9, 9.2, 9.4], 'v': [19.59, 18... 2762834 1.183995724 22832649 2021-06-01 01:40:01.123 3488 8.18 2.68 324 748 {'p': [8.4, 8.2, 8, 7.8, 7.6], 'v': [32.81, 97... {'p': [8.6, 8.8, 9, 9.2, 9.4], 'v': [0.96, 15.... 2762844 1.183995724 22832649 2021-06-01 01:40:11.136 3831 8.18 2.68 447 2285 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [46.15, 99... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [20.11, 47... 2762854 1.183995724 22832649 2021-06-01 01:40:18.201 3950 8.04 7.60 327 2469 {'p': [6.8, 6.4, 5.9, 5.1, 4.4], 'v': [4.01, 3... {'p': [10, 11, 11.5, 15, 15.5], 'v': [6.05, 0.... <p>We can see some expected behaviour as we zoom in on a particular selection</p> <ul> <li>The traded volume increases on this selection as we get closer to the jump</li> <li>The projected BSP (the <code>near_price</code> column) stays constant for a number of increments as its update is cached for 60 seconds at a time</li> <li>The sizes in the BSP pools also increases as we get closer to the jump</li> <li>The prices offered and traded closer to the jump are closer to the BSP than those at the start of the 2 minute period</li> </ul> <pre><code># Define the betfair tick ladder\ndef bfTickLadder():\n\n    tickIncrements = {\n        1.0: 0.01,\n        2.0: 0.02,\n        3.0: 0.05,\n        4.0: 0.1,\n        6.0: 0.2,\n        10.0: 0.5,\n        20.0: 1.0,\n        30.0: 2.0,\n        50.0: 5.0,\n        100.0: 10.0,\n        1000.0: 1000,\n    }\n\n    ladder = []\n\n    for index, key in enumerate(tickIncrements):\n\n        increment = tickIncrements[key]\n\n        if (index+1) == len(tickIncrements):\n            ladder.append(key)\n        else:\n            key1 = [*tickIncrements][index]\n            key2 = [*tickIncrements][index+1]\n            steps = (key2 - key1) / increment\n\n            for i in range(int(steps)):\n                ladder.append(round(key + i * increment, 2))\n\n    return(ladder)\n\nbfticks = bfTickLadder()\n\n# Round a decimal to the betfair tick value below\ndef bfTickFloor(price, includeIndex=False):\n\n    if 'bfticks' in globals():\n        global bfticks\n    else:\n        bfticks = bfTickLadder()\n\n    ind = [ n for n,i in enumerate(bfticks) if i&amp;gt;=price][0]\n    if includeIndex:\n        if bfticks[ind]==price:\n            return((ind, price))\n        else:\n            return((ind-1, bfticks[ind-1]))\n    else:\n        if bfticks[ind]==price:\n            return(price)\n        else:\n            return(bfticks[ind-1])\n\n# Calculate the numder of ticks between two tick values\ndef bfTickDelta(p1, p2):\n\n    if np.isnan(p1) or np.isnan(p2):\n        return(np.nan)\n\n    x = bfTickFloor(p1, includeIndex=True)\n    y = bfTickFloor(p2, includeIndex=True)\n    return(x[0]-y[0])\n\ndef bfTickShift(p, rungs):\n\n    if 'bfticks' in globals():\n        global bfticks\n    else:\n        bfticks = bfTickLadder()\n\n    flr = bfTickFloor(p, includeIndex = True)\n\n    return(bfticks[flr[0]+rungs])\n\n\ndef bfLadderMidPoint(p1, p2):\n\n    if np.isnan(p1) or np.isnan(p2):\n        return(np.nan)\n\n    delta = -1 * bfTickDelta(p1, p2)\n\n    if delta == 1:\n        return(p1)\n    elif delta % 2 != 0:\n        return(bfTickShift(p1, math.ceil(delta / 2)))\n    else:\n        return(bfTickShift(p1, math.floor(delta / 2)))\n</code></pre> <pre><code># Let's test a midpoint using the ladder mid point method\nbfLadderMidPoint(10,100)\n</code></pre> <pre>\n<code>25.0</code>\n</pre> <pre><code># And for illustrative purposes let's calculate the geomtric mean of these values\nnp.sqrt(10 * 100)\n</code></pre> <pre>\n<code>31.622776601683793</code>\n</pre> <p>Let's put this all together while stitching together our two core datasets.</p> <pre><code># Join and augment\ndf = (\n    selection.merge(prices, on = ['market_id', 'selection_id'])\n    .assign(sbsj = lambda x: round((x['market_time'] - x['time']).dt.total_seconds() / 10) * 10)\n    .assign(back_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atb_ladder']])\n    .assign(lay_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atl_ladder']])\n    .assign(geometric_mid_point = lambda x: round(1 / np.sqrt((1/x['back_best']) * (1/x['lay_best'])), 3))\n    .assign(ladder_mid_point = lambda x: x.apply(lambda x: bfLadderMidPoint(x.back_best, x.lay_best), axis=1))\n    .replace([np.inf, -np.inf], np.nan)\n)\n\ndf.head(3)\n</code></pre> market_id selection_id venue market_time selection_name win bsp time traded_volume near_price far_price bsp_back_pool_stake bsp_lay_pool_liability atb_ladder atl_ladder sbsj back_best lay_best geometric_mid_point ladder_mid_point 0 1.179845158 23493550 Cowra 2021-03-01 04:15:00 1. Larmour 0 6.2 2021-03-01 04:13:00.058 2465 5.93 3.17 113 238 {'p': [6.2, 6, 5.9, 5.8, 5.7], 'v': [14.63, 13... {'p': [6.6, 6.8, 7, 7.2, 7.4], 'v': [22.86, 12... 120.0 6.2 6.6 6.397 6.4 1 1.179845158 23493550 Cowra 2021-03-01 04:15:00 1. Larmour 0 6.2 2021-03-01 04:13:10.077 2848 5.93 3.17 113 238 {'p': [6, 5.9, 5.8, 5.7, 5.6], 'v': [59.93, 36... {'p': [6.4, 6.6, 6.8, 7, 7.2], 'v': [28.79, 50... 110.0 6.0 6.4 6.197 6.2 2 1.179845158 23493550 Cowra 2021-03-01 04:15:00 1. Larmour 0 6.2 2021-03-01 04:13:20.161 2866 5.93 3.17 113 238 {'p': [6.2, 6, 5.9, 5.8, 5.7], 'v': [22.91, 88... {'p': [6.6, 6.8, 7, 7.2, 7.4], 'v': [55.19, 22... 100.0 6.2 6.6 6.397 6.4 <pre><code># Volume Traded\n# _________________________\n\n\n# Extract the final time slice of data which includes the total preplay volumes traded across limit and BSP poools\nvolumeDf = df.groupby(['market_id', 'selection_id'],as_index=False).nth(-1)[['market_id', 'selection_id', 'bsp',  'traded_volume', 'bsp_back_pool_stake', 'bsp_lay_pool_liability']]\n\n# Infer the biggest of the two BSP stakes\nvolumeDf = (\n    volumeDf\n    .assign(lay_stake = lambda x: x['bsp_lay_pool_liability'] / (x['bsp']-1))\n    .assign(bsp_stake = lambda x: x[['lay_stake', 'bsp_back_pool_stake']].max(axis = 1))\n)\n\n(\n    volumeDf\n    .groupby('market_id', as_index = False)\n    .agg({'traded_volume': 'sum', 'bsp_stake': 'sum'})\n    .agg({'traded_volume': 'mean', 'bsp_stake': 'mean'})\n)\n</code></pre> <pre>\n<code>traded_volume    98025.705018\nbsp_stake         7287.524766\ndtype: float64</code>\n</pre> <p>So in an average thoroughbred market there's about 98k traded limit volume and 7,300 BSP traded stake. So approximately 7% of thoroughbred volume is traded at the BSP at least for our sample of thoroughbred races.</p> <pre><code># Extract the geomtric market mid point at time slices: 120, 90, 60, 30, and 0 seconds from the scheduled off\npreplay = df[df.sbsj.isin([120,90,60,30,0])][['market_id', 'selection_id', 'win', 'sbsj', 'geometric_mid_point']].sort_values(['market_id', 'selection_id', 'sbsj'], ascending = [True, True, False]).rename(columns={'geometric_mid_point': 'odds'}).assign(type = lambda x: \"seconds before off: \" + x['sbsj'].astype(int).astype(str))\n\n# Extract the BSP values\nbsp = df.sort_values(['market_id', 'selection_id', 'time'], ascending = [True, True, False]).groupby(['market_id', 'selection_id']).head(1)[['market_id', 'selection_id', 'win', 'sbsj', 'bsp']].rename(columns={'bsp': 'odds'}).assign(type = \"bsp\")\n\n# Append them together\naccuracyFrame = pd.concat([preplay, bsp]).dropna()\naccuracyFrame.head(5)\n</code></pre> market_id selection_id win sbsj odds type 2790008 1.171091071 10693094 0 120.0 134.164 seconds before off: 120 2790011 1.171091071 10693094 0 90.0 149.666 seconds before off: 90 2790014 1.171091071 10693094 0 60.0 200.000 seconds before off: 60 2790017 1.171091071 10693094 0 30.0 239.792 seconds before off: 30 2790020 1.171091071 10693094 0 -0.0 239.165 seconds before off: 0 <p>Now we'll filter just on our BSP records and plot the observed vs actual scatterplot</p> <pre><code># BSP Scatter\n# __________________\n\nwinRates = (\n    accuracyFrame\n    .query('type == \"bsp\"')\n    .assign(implied_chance = lambda x: round(20 * (1 / x['odds']))/20)\n    .groupby('implied_chance', as_index = False)\n    .agg({'win': 'mean'})\n)\n\nfig = px.scatter(winRates, x = \"implied_chance\", y = \"win\", template = \"plotly_white\", title = \"BSP: implied win vs actual win\")\nfig.add_trace(\n    go.Scatter(\n        x = winRates.implied_chance, y = winRates.implied_chance, name = 'no bias', line_color = 'rgba(8,61,119, 0.3)'\n    )\n)\nfig.show(\"png\")\n</code></pre> <p>Ok aside from some small sample noise at the top end (there's very few horses that run at sub 1.20 BSPs) we can see that the BSP is pretty perfectly.... efficient? Is that the right word? I'd argue that it's very much not the right word. Let me illustrate with a counter example. Let's plot the same chart for the BSP as well as our 5 other price points.</p> <pre><code># Bsp + Other Odds Scatter\n# __________________\n\nwinRates = (\n    accuracyFrame\n    .assign(implied_chance = lambda x: round(20 * (1 / x['odds']))/20)\n    .groupby(['type', 'implied_chance'], as_index = False)\n    .agg({'win': 'mean'})\n)\n\nfig = px.scatter(winRates, x = \"implied_chance\", y = \"win\", color = 'type', template = \"plotly_white\", title = \"Comparing Price Points: implied win vs actual win\")\nfig.add_trace(\n    go.Scatter(\n        x = winRates.implied_chance, y = winRates.implied_chance, name = 'no bias', line_color = 'rgba(8,61,119, 0.3)'\n    )\n)\nfig.show(\"png\")\n</code></pre> <p>So they're all efficient? And indecernibly as efficient as one another?</p> <p>Well, to cut a long and possibly boring story short this isn't the right way to measure efficiency. What we're measure here is bias. All my scatter plot here tells me is if there's any systematic bias in the BSP, i.e. groups of BSPs that aren't well calibrated with actual outcomes. That is, for example, that perhaps randomly the group of horses that BSP around 2 don't happen to win 50% of the time maybe there was a sytemic bias that short favourites were underbet and these selections actually won 55% of the time. That would be a price bias in the BSP that someone could take advatange at just by looking at historical prices and outcomes alone.</p> <p>For an even simpler counter point: I could create a perfectly well calibrated estimate that assigned a single odds value to every horse which was the overall horse empirical win rate over our sample: 10.25% (which is merely a reflection of field sizes). This estimate would be unbiased, and would pass through our scatterplot method unscathed but would it be an efficient estimate? Clearly not.</p> <pre><code>df.agg({'win': 'mean'})\n</code></pre> <pre>\n<code>win    0.102595\ndtype: float64</code>\n</pre> <p>Bias only tells us if there's a systematic way of exploiting the odds values themselves. I could have told you that this was unlikely but the scatterplot proves it.</p> <p>How else could we measure efficiency? I propose using the <code>logloss</code> metric.</p> <p>Let's calculate the logloss of the BSP</p> <pre><code># Logloss ++++++++++++++++++++++++++++++++++++++++\n\nfrom sklearn.metrics import log_loss\n\n# Overall Logloss\n# _________________\n\nbspLoss = log_loss(\n    y_pred = 1 / accuracyFrame.query('type == \"bsp\"')['odds'],\n    y_true = accuracyFrame.query('type == \"bsp\"')['win']\n)\n\nprint(f'The overall logloss of the BSP is {round(bspLoss,4)}')\n</code></pre> <pre>\n<code>The overall logloss of the BSP is 0.2757\n</code>\n</pre> <p>Ok what does this mean? Well nothing really. This metric won't tell you anything by itself it's just useful for relative comparisons. Let's plot the logloss of our geometric midpoint at our various timeslices.</p> <pre><code># Logloss at Different Time Points\n# _________________\n\naccuracyFrame.groupby('type', as_index = False).apply(lambda x: log_loss(y_pred=1/x['odds'],y_true=x['win'])).rename(columns = {None: 'logloss'}).sort_values('logloss')\n</code></pre> type logloss 0 bsp 0.275679 1 seconds before off: 0 0.275824 3 seconds before off: 30 0.275999 4 seconds before off: 60 0.276044 5 seconds before off: 90 0.276206 2 seconds before off: 120 0.276256 <pre><code># And in chart form\nfig = px.bar(\n    accuracyFrame.groupby('type', as_index = False).apply(lambda x: log_loss(y_pred=1/x['odds'],y_true=x['win'])).rename(columns = {None: 'logloss'}).sort_values('logloss', ascending = False),\n    x = \"type\",\n    y = \"logloss\",\n    template = \"plotly_white\",\n    title = \"Logloss Of Odds At Various Time Points\"\n)\nfig.update_yaxes(range=[.2755, .2765])\nfig.show(\"png\")\n</code></pre> <p>Now this is a cool graph. This is exactly like we would have intiuited. The market sharpens monotonically as we approach the market jump with the BSP being the most effiecient of all the prices!</p> <p>Hopefully you can now see the logical failing of measuring bias over market efficiency and it changes the way you think about your bet placement.</p> <p>Let's move on to what we're here for: is it possible to predict the BSP.</p> <pre><code># Lets take a sample of a market and a selection\ndSlice = df.query('market_id == \"1.182394184\" and selection_id == \"39243409\"').dropna()\n</code></pre> <pre><code>def chartClosingPrices(d):\n\n    fig = px.line(\n        pd.melt(d[:-1][['sbsj', 'back_best', 'near_price']], id_vars = 'sbsj', var_name = 'price'), \n            x='sbsj', y='value',\n            color = 'price',\n            template='plotly_white',\n            title=\"Selection\",\n            labels = {\n                'sbsj': \"Seconds Before Scheduled Jump\"\n            }\n    )\n    fig.update_layout(font_family=\"Roboto\")\n    fig.add_trace(\n        go.Line(x = dSlice.sbsj, y = dSlice.bsp, name = 'BSP', line_color = 'rgba(8,61,119, 0.3)', mode = \"lines\")\n    )\n    fig['layout']['xaxis']['autorange'] = \"reversed\"\n    fig.show(\"png\")\n\nchartClosingPrices(dSlice)\n</code></pre> <pre>\n<code>/home/tmbish/.local/lib/python3.9/site-packages/plotly/graph_objs/_deprecations.py:378: DeprecationWarning:\n\nplotly.graph_objs.Line is deprecated.\nPlease replace it with one of the following more specific types\n  - plotly.graph_objs.scatter.Line\n  - plotly.graph_objs.layout.shape.Line\n  - etc.\n\n\n</code>\n</pre> <p>The red line is the projected BSP, you can see that it's not very responsive. As the best back price comes in from ~3 to 2.9 leading up to the jump the projected SP doesn't move because it's cached. If you were relying on this number for something important and you were using it in that period you were using stale information and you'd be worse off for it. In this instance the final SP was 2.79 so you may have made the wrong betting decision.</p> <p>This is somewhat counter intuitive because the projected sp (the so called near price) should be a good estimate of the BSP because it synthetically runs the BSP algorithm on the current market state and produces and estimate, so you would think that it'd be a pretty good estimate.</p> <p>Let's widen our sample a bit and see how it performs across our entire sample. We'll slice the data at the exact scheduled off and see how accurate various price points are at predicting what the final BSP is. We'll use mean absolute error (MAE) as our error metric. We'll assess 6 price points:</p> <ul> <li>The near price (projected sp)</li> <li>The far price (projected sp excluding limit orders)</li> <li>The best back price</li> <li>The best lay price</li> <li>The ladder midpoint price</li> <li>The geometric midpoint price</li> </ul> <pre><code># Measurement\n# ________________________\n\nestimatesDf = df[df.sbsj == 0][['bsp', 'near_price', 'far_price', 'back_best', 'lay_best', 'geometric_mid_point', 'ladder_mid_point']]\n\n(\n    pd.melt(estimatesDf, id_vars = 'bsp', var_name = 'estimate')\n    .assign(error = lambda x: abs(x['value'] - x['bsp']) / x['bsp'])\n    .groupby('estimate', as_index=False)\n    .agg({'error': 'mean'})\n    .sort_values('error')\n)\n</code></pre> estimate error 0 back_best 0.091702 3 ladder_mid_point 0.093181 2 geometric_mid_point 0.094405 4 lay_best 0.103142 5 near_price 0.121266 1 far_price 0.578425 <p>So a bit surprisingly, in thoroughbred markets at the scheduled off your best to just use the current best back price as your estimate of the BSP. It significantly outperforms the projected SP and even some of our midpoint methods. </p> <p>Let's change the timeslice a little and take the very last moment before the market settles and see which performs best.</p> <pre><code>lastEstimatesDf = df.groupby(['market_id', 'selection_id'],as_index=False).nth(-1)[['bsp', 'near_price', 'far_price', 'back_best', 'lay_best', 'geometric_mid_point', 'ladder_mid_point']]\n\n(\n    pd.melt(lastEstimatesDf, id_vars = 'bsp', var_name = 'estimate')\n    .assign(error = lambda x: abs(x['value'] - x['bsp']) / x['bsp'])\n    .groupby('estimate', as_index=False)\n    .agg({'error': 'mean'})\n    .sort_values('error')\n)\n</code></pre> estimate error 2 geometric_mid_point 0.039627 3 ladder_mid_point 0.042526 0 back_best 0.063622 5 near_price 0.077108 4 lay_best 0.098198 1 far_price 0.290777 <p>First thing to notice is the estimates get a lot better than at the scheduled off, as we'd expect. A bit surprisingly the projected SP is still very weak due to the caching issue. In this scenario the geometric mid point beforms significantly better than the current best back price which suggests that as the late market is forming the back and lay spread with start converging to the fair price and eventual BSP. I personally use the geometric midpoint as my BSP estimate as it's a quick and easy metric that performs pretty well.</p> <p>What if you want more though? Is it possible to do better than these metrics? These simple price points use no information about what's in the BSP pools, surely if we used this information we'd be able to do better. Let's try to use machine learning to synthesise all this information at once.</p> <pre><code>def wapToGetBack(pool, ladder):\n    price = ladder['p']\n    volume = ladder['v']\n    try:\n        indmax = min([ i for (i,j) in enumerate(cVolume) if j &amp;gt; pool ])+1\n    except:\n        indmax = len(volume)\n    return(round(sum([a * b for a, b in zip(price[:indmax], volume[:indmax])]) / sum(volume[:indmax]),4))\n\ndef wapToGetLay(liability_pool, ladder):\n    price = ladder['p']\n    volume = ladder['v']\n    liability = [(a-1) * b for a, b in zip(price, volume)]\n    cLiability = np.cumsum(liability)\n    try:\n        indmax = min([ i for (i,j) in enumerate(cLiability) if j &amp;gt; liability_pool ])+1\n    except:\n        indmax = len(volume)\n    return(round(sum([a * b for a, b in zip(price[:indmax], volume[:indmax])]) / sum(volume[:indmax]),4))\n</code></pre> <p>Now we'll set up our model matrix which will be the market state at the exact scheduled off. We'll also add our custom features.</p> <pre><code>model_matrix = df[['sbsj', 'atb_ladder', 'atl_ladder','bsp', 'traded_volume', 'near_price', 'far_price', 'bsp_back_pool_stake', 'bsp_lay_pool_liability', 'back_best', 'lay_best', 'geometric_mid_point', 'ladder_mid_point']]\n\n# Filter at scheduled jump\nmodel_matrix = model_matrix[model_matrix.sbsj == 0].dropna()\n\nmodel_matrix = (\n    model_matrix\n    .assign(wap_to_get_back_pool = lambda x: x.apply(lambda x: wapToGetBack(x.bsp_back_pool_stake, x.atb_ladder), axis=1))\n    .assign(wap_to_get_lay_pool = lambda x: x.apply(lambda x: wapToGetLay(x.bsp_lay_pool_liability, x.atl_ladder), axis=1))\n)\n\n# Drop other columns\nmodel_matrix.drop(columns = ['sbsj', 'atb_ladder', 'atl_ladder'], inplace = True)\n\nmodel_matrix.head(3)\n</code></pre> bsp traded_volume near_price far_price bsp_back_pool_stake bsp_lay_pool_liability back_best lay_best geometric_mid_point ladder_mid_point wap_to_get_back_pool wap_to_get_lay_pool 12 6.20 6891 5.74 4.22 518 1580 6.00 6.2 6.099 6.00 5.7762 6.3731 28 3.60 13579 3.57 1.73 1023 1771 3.45 3.6 3.524 3.55 3.3010 3.7007 44 6.62 5911 5.81 1.59 845 1378 6.20 6.6 6.397 6.40 5.9156 7.1167 <p>Now the machine learning. Sklearn makes this very simple, in our case it's a few lines only. We'll split our data into train and test sets and train a small random forrest to predict the BSP.</p> <pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\n# Setup Train / Test\ntrain_features, test_features, train_labels, test_labels = train_test_split(model_matrix.drop(columns = ['bsp']), model_matrix['bsp'], test_size = 0.25)\n\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)\n\n# Instantiate Model\nrf = RandomForestRegressor(n_estimators = 100)\n\n# Train Model\nrf.fit(train_features, train_labels)\n</code></pre> <pre>\n<code>Training Features Shape: (119822, 11)\nTraining Labels Shape: (119822,)\nTesting Features Shape: (39941, 11)\nTesting Labels Shape: (39941,)\n</code>\n</pre> <pre>\n<code>RandomForestRegressor()</code>\n</pre> <p>Let's check out our predictions on the test set (remember our model hasn't seen any of this data so it should be a true reflection on how we'd perform on some new races that would happen this afternoon say)</p> <pre><code># Use the forest's predict method on the test data\npredicted_bsp = rf.predict(test_features)\npredicted_bsp\n</code></pre> <pre>\n<code>array([268.9971,   4.9892,  24.2727, ...,  29.067 ,   4.6216,  16.3991])</code>\n</pre> <p>Seems reasonable. All well and good though is the prediction any good? Let's measure it using MAE in the same way as we did before.</p> <pre><code># Let's test our estimate vs our others in the same way as before\n\ntestDf = test_features\ntestDf['bsp'] = test_labels\ntestDf['rf_bsp_prediction'] = predicted_bsp\n\n\n(\n    pd.melt(testDf[['bsp', 'near_price', 'far_price', 'back_best', 'lay_best', 'geometric_mid_point', 'ladder_mid_point', 'rf_bsp_prediction']], id_vars = 'bsp', var_name = 'estimate')\n    .assign(error = lambda x: abs(x['value'] - x['bsp']) / x['bsp'])\n    .groupby('estimate', as_index=False)\n    .agg({'error': 'mean'})\n    .sort_values('error')\n)\n</code></pre> estimate error 6 rf_bsp_prediction 0.088898 0 back_best 0.091860 3 ladder_mid_point 0.093477 2 geometric_mid_point 0.094702 4 lay_best 0.103435 5 near_price 0.121456 1 far_price 0.575946 <p>Nice that's significantly better than the best previous estimate at this time slice. To validate it further let's use the same model to predict the BSP using the market state 10 seconds after the scheduled jump instead of at the exact scheduled off. None of the rows (or samples) in this time slice have been seen by the model during the training step so it should provide a robust out of sample estimate of the models performance on unseen data.</p> <pre><code># Validate it on a completely different time point - 10 seconds after scheduled jump\n\noutOfSample = df[['sbsj', 'atb_ladder', 'atl_ladder','bsp', 'traded_volume', 'near_price', 'far_price', 'bsp_back_pool_stake', 'bsp_lay_pool_liability', 'back_best', 'lay_best', 'geometric_mid_point', 'ladder_mid_point']]\n\noutOfSample = outOfSample[outOfSample.sbsj == -10].dropna()\n\noutOfSample = (\n    outOfSample\n    .assign(wap_to_get_back_pool = lambda x: x.apply(lambda x: wapToGetBack(x.bsp_back_pool_stake, x.atb_ladder), axis=1))\n    .assign(wap_to_get_lay_pool = lambda x: x.apply(lambda x: wapToGetLay(x.bsp_lay_pool_liability, x.atl_ladder), axis=1))\n)\n\n# Produce Predictions\noutofsamplebspprediction = rf.predict(outOfSample.drop(columns = ['bsp', 'sbsj', 'atb_ladder', 'atl_ladder']))\noutofsamplebspprediction\n</code></pre> <pre>\n<code>array([  6.395 ,   3.5781,   6.3449, ..., 503.9829, 220.1171,  54.7511])</code>\n</pre> <pre><code>outOfSample['rf_bsp_prediction'] = outofsamplebspprediction\n\n(\n    pd.melt(outOfSample[['bsp', 'near_price', 'far_price', 'back_best', 'lay_best', 'geometric_mid_point', 'ladder_mid_point', 'rf_bsp_prediction']], id_vars = 'bsp', var_name = 'estimate')\n    .assign(error = lambda x: abs(x['value'] - x['bsp']) / x['bsp'])\n    .groupby('estimate', as_index=False)\n    .agg({'error': 'mean'})\n    .sort_values('error')\n)\n</code></pre> estimate error 6 rf_bsp_prediction 0.079198 0 back_best 0.084658 3 ladder_mid_point 0.086128 2 geometric_mid_point 0.087326 4 lay_best 0.098311 5 near_price 0.109640 1 far_price 0.501813 <p>Still significantly better on the out of sample set which is a really positive sign.</p> <pre><code># %% [markdown]\n# # Analysing and Predicting The BSP\n# \n# \n# ## 0.1 Setup\n# \n# Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language.\n# \n# Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer locally where you want to store the intermediate data files.\n# \n# You'll also need `betfairlightweight` which you can install with something like `pip install betfairlightweight`.\n\n# %%\nimport pandas as pd\nimport numpy as np\nimport requests\nimport os\nimport re\nimport csv\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport math\nimport logging\nimport yaml\nimport csv\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\nimport ast\n\nfrom datetime import date, timedelta\nfrom unittest.mock import patch\nfrom typing import List, Set, Dict, Tuple, Optional\nfrom itertools import zip_longest\nimport betfairlightweight\nfrom betfairlightweight import StreamListener\nfrom betfairlightweight.resources.bettingresources import (\n    PriceSize,\n    MarketBook\n)\n\n# %% [markdown]\n# ## 0.2 Context\n# \n# The BSP is betting product offered by betfair (on large enough markets) that gives customers a chance to back or lay any selection at a \"fair\" price. Without getting too complex too quickly, the BSP allows you lock in a bet at any time after the market is openened and for as much stake as you can afford. The BSP is a good option for many different segments of customers:\n# \n# - Recreational punters that don't have a particular strategy for trying to get the best odds can lock in a price that is (in the aggregate) a lot better than what they'd get at a corporate book or they'd get by taking limit bets early in a market's trading\n# - Automated customers that don't want the hassle of managing live market trading can implement automated strategies a lot easier whilst also protecting them from edge cases like race reschedules \n# - Is perfect for simply backtesting fundemental models as it's a resiliant and robust single price\n# \n# Despite it being a good option for a lot of customers it's also a fairly contraversial topic for some other types of customers. Some people firmly believe that the BSP on big markets reflects the \"true chance\" of a selection so betting it is a fools errand that will simply lose you commission over the long run. You might have heard a version of this story before: given the large pool sizes, the 0% overround, the settlement at the exact moment the market is suspended the BSP perfectly synthesises all available public information and demand and arrives at a true fair odds. Some will attempt to prove this to you by showing you a predicted chance vs observed win rate scatterplot which shows a perfect correlation between chance implied by the BSP and a horses true chance. Whilst I don't disagree that the BSP is a **very strong** estimate of a selections chance it's pretty obviously not perfect. \n# \n# Furthermore, it presents some other tricky challenges to use in practical situations. It's not knowable perfectly before it's the exact moment of market suspension so many model or strategy builders make the mistake of unknowingly leaking it into their preplay model development or their theoretical staking calculations. Where the final number will land is actually another source of uncertainty in your processes which presents anothing forecasting / predictive modelling application as I'll explore later in this piece. I'll take you through how I'd measure the accuracy of the BSP, show you how it's traded on the exchange, and take you through a host of methods of estimating the BSP and build a custom machine learning approach that's better than each of them.\n# \n# ## 0.3 The Algorithm\n# \n# The actual logic of how betfair arrives at the final BSP number is quite complex and for a few reasons you won't be able to perfectly replicate it at home. However, the general gist of the BSP reconciliation algorithm that is executed just as the market suspended goes something like:\n# \n# - The algorithm combines 4 distinct groups of open bets for a given selection: \n#     + Non price limited BSP orders on both the back and lay side (`market_on_close` orders)\n#     + Price limited orders on both the back and lay side (`limit_on_close` orders)\n#     + All non filled open lay orders\n#     + All non filled open back orders \n# - It then combines them all together, passes a sophisticated balancing algorithm over the top of them and arrives at a single fair price for the BSP that balances the demand on either side of the ledger\n# \n# ## 0.4 This Example\n# \n# For this exercise we'll again take advantage of the betfair historical stream json files. The slice of betfair markets I'll be analysing is all thoroughbred races over July 2020 - June 2021. \n# \n# As an aside the projected BSP number you see on the betfair website isn't collected inside betfair's own internal database of orders, so any custom data request you may be able to get as a VIP won't include this number. So if you were planning to include it in any kind of projection or bet placement logic operation you were making the only way to anlayse it historically is to mine these data files. Another good reason to learn the skills to do so! \n\n# %% [markdown]\n# # 1.0 Data\n# \n# Like the previous tutorial we won't be able to collapse the stream data down into a single row per runner because I'm interested in anlaysing how the projected BSP moves late in betfair markets. I'm also interested in plotting the efficiency of certain odds values at certain distinct time points leading up the the races so I need multiple records per runner.\n# \n# Like in the previous tutorial I'll split out the selection metadata, BSP and win flag values as a seperate data file to reduce the size of the datafiles extracted for this analysis.\n# \n# For the preplay prices dataset I'll:\n# \n# - Start extraction at 2 mins before the scheduled off\n# - Extract prices every 10 seconds thereafter until the market is suspended\n# - I'll also extract the final market state the instant before the market is suspended\n# \n# ## 1.1 Sourcing Data\n# \n# First you'll need to source the stream file TAR archive files. I'll be analysing 12 months of Australian thoroughbred Pro files. Aask automation@betfair.com.au for more info if you don't know how to do this. Once you've gotten access download them to your computer and store them together in a folder.\n# \n# ## 1.2 Utility functions\n# \n# First like always we'll need some general utility functions that you may have seen before:\n\n# %%\n# General Utility Functions\n# _________________________________\n\ndef split_anz_horse_market_name(market_name: str) -&amp;gt; (str, str, str):\n    parts = market_name.split(' ')\n    race_no = parts[0] # return example R6\n    race_len = parts[1] # return example 1400m\n    race_type = parts[2].lower() # return example grp1, trot, pace\n    return (race_no, race_len, race_type)\n\n\ndef load_markets(file_paths):\n    for file_path in file_paths:\n        print(file_path)\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n\n    return None\n\ndef pull_ladder(availableLadder, n = 5):\n        out = {}\n        price = []\n        volume = []\n        if len(availableLadder) == 0:\n            return(out)        \n        else:\n            for rung in availableLadder[0:n]:\n                price.append(rung.price)\n                volume.append(rung.size)\n\n            out[\"p\"] = price\n            out[\"v\"] = volume\n            return(out)\n\ndef filter_market(market: MarketBook) -&amp;gt; bool: \n\n    d = market.market_definition\n\n    return (d.country_code == 'AU' \n        and d.market_type == 'WIN' \n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n\n\n# %% [markdown]\n# ## 1.3 Selection Metadata\n# \n# Given that the detailed price data will have so many records we will split out the selection metadata (including the selection win outcome flag and the bsp) into it's own dataset much you would do in a relational database to manage data volumes.\n\n# %%\n\ndef final_market_book(s):\n    with patch(\"builtins.open\", lambda f, _: f):\n        gen = s.get_generator()\n        for market_books in gen():\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    return(None)\n            for market_book in market_books:\n                last_market_book = market_book\n        return(last_market_book)\n\ndef parse_final_selection_meta(dir, out_file):\n\n    with open(out_file, \"w+\") as output:\n\n        output.write(\"market_id,selection_id,venue,market_time,selection_name,win,bsp\\n\")\n\n        for file_obj in load_markets(dir):\n\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            last_market_book = final_market_book(stream)\n            if last_market_book is None:\n                continue \n\n            # Extract Info ++++++++++++++++++++++++++++++++++\n            runnerMeta = [\n                {\n                    'selection_id': r.selection_id,\n                    'selection_name': next((rd.name for rd in last_market_book.market_definition.runners if rd.selection_id == r.selection_id), None),\n                    'selection_status': r.status,\n                    'win': np.where(r.status == \"WINNER\", 1, 0),\n                    'sp': r.sp.actual_sp\n                }\n                for r in last_market_book.runners \n            ]\n\n            # Return Info ++++++++++++++++++++++++++++++++++\n            for runnerMeta in runnerMeta:\n                if runnerMeta['selection_status'] != 'REMOVED':\n                    output.write(\n                        \"{},{},{},{},{},{},{}\\n\".format(\n                            str(last_market_book.market_id),\n                            runnerMeta['selection_id'],\n                            last_market_book.market_definition.venue,\n                            last_market_book.market_definition.market_time,\n                            runnerMeta['selection_name'],\n                            runnerMeta['win'],\n                            runnerMeta['sp']\n                        )\n                    )\n\n# %%\nselection_meta = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\"\nstream_files = glob.glob(\"[PATH TO STREAM FILES]*.tar\")\n# trading = betfairlightweight.APIClient(\"username\", \"password\")\n# listener = StreamListener(max_latency=None)\n\nprint(\"__ Parsing Selection Metadata ___ \")\n# parse_final_selection_meta(stream_files, selection_meta)\n\n# %% [markdown]\n# ## 1.4 Preplay Prices and Projections\n# \n# In this set of preplay prices I'm interested in many of the same fields as we've extracted in previous tutorials as well as fields relating to the current state of the BSP.\n# \n# These objects sit under the `sp` slot within the returned `runner` object. The fields we'll extract are:\n# \n# - The so called \"near price\"\n#     + The near price is the projected SP value you can see on the website\n#     + It includes both bets already placed into the SP pools as well as open limit orders to estimate what the final BSP value will be\n# - The so called \"far price\"\n#     + This is the same as the near price except it excludes limit orders on the exchange\n#     + This makes it fairly redundant value and we'll see how poor of an estimator it is a bit later\n# - The volume currently bet into the BSP back pool\n# - The liability currently laid into the BSP lay pool\n# \n# We'll also extract the top 5 rungs of the available to back and available to lay ladders as well as the traded volume of limit bets.\n# \n# It's worth noting that I am discarding some key information about the BSP pools that I could have extracted if I wanted to. The current SP bets are laid out in a way that I could split out `limit_on_close` as well as `market_on_close` sp bets but I've rolled everything together in SP stake on the back side and sp liability on the lay side. This is just to reduce complexity of this article but including it would increase the predictive power of the BSP model in the final step.\n\n# %%\ndef loop_preplay_prices(s, o):\n\n    with patch(\"builtins.open\", lambda f, _: f):\n\n        gen = s.get_generator()\n\n        marketID = None\n        tradeVols = None\n        time = None\n        last_book_recorded = False\n        prev_book = None\n\n        for market_books in gen():\n\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    break\n\n            for market_book in market_books:\n\n                # Time Step Management ++++++++++++++++++++++++++++++++++\n\n                if marketID is None:\n                    # No market initialised\n                    marketID = market_book.market_id\n                    time =  market_book.publish_time\n                elif market_book.inplay and last_book_recorded:\n                    break\n                else:\n\n                    seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n\n                    if seconds_to_start &amp;gt; 120:\n                        # Too early before off to start logging prices\n                        prev_book = market_book\n                        continue\n                    else:\n\n                        # Update data at different time steps depending on seconds to off\n                        wait = 10\n\n                        # New Market\n                        if market_book.market_id != marketID:\n                            last_book_recorded = False\n                            marketID = market_book.market_id\n                            time =  market_book.publish_time\n                            continue\n                        # (wait) seconds elapsed since last write\n                        elif (market_book.publish_time - time).total_seconds() &amp;gt; wait:\n                            time = market_book.publish_time\n                        # if current marketbook is inplay want to record the previous market book as it's the last preplay marketbook\n                        elif market_book.inplay:\n                            last_book_recorded = True\n                            market_book = prev_book\n                        # fewer than (wait) seconds elapsed continue to next loop\n                        else:\n                            prev_book = market_book\n                            continue\n\n                # Execute Data Logging ++++++++++++++++++++++++++++++++++\n                for runner in market_book.runners:\n\n                    try:\n                        atb_ladder = pull_ladder(runner.ex.available_to_back, n = 5)\n                        atl_ladder = pull_ladder(runner.ex.available_to_lay, n = 5)\n                    except:\n                        atb_ladder = {}\n                        atl_ladder = {}\n\n                    limitTradedVol = sum([rung.size for rung in runner.ex.traded_volume])\n\n                    o.writerow(\n                        (\n                            market_book.market_id,\n                            runner.selection_id,\n                            market_book.publish_time,\n                            int(limitTradedVol),\n                            # SP Fields\n                            runner.sp.near_price,\n                            runner.sp.far_price,\n                            int(sum([ps.size for ps in runner.sp.back_stake_taken])),\n                            int(sum([ps.size for ps in runner.sp.lay_liability_taken])),\n                            # Limit bets available\n                            str(atb_ladder).replace(' ',''), \n                            str(atl_ladder).replace(' ','')\n                        )\n                    )\n\n                prev_book = market_book\n\ndef parse_preplay_prices(dir, out_file):\n\n    with open(out_file, \"w+\") as output:\n\n        writer = csv.writer(\n            output, \n            delimiter=',',\n            lineterminator='\\r\\n',\n            quoting=csv.QUOTE_ALL\n        )\n        writer.writerow((\"market_id\",\"selection_id\",\"time\",\"traded_volume\",\"near_price\",\"far_price\",\"bsp_back_pool_stake\",\"bsp_lay_pool_liability\",\"atb_ladder\",'atl_ladder'))\n\n        for file_obj in load_markets(dir):\n\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            loop_preplay_prices(stream, writer)\n\n# %%\nprice = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\"\nstream_files = glob.glob(\"[PATH TO STREAM FILES]*.tar\")\n# trading = betfairlightweight.APIClient(\"username\", \"password\")\n# listener = StreamListener(max_latency=None)\n\nprint(\"__ Parsing Selection Prices ___ \")\n# parse_final_selection_meta(stream_files, price)\n\n# %% [markdown]\n# # 2.0 Analysis\n# \n# First step let's boot up the datasets we extracted in the previous steps and take a look at what we've managed to extract from the raw stream files.\n# \n# ## 2.1 Load and Inspect\n# \n# First we have the highlevel selection metadata as we have already seen in other tutorials\n\n# %%\nselection = pd.read_csv(\"[PATH TO YOUR SELECTION METADATA FILE]\", dtype={'market_id': object, 'selection_id': object}, parse_dates = ['market_time'])\n\nselection.head(3)\n\n# %% [markdown]\n# Now let's load the prices file. We'll apply some extra logic to parse the ladder columns into dictionaries and also remove the first odds record per group as it's the first record as the market was instantiated.\n\n# %%\nprices = pd.read_csv(\n    \"[PATH TO YOUR PRICES FILE]\", \n    quoting=csv.QUOTE_ALL,\n    dtype={'market_id': 'string', 'selection_id': 'string', 'atb_ladder': 'string', 'atl_ladder': 'string'},\n    parse_dates=['time']\n)\n\n# Parse ladder columns\nprices['atb_ladder'] = [ast.literal_eval(x) for x in prices['atb_ladder']]\nprices['atl_ladder'] = [ast.literal_eval(x) for x in prices['atl_ladder']]\n\n# Drop the first row within each group\nprices = prices.drop(prices.groupby(['market_id', 'selection_id'],as_index=False).nth(0).index)\n\nprices.head(3)\n\n# %%\nf'The shape of the prices data file is {prices.shape[0]} rows and {prices.shape[1]} columns'\n\n# %%\n# Let's have a look at the prices datafile for a distinct market and selection\nprices.query('market_id == \"1.183995724\" and selection_id == \"22832649\"')\n\n# %% [markdown]\n# We can see some expected behaviour as we zoom in on a particular selection\n# \n# - The traded volume increases on this selection as we get closer to the jump\n# - The projected BSP (the `near_price` column) stays constant for a number of increments as its update is cached for 60 seconds at a time\n# - The sizes in the BSP pools also increases as we get closer to the jump\n# - The prices offered and traded closer to the jump are closer to the BSP than those at the start of the 2 minute period\n\n# %% [markdown]\n# ## 2.2 Transform and Assemble\n# \n# We have our 2 core datasets, but we'd prefer to work with one now. We'd also like to add some key columns that will be reused throughout our analysis so we'll add those now too.\n# \n# \n# ### 2.2.1 Mid points\n# \n# The first semi-interesting thing we'll do in this analysis is add selection mid-points to our dataset. Eventually we're going to be interested in estimating the BSP and measuring the efficiency of certain prices at various points leading up to the race. \n# \n# Betfair markets work like all markets with bids and spreads. The market equilibrium forms around the best price offered on either side of the market to back or to lay. These top prices each have some inherent advantage built into it for the offerer. For example in early markets the best offers on either side of the market might be really wide (say 1.80 as a best back and 2.50 as a best lay). Given the price discovery process is still immature each bidder gets a large premium, backed into their offer price, which compensates them for providing betting opportunities with little to no information provided from other market participants. This spread will naturally get tighter and tighter as the market matures and more participants seek to get volume down and must be more and more competitive. But what's the price \"equilibrium\" in each case?\n# \n# Well it's up to you but I'll provide you two ways of finding the central mid-point of a bid-ask spread on betfair markets. The problem we're solving for here is the non-linearity of prices in odds space. We have some intuition for this: when we see a market spread of 10-100 in early trading we have an understanding that the true midpoint of this market is somewhere around 25-35 not the 55 you'd get if you simply took the (arithmetic) mean of those two numbers.\n# \n# Two techniquest for accounting for that non-linearity are as follows.\n# \n# **Ladder Midpoint**\n# \n# The ladder midpoint method takes advantage of the fact that the betfair price ladder itself accounts for the nonlinearity of prices in odds space. The method calculated the difference in number of rungs on the betfair ladder, halves it, and shifts the best back or lay price that number of rungs towards the centre. This will generally provide a much better idea of the market midpoint than a simple arithmetic mean of the two prices.\n# \n# **Geometric Mean**\n# \n# Unfortunately the ladder method is a little computationally expensive. A good approximation for this approach is to take the geometric mean of the best back and best lay values. The geometric mean is a special kind of mean that you may have never used before that is more appropriate for purposes like this. It is calculated like: `sqrt(x1 * x2 * ...)`. This number will also provide a much better estimate of the market midpoint than the simple arithmetic mean.\n# \n# The latter calculation is trivial. The former requires a suite of betfair tick arithmetic functions that I'll put below. It may seem like overkill for this exercise (and it is) but hopefully these functions might be of use to you for other purposes.\n\n# %%\n# Define the betfair tick ladder\ndef bfTickLadder():\n\n    tickIncrements = {\n        1.0: 0.01,\n        2.0: 0.02,\n        3.0: 0.05,\n        4.0: 0.1,\n        6.0: 0.2,\n        10.0: 0.5,\n        20.0: 1.0,\n        30.0: 2.0,\n        50.0: 5.0,\n        100.0: 10.0,\n        1000.0: 1000,\n    }\n\n    ladder = []\n\n    for index, key in enumerate(tickIncrements):\n\n        increment = tickIncrements[key]\n\n        if (index+1) == len(tickIncrements):\n            ladder.append(key)\n        else:\n            key1 = [*tickIncrements][index]\n            key2 = [*tickIncrements][index+1]\n            steps = (key2 - key1) / increment\n\n            for i in range(int(steps)):\n                ladder.append(round(key + i * increment, 2))\n\n    return(ladder)\n\nbfticks = bfTickLadder()\n\n# Round a decimal to the betfair tick value below\ndef bfTickFloor(price, includeIndex=False):\n\n    if 'bfticks' in globals():\n        global bfticks\n    else:\n        bfticks = bfTickLadder()\n\n    ind = [ n for n,i in enumerate(bfticks) if i&amp;gt;=price][0]\n    if includeIndex:\n        if bfticks[ind]==price:\n            return((ind, price))\n        else:\n            return((ind-1, bfticks[ind-1]))\n    else:\n        if bfticks[ind]==price:\n            return(price)\n        else:\n            return(bfticks[ind-1])\n\n# Calculate the numder of ticks between two tick values\ndef bfTickDelta(p1, p2):\n\n    if np.isnan(p1) or np.isnan(p2):\n        return(np.nan)\n\n    x = bfTickFloor(p1, includeIndex=True)\n    y = bfTickFloor(p2, includeIndex=True)\n    return(x[0]-y[0])\n\ndef bfTickShift(p, rungs):\n\n    if 'bfticks' in globals():\n        global bfticks\n    else:\n        bfticks = bfTickLadder()\n\n    flr = bfTickFloor(p, includeIndex = True)\n\n    return(bfticks[flr[0]+rungs])\n\n\ndef bfLadderMidPoint(p1, p2):\n\n    if np.isnan(p1) or np.isnan(p2):\n        return(np.nan)\n\n    delta = -1 * bfTickDelta(p1, p2)\n\n    if delta == 1:\n        return(p1)\n    elif delta % 2 != 0:\n        return(bfTickShift(p1, math.ceil(delta / 2)))\n    else:\n        return(bfTickShift(p1, math.floor(delta / 2)))\n\n# %%\n# Let's test a midpoint using the ladder mid point method\nbfLadderMidPoint(10,100)\n\n# %%\n# And for illustrative purposes let's calculate the geomtric mean of these values\nnp.sqrt(10 * 100)\n\n# %% [markdown]\n# Let's put this all together while stitching together our two core datasets.\n\n# %%\n# Join and augment\ndf = (\n    selection.merge(prices, on = ['market_id', 'selection_id'])\n    .assign(sbsj = lambda x: round((x['market_time'] - x['time']).dt.total_seconds() / 10) * 10)\n    .assign(back_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atb_ladder']])\n    .assign(lay_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atl_ladder']])\n    .assign(geometric_mid_point = lambda x: round(1 / np.sqrt((1/x['back_best']) * (1/x['lay_best'])), 3))\n    .assign(ladder_mid_point = lambda x: x.apply(lambda x: bfLadderMidPoint(x.back_best, x.lay_best), axis=1))\n    .replace([np.inf, -np.inf], np.nan)\n)\n\ndf.head(3)\n\n# %% [markdown]\n# ## 2.3 Analysing The BSP\n# \n# Before we embark on our predictive exercise let's analyse the BSP to get a feel for it as an entity.\n# \n# ### 2.3.1 Volumes\n# \n# Ever wondered how much volume is traded on the BSP? How does it compare to limit bets? Well with our parsed stream data we can answer those questions! Now the BSP volume will be the bigger of the BSP back stake and the lay stake (which you can infer by the final BSP and the total lay liability).\n# \n# \n\n# %%\n# Volume Traded\n# _________________________\n\n\n# Extract the final time slice of data which includes the total preplay volumes traded across limit and BSP poools\nvolumeDf = df.groupby(['market_id', 'selection_id'],as_index=False).nth(-1)[['market_id', 'selection_id', 'bsp',  'traded_volume', 'bsp_back_pool_stake', 'bsp_lay_pool_liability']]\n\n# Infer the biggest of the two BSP stakes\nvolumeDf = (\n    volumeDf\n    .assign(lay_stake = lambda x: x['bsp_lay_pool_liability'] / (x['bsp']-1))\n    .assign(bsp_stake = lambda x: x[['lay_stake', 'bsp_back_pool_stake']].max(axis = 1))\n)\n\n(\n    volumeDf\n    .groupby('market_id', as_index = False)\n    .agg({'traded_volume': 'sum', 'bsp_stake': 'sum'})\n    .agg({'traded_volume': 'mean', 'bsp_stake': 'mean'})\n)\n\n# %% [markdown]\n# So in an average thoroughbred market there's about 98k traded limit volume and 7,300 BSP traded stake. So approximately 7% of thoroughbred volume is traded at the BSP at least for our sample of thoroughbred races.\n# \n# \n# ## 2.3.2 Efficiency?\n# \n# Now you may have heard this story before: **you can't beat the BSP it's too efficient!**. I'm not sure people really have a firm idea about what they're talking about when they say this.\n# \n# Typically what you'll see in a discussion about efficiency is the predicted vs observed scatterplot. Let's see if we can reproduce this chart.\n# \n# First let's assemble a dataframe that we can use for this chart as well as others. What we'll do is we'll extract the BSP and a price value at 5 different slices before the race starts. We could chose any price point (we'll analyse the difference between them in a subsequent step) but for this section I'm going to take the preplay market estimate as the geometric market midpoint (you'll have to trust me for now that this is a sensible decision).\n\n# %%\n# Extract the geomtric market mid point at time slices: 120, 90, 60, 30, and 0 seconds from the scheduled off\npreplay = df[df.sbsj.isin([120,90,60,30,0])][['market_id', 'selection_id', 'win', 'sbsj', 'geometric_mid_point']].sort_values(['market_id', 'selection_id', 'sbsj'], ascending = [True, True, False]).rename(columns={'geometric_mid_point': 'odds'}).assign(type = lambda x: \"seconds before off: \" + x['sbsj'].astype(int).astype(str))\n\n# Extract the BSP values\nbsp = df.sort_values(['market_id', 'selection_id', 'time'], ascending = [True, True, False]).groupby(['market_id', 'selection_id']).head(1)[['market_id', 'selection_id', 'win', 'sbsj', 'bsp']].rename(columns={'bsp': 'odds'}).assign(type = \"bsp\")\n\n# Append them together\naccuracyFrame = pd.concat([preplay, bsp]).dropna()\naccuracyFrame.head(5)\n\n# %% [markdown]\n# Now we'll filter just on our BSP records and plot the observed vs actual scatterplot\n\n# %%\n# BSP Scatter\n# __________________\n\nwinRates = (\n    accuracyFrame\n    .query('type == \"bsp\"')\n    .assign(implied_chance = lambda x: round(20 * (1 / x['odds']))/20)\n    .groupby('implied_chance', as_index = False)\n    .agg({'win': 'mean'})\n)\n\nfig = px.scatter(winRates, x = \"implied_chance\", y = \"win\", template = \"plotly_white\", title = \"BSP: implied win vs actual win\")\nfig.add_trace(\n    go.Scatter(\n        x = winRates.implied_chance, y = winRates.implied_chance, name = 'no bias', line_color = 'rgba(8,61,119, 0.3)'\n    )\n)\nfig.show(\"png\")\n\n# %% [markdown]\n# Ok aside from some small sample noise at the top end (there's very few horses that run at sub 1.20 BSPs) we can see that the BSP is pretty perfectly.... efficient? Is that the right word? I'd argue that it's very much not the right word. Let me illustrate with a counter example. Let's plot the same chart for the BSP as well as our 5 other price points.\n\n# %%\n# Bsp + Other Odds Scatter\n# __________________\n\nwinRates = (\n    accuracyFrame\n    .assign(implied_chance = lambda x: round(20 * (1 / x['odds']))/20)\n    .groupby(['type', 'implied_chance'], as_index = False)\n    .agg({'win': 'mean'})\n)\n\nfig = px.scatter(winRates, x = \"implied_chance\", y = \"win\", color = 'type', template = \"plotly_white\", title = \"Comparing Price Points: implied win vs actual win\")\nfig.add_trace(\n    go.Scatter(\n        x = winRates.implied_chance, y = winRates.implied_chance, name = 'no bias', line_color = 'rgba(8,61,119, 0.3)'\n    )\n)\nfig.show(\"png\")\n\n# %% [markdown]\n# So they're all efficient? And indecernibly as efficient as one another?\n# \n# Well, to cut a long and possibly boring story short this isn't the right way to measure efficiency. What we're measure here is **bias**. All my scatter plot here tells me is if there's any systematic bias in the BSP, ie groups of BSPs that aren't well calibrated with actual outcomes. That is, for example, that perhaps randomly the group of horses that BSP around 2 don't happen to win 50% of the time maybe there was a sytemic bias that short favourites were underbet and these selections actually won 55% of the time. That would be a price bias in the BSP that someone could take advatange at just by looking at historical prices and outcomes alone.\n# \n# For and even simpler counter point: I could create a perfectly well calibrated estimate that assigned a single odds value to every horse which was the overall horse empirical win rate over our sample: 10.25% (which is merely a reflection of field sizes). This estimate would be unbiased, and would pass through our scatterplot method unscathed but would it be an efficient estimate? Clearly not.\n\n# %%\ndf.agg({'win': 'mean'})\n\n# %% [markdown]\n# Bias only tells us if there's a systematic way of exploiting the odds values themselves. I could have told you that this was unlikely but the scatterplot proves it.\n# \n# How else could we measure efficiency? I propose using the `logloss` metric.\n# \n# Let's calculate the logloss of the BSP\n\n# %%\n# Logloss ++++++++++++++++++++++++++++++++++++++++\n\nfrom sklearn.metrics import log_loss\n\n# Overall Logloss\n# _________________\n\nbspLoss = log_loss(\n    y_pred = 1 / accuracyFrame.query('type == \"bsp\"')['odds'],\n    y_true = accuracyFrame.query('type == \"bsp\"')['win']\n)\n\nprint(f'The overall logloss of the BSP is {round(bspLoss,4)}')\n\n# %% [markdown]\n# Ok what does this mean? Well nothing really. This metric won't tell you anything by itself it's just useful for relative comparisons. Let's plot the logloss of our geometric midpoint at our various timeslices.\n# \n\n# %%\n\n# Logloss at Different Time Points\n# _________________\n\naccuracyFrame.groupby('type', as_index = False).apply(lambda x: log_loss(y_pred=1/x['odds'],y_true=x['win'])).rename(columns = {None: 'logloss'}).sort_values('logloss')\n\n# %%\n# And in chart form\nfig = px.bar(\n    accuracyFrame.groupby('type', as_index = False).apply(lambda x: log_loss(y_pred=1/x['odds'],y_true=x['win'])).rename(columns = {None: 'logloss'}).sort_values('logloss', ascending = False),\n    x = \"type\",\n    y = \"logloss\",\n    template = \"plotly_white\",\n    title = \"Logloss Of Odds At Various Time Points\"\n)\nfig.update_yaxes(range=[.2755, .2765])\nfig.show(\"png\")\n\n# %% [markdown]\n# Now this is a cool graph. This is exactly like we would have intiuited. The market sharpens monotonically as we approach the market jump with the BSP being the most effiecient of all the prices!\n# \n# Hopefully you can now see the logical failing of measuring bias over market efficiency and it changes the way you think about your bet placement.\n# \n# Let's move on to what we're here for: is it possible to predict the BSP.\n\n# %% [markdown]\n# ## 2.4 Predicting the BSP\n# \n# Ok so I'm interested in finding the answer to the question: which estimate of BSP should i use when betting on the exchange and is it possible to beat the projected SP provided on the website and through the API?\n# \n# Well the first thing we should recognise about this projection is that it's cached. What does that mean? It means it only updated every 60 seconds. This suprised me when i first learned it and it was actually causing issues in my bet placement logic for the SP.\n# \n# Let's have a look at a selection to see how this works in practice\n\n# %%\n# Lets take a sample of a market and a selection\ndSlice = df.query('market_id == \"1.182394184\" and selection_id == \"39243409\"').dropna()\n\n# %%\ndef chartClosingPrices(d):\n\n    fig = px.line(\n        pd.melt(d[:-1][['sbsj', 'back_best', 'near_price']], id_vars = 'sbsj', var_name = 'price'), \n            x='sbsj', y='value',\n            color = 'price',\n            template='plotly_white',\n            title=\"Selection\",\n            labels = {\n                'sbsj': \"Seconds Before Scheduled Jump\"\n            }\n    )\n    fig.update_layout(font_family=\"Roboto\")\n    fig.add_trace(\n        go.Line(x = dSlice.sbsj, y = dSlice.bsp, name = 'BSP', line_color = 'rgba(8,61,119, 0.3)', mode = \"lines\")\n    )\n    fig['layout']['xaxis']['autorange'] = \"reversed\"\n    fig.show(\"png\")\n\nchartClosingPrices(dSlice)\n\n# %% [markdown]\n# The red line is the projected BSP, you can see that it's not very responsive. As the best back price comes in from ~3 to 2.9 leading up to the jump the projected SP doesn't move because it's cached. If you were relying on this number for something important and you were using it in that period you were using stale information and you'd be worse off for it. In this instance the final SP was 2.79 so you may have made the wrong betting decision.\n# \n# This is somewhat counter intuitive because the projected sp (the so called near price) should be a good estimate of the BSP because it synthetically runs the BSP algorithm on the current market state and produces and estimate, so you would think that it'd be a pretty good estimate.\n# \n# Let's widen our sample a bit and see how it performs across our entire sample. We'll slice the data at the exact scheduled off and see how accurate various price points are at predicting what the final BSP is. We'll use mean absolute error (MAE) as our error metric. We'll assess 6 price points:\n# \n# - The near price (projected sp)\n# - The far price (projected sp excluding limit orders)\n# - The best back price\n# - The best lay price\n# - The ladder midpoint price\n# - The geometric midpoint price\n\n# %%\n# Measurement\n# ________________________\n\nestimatesDf = df[df.sbsj == 0][['bsp', 'near_price', 'far_price', 'back_best', 'lay_best', 'geometric_mid_point', 'ladder_mid_point']]\n\n(\n    pd.melt(estimatesDf, id_vars = 'bsp', var_name = 'estimate')\n    .assign(error = lambda x: abs(x['value'] - x['bsp']) / x['bsp'])\n    .groupby('estimate', as_index=False)\n    .agg({'error': 'mean'})\n    .sort_values('error')\n)\n\n# %% [markdown]\n# So a bit surprisingly, in thoroughbred markets at the scheduled off your best to just use the current best back price as your estimate of the BSP. It significantly outperforms the projected SP and even some of our midpoint methods. \n# \n# Let's change the timeslice a little and take the very last moment before the market settles and see which performs best.\n\n# %%\nlastEstimatesDf = df.groupby(['market_id', 'selection_id'],as_index=False).nth(-1)[['bsp', 'near_price', 'far_price', 'back_best', 'lay_best', 'geometric_mid_point', 'ladder_mid_point']]\n\n(\n    pd.melt(lastEstimatesDf, id_vars = 'bsp', var_name = 'estimate')\n    .assign(error = lambda x: abs(x['value'] - x['bsp']) / x['bsp'])\n    .groupby('estimate', as_index=False)\n    .agg({'error': 'mean'})\n    .sort_values('error')\n)\n\n\n# %% [markdown]\n# First thing to notice is the estimates get a lot better than at the scheduled off, as we'd expect. A bit surprisingly the projected SP is still very weak due to the caching issue. In this scenario the geometric mid point beforms significantly better than the current best back price which suggests that as the late market is forming the back and lay spread with start converging to the fair price and eventual BSP. I personally use the geometric midpoint as my BSP estimate as it's a quick and easy metric that performs pretty well.\n# \n# What if you want more though? Is it possible to do better than these metrics? These simple price points use no information about what's in the BSP pools, surely if we used this information we'd be able to do better. Let's try to use machine learning to synthesise all this information at once.\n# \n# ### 2.3.4 Machine Learning\n# \n# We'll build a quite random forest model to estimate the BSP with current price and pool size information. This is a very simple application of machine learning so hopefully gives you an idea of its power without being too complex.\n# \n# Now we need an intelligent way of turning our pool and ladder information into a feature to insert into our model, how could we engineer this feature? Well what we'll do is calculate a WAP required to fill our pool stake on the back and lay side. What does that mean? Say we've got $200 sitting in the BSP back pool and $200 sitting on the top box of 2.5 on the back side, in this instance our WAP value would be exactly 2.5 cause we can fill it all at the top box. But if however, there was only $100 in the top box then we'd need to move down the ladder to fill the remaining $100 volume. Our feature will simulate this allocation logic and return the final weighted average price required to fill the total BSP pool. Here's the functions to do it on the back and lay side respectively:\n\n# %%\ndef wapToGetBack(pool, ladder):\n    price = ladder['p']\n    volume = ladder['v']\n    try:\n        indmax = min([ i for (i,j) in enumerate(cVolume) if j &amp;gt; pool ])+1\n    except:\n        indmax = len(volume)\n    return(round(sum([a * b for a, b in zip(price[:indmax], volume[:indmax])]) / sum(volume[:indmax]),4))\n\ndef wapToGetLay(liability_pool, ladder):\n    price = ladder['p']\n    volume = ladder['v']\n    liability = [(a-1) * b for a, b in zip(price, volume)]\n    cLiability = np.cumsum(liability)\n    try:\n        indmax = min([ i for (i,j) in enumerate(cLiability) if j &amp;gt; liability_pool ])+1\n    except:\n        indmax = len(volume)\n    return(round(sum([a * b for a, b in zip(price[:indmax], volume[:indmax])]) / sum(volume[:indmax]),4))\n\n\n# %% [markdown]\n# Now we'll set up our model matrix which will be the market state at the exact scheduled off. We'll also add our custom features.\n\n# %%\nmodel_matrix = df[['sbsj', 'atb_ladder', 'atl_ladder','bsp', 'traded_volume', 'near_price', 'far_price', 'bsp_back_pool_stake', 'bsp_lay_pool_liability', 'back_best', 'lay_best', 'geometric_mid_point', 'ladder_mid_point']]\n\n# Filter at scheduled jump\nmodel_matrix = model_matrix[model_matrix.sbsj == 0].dropna()\n\nmodel_matrix = (\n    model_matrix\n    .assign(wap_to_get_back_pool = lambda x: x.apply(lambda x: wapToGetBack(x.bsp_back_pool_stake, x.atb_ladder), axis=1))\n    .assign(wap_to_get_lay_pool = lambda x: x.apply(lambda x: wapToGetLay(x.bsp_lay_pool_liability, x.atl_ladder), axis=1))\n)\n\n# Drop other columns\nmodel_matrix.drop(columns = ['sbsj', 'atb_ladder', 'atl_ladder'], inplace = True)\n\nmodel_matrix.head(3)\n\n# %% [markdown]\n# Now the machine learning. Sklearn make this very simple, in our case it's a few lines only. We'll split our data into train and test sets and train a small random forrest to predict the BSP.\n\n# %%\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\n# Setup Train / Test\ntrain_features, test_features, train_labels, test_labels = train_test_split(model_matrix.drop(columns = ['bsp']), model_matrix['bsp'], test_size = 0.25)\n\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)\n\n# Instantiate Model\nrf = RandomForestRegressor(n_estimators = 100)\n\n# Train Model\nrf.fit(train_features, train_labels)\n\n# %% [markdown]\n# Let's check out our predictions on the test set (remember our model hasn't seen any of this data so it should be a true reflection on how we'd perform on some new races that would happen this afternoon say)\n\n# %%\n# Use the forest's predict method on the test data\npredicted_bsp = rf.predict(test_features)\npredicted_bsp\n\n# %% [markdown]\n# Seems reasonable. All well and good though is the prediction any good? Let's measure it using MAE in the same way as we did before.\n\n# %%\n# Let's test our estimate vs our others in the same way as before\n\ntestDf = test_features\ntestDf['bsp'] = test_labels\ntestDf['rf_bsp_prediction'] = predicted_bsp\n\n\n(\n    pd.melt(testDf[['bsp', 'near_price', 'far_price', 'back_best', 'lay_best', 'geometric_mid_point', 'ladder_mid_point', 'rf_bsp_prediction']], id_vars = 'bsp', var_name = 'estimate')\n    .assign(error = lambda x: abs(x['value'] - x['bsp']) / x['bsp'])\n    .groupby('estimate', as_index=False)\n    .agg({'error': 'mean'})\n    .sort_values('error')\n)\n\n# %% [markdown]\n# Nice that's significantly better than the best previous estimate at this time slice. To validate it further let's use the same model to predict the BSP using the market state 10 seconds after the scheduled jump instead of at the exact scheduled off. None of the rows (or samples) in this time slice have been seen by the model during the training step so it should provide a robust out of sample estimate of the models performance on unseen data.\n\n# %%\n# Validate it on a completely different time point - 10 seconds after scheduled jump\n\noutOfSample = df[['sbsj', 'atb_ladder', 'atl_ladder','bsp', 'traded_volume', 'near_price', 'far_price', 'bsp_back_pool_stake', 'bsp_lay_pool_liability', 'back_best', 'lay_best', 'geometric_mid_point', 'ladder_mid_point']]\n\noutOfSample = outOfSample[outOfSample.sbsj == -10].dropna()\n\noutOfSample = (\n    outOfSample\n    .assign(wap_to_get_back_pool = lambda x: x.apply(lambda x: wapToGetBack(x.bsp_back_pool_stake, x.atb_ladder), axis=1))\n    .assign(wap_to_get_lay_pool = lambda x: x.apply(lambda x: wapToGetLay(x.bsp_lay_pool_liability, x.atl_ladder), axis=1))\n)\n\n# Produce Predictions\noutofsamplebspprediction = rf.predict(outOfSample.drop(columns = ['bsp', 'sbsj', 'atb_ladder', 'atl_ladder']))\noutofsamplebspprediction\n\n# %%\noutOfSample['rf_bsp_prediction'] = outofsamplebspprediction\n\n(\n    pd.melt(outOfSample[['bsp', 'near_price', 'far_price', 'back_best', 'lay_best', 'geometric_mid_point', 'ladder_mid_point', 'rf_bsp_prediction']], id_vars = 'bsp', var_name = 'estimate')\n    .assign(error = lambda x: abs(x['value'] - x['bsp']) / x['bsp'])\n    .groupby('estimate', as_index=False)\n    .agg({'error': 'mean'})\n    .sort_values('error')\n)\n\n# %% [markdown]\n# Still significantly better on the out of sample set which is a really positive sign.\n# \n# ## 2.3.5 Next Steps\n# \n# To improve this model I'd include multiple time slices in the training sample and use the seconds before scheduled jump as a feature as I would estimate that the predictive dynamics of each of these features is dynamic and affected by how mature + how close to settlement the market is. \n# \n# To implement this model in your bet placement code you'd simply need to save the model object (some info about how to do this with sklearn can be found here [here](https://scikit-learn.org/stable/modules/model_persistence.html)). Your key challenge will be making sure you can produce the exact inputs you've created in this development process from the live stream or polling API responses, but if you've gotten this far it won't be a huge challenge for you.\n# \n# # 3.0 Conclusion\n# \n# I've taken you through a quick crash course in the Betfair BSP including:\n# \n# - What it is\n# - How it's created\n# - How it's traded on betfair Australian thoroughbred markets\n# - How efficient it is and a methodology for measuring its efficiency in different contexts\n# - The accuracy of the projected SP and how it compares with other estimates\n# - How to build your own custom projection that's better than anything available out of the box\n# \n# The analysis focused on thoroughbred markets but could easily be extended to other racing codes or sports markets that have BSP enabled. The custom SP projection methodology could be used for anything from staking your model more accurately or with some improvement maybe as part of a automated trading strategy.\n</code></pre>"},{"location":"tutorials/analysingAndPredictingBSP/#wisdom-of-the-crowd-analysing-understanding-bsp","title":"Wisdom of the crowd? Analysing &amp; understanding BSP","text":""},{"location":"tutorials/analysingAndPredictingBSP/#cheat-sheet","title":"Cheat sheet","text":"<ul> <li>This is presented as a Jupyter notebook as this format is interactive and lets you run snippets of code from wihtin the notebook. To use this functionality you'll need to download a copy of the <code>ipynb</code> file locally and open it in a text editor (i.e. VS code).</li> <li>If you're looking for the complete code head to the bottom of the page or download the script from Github.</li> <li>To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code), make sure you've navigated in the terminal to the folder you've saved the script in and then type <code>py main.py</code> (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. </li> <li>Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site. We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. </li> <li>We're using the <code>betfairlightweight</code> package to do the heavy lifting</li> <li>We've also posted the completed code logic on the <code>betfair-downunder</code> Github repo.</li> </ul>"},{"location":"tutorials/analysingAndPredictingBSP/#00-setup","title":"0.0 Setup","text":""},{"location":"tutorials/analysingAndPredictingBSP/#01-importing-libraries","title":"0.1 Importing libraries","text":"<p>Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language.</p> <p>Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer locally where you want to store the intermediate data files.</p> <p>You'll also need <code>betfairlightweight</code> which you can install with something like <code>pip install betfairlightweight</code>.</p>"},{"location":"tutorials/analysingAndPredictingBSP/#02-context","title":"0.2 Context","text":"<p>The Betfair Starting Price (BSP) is a starting price product offered by Betfair (on large enough markets, almost all racing and some sports) that gives customers a chance to back or lay any selection at a \"fair\" price. Without getting too complex too quickly, the BSP allows you lock in a bet at any time after the market is opened and for as much stake as you can afford. The BSP is a good option for many different segments of customers:</p> <ul> <li>Recreational punters that don't have a particular strategy for trying to get the best odds can lock in a price that is (in the aggregate) a lot better than what they'd get at a corporate book or they'd get by taking limit bets early in a market's trading</li> <li>Automated customers that don't want the hassle of managing live market trading can implement automated strategies a lot easier whilst also protecting them from edge cases like race reschedules </li> <li>Is perfect for simply backtesting fundamental models as it's a resilient and robust single price</li> </ul> <p>There\u2019s a lot of background reading available on BSP if you\u2019re keen to understand it better. Here are some resources that might be interesting if you want to learn more: </p> <ul> <li>Overview</li> <li>FAQ summary</li> <li>Detailed explanation</li> <li>Worked examples of the reconciliation process</li> <li>How the projected odds work</li> </ul> <p>Despite it being a good option for a lot of customers it's also a fairly controversial topic for some other types of customers. Some people firmly believe that the BSP on big markets reflects the \"true chance\" of a selection so betting it is a fool's errand that will simply lose you commission over the long run. You might have heard a version of this story before: given the large pool sizes, the 0% overround, the settlement at the exact moment the market is suspended the BSP perfectly synthesises all available public information and demand and arrives at a true fair odds. Some will attempt to prove this to you by showing you a predicted chance vs observed win rate scatterplot which shows a perfect correlation between chance implied by the BSP and a horse's true chance. Whilst I don't disagree that the BSP is a very strong estimate of a selections chance it's pretty obviously not perfect. </p> <p>Furthermore, it presents some other tricky challenges to use in practical situations. It's not knowable perfectly before it's the exact moment of market suspension so many model or strategy builders make the mistake of unknowingly leaking it into their preplay model development or their theoretical staking calculations. Where the final number will land is actually another source of uncertainty in your processes which presents anothing forecasting / predictive modelling application as I'll explore later in this piece. I'll take you through how I'd measure the accuracy of the BSP, show you how it's traded on the exchange, and take you through a host of methods of estimating the BSP and build a custom machine learning approach that's better than each of them.</p>"},{"location":"tutorials/analysingAndPredictingBSP/#03-the-algorithm","title":"0.3 The Algorithm","text":"<p>The actual logic of how Betfair arrives at the final BSP number is quite complex and for a few reasons you won't be able to perfectly replicate it at home. However, the general gist of the BSP reconciliation algorithm that is executed just as the market suspended goes something like:</p> <ul> <li>The algorithm combines 4 distinct groups of open bets for a given selection: <ul> <li>Non-price-limited BSP orders on both the back and lay side (<code>market_on_close</code> orders)</li> <li>Price-limited orders on both the back and lay side (<code>limit_on_close</code> orders)</li> <li>All non-filled open lay orders</li> <li>All non-filled open back orders </li> </ul> </li> <li>It then combines them all together, passes a sophisticated balancing algorithm over the top of them and arrives at a single fair price for the BSP that balances the demand on either side of the ledger</li> </ul>"},{"location":"tutorials/analysingAndPredictingBSP/#04-this-example","title":"0.4 This Example","text":"<p>For this exercise we'll again take advantage of the betfair historical stream json files. The slice of Betfair markets I'll be analysing is all thoroughbred races over July 2020 - June 2021. </p> <p>As an aside the projected BSP number you see on the betfair website isn't collected inside betfair's own internal database of orders, so any custom data request you may be able to get as a VIP won't include this number. So if you were planning to include it in any kind of projection or bet placement logic operation you were making the only way to anlayse it historically is to mine these data files. Another good reason to learn the skills to do so! </p>"},{"location":"tutorials/analysingAndPredictingBSP/#10-data","title":"1.0 Data","text":"<p>Like the previous tutorial we won't be able to collapse the stream data down into a single row per runner because I'm interested in analysing how the projected BSP moves late in betfair markets. I'm also interested in plotting the efficiency of certain odds values at certain distinct time points leading up the the races so I need multiple records per runner.</p> <p>Like in the previous tutorial I'll split out the selection metadata, BSP and win flag values as a separate data file to reduce the size of the data-files extracted for this analysis.</p> <p>For the preplay prices dataset I'll:</p> <ul> <li>Start extraction at 2 mins before the scheduled off</li> <li>Extract prices every 10 seconds thereafter until the market is suspended</li> <li>I'll also extract the final market state the instant before the market is suspended</li> </ul>"},{"location":"tutorials/analysingAndPredictingBSP/#11-sourcing-data","title":"1.1 Sourcing Data","text":"<p>First you'll need to source the stream file TAR archive files. I'll be analysing 12 months of Australian thoroughbred Pro files. Ask automation@betfair.com.au for more info if you don't know how to do this. Once you've gotten access, download them to your computer and store them together in a folder.</p>"},{"location":"tutorials/analysingAndPredictingBSP/#12-utility-functions","title":"1.2 Utility functions","text":"<p>First like always we'll need some general utility functions that you may have seen before:</p>"},{"location":"tutorials/analysingAndPredictingBSP/#13-selection-metadata","title":"1.3 Selection Metadata","text":"<p>Given that the detailed price data will have so many records we will split out the selection metadata (including the selection win outcome flag and the bsp) into it's own dataset much you would do in a relational database to manage data volumes.</p>"},{"location":"tutorials/analysingAndPredictingBSP/#14-preplay-prices-and-projections","title":"1.4 Preplay Prices and Projections","text":"<p>In this set of preplay prices I'm interested in many of the same fields as we've extracted in previous tutorials as well as fields relating to the current state of the BSP.</p> <p>These objects sit under the <code>sp</code> slot within the returned <code>runner</code> object. The fields we'll extract are:</p> <ul> <li>The so called \"near price\"<ul> <li>The near price is the projected SP value you can see on the website</li> <li>It includes both bets already placed into the SP pools as well as open limit orders to estimate what the final BSP value will be</li> </ul> </li> <li>The so called \"far price\"<ul> <li>This is the same as the near price except it excludes limit orders on the exchange</li> <li>This makes it fairly redundant value and we'll see how poor of an estimator it is a bit later</li> </ul> </li> <li>The volume currently bet into the BSP back pool</li> <li>The liability currently laid into the BSP lay pool</li> </ul> <p>We'll also extract the top 5 rungs of the available to back and available to lay ladders as well as the traded volume of limit bets.</p> <p>It's worth noting that I am discarding some key information about the BSP pools that I could have extracted if I wanted to. The current SP bets are laid out in a way that I could split out <code>limit_on_close</code> as well as <code>market_on_close</code> sp bets but I've rolled everything together in SP stake on the back side and sp liability on the lay side. This is just to reduce complexity of this article but including it would increase the predictive power of the BSP model in the final step.</p>"},{"location":"tutorials/analysingAndPredictingBSP/#20-analysis","title":"2.0 Analysis","text":"<p>First step let's boot up the datasets we extracted in the previous steps and take a look at what we've managed to extract from the raw stream files.</p>"},{"location":"tutorials/analysingAndPredictingBSP/#21-load-inspect","title":"2.1 Load &amp; Inspect","text":"<p>First we have the high-level selection metadata as we have already seen in other tutorials</p>"},{"location":"tutorials/analysingAndPredictingBSP/#22-transform-and-assemble","title":"2.2 Transform and Assemble","text":"<p>We have our 2 core datasets, but we'd prefer to work with one now. We'd also like to add some key columns that will be reused throughout our analysis so we'll add those now too.</p>"},{"location":"tutorials/analysingAndPredictingBSP/#221-mid-points","title":"2.2.1 Mid points","text":"<p>The first semi-interesting thing we'll do in this analysis is add selection mid-points to our dataset. Eventually we're going to be interested in estimating the BSP and measuring the efficiency of certain prices at various points leading up to the race. </p> <p>Betfair markets work like all markets with bids and spreads. The market equilibrium forms around the best price offered on either side of the market to back or to lay. These top prices each have some inherent advantage built into it for the offerer. For example in early markets the best offers on either side of the market might be really wide (say 1.80 as a best back and 2.50 as a best lay). Given the price discovery process is still immature each bidder gets a large premium, backed into their offer price, which compensates them for providing betting opportunities with little to no information provided from other market participants. This spread will naturally get tighter and tighter as the market matures and more participants seek to get volume down and must be more and more competitive. But what's the price \"equilibrium\" in each case?</p> <p>Well it's up to you but I'll provide you two ways of finding the central mid-point of a bid-ask spread on betfair markets. The problem we're solving for here is the non-linearity of prices in odds space. We have some intuition for this: when we see a market spread of 10-100 in early trading we have an understanding that the true midpoint of this market is somewhere around 25-35 not the 55 you'd get if you simply took the (arithmetic) mean of those two numbers.</p> <p>Two techniques for accounting for that non-linearity are as follows.</p> <p>Ladder Midpoint</p> <p>The ladder midpoint method takes advantage of the fact that the Betfair price ladder itself accounts for the nonlinearity of prices in odds space. The method calculated the difference in number of rungs on the Betfair ladder, halves it, and shifts the best back or lay price that number of rungs towards the centre. This will generally provide a much better idea of the market midpoint than a simple arithmetic mean of the two prices.</p> <p>Geometric Mean</p> <p>Unfortunately the ladder method is a little computationally expensive. A good approximation for this approach is to take the geometric mean of the best back and best lay values. The geometric mean is a special kind of mean that you may have never used before that is more appropriate for purposes like this. It is calculated like: <code>sqrt(x1 * x2 * ...)</code>. This number will also provide a much better estimate of the market midpoint than the simple arithmetic mean.</p> <p>The latter calculation is trivial. The former requires a suite of Betfair tick arithmetic functions that I'll put below. It may seem like overkill for this exercise (and it is) but hopefully these functions might be of use to you for other purposes.</p>"},{"location":"tutorials/analysingAndPredictingBSP/#23-analysing-the-bsp","title":"2.3 Analysing the BSP","text":"<p>Before we embark on our predictive exercise let's analyse the BSP to get a feel for it as an entity.</p>"},{"location":"tutorials/analysingAndPredictingBSP/#231-volumes","title":"2.3.1 Volumes","text":"<p>Ever wondered how much volume is traded on the BSP? How does it compare to limit bets? Well with our parsed stream data we can answer those questions! Now the BSP volume will be the bigger of the BSP back stake and the lay stake (which you can infer by the final BSP and the total lay liability).</p>"},{"location":"tutorials/analysingAndPredictingBSP/#232-efficiency","title":"2.3.2 Efficiency?","text":"<p>Now you may have heard this story before: you can't beat the BSP it's too efficient! I'm not sure people really have a firm idea about what they're talking about when they say this.</p> <p>Typically what you'll see in a discussion about efficiency is the predicted vs observed scatterplot. Let's see if we can reproduce this chart.</p> <p>First let's assemble a dataframe that we can use for this chart as well as others. What we'll do is we'll extract the BSP and a price value at 5 different slices before the race starts. We could chose any price point (we'll analyse the difference between them in a subsequent step) but for this section I'm going to take the preplay market estimate as the geometric market midpoint (you'll have to trust me for now that this is a sensible decision).</p>"},{"location":"tutorials/analysingAndPredictingBSP/#24-predicting-the-bsp","title":"2.4 Predicting the BSP","text":"<p>Ok so I'm interested in finding the answer to the question: which estimate of BSP should I use when betting on the exchange and is it possible to beat the projected SP provided on the website and through the API?</p> <p>Well the first thing we should recognise about this projection is that it's cached. What does that mean? It means it only updated every 60 seconds. This suprised me when I first learned it and it was actually causing issues in my bet placement logic for the SP.</p> <p>Let's have a look at a selection to see how this works in practice</p>"},{"location":"tutorials/analysingAndPredictingBSP/#234-machine-learning","title":"2.3.4 Machine Learning","text":"<p>We'll build a quick random forest model to estimate the BSP with current price and pool size information. This is a very simple application of machine learning so hopefully gives you an idea of its power without being too complex.</p> <p>Now we need an intelligent way of turning our pool and ladder information into a feature to insert into our model, how could we engineer this feature? Well what we'll do is calculate a WAP required to fill our pool stake on the back and lay side. What does that mean? Say we've got \\$200 sitting in the BSP back pool and \\$200 sitting on the top box of 2.5 on the back side, in this instance our WAP value would be exactly 2.5 cause we can fill it all at the top box. But if however, there was only $100 in the top box then we'd need to move down the ladder to fill the remaining \\$100 volume. Our feature will simulate this allocation logic and return the final weighted average price required to fill the total BSP pool. Here's the functions to do it on the back and lay side respectively:</p>"},{"location":"tutorials/analysingAndPredictingBSP/#235-next-steps","title":"2.3.5 Next Steps","text":"<p>To improve this model I'd include multiple time slices in the training sample and use the seconds before scheduled jump as a feature as I would estimate that the predictive dynamics of each of these features is dynamic and affected by how mature + how close to settlement the market is. </p> <p>To implement this model in your bet placement code you'd simply need to save the model object (some info about how to do this with sklearn can be found here here). Your key challenge will be making sure you can produce the exact inputs you've created in this development process from the live stream or polling API responses, but if you've gotten this far it won't be a huge challenge for you.</p>"},{"location":"tutorials/analysingAndPredictingBSP/#30-conclusion","title":"3.0 Conclusion","text":"<p>I've taken you through a quick crash course in the Betfair BSP including:</p> <ul> <li>What it is</li> <li>How it's created</li> <li>How it's traded on betfair Australian thoroughbred markets</li> <li>How efficient it is and a methodology for measuring its efficiency in different contexts</li> <li>The accuracy of the projected SP and how it compares with other estimates</li> <li>How to build your own custom projection that's better than anything available out of the box</li> </ul> <p>The analysis focused on thoroughbred markets but could easily be extended to other racing codes or sports markets that have BSP enabled. The custom SP projection methodology could be used for anything from staking your model more accurately or with some improvement maybe as part of a automated trading strategy.</p>"},{"location":"tutorials/analysingAndPredictingBSP/#31-over-to-you","title":"3.1 Over to you","text":"<p>We're planning on writing some more tutorials to help make it easier to work with the JSON data sets. If there are particular examples or data sets you'd like to see us walk through please reach out.</p> <p>&gt;Community support - There's a really active <code>betcode</code> Slack community that's a great place to go to ask questions about the library and get support from other people who are also working in the space</p>"},{"location":"tutorials/analysingAndPredictingBSP/#32-complete-code","title":"3.2 Complete code","text":"<p>Run the code from your ide by using <code>py &lt;filename&gt;.py</code>, making sure you amend the path to point to your input data. </p> <p>Download from Github </p>"},{"location":"tutorials/analysingAndPredictingBSP/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/","title":"Do they know? Analysing Betfair market formation & market movements","text":"<p>This tutorial was written by Tom Bishop and was originally published on Github. It is shared here with his permission.</p> <p>This tutorial follows on logically from the Automated betting angles in Python we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at that tutorial before diving into this one.</p> <p>As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements!</p> <p>This article was written more than 2 years ago and some packages used here will have changed since the article was written. Continue at your peril</p> <pre><code>import pandas as pd\nimport numpy as np\nimport requests\nimport os\nimport re\nimport csv\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport math\nimport logging\nimport yaml\nimport csv\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\nimport ast\n\nfrom datetime import date, timedelta\nfrom unittest.mock import patch\nfrom typing import List, Set, Dict, Tuple, Optional\nfrom itertools import zip_longest\nimport betfairlightweight\nfrom betfairlightweight import StreamListener\nfrom betfairlightweight.resources.bettingresources import (\n    PriceSize,\n    MarketBook\n)\n</code></pre> <pre><code># General Utility Functions\n# _________________________________\n\ndef as_str(v) -&amp;gt; str:\n    return '%.2f' % v if type(v) is float else v if type(v) is str else ''\n\ndef split_anz_horse_market_name(market_name: str) -&amp;gt; (str, str, str):\n    parts = market_name.split(' ')\n    race_no = parts[0] # return example R6\n    race_len = parts[1] # return example 1400m\n    race_type = parts[2].lower() # return example grp1, trot, pace\n    return (race_no, race_len, race_type)\n\n\ndef load_markets(file_paths):\n    for file_path in file_paths:\n        print(file_path)\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n\n    return None\n\ndef slicePrice(l, n):\n    try:\n        x = l[n].price\n    except:\n        x = \"\"\n    return(x)\n\ndef sliceSize(l, n):\n    try:\n        x = l[n].size\n    except:\n        x = \"\"\n    return(x)\n\ndef pull_ladder(availableLadder, n = 5):\n        out = {}\n        price = []\n        volume = []\n        if len(availableLadder) == 0:\n            return(out)        \n        else:\n            for rung in availableLadder[0:n]:\n                price.append(rung.price)\n                volume.append(rung.size)\n\n            out[\"p\"] = price\n            out[\"v\"] = volume\n            return(out)\n</code></pre> <p>Slicing the tracks we want we'll just adjust the market filter function used before to include some logic on the venue name</p> <pre><code>def filter_market(market: MarketBook) -&amp;gt; bool: \n\n    d = market.market_definition\n    track_filter = ['Bendigo', 'Sandown', 'Flemington', 'Caulfield', 'Moonee Valley']\n\n    return (d.country_code == 'AU' \n        and d.venue in track_filter\n        and d.market_type == 'WIN' \n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n</code></pre> <pre><code>def final_market_book(s):\n\n    with patch(\"builtins.open\", lambda f, _: f):\n\n        gen = s.get_generator()\n\n        for market_books in gen():\n\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    return(None)\n\n            for market_book in market_books:\n\n                last_market_book = market_book\n\n        return(last_market_book)\n\ndef parse_final_selection_meta(dir, out_file):\n\n    with open(out_file, \"w+\") as output:\n\n        output.write(\"market_id,selection_id,venue,market_time,selection_name,win,bsp\\n\")\n\n        for file_obj in load_markets(dir):\n\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            last_market_book = final_market_book(stream)\n\n            if last_market_book is None:\n                continue \n\n            # Extract Info ++++++++++++++++++++++++++++++++++\n\n            runnerMeta = [\n                {\n                    'selection_id': r.selection_id,\n                    'selection_name': next((rd.name for rd in last_market_book.market_definition.runners if rd.selection_id == r.selection_id), None),\n                    'selection_status': r.status,\n                    'win': np.where(r.status == \"WINNER\", 1, 0),\n                    'sp': r.sp.actual_sp\n                }\n                for r in last_market_book.runners \n            ]\n\n            # Return Info ++++++++++++++++++++++++++++++++++\n\n            for runnerMeta in runnerMeta:\n\n                if runnerMeta['selection_status'] != 'REMOVED':\n\n                    output.write(\n                        \"{},{},{},{},{},{},{}\\n\".format(\n                            str(last_market_book.market_id),\n                            runnerMeta['selection_id'],\n                            last_market_book.market_definition.venue,\n                            last_market_book.market_definition.market_time,\n                            runnerMeta['selection_name'],\n                            runnerMeta['win'],\n                            runnerMeta['sp']\n                        )\n                    )\n</code></pre> <pre><code>selection_meta = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\"\nstream_files = glob.glob(\"[PATH TO STREAM FILES]*.tar\")\ntrading = betfairlightweight.APIClient(username = \"username\", password = \"password\", app_key=\"app_key\")\nlistener = StreamListener(max_latency=None)\n\nprint(\"__ Parsing Selection Metadata ___ \")\n# parse_final_selection_meta(stream_files, selection_meta)\n</code></pre> <pre>\n<code>__ Parsing Selection Metadata ___ \n</code>\n</pre> <pre><code>def loop_preplay_prices(s, o):\n\n    with patch(\"builtins.open\", lambda f, _: f):\n\n        gen = s.get_generator()\n\n        marketID = None\n        tradeVols = None\n        time = None\n\n        for market_books in gen():\n\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    break\n\n            for market_book in market_books:\n\n                # Time Step Management ++++++++++++++++++++++++++++++++++\n\n                if marketID is None:\n\n                    # No market initialised\n                    marketID = market_book.market_id\n                    time =  market_book.publish_time\n\n                elif market_book.inplay:\n\n                    # Stop once market goes inplay\n                    break\n\n                else:\n\n                    seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n\n                    if seconds_to_start &amp;gt; log1_Start:\n\n                        # Too early before off to start logging prices\n                        continue\n\n                    else:\n\n                        # Update data at different time steps depending on seconds to off\n                        wait = np.where(seconds_to_start &amp;lt;= log2_Start, log2_Step, log1_Step)\n\n                        # New Market\n                        if market_book.market_id != marketID:\n                            marketID = market_book.market_id\n                            time =  market_book.publish_time\n                        # (wait) seconds elapsed since last write\n                        elif (market_book.publish_time - time).total_seconds() &amp;gt; wait:\n                            time = market_book.publish_time\n                        # fewer than (wait) seconds elapsed continue to next loop\n                        else:\n                            continue\n\n                # Execute Data Logging ++++++++++++++++++++++++++++++++++\n\n                for runner in market_book.runners:\n\n                    try:\n                        atb_ladder = pull_ladder(runner.ex.available_to_back, n = 10)\n                        atl_ladder = pull_ladder(runner.ex.available_to_lay, n = 10)\n                    except:\n                        atb_ladder = {}\n                        atl_ladder = {}\n\n                    # Calculate Current Traded Volume + Tradedd WAP\n                    limitTradedVol = sum([rung.size for rung in runner.ex.traded_volume])\n                    if limitTradedVol == 0:\n                        limitWAP = \"\"\n                    else:\n                        limitWAP = sum([rung.size * rung.price for rung in runner.ex.traded_volume]) / limitTradedVol\n                        limitWAP = round(limitWAP, 2)\n\n                    o.writerow(\n                        (\n                            market_book.market_id,\n                            runner.selection_id,\n                            market_book.publish_time,\n                            limitTradedVol,\n                            limitWAP,\n                            runner.last_price_traded or \"\",\n                            str(atb_ladder).replace(' ',''), \n                            str(atl_ladder).replace(' ','')\n                        )\n                    )\n\n\n\n\ndef parse_preplay_prices(dir, out_file):\n\n    with open(out_file, \"w+\") as output:\n\n        writer = csv.writer(\n            output, \n            delimiter=',',\n            lineterminator='\\r\\n',\n            quoting=csv.QUOTE_ALL\n        )\n\n        writer.writerow((\"market_id\",\"selection_id\",\"time\",\"traded_volume\",\"wap\",\"ltp\",\"atb_ladder\",\"atl_ladder\"))\n\n        for file_obj in load_markets(dir):\n\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            loop_preplay_prices(stream, writer)\n</code></pre> <pre><code>preplay_price_file =  \"[OUTPUT PATH TO CSV FOR PREPLAY PRICES]\"\nstream_files = glob.glob(\"[PATH TO STREAM FILES]*.tar\")\n\n\nlog1_Start = 60 * 30 # Seconds before scheduled off to start recording data for data segment one\nlog1_Step = 60       # Seconds between log steps for first data segment\nlog2_Start = 60 * 10  # Seconds before scheduled off to start recording data for data segment two\nlog2_Step = 1        # Seconds between log steps for second data segment\n\nprint(\"__ Parsing Detailed Preplay Prices ___ \")\n# parse_preplay_prices(stream_files, preplay_price_file)\n</code></pre> <pre>\n<code>__ Parsing Detailed Preplay Prices ___ \n</code>\n</pre> <pre><code>selection = pd.read_csv(\"[PATH TO METADATA FILE]\" , dtype={'market_id': object, 'selection_id': object}, parse_dates = ['market_time'])\nselection.head(3)\n</code></pre> market_id selection_id venue market_time selection_name win bsp 0 1.179904683 38629713 Bendigo 2021-03-02 03:00:00 1. Bigdoorprize 0 19.37 1 1.179904683 3593031 Bendigo 2021-03-02 03:00:00 2. Cornucopia 0 6.64 2 1.179904683 38629714 Bendigo 2021-03-02 03:00:00 3. Danejararose 0 21.66 <p>And we have the detailed preplay price data for these markets + selections:</p> <pre><code>prices = pd.read_csv(\n    \"[PATH TO PRICES FILE]\", \n    quoting=csv.QUOTE_ALL,\n    dtype={'market_id': 'string', 'selection_id': 'string', 'atb_ladder': 'string', 'atl_ladder': 'string'},\n    parse_dates=['time']\n)\nprices.head(3)\n</code></pre> market_id selection_id time traded_volume wap ltp atb_ladder atl_ladder 0 1.179904683 38629713 2021-03-01 05:15:09.480 0.0 NaN NaN {} {} 1 1.179904683 3593031 2021-03-01 05:15:09.480 0.0 NaN NaN {} {} 2 1.179904683 38629714 2021-03-01 05:15:09.480 0.0 NaN NaN {} {} <p>Now it's important to observe how much data we have here.</p> <pre><code>prices.shape\n</code></pre> <pre>\n<code>(7774392, 8)</code>\n</pre> <p>We've got 7 million rows of price data here just for races at 5 thoroughbred tracks over the last year. Now it's not really \"big data\" in the sense you might have heard before but it's certainly a lot of rows and we'll have to think about the performance of our code a little bit more than we would if we were dealining with 1 row per selection style datasets.</p> <p>We need pandas to correctly interpret the dictionary columns as dictionaries so we'll run this code:</p> <pre><code># To get pandas to correctly recognise the ladder columns as dictionaries\nprices['atb_ladder'] = [ast.literal_eval(x) for x in prices['atb_ladder']]\nprices['atl_ladder'] = [ast.literal_eval(x) for x in prices['atl_ladder']]\n</code></pre> <p>Now we'll join the 2 data sets together to form a nice normalised dataframe:</p> <pre><code># Simple join on market and selection_id initially\ndf = selection.merge(prices, on = ['market_id', 'selection_id'])\ndf.head(3)\n</code></pre> market_id selection_id venue market_time selection_name win bsp time traded_volume wap ltp atb_ladder atl_ladder 0 1.179904683 38629713 Bendigo 2021-03-02 03:00:00 1. Bigdoorprize 0 19.37 2021-03-01 05:15:09.480 0.00 NaN NaN {} {} 1 1.179904683 38629713 Bendigo 2021-03-02 03:00:00 1. Bigdoorprize 0 19.37 2021-03-02 02:30:00.552 15.95 15.44 22.0 {'p': [18.5, 18, 17.5, 17, 16, 15, 14, 13.5, 1... {'p': [24, 25, 29, 30, 38, 55, 65, 70, 90, 140... 2 1.179904683 38629713 Bendigo 2021-03-02 03:00:00 1. Bigdoorprize 0 19.37 2021-03-02 02:31:06.716 15.95 15.44 22.0 {'p': [19, 18.5, 18, 17, 16, 15, 14, 13.5, 13,... {'p': [25, 29, 30, 38, 55, 65, 70, 90, 140, 24... <pre><code>df = (\n    df\n    .assign(back_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atb_ladder']])\n    .assign(lay_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atl_ladder']])\n    .assign(back_vwap = lambda x: [np.nan if d.get('p') is None else round(sum([a*b for a,b in zip(d.get('p')[0:3],d.get('v')[0:3])]) / sum(d.get('v')[0:3]),3) for d in x['atb_ladder']])\n    .assign(lay_vwap = lambda x: [np.nan if d.get('p') is None else round(sum([a*b for a,b in zip(d.get('p')[0:3],d.get('v')[0:3])]) / sum(d.get('v')[0:3]),3) for d in x['atl_ladder']])\n    .assign(seconds_before_scheduled_jump = lambda x: round((x['market_time'] - x['time']).dt.total_seconds()))\n    .query('seconds_before_scheduled_jump &amp;lt; 1800 and seconds_before_scheduled_jump &amp;gt; -120')\n)\n</code></pre> <pre><code>traded_volume_values = df[['market_id', 'selection_id', 'venue', 'bsp', 'seconds_before_scheduled_jump', 'traded_volume']]\nall_sbj = pd.DataFrame(data = {'seconds_before_scheduled_jump': traded_volume_values.seconds_before_scheduled_jump.unique()}).assign(join = 1)\ntraded_volume_explode = traded_volume_values[['market_id', 'selection_id', 'venue', 'bsp']].drop_duplicates().assign(join = 1).merge(all_sbj).drop('join', 1)\ntraded_volume_df = traded_volume_explode.merge(traded_volume_values, how = \"left\")\ntraded_volume_df = traded_volume_df.sort_values(['market_id', 'selection_id', 'venue', 'seconds_before_scheduled_jump'], ascending=[True, True, True, False])\ntraded_volume_df.update(traded_volume_df.groupby(['market_id', 'selection_id', 'venue'])[['seconds_before_scheduled_jump', 'traded_volume']].ffill().fillna(0))\n</code></pre> <pre>\n<code>/tmp/ipykernel_327971/677704215.py:3: FutureWarning:\n\nIn a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n\n</code>\n</pre> <pre><code># Group by market, sum volume over selections at a given time, average over times for total\ndef chunkSBJ(sbj):\n    if sbj &amp;lt; 600:\n        return(sbj)\n    else:\n        return(int(math.floor(sbj / 60) * 60))\n\ntradedVolumes_1 = (\n    traded_volume_df\n    .groupby([\"market_id\", \"seconds_before_scheduled_jump\"], as_index = False)\n    .agg({'traded_volume': 'sum'})\n    .assign(seconds_before_scheduled_jump = lambda x: x['seconds_before_scheduled_jump'].apply(chunkSBJ))\n    .groupby(\"seconds_before_scheduled_jump\", as_index = False)\n    .agg({'traded_volume': 'mean'})\n    .sort_values('seconds_before_scheduled_jump', ascending=False)\n)\n\nfig = px.area(\n    tradedVolumes_1, \n    x='seconds_before_scheduled_jump', y='traded_volume', \n    template='plotly_white',\n    title=\"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\",\n    labels = {\n        'seconds_before_scheduled_jump': \"Seconds Before Scheduled Jump\",\n        \"traded_volume\": \"Average Cumulative Traded Volume\"\n    }\n    #subtitle = \"Top 5 Biggest Vic Track Sample\"\n)\nfig.update_layout(font_family=\"Roboto\")\nfig['layout']['xaxis']['autorange'] = \"reversed\"\nfig.show(\"png\")\n</code></pre> <p>The discontinuity in the chart highlights the switch point between the two time granularities that we extracted from the stream. Pre 600 seconds (10 minutes) before the scheduled off I can plot 1 data point per minute and after I'm plotting 60 data points per minute.</p> <p>The traded volume chart looks like an exponential chart: the total traded volume doubles from 10 minutes out to 4 minutes out, then it doubles again between then and 1 minute out, then nearly doubling again in the last minute and a bit of trading. Even a simple visual like this can help you with your bet placement on Betfair markets. For example if you're planning to get large volumes down on Betfair thoroughbred markets it's probably best to view prices &gt;10 minutes out with a skeptical eye even if the market is tight - because you won't find the requisite lay volume that early as the majority of traded volume happens in the last 2-5 minutes of trading.</p> <p>Now like most analysis the average is definitely hiding lots of interesting things about this data. Let's split out this data by our 5 tracks:</p> <pre><code>tradedVolumes_2 = (\n   traded_volume_df\n    .assign(seconds_before_scheduled_jump_chunk = lambda x: x['seconds_before_scheduled_jump'].apply(chunkSBJ))\n    .groupby([\"market_id\", \"venue\", \"seconds_before_scheduled_jump_chunk\", \"seconds_before_scheduled_jump\"], as_index = False)\n    .agg({'traded_volume': 'sum'})\n    .groupby([\"venue\", \"seconds_before_scheduled_jump_chunk\"], as_index = False)\n    .agg({'traded_volume': 'mean'})\n    .sort_values('seconds_before_scheduled_jump_chunk', ascending=False)\n)\n\nfig_2 = px.line(\n    tradedVolumes_2, \n    x='seconds_before_scheduled_jump_chunk', y='traded_volume',  color='venue',\n    template='plotly_white',\n    title=\"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\",\n    labels = {\n        'seconds_before_scheduled_jump_chunk': \"Seconds Before Scheduled Jump\",\n        \"traded_volume\": \"Average Cumulative Traded Volume\"\n    }\n)\nfig_2.update_layout(font_family=\"Roboto\")\nfig_2['layout']['xaxis']['autorange'] = \"reversed\"\nfig_2.show(\"png\")\n</code></pre> <p>As expected an average Flemington race trades nearly 500k whereas an average Bendigo race trades only ~120k volume.</p> <p>How about if we split our selections by odds range. Intuitively we know that odds-on horses will trade significantly more volume than a \\$50 shot but let's visualise the difference.</p> <p>We'll chunk the BSP of each horse into 5 groups: - Odds on (&lt;50% chance of winning) - Odds between 2 and 5 - Odds between 5 and 15 - Odds between 15 and 50 - Odds 50+</p> <pre><code>def chunkBsp(bsp):\n    if bsp &amp;lt;= 2:\n        return(\"1. Odds On\")\n    elif bsp &amp;lt;= 5:\n        return(\"2. (2, 5]\")\n    elif bsp &amp;lt;= 15:\n        return(\"3. (5, 15]\")\n    elif bsp &amp;lt;= 50:\n        return(\"4. (15, 50]\")\n    else:\n        return(\"5. 50+\")\n\n# Group by odds range\ntradedVolumes_3 = (\n    traded_volume_df\n    .assign(seconds_before_scheduled_jump_chunk = lambda x: x['seconds_before_scheduled_jump'].apply(chunkSBJ))\n    .assign(bsp = lambda x: x['bsp'].apply(chunkBsp))\n    .groupby([\"market_id\", \"bsp\", \"seconds_before_scheduled_jump_chunk\", \"seconds_before_scheduled_jump\"], as_index = False)\n    .agg({'traded_volume': 'sum'})\n    .groupby([\"bsp\", \"seconds_before_scheduled_jump_chunk\"], as_index = False)\n    .agg({'traded_volume': 'mean'})\n)\n\nfig_3 = px.line(\n    tradedVolumes_3, \n    x='seconds_before_scheduled_jump_chunk', y='traded_volume', color='bsp',\n    template='plotly_white',\n    title=\"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\",\n    labels = {\n        'seconds_before_scheduled_jump_chunk': \"Seconds Before Scheduled Jump\",\n        \"traded_volume\": \"Average Cumulative Traded Volume\"\n    }\n)\nfig_3.update_layout(font_family=\"Roboto\")\nfig_3['layout']['xaxis']['autorange'] = \"reversed\"\nfig_3.show(\"png\")\n</code></pre> <p>Again as expected the traded volume is strongly inversely proportional to the implied chance. There's a few reasons for this:  - Inherently exposure is inversely proportional to odds so the same stake can produce widely different exposures for lay betting - Non model based participants have limited resources to manually analyse the form and thus focus on the top end of the market - Higher chance events reduce variance which is captured in staking schemes like the kelly criterion (which overweight stakes on larged percieved advantages on high probability events) that sophisticated participants tend to use</p> <p>Knowing where a majority of the traded volume is concentrated is another thing you should be aware of whether your betting on horse racing or elections and everything in between.</p> <p></p> <pre><code>averageMarketPct = (\n    df[['market_id', 'selection_id', 'seconds_before_scheduled_jump', 'back_best', 'lay_best']]\n    .query('seconds_before_scheduled_jump &amp;gt;= -20')\n    .assign(seconds_before_scheduled_jump_chunk = lambda x: x['seconds_before_scheduled_jump'].apply(chunkSBJ))\n    .assign(\n        back_best = lambda x: 1 / x['back_best'],\n        lay_best = lambda x: 1 / x['lay_best']\n    )\n    .groupby([\"market_id\", \"seconds_before_scheduled_jump_chunk\", \"seconds_before_scheduled_jump\"], as_index = False)\n    .agg({'back_best': 'sum', 'lay_best': 'sum'})\n    .groupby(\"seconds_before_scheduled_jump_chunk\", as_index = False)\n    .agg({'back_best': 'mean', 'lay_best': 'mean'})\n    .sort_values('seconds_before_scheduled_jump_chunk', ascending=False)\n)\n\nfig_4 = go.Figure()\nfig_4.add_trace(go.Scatter(x=averageMarketPct['seconds_before_scheduled_jump_chunk'], y=averageMarketPct['back_best'],\n                    mode='lines',\n                    name='Back Market Overround'))\nfig_4.add_trace(go.Scatter(x=averageMarketPct['seconds_before_scheduled_jump_chunk'], y=averageMarketPct['lay_best'],\n                    mode='lines',\n                    name='Lay Market Overround'))\nfig_4.update_layout(\n    font_family=\"Roboto\", \n    template=\"plotly_white\",\n    title='Average Back + Lay Market Overound Vic Thoroughbreds'\n)\nfig_4.update_xaxes(title = \"Seconds Before Scheduled Jump\")\nfig_4['layout']['xaxis']['autorange'] = \"reversed\"\nfig_4.show(\"png\")\n</code></pre> <p>As you can see the back and lay market % converge to 1 as the market gets closer to the jump. However, these are generally great markets even 30 mins before the off to have overrounds of only 4% is very good for racing markets. </p> <p>If you were analysing different kinds of racing markets, however, (harness or greyhound markets or thoroughbred races for country meets) you may need to conduct this kind of analysis to see when the earliest time you're likely to be able to get fair prices on either side of the market.</p> <p>Another way we can measure the tightness of Betfair markets is the market spread. I'm going to define the market spread as the number of ticks between the best back and best lay prices. This can give some extra granularity when measuring the market tightness for an individual selection</p> <p></p> <p>In this market for example we can see that the first selection has a spread of 5 ticks between 11 and 13.5 (where ticks are 0.5 apart) whereas there's only 2 ticks between the best back and lay for the market favourite 3.4 and 3.5 (where ticks are 0.05 apart).</p> <p>First we'll need to create some custom functions that will create the Betfair ladder and do Betfair \"tick arithmetic\" for us. Part of the reason that I'm creating a different view for market spread is as a reason to introduce this Betfair tick ladder concept. Measuring odds differences and movement between odds values can be tricky because prices are fairly non-linear in probability space (you see far more horses between 2 and 3 than you do between 802 and 803 for example). Converting prices to a rank on the Betfair tick ladder creates a nice output mapping that can be used for all kinds of other purposes. Betfair actually has some Betfair arithmetic tick functions available on Github</p> <pre><code># Define the betfair tick ladder\ndef bfTickLadder():\n\n    tickIncrements = {\n        1.0: 0.01,\n        2.0: 0.02,\n        3.0: 0.05,\n        4.0: 0.1,\n        6.0: 0.2,\n        10.0: 0.5,\n        20.0: 1.0,\n        30.0: 2.0,\n        50.0: 5.0,\n        100.0: 10.0,\n        1000.0: 1000,\n    }\n\n    ladder = []\n\n    for index, key in enumerate(tickIncrements):\n\n        increment = tickIncrements[key]\n\n        if (index+1) == len(tickIncrements):\n            ladder.append(key)\n        else:\n            key1 = [*tickIncrements][index]\n            key2 = [*tickIncrements][index+1]\n            steps = (key2 - key1) / increment\n\n            for i in range(int(steps)):\n                ladder.append(round(key + i * increment, 2))\n\n    return(ladder)\n\nbfticks = bfTickLadder()\n\n# Round a decimal to the betfair tick value below\ndef bfTickFloor(price, includeIndex=False):\n\n    if 'bfticks' in globals():\n        global bfticks\n    else:\n        bfticks = bfTickLadder()\n\n    ind = [ n for n,i in enumerate(bfticks) if i&amp;gt;=price][0]\n    if includeIndex:\n        if bfticks[ind]==price:\n            return((ind, price))\n        else:\n            return((ind-1, bfticks[ind-1]))\n    else:\n        if bfticks[ind]==price:\n            return(price)\n        else:\n            return(bfticks[ind-1])\n\n# Calculate the numder of ticks between two tick values\ndef bfTickDelta(p1, p2):\n\n    if np.isnan(p1) or np.isnan(p2):\n        return(np.nan)\n\n    x = bfTickFloor(p1, includeIndex=True)\n    y = bfTickFloor(p2, includeIndex=True)\n    return(x[0]-y[0])\n</code></pre> <pre><code>bfTickDelta(13.5, 11)\n</code></pre> <pre>\n<code>5</code>\n</pre> <pre><code>bfTickDelta(3.5, 3.4)\n</code></pre> <pre>\n<code>2</code>\n</pre> <p>Now that we have our functions let's plot the average market spread leading up to the jump.</p> <pre><code># Group by odds range\naverageMarketSpread = (\n    df[['market_id', 'selection_id', 'seconds_before_scheduled_jump', 'back_best', 'lay_best']]\n    .assign(seconds_before_scheduled_jump_chunk = lambda x: x['seconds_before_scheduled_jump'].apply(chunkSBJ))\n    .assign(market_spread = lambda x: x.apply(lambda x: bfTickDelta(x.lay_best, x.back_best), axis=1))\n    .groupby([\"seconds_before_scheduled_jump_chunk\"], as_index = False)\n    .agg({'market_spread': 'mean'})\n)\n</code></pre> <pre><code>fig_5 = px.line(\n    averageMarketSpread, \n    x='seconds_before_scheduled_jump_chunk', y='market_spread',\n    template='plotly_white',\n    title=\"Market Spread Leading Up To Jump\",\n    labels = {\n        'seconds_before_scheduled_jump_chunk': \"Seconds Before Scheduled Jump\",\n        \"market_spread\": \"Average Spread (b/w best back and lay)\"\n    }\n)\nfig_5.update_layout(font_family=\"Roboto\")\nfig_5['layout']['xaxis']['autorange'] = \"reversed\"\nfig_5.show(\"png\")\n</code></pre> <p>So much the same story as before, the spread tightens pretty dramatically leading all the way up to the jump. Now that we have a measure for market tightness on a selection level we can split it by odds range to see how tightness varies across odds range.</p> <pre><code># Market Spread By Odds Group\n# _______________________\n\n# Group by odds range\naverageMarketSpreadOddsgrp = (\n    df[['market_id', 'selection_id', 'bsp','seconds_before_scheduled_jump', 'back_best', 'lay_best']]\n    .assign(seconds_before_scheduled_jump_chunk = lambda x: x['seconds_before_scheduled_jump'].apply(chunkSBJ))\n    .assign(bsp = lambda x: x['bsp'].apply(chunkBsp))\n    .assign(market_spread = lambda x: x.apply(lambda x: bfTickDelta(x.lay_best, x.back_best), axis=1))\n    .groupby([\"bsp\", \"seconds_before_scheduled_jump_chunk\"], as_index = False)\n    .agg({'market_spread': 'mean'})\n)\n\nfig_6 = px.line(\n    averageMarketSpreadOddsgrp, \n    x='seconds_before_scheduled_jump_chunk', y='market_spread', color = \"bsp\",\n    template='plotly_white',\n    title=\"Market Spread Leading Up To Jump\",\n    labels = {\n        'seconds_before_scheduled_jump_chunk': \"Seconds Before Scheduled Jump\",\n        \"market_spread\": \"Average Spread (b/w best back and lay)\"\n    }\n)\nfig_6.update_layout(font_family=\"Roboto\")\nfig_6['layout']['xaxis']['autorange'] = \"reversed\"\nfig_6.show(\"png\")\n</code></pre> <p>Now this gives us visibility of how the market tightens across groups of selections. Clearly the bottom end of the market takes a bit longer to tighten up which fits our understanding that there's generally less money, and less very sharp money on these selections.</p> <pre><code># Big Movers\n# ____________________\n\n(\n    df\n    .groupby(['market_id', 'selection_id', 'venue', 'selection_name', 'win', 'bsp'], as_index = False)\n    .agg({'ltp': 'max'})\n    .rename(columns = {'ltp': 'max_traded'})\n    .assign(ticks_in = lambda x: x.apply(lambda x: bfTickDelta(x.max_traded, x.bsp), axis=1))\n    .query('max_traded &amp;lt; 500')\n    .sort_values('ticks_in', ascending = False)\n    .head(10)\n)\n</code></pre> market_id selection_id venue selection_name win bsp max_traded ticks_in 9209 1.182760338 39317622 Bendigo 6. San Fabrizio 0 2.81 10.50 71 2277 1.173069469 35981261 Bendigo 13. Shotmaker 1 2.32 4.20 56 4853 1.176708684 37456112 Bendigo 6. Zoutons 1 1.94 3.00 56 7795 1.180291241 38744938 Sandown 4. Summit Reach 0 2.42 4.50 54 4404 1.175952680 36995397 Bendigo 2. Nikau Spur 0 1.68 2.40 52 6413 1.178722039 27445196 Sandown 6. Miss Five Hundred 0 2.05 3.20 52 10216 1.184305249 39800435 Sandown 18. Lucabelle 0 48.00 470.00 48 5872 1.177927858 38197699 Flemington 2. Long Arm 0 9.90 80.00 47 1519 1.172217104 35609408 Moonee Valley 4. Tailleur 1 1.61 2.14 46 8856 1.182235083 25370687 Bendigo 3. Cernan 0 2.34 3.65 46 <p>Some of those moves are astounding: San Fabrizio traded at 10.50 in the last 30 mins before the jump and had a final BSP of 2.80, but lost. </p> <p>Like we discussed before observing a market plunge in hindsight may seem like a more powerful predictor than it actually is. For example, if we had have flat staked these 10 giant moves after they happened (say at the BSP) we'd be running at ~70% POT even though our intuition would have told you at the time that these horses couldn't lose!</p> <pre><code>m10 = (\n    df\n    .groupby(['market_id', 'selection_id'], as_index = False)\n    .apply(lambda g: None if g[g['seconds_before_scheduled_jump'] &amp;lt; 600].shape[0] == 0 else g[g['seconds_before_scheduled_jump'] &amp;lt; 600].sort_values('seconds_before_scheduled_jump', ascending = False).iloc[0])\n    .filter(items = ['market_id', 'selection_id', 'venue', 'selection_name', 'win', 'back_best', 'lay_best'])\n    .rename(columns = {'back_best': 'back_best_10m', 'lay_best': 'lay_best_10m'})\n)\n\nm0 = (\n    df\n    .groupby(['market_id', 'selection_id'], as_index = False)\n    .apply(lambda g: None if g[g['seconds_before_scheduled_jump'] &amp;lt; 0].shape[0] == 0 else g[g['seconds_before_scheduled_jump'] &amp;lt; 0].sort_values('seconds_before_scheduled_jump', ascending = False).iloc[0])\n    .filter(items = ['market_id', 'selection_id', 'venue', 'selection_name', 'win', 'back_best', 'lay_best'])\n)\n</code></pre> <pre><code># Back or lay according to large plunges\npd.DataFrame( \n    m10.merge(m0)\n    .assign(ticks_in = lambda x: x.apply(lambda x: bfTickDelta(x.back_best_10m, x.back_best), axis=1))\n    .query('ticks_in &amp;gt;= 15')\n    .assign(back_pl = lambda x:  np.where(x['win'] == 1, 0.95 * (x['back_best']-1), -1))\n    .assign(lay_pl = lambda x:  np.where(x['win'] == 1, -1*(x['lay_best']-1), 0.95))\n    .assign(stake = 1)\n    .agg({'stake': 'sum', 'back_pl': 'sum', 'lay_pl': 'sum'})\n)\n</code></pre> 0 stake 241.000 back_pl -4.824 lay_pl -17.620 <pre><code># Back or lay according to large plunges - grouped by track\n( \n    m10.merge(m0)\n    .assign(ticks_in = lambda x: x.apply(lambda x: bfTickDelta(x.back_best_10m, x.back_best), axis=1))\n    .query('ticks_in &amp;gt;= 15')\n    .assign(back_pl = lambda x:  np.where(x['win'] == 1, 0.95 * (x['back_best']-1), -1))\n    .assign(lay_pl = lambda x:  np.where(x['win'] == 1, -1*(x['lay_best']-1), 0.95))\n    .assign(stake = 1)\n    .groupby('venue', as_index = False)\n    .agg({'stake': 'sum', 'back_pl': 'sum', 'lay_pl': 'sum'})\n)\n</code></pre> venue stake back_pl lay_pl 0 Bendigo 73 -14.7225 8.64 1 Caulfield 32 21.1305 -25.20 2 Flemington 36 -8.0095 5.13 3 Moonee Valley 34 -18.8730 16.62 4 Sandown 66 15.6505 -22.81 <p>As you can see there's not much of a pattern here to take advantage of here. At least for this sample of tracks, and for this time slice, the value has been sucked dry from the market on these selections.  To illustrate this let's check what the backers would have profited if they were the ones who could identify these selections prior to their big market moves</p> <pre><code># Backing before the plunge?\n( \n    m10.merge(m0)\n    .assign(ticks_in = lambda x: x.apply(lambda x: bfTickDelta(x.back_best_10m, x.back_best), axis=1))\n    .query('ticks_in &amp;gt;= 15')\n    .assign(back_pl = lambda x:  np.where(x['win'] == 1, 0.95 * (x['back_best_10m']-1), -1))\n    .assign(lay_pl = lambda x:  np.where(x['win'] == 1, -1*(x['lay_best_10m']-1), 0.95))\n    .assign(stake = 1)\n    .agg({'stake': 'sum', 'back_pl': 'sum', 'lay_pl': 'sum'})\n)\n</code></pre> <pre>\n<code>stake      241.0000\nback_pl    108.6155\nlay_pl    -154.6100\ndtype: float64</code>\n</pre> <p>I could sign up for winning at 50% POT! Now I just have to pick 100% of the large thoroughbred plunges before they happen... Obviously no one individual person or group is predicting these movements 100%. Even the sharpest players will have movements go against them but this highlights the essence of the problem and the contradiction of retrospective price movement analysis.</p> <p>These movements are important. There is a lot to be gained by purely trading Betfair markets and how they move. However, identifting large moves after they occur isn't good enough. The large players sharpen the odds down to a good and fair implied chance and leave those watching on the sidelines with little to no value either side. To profit then we must capture some of the value as they are and jump on momentum as it's happening not after it's come to a halt.</p> <pre><code># Candlestick Plotly Chart With Plotly\n# _____________________________\n\ndef priceCandlestickPlotly(d):\n\n    d['time_chunk'] = d['time'].dt.round('2min')  \n\n    selectionName = d.selection_name.iloc[0]\n    track = d.venue.iloc[0]\n    startTime = d.market_time.iloc[0]\n\n    candleStickInput = d.groupby('time_chunk', as_index=False).agg({\"ltp\": ['first', 'last', 'min', 'max']})\n\n    candleStickInput.columns = [\"_\".join(pair) for pair in candleStickInput.columns]\n\n    fig = go.Figure(data=[go.Candlestick(x=candleStickInput['time_chunk_'],\n                    open=candleStickInput['ltp_first'],\n                    high=candleStickInput['ltp_max'],\n                    low=candleStickInput['ltp_min'],\n                    close=candleStickInput['ltp_last'])])\n    fig.update_layout(\n        template=\"plotly_white\", \n        xaxis_rangeslider_visible=False,\n        title=f'{selectionName} at {track} race started at {startTime} UTC'\n    )\n    fig.show(\"png\")\n</code></pre> <p>Let's visualise the 2 biggest moves with this chart function.</p> <pre><code>priceCandlestickPlotly(df.query('market_id == \"1.182760338\" and selection_id == \"39317622\"'))\n</code></pre> <pre>\n<code>/home/tmbish/.local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n</code>\n</pre> <p>This looks exactly like what we'd expect from a extreme plunge: consistent, sustained and unrelenting support on the back side as the horse is pushed from 8-10 odds down to it's fair odds of around 3.</p> <p>Let's look at the second big plunge:</p> <pre><code>priceCandlestickPlotly(df.query('market_id == \"1.173069469\" and selection_id == \"35981261\"'))\n</code></pre> <p>This movement chart is a bit different. The market has 4 distinct segments:</p> <ul> <li>About 30 minutes from the off the market changes and the horse is supported from 4 into 3</li> <li>It trades at around 3 for the next 25 minutes</li> <li>It then drifts back out sharply to a bit over 4</li> <li>It then gets crunched back in all the way into a BSP of 2.30</li> </ul> <p>Clearly different segments of the participants had very different perceptions of this horses chances. How do we make sense of it?</p> <p>Let's try a different candlestick visualistion, which includes the traded volume.</p> <pre><code>import mplfinance as fplt\n\ndef priceCandlestickMpl(d):\n\n    d['time_chunk'] = d['time'].dt.round('2min')  \n\n    selectionName = d.selection_name.iloc[0]\n    track = d.venue.iloc[0]\n    startTime = d.market_time.iloc[0]\n\n    # Add traded volume in last interval\n    d = d.groupby(['market_id', 'selection_id']).apply(lambda x: x.assign(traded_volume_delta = lambda y: (y['traded_volume'] - y['traded_volume'].shift(1)).mask(pd.isnull, 0)))\n\n    candleStickInput = d.groupby('time_chunk', as_index=False).agg({\"ltp\": ['first', 'last', 'min', 'max'], \"traded_volume_delta\": 'sum'})\n\n    candleStickInput.columns = [\"_\".join(pair) for pair in candleStickInput.columns]\n\n    candleStickInput = candleStickInput.rename(columns = {'time_chunk_': 'date','ltp_first': 'open', 'ltp_last': 'close', 'ltp_min': 'low', 'ltp_max': 'high', 'traded_volume_delta_sum': 'volume'})\n\n    candleStickInput = candleStickInput.set_index('date')\n\n    fplt.plot(\n        candleStickInput,\n        type='candle',\n        style='yahoo',\n        title= f'{selectionName} at {track} race started at {startTime} UTC',\n        ylabel='Odds',\n        volume=True,\n        ylabel_lower='Volume Traded',\n    )\n</code></pre> <p>First the first horse:</p> <pre><code>priceCandlestickMpl(df.query('market_id == \"1.182760338\" and selection_id == \"39317622\"'))\n</code></pre> <pre>\n<code>/home/tmbish/.local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n</code>\n</pre> 2021-08-30T11:43:18.549484 image/svg+xml Matplotlib v3.4.3, https://matplotlib.org/ <p>Now the second selection:</p> <pre><code>priceCandlestickMpl(df.query('market_id == \"1.173069469\" and selection_id == \"35981261\"'))\n</code></pre> <pre>\n<code>/home/tmbish/.local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n</code>\n</pre> 2021-08-30T11:43:22.999632 image/svg+xml Matplotlib v3.4.3, https://matplotlib.org/ <p>So as the majority of the volume was traded right before the jump, the price was hammered into 2.30. This is a pattern that you might see a lot in Betfair markets: prices may oscillate around or even drift consistently in one direction until a point at which certain large participants or groups of participants enter the market and push the selection in another direction completely.</p> <p>So anticipating a move won't be as simple as analysing the price as a time series, it's one piece of the puzzle. Really sophisticated market trading algorithms will need to historically analyse a raft of market metrics and correlate them with these movements historically within the market type of interest. In the next section I'll start off down that road to give you an idea of how I'd tackle the problem but it's more complicated than we can uncover in a single article.</p> <pre><code># Start Analysis\n# ______________________________\n\ndfPredict = df.query('seconds_before_scheduled_jump &amp;lt;= 600 and seconds_before_scheduled_jump &amp;gt;= 0')\n\n# Target\n# ____________________________\n\ndfPredict['best_back_30s_in_future'] =  dfPredict.groupby(['market_id', 'selection_id'])['back_best'].shift(-30)\ndfPredict['best_lay_30s_in_future'] = dfPredict.groupby(['market_id', 'selection_id'])['back_best'].shift(-30)\n\ndfPredict['back_target'] = dfPredict.apply(lambda x: bfTickDelta(x.back_best, x.best_back_30s_in_future), axis=1)\ndfPredict['lay_target'] =  dfPredict.apply(lambda x: bfTickDelta(x.lay_best, x.best_lay_30s_in_future), axis=1)\n\n# Movement \n# ____________________________\n\ndfPredict['back_best_30s_ago'] = dfPredict.groupby(['market_id', 'selection_id'])['back_best'].shift(30)\ndfPredict['back_lay_30s_ago'] = dfPredict.groupby(['market_id', 'selection_id'])['lay_best'].shift(30)\n\ndfPredict = (\n    dfPredict\n    .assign(back_ticks_in_30s = lambda x: x.apply(lambda x: bfTickDelta(x.back_best_30s_ago, x.back_best), axis=1))\n    .assign(lay_ticks_in_30s = lambda x: x.apply(lambda x: bfTickDelta(x.back_lay_30s_ago, x.lay_best), axis=1))\n)\n\n# Weight Of Money\n# ____________________________\n\natb_ladder = dfPredict.atb_ladder.iloc[0]\natl_ladder = dfPredict.atl_ladder.iloc[0]\n\ndef wom(back_ladder, lay_ladder):\n\n    if not back_ladder or not lay_ladder:\n        return((None, None))\n\n    total_volume = round(sum(back_ladder['v']) + sum(lay_ladder['v']),2)\n\n    return((round(sum(back_ladder['v']) / total_volume,3), round(sum(lay_ladder['v']) / total_volume,3)))\n\n\ndfPredict['wom'] = dfPredict.apply(lambda x: wom(x['atb_ladder'], x['atl_ladder']), axis = 1)\n\ndfPredict[['back_wom', 'lay_wom']] = pd.DataFrame(dfPredict['wom'].tolist(), index=dfPredict.index)\n\n# Top Box Support\n# ________________________\n\ndfPredict['back_best_support'] = dfPredict['back_vwap'] / dfPredict['back_best']\ndfPredict['lay_best_support'] = dfPredict['lay_best'] / dfPredict['lay_vwap']\n\n# Recent Movement\n# _____________________\n\ndfPredict['wap_movement_10s'] = dfPredict['wap'] / dfPredict.groupby(['market_id', 'selection_id'])['wap'].shift(-10)\n</code></pre> <pre>\n<code>/home/tmbish/.local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n</code>\n</pre> <p>Now that we've created the candidate factors let's use a spearman correlation coefficient to analyse their correlation (positive or negative we're mostly focussed on the absolute size of the coefficient).</p> <pre><code>corrMatrix = dfPredict[['back_target', 'back_ticks_in_30s', 'lay_ticks_in_30s', 'back_wom', 'lay_wom', 'back_best_support', 'lay_best_support', 'wap_movement_10s']].dropna().corr(method = \"spearman\")\ncorrMatrix[['back_target']]\n</code></pre> back_target back_target 1.000000 back_ticks_in_30s -0.080772 lay_ticks_in_30s -0.036798 back_wom 0.259926 lay_wom -0.259927 back_best_support -0.142726 lay_best_support 0.013173 wap_movement_10s -0.316167 <pre><code>import seaborn as sns\n\ncorPlot = sns.heatmap(\n    corrMatrix, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\ncorPlot.set_xticklabels(\n    corPlot.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n)\n</code></pre> <pre>\n<code>[Text(0.5, 0, 'back_target'),\n Text(1.5, 0, 'back_ticks_in_30s'),\n Text(2.5, 0, 'lay_ticks_in_30s'),\n Text(3.5, 0, 'back_wom'),\n Text(4.5, 0, 'lay_wom'),\n Text(5.5, 0, 'back_best_support'),\n Text(6.5, 0, 'lay_best_support'),\n Text(7.5, 0, 'wap_movement_10s')]</code>\n</pre> 2021-08-30T12:26:29.189464 image/svg+xml Matplotlib v3.4.3, https://matplotlib.org/ <p>We're entirely interested in the <code>back_target</code> column in the matrix and the heatmap. We can see that some of our candidate features have decent correlation with the number of ticks the price will move over the course of the next 30 seconds.</p> <pre><code># %% [markdown]\n# # Do #theyknow? Analysing betfair market formation and market movements\n\n# %% [markdown]\n# ## 0.1 Setup\n# \n# Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language.\n# \n# Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer locally where you want to store the intermediate data files.\n# \n# You'll also need `betfairlightweight` which you can install with something like `pip install betfairlightweight`.\n\n# %%\nimport pandas as pd\nimport numpy as np\nimport requests\nimport os\nimport re\nimport csv\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport math\nimport logging\nimport yaml\nimport csv\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\nimport ast\n\nfrom datetime import date, timedelta\nfrom unittest.mock import patch\nfrom typing import List, Set, Dict, Tuple, Optional\nfrom itertools import zip_longest\nimport betfairlightweight\nfrom betfairlightweight import StreamListener\nfrom betfairlightweight.resources.bettingresources import (\n    PriceSize,\n    MarketBook\n)\n\n# %% [markdown]\n# ## 0.2 Context\n# \n# You may have seen the hashtag if you're on australian racing twitter #theyknow following a dramatic late market move for a horse that's followed by a decisive race victory. Sometimes it can seem eerie how accurate these moves are after the fact. In betfair racing markets there's usually a flurry of activity leading up to the race start as players look to get their bets down at the best price without tipping their hand (opinion on the race) too much. Large moves can happend when large players rally around a selection who's implied chance in early trading isn't close to what it's true chance is in the upcoming race. Large moves can also happen when there's some inside information - not able to be gleaned from analysis of the horses previous races - that slowly filters out of the stable or training group.\n# \n# This creates opportunity in the secondary market as punters try to read these movements to make bets themselves. The task often becomes identifying which movements are caused by these sophisticated players or represent real signals of strength and which aren't.\n# \n# So do #theyknow generally? Before even looking at the data I can assure you that yes they do know pretty well. Strong movements in betting markets are usually pretty reliable indicators about what is about to happen. However, these moves can be overvalued by recreationals. When observing a horse plumet in from $3.50 to $2 you are usually suprised if the horse loses, but the general efficiency of late prices would suggest that this horse is going to still lose 50% of time. If you simply observe the large moves after the sharp players have corrected the market landscape you're in no better a position to bet than before the move happened. On the other hand what if would could reliably identify the move as it was happening or about to happen? That would be a recipe for successful trading of horse racing markets and no doubt this is what many players in this secondary market (analysis of betfair markets rather than the races themselves) try to do.\n# \n# If you were to build up a manual qualitative strategy for this kind of market trading you need to understand:\n# - Who the participants are\n# - How sophisticated they are at the top end\n# - What types of races do they bet on and for how much\n# - When the different types of participants typically enter the market \n# - What do bet placement patterns look like for these participants\n# - etc.\n# \n# This is the kind of task that takes a lot research, years of experience watching markets, a lot of trial and error, and a lot of industry know-how. Given I'm a lazy quantitative person I'll try to see if I can uncover any of these patterns in the data alone.\n# \n# Put simply the central question for the second half of this piece will be:\n# \n# &amp;gt; How should you think about large price moves in betfair markets and how could you look to build secondary betting strategies by quantitatively analysing historical data\n# \n# I'll just be analysing historical thoroughbred markets but the same approach could be applied to any sport or racing code of your interest.\n\n# %% [markdown]\n# ## 0.3 This Example\n# \n# Building market based trading strategies is a broad and fertile ground for many quantitative betfair customers; too big to cover in a single article. I'll try to zero in on a small slice of thoroughbred markets and analyse how these markets form and how I'd start the process of trying to find the patterns in the market movements. Again hopefully this is some inspiration for you and you can pick up some of the ideas and build them out.\n# \n# Given volume of data (when analysing second by second slices of market data) I'll be looking at a years worth of thoroughbred races from the 5 largest Victorian tracks: Flemington, Caulfield, Moonee Valley, Bendigo and Sandown.\n\n# %% [markdown]\n# # 1.0 Data\n# \n# Unlike in some of the previous tutorials we aren't going to collapse the stream data into a single row per runner. In those examples we were interested in analysing some discrete things about selections in betfair markets like:\n# \n# - Their final odds (bsp or last traded price)\n# - Their odds at some fixed time point or time points before the scheduled race start\n# - Other single number descriptors of the trading activity on a selection (eg total traded volume)\n# \n# \n# In this analysis I want to analyse how markets form and prices move for selections as markets evolve. So we'll need to pull out multiple price points per runner - so we'll have multiple rows per runner in our parsed output dataset.\n# \n# To output a row for every stream update for every selection in every thoroughbred race over the last 12 months would produce a dataset far too big too analyse using normal data analysis tools - we're about 10s to 100s of billions of rows.\n# \n# To chop our sample down into a manageable slice I'm going to filter on some select tracks of interest (as mentioned above) and I'm also going to have 3 sections of data granularity:\n# \n# - I won't log any of the odds or traded volumes &amp;gt; 30mins before the scheduled off\n#     + In thoroughbreds there is non-trivial action before this point you may want to study, but it's not what I want to study here\n# - Between 30 and 10 minutes before the scheduled off I'll log data every 60 seconds\n# - 10 minutes or less to the scheuled off I'll log prices every second\n# \n# The code to manage this windowed granularity is in the below parsing code tweak as you wish if you want to tighten or broaden the analysis.\n\n# %% [markdown]\n# ## 1.1 Sourcing Data\n# \n# First you'll need to source the stream file TAR archive files. I'll be analysing 12 months of Australian thoroughbred Pro files. Aask automation@betfair.com.au for more info if you don't know how to do this. Once you've gotten access download them to your computer and store them together in a folder.\n\n# %% [markdown]\n# ## 1.2 Utility Functions\n\n# %%\n# General Utility Functions\n# _________________________________\n\ndef as_str(v) -&amp;gt; str:\n    return '%.2f' % v if type(v) is float else v if type(v) is str else ''\n\ndef split_anz_horse_market_name(market_name: str) -&amp;gt; (str, str, str):\n    parts = market_name.split(' ')\n    race_no = parts[0] # return example R6\n    race_len = parts[1] # return example 1400m\n    race_type = parts[2].lower() # return example grp1, trot, pace\n    return (race_no, race_len, race_type)\n\n\ndef load_markets(file_paths):\n    for file_path in file_paths:\n        print(file_path)\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n\n    return None\n\ndef slicePrice(l, n):\n    try:\n        x = l[n].price\n    except:\n        x = \"\"\n    return(x)\n\ndef sliceSize(l, n):\n    try:\n        x = l[n].size\n    except:\n        x = \"\"\n    return(x)\n\ndef pull_ladder(availableLadder, n = 5):\n        out = {}\n        price = []\n        volume = []\n        if len(availableLadder) == 0:\n            return(out)        \n        else:\n            for rung in availableLadder[0:n]:\n                price.append(rung.price)\n                volume.append(rung.size)\n\n            out[\"p\"] = price\n            out[\"v\"] = volume\n            return(out)\n\n# %% [markdown]\n# Slicing the tracks we want we'll just adjust the market filter function used before to include some logic on the venue name\n\n# %%\ndef filter_market(market: MarketBook) -&amp;gt; bool: \n\n    d = market.market_definition\n    track_filter = ['Bendigo', 'Sandown', 'Flemington', 'Caulfield', 'Moonee Valley']\n\n    return (d.country_code == 'AU' \n        and d.venue in track_filter\n        and d.market_type == 'WIN' \n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n\n# %% [markdown]\n# ## 1.3 Selection Metadata\n# \n# Given that the detailed price data will have so many records we will split out the selection metadata (including the selection win outcome flag and the bsp) into it's own dataset much you would do in a relational database to manage data volumes.\n# \n# This means we'll have to parse over the data twice but our outputs will be much smaller than if we duplicated the selection name 800 times for example.\n\n# %%\ndef final_market_book(s):\n\n    with patch(\"builtins.open\", lambda f, _: f):\n\n        gen = s.get_generator()\n\n        for market_books in gen():\n\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    return(None)\n\n            for market_book in market_books:\n\n                last_market_book = market_book\n\n        return(last_market_book)\n\ndef parse_final_selection_meta(dir, out_file):\n\n    with open(out_file, \"w+\") as output:\n\n        output.write(\"market_id,selection_id,venue,market_time,selection_name,win,bsp\\n\")\n\n        for file_obj in load_markets(dir):\n\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            last_market_book = final_market_book(stream)\n\n            if last_market_book is None:\n                continue \n\n            # Extract Info ++++++++++++++++++++++++++++++++++\n\n            runnerMeta = [\n                {\n                    'selection_id': r.selection_id,\n                    'selection_name': next((rd.name for rd in last_market_book.market_definition.runners if rd.selection_id == r.selection_id), None),\n                    'selection_status': r.status,\n                    'win': np.where(r.status == \"WINNER\", 1, 0),\n                    'sp': r.sp.actual_sp\n                }\n                for r in last_market_book.runners \n            ]\n\n            # Return Info ++++++++++++++++++++++++++++++++++\n\n            for runnerMeta in runnerMeta:\n\n                if runnerMeta['selection_status'] != 'REMOVED':\n\n                    output.write(\n                        \"{},{},{},{},{},{},{}\\n\".format(\n                            str(last_market_book.market_id),\n                            runnerMeta['selection_id'],\n                            last_market_book.market_definition.venue,\n                            last_market_book.market_definition.market_time,\n                            runnerMeta['selection_name'],\n                            runnerMeta['win'],\n                            runnerMeta['sp']\n                        )\n                    )\n\n# %%\nselection_meta = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\"\nstream_files = glob.glob(\"[PATH TO STREAM FILES]*.tar\")\ntrading = betfairlightweight.APIClient(username = \"username\", password = \"password\", app_key=\"app_key\")\nlistener = StreamListener(max_latency=None)\n\nprint(\"__ Parsing Selection Metadata ___ \")\n# parse_final_selection_meta(stream_files, selection_meta)\n\n# %% [markdown]\n# ## 1.4 Detailed Preplay Odds\n# \n# Like mentioned above there will be some time control logic injected to control the time granularity that odds are recorded in each step.\n# \n# Instead of widening the available to bet price ladder I'll extract the top 10 rungs of the available to back (atb) and available to lay (atl) ladders and write them both to the output file. That will give me more flexibility during the analysis to pull out things that interest me. So in total I'll extract:\n# \n# - Top 10 ATB Ladder\n# - Top 10 ATL Ladder\n# - Total Traded Volume\n# - Volume weighted average traded price up till the current time\n# - Last Traded price\n# \n\n# %%\ndef loop_preplay_prices(s, o):\n\n    with patch(\"builtins.open\", lambda f, _: f):\n\n        gen = s.get_generator()\n\n        marketID = None\n        tradeVols = None\n        time = None\n\n        for market_books in gen():\n\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    break\n\n            for market_book in market_books:\n\n                # Time Step Management ++++++++++++++++++++++++++++++++++\n\n                if marketID is None:\n\n                    # No market initialised\n                    marketID = market_book.market_id\n                    time =  market_book.publish_time\n\n                elif market_book.inplay:\n\n                    # Stop once market goes inplay\n                    break\n\n                else:\n\n                    seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n\n                    if seconds_to_start &amp;gt; log1_Start:\n\n                        # Too early before off to start logging prices\n                        continue\n\n                    else:\n\n                        # Update data at different time steps depending on seconds to off\n                        wait = np.where(seconds_to_start &amp;lt;= log2_Start, log2_Step, log1_Step)\n\n                        # New Market\n                        if market_book.market_id != marketID:\n                            marketID = market_book.market_id\n                            time =  market_book.publish_time\n                        # (wait) seconds elapsed since last write\n                        elif (market_book.publish_time - time).total_seconds() &amp;gt; wait:\n                            time = market_book.publish_time\n                        # fewer than (wait) seconds elapsed continue to next loop\n                        else:\n                            continue\n\n                # Execute Data Logging ++++++++++++++++++++++++++++++++++\n\n                for runner in market_book.runners:\n\n                    try:\n                        atb_ladder = pull_ladder(runner.ex.available_to_back, n = 10)\n                        atl_ladder = pull_ladder(runner.ex.available_to_lay, n = 10)\n                    except:\n                        atb_ladder = {}\n                        atl_ladder = {}\n\n                    # Calculate Current Traded Volume + Tradedd WAP\n                    limitTradedVol = sum([rung.size for rung in runner.ex.traded_volume])\n                    if limitTradedVol == 0:\n                        limitWAP = \"\"\n                    else:\n                        limitWAP = sum([rung.size * rung.price for rung in runner.ex.traded_volume]) / limitTradedVol\n                        limitWAP = round(limitWAP, 2)\n\n                    o.writerow(\n                        (\n                            market_book.market_id,\n                            runner.selection_id,\n                            market_book.publish_time,\n                            limitTradedVol,\n                            limitWAP,\n                            runner.last_price_traded or \"\",\n                            str(atb_ladder).replace(' ',''), \n                            str(atl_ladder).replace(' ','')\n                        )\n                    )\n\n\n\n\ndef parse_preplay_prices(dir, out_file):\n\n    with open(out_file, \"w+\") as output:\n\n        writer = csv.writer(\n            output, \n            delimiter=',',\n            lineterminator='\\r\\n',\n            quoting=csv.QUOTE_ALL\n        )\n\n        writer.writerow((\"market_id\",\"selection_id\",\"time\",\"traded_volume\",\"wap\",\"ltp\",\"atb_ladder\",\"atl_ladder\"))\n\n        for file_obj in load_markets(dir):\n\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            loop_preplay_prices(stream, writer)\n\n# %%\npreplay_price_file =  \"[OUTPUT PATH TO CSV FOR PREPLAY PRICES]\"\nstream_files = glob.glob(\"[PATH TO STREAM FILES]*.tar\")\n\n\nlog1_Start = 60 * 30 # Seconds before scheduled off to start recording data for data segment one\nlog1_Step = 60       # Seconds between log steps for first data segment\nlog2_Start = 60 * 10  # Seconds before scheduled off to start recording data for data segment two\nlog2_Step = 1        # Seconds between log steps for second data segment\n\nprint(\"__ Parsing Detailed Preplay Prices ___ \")\n# parse_preplay_prices(stream_files, preplay_price_file)\n\n# %% [markdown]\n# # 2.0 Analysis\n\n# %% [markdown]\n# ## 2.1 Load and Assemble\n# \n# First let's load the raw datafiles we created in the previous step. \n# \n# ## 2.1.1 Load\n\n# %% [markdown]\n# We have the highlevel selection metadata (1 row per selection):\n# \n\n# %%\nselection_meta_path = \"[PATH TO METADATA FILE]\"\nselection = pd.read_csv(selection_meta_path , dtype={'market_id': object, 'selection_id': object}, parse_dates = ['market_time'])\nselection.head(3)\n\n# %% [markdown]\n# And we have the detailed preplay price data for these markets + selections:\n\n# %%\nprices_path = \"[PATH TO PRICES FILE]\"\n\nprices = pd.read_csv(\n    prices_path, \n    quoting=csv.QUOTE_ALL,\n    dtype={'market_id': 'string', 'selection_id': 'string', 'atb_ladder': 'string', 'atl_ladder': 'string'},\n    parse_dates=['time']\n)\nprices.head(3)\n\n# %% [markdown]\n# Now it's important to observe how much data we have here.\n\n# %%\nprices.shape\n\n# %% [markdown]\n# We've got 7 million rows of price data here just for races at 5 thoroughbred tracks over the last year. Now it's not really \"big data\" in the sense you might have heard before but it's certainly a lot of rows and we'll have to think about the performance of our code a little bit more than we would if we were dealining with 1 row per selection style datasets.\n\n# %% [markdown]\n# We need pandas to correctly interpret the dictionary columns as dictionaries so we'll run this code:\n\n# %%\n# To get pandas to correctly recognise the ladder columns as dictionaries\nprices['atb_ladder'] = [ast.literal_eval(x) for x in prices['atb_ladder']]\nprices['atl_ladder'] = [ast.literal_eval(x) for x in prices['atl_ladder']]\n\n# %% [markdown]\n# ## 2.1.2 Assemble\n\n# %% [markdown]\n# Now we'll join the 2 data sets together to form a nice normalised dataframe:\n\n# %%\n# Simple join on market and selection_id initially\ndf = selection.merge(prices, on = ['market_id', 'selection_id'])\ndf.head(3)\n\n# %% [markdown]\n# ## 2.1.3 Transform\n# \n# Next we'll do some processing on the joined dataframe to add some columns that we can use in our analysis including calculating a numeric #seconds before the scheduled jump field that we'll use extensively.\n\n# %%\ndf = (\n    df\n    .assign(back_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atb_ladder']])\n    .assign(lay_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atl_ladder']])\n    .assign(back_vwap = lambda x: [np.nan if d.get('p') is None else round(sum([a*b for a,b in zip(d.get('p')[0:3],d.get('v')[0:3])]) / sum(d.get('v')[0:3]),3) for d in x['atb_ladder']])\n    .assign(lay_vwap = lambda x: [np.nan if d.get('p') is None else round(sum([a*b for a,b in zip(d.get('p')[0:3],d.get('v')[0:3])]) / sum(d.get('v')[0:3]),3) for d in x['atl_ladder']])\n    .assign(seconds_before_scheduled_jump = lambda x: round((x['market_time'] - x['time']).dt.total_seconds()))\n    .query('seconds_before_scheduled_jump &amp;lt; 1800 and seconds_before_scheduled_jump &amp;gt; -120')\n)\n\n# %% [markdown]\n# ## 2.2 Market Formation\n# \n# Before we analyse how prices for selections move let's understand some basic things about how thoroughbred markets form.\n\n# %% [markdown]\n# ## 2.2.1 Traded Volumes\n# \n# Let's look at how a typical market (at one of these 5 tracks) trades leading up to the scheduled race start.\n# \n# To make some of the analysis a little bit cleaner we need to pad out missing odds updates. For example for a given market we might have a market update 140 seconds before the jump but not another one till 132 seconds before the jump. We'll add rows for those interim 8 seconds by filling the values from the the previous row, this is required to iron out some idiosyncracies in the aggregations, it's not that important to follow if you don't understand it.\n\n# %%\ntraded_volume_values = df[['market_id', 'selection_id', 'venue', 'bsp', 'seconds_before_scheduled_jump', 'traded_volume']]\nall_sbj = pd.DataFrame(data = {'seconds_before_scheduled_jump': traded_volume_values.seconds_before_scheduled_jump.unique()}).assign(join = 1)\ntraded_volume_explode = traded_volume_values[['market_id', 'selection_id', 'venue', 'bsp']].drop_duplicates().assign(join = 1).merge(all_sbj).drop('join', 1)\ntraded_volume_df = traded_volume_explode.merge(traded_volume_values, how = \"left\")\ntraded_volume_df = traded_volume_df.sort_values(['market_id', 'selection_id', 'venue', 'seconds_before_scheduled_jump'], ascending=[True, True, True, False])\ntraded_volume_df.update(traded_volume_df.groupby(['market_id', 'selection_id', 'venue'])[['seconds_before_scheduled_jump', 'traded_volume']].ffill().fillna(0))\n\n# %%\n# Group by market, sum volume over selections at a given time, average over times for total\ndef chunkSBJ(sbj):\n    if sbj &amp;lt; 600:\n        return(sbj)\n    else:\n        return(int(math.floor(sbj / 60) * 60))\n\ntradedVolumes_1 = (\n    traded_volume_df\n    .groupby([\"market_id\", \"seconds_before_scheduled_jump\"], as_index = False)\n    .agg({'traded_volume': 'sum'})\n    .assign(seconds_before_scheduled_jump = lambda x: x['seconds_before_scheduled_jump'].apply(chunkSBJ))\n    .groupby(\"seconds_before_scheduled_jump\", as_index = False)\n    .agg({'traded_volume': 'mean'})\n    .sort_values('seconds_before_scheduled_jump', ascending=False)\n)\n\nfig = px.area(\n    tradedVolumes_1, \n    x='seconds_before_scheduled_jump', y='traded_volume', \n    template='plotly_white',\n    title=\"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\",\n    labels = {\n        'seconds_before_scheduled_jump': \"Seconds Before Scheduled Jump\",\n        \"traded_volume\": \"Average Cumulative Traded Volume\"\n    }\n    #subtitle = \"Top 5 Biggest Vic Track Sample\"\n)\nfig.update_layout(font_family=\"Roboto\")\nfig['layout']['xaxis']['autorange'] = \"reversed\"\nfig.show(\"png\")\n\n# %% [markdown]\n# The discontinuity in the chart highlights the switch point between the two time granularities that we extracted from the stream. Pre 600 seconds (10 minutes) before the scheduled off I can plot 1 data point per minute and after I'm plotting 60 data points per minute.\n# \n# The traded volume chart looks like an exponential chart: the total traded volume doubles from 10 minutes out to 4 minutes out, then it doubles again between then and 1 minute out, then nearly doubling again in the last minute and a bit of trading. Even a simple visual like this can help you with your bet placement on betfair markets. For example if you're planning to get large volumes down on betfair thoroughbred markets it's probably best to view prices &amp;gt;10 minutes out with a skeptical eye even if the market is tight - because you won't find the requisite lay volume that early as the majority of traded volume happens in the last 2-5 minutes of trading.\n# \n# Now like most analysis the average is definitely hiding lots of interesting things about this data. Let's split out this data by our 5 tracks:\n\n# %%\ntradedVolumes_2 = (\n   traded_volume_df\n    .assign(seconds_before_scheduled_jump_chunk = lambda x: x['seconds_before_scheduled_jump'].apply(chunkSBJ))\n    .groupby([\"market_id\", \"venue\", \"seconds_before_scheduled_jump_chunk\", \"seconds_before_scheduled_jump\"], as_index = False)\n    .agg({'traded_volume': 'sum'})\n    .groupby([\"venue\", \"seconds_before_scheduled_jump_chunk\"], as_index = False)\n    .agg({'traded_volume': 'mean'})\n    .sort_values('seconds_before_scheduled_jump_chunk', ascending=False)\n)\n\nfig_2 = px.line(\n    tradedVolumes_2, \n    x='seconds_before_scheduled_jump_chunk', y='traded_volume',  color='venue',\n    template='plotly_white',\n    title=\"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\",\n    labels = {\n        'seconds_before_scheduled_jump_chunk': \"Seconds Before Scheduled Jump\",\n        \"traded_volume\": \"Average Cumulative Traded Volume\"\n    }\n)\nfig_2.update_layout(font_family=\"Roboto\")\nfig_2['layout']['xaxis']['autorange'] = \"reversed\"\nfig_2.show(\"png\")\n\n# %% [markdown]\n# As expected an average Flemington race trades nearly 500k whereas an average Bendigo race trades only ~120k volume.\n# \n# How about if we split our selections by odds range. Intuitively we know that odds-on horses will trade significantly more volume than a $50 shot but let's visualise the difference.\n# \n# We'll chunk the BSP of each horse into 5 groups:\n# - Odds on (&amp;lt;50% chance of winning)\n# - Odds between 2 and 5\n# - Odds between 5 and 15\n# - Odds between 15 and 50\n# - Odds 50+\n\n# %%\ndef chunkBsp(bsp):\n    if bsp &amp;lt;= 2:\n        return(\"1. Odds On\")\n    elif bsp &amp;lt;= 5:\n        return(\"2. (2, 5]\")\n    elif bsp &amp;lt;= 15:\n        return(\"3. (5, 15]\")\n    elif bsp &amp;lt;= 50:\n        return(\"4. (15, 50]\")\n    else:\n        return(\"5. 50+\")\n\n# Group by odds range\ntradedVolumes_3 = (\n    traded_volume_df\n    .assign(seconds_before_scheduled_jump_chunk = lambda x: x['seconds_before_scheduled_jump'].apply(chunkSBJ))\n    .assign(bsp = lambda x: x['bsp'].apply(chunkBsp))\n    .groupby([\"market_id\", \"bsp\", \"seconds_before_scheduled_jump_chunk\", \"seconds_before_scheduled_jump\"], as_index = False)\n    .agg({'traded_volume': 'sum'})\n    .groupby([\"bsp\", \"seconds_before_scheduled_jump_chunk\"], as_index = False)\n    .agg({'traded_volume': 'mean'})\n)\n\nfig_3 = px.line(\n    tradedVolumes_3, \n    x='seconds_before_scheduled_jump_chunk', y='traded_volume', color='bsp',\n    template='plotly_white',\n    title=\"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\",\n    labels = {\n        'seconds_before_scheduled_jump_chunk': \"Seconds Before Scheduled Jump\",\n        \"traded_volume\": \"Average Cumulative Traded Volume\"\n    }\n)\nfig_3.update_layout(font_family=\"Roboto\")\nfig_3['layout']['xaxis']['autorange'] = \"reversed\"\nfig_3.show(\"png\")\n\n# %% [markdown]\n# Again as expected the traded volume is strongly inversely proportional to the implied chance. There's a few reasons for this: \n# - Inherently exposure is inversely proportional to odds so the same stake can produce widely different exposures for lay betting\n# - Non model based participants have limited resources to manually analyse the form and thus focus on the top end of the market\n# - Higher chance events reduce variance which is captured in staking schemes like the kelly criterion (which overweight stakes on larged percieved advantages on high probability events) that sophisticated participants tend to use\n# \n# Knowing where a majority of the traded volume is concentrated is another thing you should be aware of whether your betting on horse racing or elections and everything in between.\n\n# %% [markdown]\n# ## 2.2.2 Market Tightness\n# \n# Understanding how the market tightens before the off is also another key conceptual component to market formation. I will consider two different variations on this concept:\n# \n# - Market overround or Market Percentage\n#     + The sum of probabilities across all outcomes\n#     + Back % are always above 1 (else there exists an arbitrage opportunity)\n#     + Lay % are always below 1 (else there exists an arbitrage opportunity)\n# - Market Spread\n#     + The # of ticks / rungs between the best available back price and the best available lay price\n# \n# The market % is the value displayed on the betfair website here\n\n# %% [markdown]\n# ![](img/overround.png)\n\n# %%\naverageMarketPct = (\n    df[['market_id', 'selection_id', 'seconds_before_scheduled_jump', 'back_best', 'lay_best']]\n    .query('seconds_before_scheduled_jump &amp;gt;= -20')\n    .assign(seconds_before_scheduled_jump_chunk = lambda x: x['seconds_before_scheduled_jump'].apply(chunkSBJ))\n    .assign(\n        back_best = lambda x: 1 / x['back_best'],\n        lay_best = lambda x: 1 / x['lay_best']\n    )\n    .groupby([\"market_id\", \"seconds_before_scheduled_jump_chunk\", \"seconds_before_scheduled_jump\"], as_index = False)\n    .agg({'back_best': 'sum', 'lay_best': 'sum'})\n    .groupby(\"seconds_before_scheduled_jump_chunk\", as_index = False)\n    .agg({'back_best': 'mean', 'lay_best': 'mean'})\n    .sort_values('seconds_before_scheduled_jump_chunk', ascending=False)\n)\n\nfig_4 = go.Figure()\nfig_4.add_trace(go.Scatter(x=averageMarketPct['seconds_before_scheduled_jump_chunk'], y=averageMarketPct['back_best'],\n                    mode='lines',\n                    name='Back Market Overround'))\nfig_4.add_trace(go.Scatter(x=averageMarketPct['seconds_before_scheduled_jump_chunk'], y=averageMarketPct['lay_best'],\n                    mode='lines',\n                    name='Lay Market Overround'))\nfig_4.update_layout(\n    font_family=\"Roboto\", \n    template=\"plotly_white\",\n    title='Average Back + Lay Market Overound Vic Thoroughbreds'\n)\nfig_4.update_xaxes(title = \"Seconds Before Scheduled Jump\")\nfig_4['layout']['xaxis']['autorange'] = \"reversed\"\nfig_4.show(\"png\")\n\n# %% [markdown]\n# As you can see the back and lay market % converge to 1 as the market gets closer to the jump. However, these are generally great markets even 30 mins before the off to have overrounds of only 4% is very good for racing markets. \n# \n# If you were analysing different kinds of racing markets, however, (harness or greyhound markets or thoroughbred races for country meets) you may need to conduct this kind of analysis to see when the earliest time you're likely to be able to get fair prices on either side of the market.\n# \n# Another way we can measure the tightness of betfair markets is the market spread. I'm going to define the market spread as the number of ticks between the best back and best lay prices. This can give some extra granularity when measuring the market tightness for an individual selection\n\n# %% [markdown]\n# ![](img/market-spread.png)\n\n# %% [markdown]\n# In this market for example we can see that the first selection has a spread of 5 ticks between 11 and 13.5 (where ticks are 0.5 apart) whereas there's only 2 ticks between the best back and lay for the market favourite 3.4 and 3.5 (where ticks are 0.05 apart).\n# \n# First we'll need to create some custom functions that will create the betfair ladder and do betfair \"tick arithmetic\" for us. Part of the reason that I'm creating a different view for market spread is as a reason to introduce this betfair tick ladder concept. Measuring odds differences and movement between odds values can be tricky because prices are fairly non-linear in probability space (you see far more horses between 2 and 3 than you do between 802 and 803 for example). Converting prices to a rank on the betfair tick ladder creates a nice output mapping that can be used for all kinds of other purposes.\n\n# %%\n# Define the betfair tick ladder\ndef bfTickLadder():\n\n    tickIncrements = {\n        1.0: 0.01,\n        2.0: 0.02,\n        3.0: 0.05,\n        4.0: 0.1,\n        6.0: 0.2,\n        10.0: 0.5,\n        20.0: 1.0,\n        30.0: 2.0,\n        50.0: 5.0,\n        100.0: 10.0,\n        1000.0: 1000,\n    }\n\n    ladder = []\n\n    for index, key in enumerate(tickIncrements):\n\n        increment = tickIncrements[key]\n\n        if (index+1) == len(tickIncrements):\n            ladder.append(key)\n        else:\n            key1 = [*tickIncrements][index]\n            key2 = [*tickIncrements][index+1]\n            steps = (key2 - key1) / increment\n\n            for i in range(int(steps)):\n                ladder.append(round(key + i * increment, 2))\n\n    return(ladder)\n\nbfticks = bfTickLadder()\n\n# Round a decimal to the betfair tick value below\ndef bfTickFloor(price, includeIndex=False):\n\n    if 'bfticks' in globals():\n        global bfticks\n    else:\n        bfticks = bfTickLadder()\n\n    ind = [ n for n,i in enumerate(bfticks) if i&amp;gt;=price][0]\n    if includeIndex:\n        if bfticks[ind]==price:\n            return((ind, price))\n        else:\n            return((ind-1, bfticks[ind-1]))\n    else:\n        if bfticks[ind]==price:\n            return(price)\n        else:\n            return(bfticks[ind-1])\n\n# Calculate the numder of ticks between two tick values\ndef bfTickDelta(p1, p2):\n\n    if np.isnan(p1) or np.isnan(p2):\n        return(np.nan)\n\n    x = bfTickFloor(p1, includeIndex=True)\n    y = bfTickFloor(p2, includeIndex=True)\n    return(x[0]-y[0])\n\n# %%\nbfTickDelta(13.5, 11)\n\n# %%\nbfTickDelta(3.5, 3.4)\n\n# %% [markdown]\n# Now that we have our functions let's plot the average market spread leading up to the jump.\n\n# %%\n# Group by odds range\naverageMarketSpread = (\n    df[['market_id', 'selection_id', 'seconds_before_scheduled_jump', 'back_best', 'lay_best']]\n    .assign(seconds_before_scheduled_jump_chunk = lambda x: x['seconds_before_scheduled_jump'].apply(chunkSBJ))\n    .assign(market_spread = lambda x: x.apply(lambda x: bfTickDelta(x.lay_best, x.back_best), axis=1))\n    .groupby([\"seconds_before_scheduled_jump_chunk\"], as_index = False)\n    .agg({'market_spread': 'mean'})\n)\n\n# %%\n\nfig_5 = px.line(\n    averageMarketSpread, \n    x='seconds_before_scheduled_jump_chunk', y='market_spread',\n    template='plotly_white',\n    title=\"Market Spread Leading Up To Jump\",\n    labels = {\n        'seconds_before_scheduled_jump_chunk': \"Seconds Before Scheduled Jump\",\n        \"market_spread\": \"Average Spread (b/w best back and lay)\"\n    }\n)\nfig_5.update_layout(font_family=\"Roboto\")\nfig_5['layout']['xaxis']['autorange'] = \"reversed\"\nfig_5.show(\"png\")\n\n# %% [markdown]\n# So much the same story as before, the spread tightens pretty dramatically leading all the way up to the jump. Now that we have a measure for market tightness on a selection level we can split it by odds range to see how tightness varies across odds range.\n\n# %%\n# Market Spread By Odds Group\n# _______________________\n\n# Group by odds range\naverageMarketSpreadOddsgrp = (\n    df[['market_id', 'selection_id', 'bsp','seconds_before_scheduled_jump', 'back_best', 'lay_best']]\n    .assign(seconds_before_scheduled_jump_chunk = lambda x: x['seconds_before_scheduled_jump'].apply(chunkSBJ))\n    .assign(bsp = lambda x: x['bsp'].apply(chunkBsp))\n    .assign(market_spread = lambda x: x.apply(lambda x: bfTickDelta(x.lay_best, x.back_best), axis=1))\n    .groupby([\"bsp\", \"seconds_before_scheduled_jump_chunk\"], as_index = False)\n    .agg({'market_spread': 'mean'})\n)\n\nfig_6 = px.line(\n    averageMarketSpreadOddsgrp, \n    x='seconds_before_scheduled_jump_chunk', y='market_spread', color = \"bsp\",\n    template='plotly_white',\n    title=\"Market Spread Leading Up To Jump\",\n    labels = {\n        'seconds_before_scheduled_jump_chunk': \"Seconds Before Scheduled Jump\",\n        \"market_spread\": \"Average Spread (b/w best back and lay)\"\n    }\n)\nfig_6.update_layout(font_family=\"Roboto\")\nfig_6['layout']['xaxis']['autorange'] = \"reversed\"\nfig_6.show(\"png\")\n\n# %% [markdown]\n# Now this gives us visibility of how the market tightens across groups of selections. Clearly the bottom end of the market takes a bit longer to tighten up which fits our understanding that there's generally less money, and less very sharp money on these selections.\n\n# %% [markdown]\n# ## 2.3 Market Moves\n# \n# Circling back to the motivation of this article: \n# \n# &amp;gt; How should you think about large price moves in betfair markets and how could you look to build secondary betting strategies by quantitatively analysing historical data\n# \n# Now that we have a good grasp on the data we've collected on how betfair markets form let's try to analyse big price moves. Here's a sample of our largest price moves in our race sample:\n# \n# \n\n# %%\n# Big Movers\n# ____________________\n\n(\n    df\n    .groupby(['market_id', 'selection_id', 'venue', 'selection_name', 'win', 'bsp'], as_index = False)\n    .agg({'ltp': 'max'})\n    .rename(columns = {'ltp': 'max_traded'})\n    .assign(ticks_in = lambda x: x.apply(lambda x: bfTickDelta(x.max_traded, x.bsp), axis=1))\n    .query('max_traded &amp;lt; 500')\n    .sort_values('ticks_in', ascending = False)\n    .head(10)\n)\n\n# %% [markdown]\n# Some of those moves are astounding: San Fabrizio traded at 10.50 in the last 30 mins before the jump and had a final BSP of 2.80, but lost. \n# \n# Like we discussed before observing a market plunge in hindsight may seem like a more powerful predictor than it actually is. For example, if we had have flat staked these 10 giant moves after they happened (say at the BSP) we'd be running at ~70% POT even though our intuition would have told you at the time that these horses couldn't lose!\n\n# %% [markdown]\n# ## 2.3.1 Reacting to moves after they happen\n# \n# Bookies, race callers and other services will often tell you what selection has been strong in the market; who the \"money\" has come for. But once you hear this is it already too late to do anything about?\n# \n# Let's extend the sample in the previous section to see if we can draw any broader conclusions.\n# \n# - I'll take a sample of the all selections 10 minutes before the scheduled jump\n# - I'll also take those same selections at the scheduled jump exactly\n# - I'll then calculate the number of ticks between the best back price at each time point to measure the movement in the last 10 minutes of trading\n# - I'll then calculate a back and lay profit using the top box at the scheduled off\n\n# %%\nm10 = (\n    df\n    .groupby(['market_id', 'selection_id'], as_index = False)\n    .apply(lambda g: None if g[g['seconds_before_scheduled_jump'] &amp;lt; 600].shape[0] == 0 else g[g['seconds_before_scheduled_jump'] &amp;lt; 600].sort_values('seconds_before_scheduled_jump', ascending = False).iloc[0])\n    .filter(items = ['market_id', 'selection_id', 'venue', 'selection_name', 'win', 'back_best', 'lay_best'])\n    .rename(columns = {'back_best': 'back_best_10m', 'lay_best': 'lay_best_10m'})\n)\n\nm0 = (\n    df\n    .groupby(['market_id', 'selection_id'], as_index = False)\n    .apply(lambda g: None if g[g['seconds_before_scheduled_jump'] &amp;lt; 0].shape[0] == 0 else g[g['seconds_before_scheduled_jump'] &amp;lt; 0].sort_values('seconds_before_scheduled_jump', ascending = False).iloc[0])\n    .filter(items = ['market_id', 'selection_id', 'venue', 'selection_name', 'win', 'back_best', 'lay_best'])\n)\n\n# %%\n# Back or lay according to large plunges\npd.DataFrame( \n    m10.merge(m0)\n    .assign(ticks_in = lambda x: x.apply(lambda x: bfTickDelta(x.back_best_10m, x.back_best), axis=1))\n    .query('ticks_in &amp;gt;= 15')\n    .assign(back_pl = lambda x:  np.where(x['win'] == 1, 0.95 * (x['back_best']-1), -1))\n    .assign(lay_pl = lambda x:  np.where(x['win'] == 1, -1*(x['lay_best']-1), 0.95))\n    .assign(stake = 1)\n    .agg({'stake': 'sum', 'back_pl': 'sum', 'lay_pl': 'sum'})\n)\n\n# %%\n# Back or lay according to large plunges - grouped by track\n( \n    m10.merge(m0)\n    .assign(ticks_in = lambda x: x.apply(lambda x: bfTickDelta(x.back_best_10m, x.back_best), axis=1))\n    .query('ticks_in &amp;gt;= 15')\n    .assign(back_pl = lambda x:  np.where(x['win'] == 1, 0.95 * (x['back_best']-1), -1))\n    .assign(lay_pl = lambda x:  np.where(x['win'] == 1, -1*(x['lay_best']-1), 0.95))\n    .assign(stake = 1)\n    .groupby('venue', as_index = False)\n    .agg({'stake': 'sum', 'back_pl': 'sum', 'lay_pl': 'sum'})\n)\n\n# %% [markdown]\n# As you can see there's not much of a pattern here to take advantage of here. At least for this sample of tracks, and for this time slice, the value has been sucked dry from the market on these selections. \n# To illustrate this let's check what the backers would have profited if they were the ones who could identify these selections prior to their big market moves\n\n# %%\n# Backing before the plunge?\n( \n    m10.merge(m0)\n    .assign(ticks_in = lambda x: x.apply(lambda x: bfTickDelta(x.back_best_10m, x.back_best), axis=1))\n    .query('ticks_in &amp;gt;= 15')\n    .assign(back_pl = lambda x:  np.where(x['win'] == 1, 0.95 * (x['back_best_10m']-1), -1))\n    .assign(lay_pl = lambda x:  np.where(x['win'] == 1, -1*(x['lay_best_10m']-1), 0.95))\n    .assign(stake = 1)\n    .agg({'stake': 'sum', 'back_pl': 'sum', 'lay_pl': 'sum'})\n)\n\n# %% [markdown]\n# I could sign up for winning at 50% POT! Now I just have to pick 100% of the large thoroughbred plunges before they happen... Obviously no one individual person or group is predicting these movements 100%. Even the sharpest players will have movements go against them but this highlights the essence of the problem and the contradiction of retrospective price movement analysis.\n# \n# These movements are important. There is a lot to be gained by purely trading betfair markets and how they move. However, identifting large moves **after** they occur isn't good enough. The large players sharpen the odds down to a good and fair implied chance and leave those watching on the sidelines with little to no value either side. To profit then we must capture some of the value as they are and jump on momentum as it's happening not after it's come to a halt.\n\n# %% [markdown]\n# ## 2.3.2 Visualising the moves\n# \n# In the final section I'll frame how I think you could go about identifying these moves as they're happening or about to happen. But first to illustrate some of dynamics let's visualise some of these large moves. How do selections firm and do they do it with different characteristics in different patterns? Candlestick charts are a good way to visualise the evolution of prices in markets and are often used in financial markets to do technical analysis.\n# \n# I'll first create a function to create a candlestick chart for a market / selection slice of our dataframe using plotly charts.\n# \n\n# %%\n# Candlestick Plotly Chart With Plotly\n# _____________________________\n\ndef priceCandlestickPlotly(d):\n\n    d['time_chunk'] = d['time'].dt.round('2min')  \n\n    selectionName = d.selection_name.iloc[0]\n    track = d.venue.iloc[0]\n    startTime = d.market_time.iloc[0]\n\n    candleStickInput = d.groupby('time_chunk', as_index=False).agg({\"ltp\": ['first', 'last', 'min', 'max']})\n\n    candleStickInput.columns = [\"_\".join(pair) for pair in candleStickInput.columns]\n\n    fig = go.Figure(data=[go.Candlestick(x=candleStickInput['time_chunk_'],\n                    open=candleStickInput['ltp_first'],\n                    high=candleStickInput['ltp_max'],\n                    low=candleStickInput['ltp_min'],\n                    close=candleStickInput['ltp_last'])])\n    fig.update_layout(\n        template=\"plotly_white\", \n        xaxis_rangeslider_visible=False,\n        title=f'{selectionName} at {track} race started at {startTime} UTC'\n    )\n    fig.show(\"png\")\n\n# %% [markdown]\n# Let's visualise the 2 biggest moves with this chart function.\n\n# %%\npriceCandlestickPlotly(df.query('market_id == \"1.182760338\" and selection_id == \"39317622\"'))\n\n# %% [markdown]\n# This looks exactly like what we'd expect from a extreme plunge: consistent, sustained and unrelenting support on the back side as the horse is pushed from 8-10 odds down to it's fair odds of around 3.\n# \n# Let's look at the second big plunge:\n\n# %%\npriceCandlestickPlotly(df.query('market_id == \"1.173069469\" and selection_id == \"35981261\"'))\n\n# %% [markdown]\n# This movement chart is a bit different. The market has 4 distinct segments:\n# \n# - About 30 minutes from the off the market changes and the horse is supported from 4 into 3\n# - It trades at around 3 for the next 25 minutes\n# - It then drifts back out sharply to a bit over 4\n# - It then gets crunched back in all the way into a BSP of 2.30\n# \n# Clearly different segments of the participants had very different perceptions of this horses chances. How do we make sense of it?\n# \n# Let's try a different candlestick visualistion, which includes the traded volume.\n\n# %%\nimport mplfinance as fplt\n\ndef priceCandlestickMpl(d):\n\n    d['time_chunk'] = d['time'].dt.round('2min')  \n\n    selectionName = d.selection_name.iloc[0]\n    track = d.venue.iloc[0]\n    startTime = d.market_time.iloc[0]\n\n    # Add traded volume in last interval\n    d = d.groupby(['market_id', 'selection_id']).apply(lambda x: x.assign(traded_volume_delta = lambda y: (y['traded_volume'] - y['traded_volume'].shift(1)).mask(pd.isnull, 0)))\n\n    candleStickInput = d.groupby('time_chunk', as_index=False).agg({\"ltp\": ['first', 'last', 'min', 'max'], \"traded_volume_delta\": 'sum'})\n\n    candleStickInput.columns = [\"_\".join(pair) for pair in candleStickInput.columns]\n\n    candleStickInput = candleStickInput.rename(columns = {'time_chunk_': 'date','ltp_first': 'open', 'ltp_last': 'close', 'ltp_min': 'low', 'ltp_max': 'high', 'traded_volume_delta_sum': 'volume'})\n\n    candleStickInput = candleStickInput.set_index('date')\n\n    fplt.plot(\n        candleStickInput,\n        type='candle',\n        style='yahoo',\n        title= f'{selectionName} at {track} race started at {startTime} UTC',\n        ylabel='Odds',\n        volume=True,\n        ylabel_lower='Volume Traded',\n    )\n\n# %% [markdown]\n# First the first horse:\n\n# %%\npriceCandlestickMpl(df.query('market_id == \"1.182760338\" and selection_id == \"39317622\"'))\n\n# %% [markdown]\n# Now the second selection:\n\n# %%\npriceCandlestickMpl(df.query('market_id == \"1.173069469\" and selection_id == \"35981261\"'))\n\n# %% [markdown]\n# So as the majority of the volume was traded right before the jump, the price was hammered into 2.30. This is a pattern that you might see a lot in betfair markets: prices may oscillate around or even drift consistently in one direction until a point at which certain large participants or groups of participants enter the market and push the selection in another direction completely.\n# \n# So anticipating a move won't be as simple as analysing the price as a time series, it's one piece of the puzzle. Really sophisticated market trading algorithms will need to historically analyse a raft of market metrics and correlate them with these movements historically within the market type of interest. In the next section I'll start off down that road to give you an idea of how I'd tackle the problem but it's more complicated than we can uncover in a single article.\n\n# %% [markdown]\n# ## 2.3.3 Anticipating a move\n# \n# Our retrospective analysis was good to help us understand the dynamics of movements, the efficiency of markets, how they form, and what things we could put in a predictive analysis. The next step is to create forward looking estimates about where the market is headed next.\n# \n# You get setup your problem in lots of different ways including creating rules based strategies like discussed in a [previous piece](https://betfair-datascientists.github.io/tutorials/automatedBettingAnglesTutorial/). Or you could go down a formalised machine learning approach. The first step to both is to build up some key features and test their predictive power.\n# \n# The factors I'll consider in this section will be:\n# \n# - The movement over the last 30 seconds\n# - The weight of money on the back side\n# - The weight of money on the lay side\n# - The current best back / volume weighted average top 3 back (top box support on back side)\n# - The current best lay / volume weighted average top 3 lay (top box support on lay side)\n# - The current best back / The volume weighted traded price over the last increment\n# - The current best lay  / The volume weighted traded price over the last increment\n# \n# And im interested in correlating these variables with the number + direction of ticks moved over the **next 30 seconds** to see if I can find anything interesting.\n# \n\n# %%\n# Start Analysis\n# ______________________________\n\ndfPredict = df.query('seconds_before_scheduled_jump &amp;lt;= 600 and seconds_before_scheduled_jump &amp;gt;= 0')\n\n# Target\n# ____________________________\n\ndfPredict['best_back_30s_in_future'] =  dfPredict.groupby(['market_id', 'selection_id'])['back_best'].shift(-30)\ndfPredict['best_lay_30s_in_future'] = dfPredict.groupby(['market_id', 'selection_id'])['back_best'].shift(-30)\n\ndfPredict['back_target'] = dfPredict.apply(lambda x: bfTickDelta(x.back_best, x.best_back_30s_in_future), axis=1)\ndfPredict['lay_target'] =  dfPredict.apply(lambda x: bfTickDelta(x.lay_best, x.best_lay_30s_in_future), axis=1)\n\n# Movement \n# ____________________________\n\ndfPredict['back_best_30s_ago'] = dfPredict.groupby(['market_id', 'selection_id'])['back_best'].shift(30)\ndfPredict['back_lay_30s_ago'] = dfPredict.groupby(['market_id', 'selection_id'])['lay_best'].shift(30)\n\ndfPredict = (\n    dfPredict\n    .assign(back_ticks_in_30s = lambda x: x.apply(lambda x: bfTickDelta(x.back_best_30s_ago, x.back_best), axis=1))\n    .assign(lay_ticks_in_30s = lambda x: x.apply(lambda x: bfTickDelta(x.back_lay_30s_ago, x.lay_best), axis=1))\n)\n\n# Weight Of Money\n# ____________________________\n\natb_ladder = dfPredict.atb_ladder.iloc[0]\natl_ladder = dfPredict.atl_ladder.iloc[0]\n\ndef wom(back_ladder, lay_ladder):\n\n    if not back_ladder or not lay_ladder:\n        return((None, None))\n\n    total_volume = round(sum(back_ladder['v']) + sum(lay_ladder['v']),2)\n\n    return((round(sum(back_ladder['v']) / total_volume,3), round(sum(lay_ladder['v']) / total_volume,3)))\n\n\ndfPredict['wom'] = dfPredict.apply(lambda x: wom(x['atb_ladder'], x['atl_ladder']), axis = 1)\n\ndfPredict[['back_wom', 'lay_wom']] = pd.DataFrame(dfPredict['wom'].tolist(), index=dfPredict.index)\n\n# Top Box Support\n# ________________________\n\ndfPredict['back_best_support'] = dfPredict['back_vwap'] / dfPredict['back_best']\ndfPredict['lay_best_support'] = dfPredict['lay_best'] / dfPredict['lay_vwap']\n\n# Recent Movement\n# _____________________\n\ndfPredict['wap_movement_10s'] = dfPredict['wap'] / dfPredict.groupby(['market_id', 'selection_id'])['wap'].shift(-10)\n\n# %% [markdown]\n# Now that we've created the candidate factors let's use a spearman correlation coefficient to analyse their correlation (positive or negative we're mostly focussed on the absolute size of the coefficient).\n\n# %%\ncorrMatrix = dfPredict[['back_target', 'back_ticks_in_30s', 'lay_ticks_in_30s', 'back_wom', 'lay_wom', 'back_best_support', 'lay_best_support', 'wap_movement_10s']].dropna().corr(method = \"spearman\")\ncorrMatrix[['back_target']]\n\n# %%\nimport seaborn as sns\n\ncorPlot = sns.heatmap(\n    corrMatrix, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\ncorPlot.set_xticklabels(\n    corPlot.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n)\n\n# %% [markdown]\n# We're entirely interested in the `back_target` column in the matrix and the heatmap. We can see that some of our candidate features have decent correlation with the number of ticks the price will move over the course of the next 30 seconds.\n# \n# \n# ## 2.3.4 Next Steps\n# \n# This marks the end of this analysis however if you're interested in turning this kind of analysis / approach into something more I'd suggest there's 2 main paths you could go down from here.\n# \n# 1. Angles + Rules\n# \n# You could dig into the data a little more a find specific situations where some of these features (or ones like them) preceeded certain things happening in the market (movements or increases in trading volatility etc) You could then construct trading rules based on these findings an try to automate them or bake them into trading rules inside a 3rd party tool.\n# \n# 2. Predictive Modelling\n# \n# If you were comfortable with statistical models or machine learning you could easily feed this data into a predictive modelling workflow. Once honed, the predictions from this workflow could be turned into recommended betting decisions which could form a part of an automated algorithmic betting framework. The work to fill out a project like this would be significant but so would the reward.\n# \n# \n# # 3.0 Conclusion\n# \n# Like previous articles this analysis is a sketch of what can be accomplished analysing the historical stream files. In this instance we focussed on the markets themselves, with a particular focussing on a small slice of markets: victorian thoroughbred markets. We analysed how these markets form, how to interpret the price movements on certain selections and the first steps to building out automated strategies based on these features alone. \n# \n# Building automated betting strategies based on the markets alone is path that plenty of quantitatively inclined betfair customers go down as it minimises complexity in source data (there's only 1!) and there's plenty of value in it left untapped for you to try to capture.\n# \n# \n</code></pre>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#do-theyknow-analysing-betfair-market-formation-market-movements","title":"Do #theyknow? Analysing Betfair market formation &amp; market movements","text":""},{"location":"tutorials/analysingAndPredictingMarketMovements/#cheat-sheet","title":"Cheat sheet","text":"<ul> <li> <p>If you're looking for the complete code head to the bottom of the page or download the script from Github.</p> </li> <li> <p>To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code), make sure you've navigated in the terminal to the folder you've saved the script in and then type <code>py main.py</code> (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. </p> </li> <li> <p>Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site. We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. </p> </li> <li> <p>We're using the <code>betfairlightweight</code> package to do the heavy lifting</p> </li> <li> <p>We've also posted the completed code logic on the <code>betfair-downunder</code> Github repo.</p> </li> </ul>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#00-setup","title":"0.0 Setup","text":""},{"location":"tutorials/analysingAndPredictingMarketMovements/#01-importing-libraries","title":"0.1 Importing libraries","text":"<p>Once again I'll be presenting the analysis in a Jupyter notebook and will be using Python as a programming language.</p> <p>Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer locally where you want to store the intermediate data files.</p> <p>You'll also need <code>betfairlightweight</code> which you can install with something like <code>pip install betfairlightweight</code>.</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#02-context","title":"0.2 Context","text":"<p>You may have seen the hashtag if you're on Australian racing twitter #theyknow following a dramatic late market move for a horse that's followed by a decisive race victory. Sometimes it can seem eerie how accurate these moves are after the fact. In Betfair racing markets there's usually a flurry of activity leading up to the race start as players look to get their bets down at the best price without tipping their hand (opinion on the race) too much. Large moves can happen when large players rally around a selection who's implied chance in early trading isn't close to what it's true chance is in the upcoming race. Large moves can also happen when there's some inside information - not able to be gleaned from analysis of the horses previous races - that slowly filters out of the stable or training group.</p> <p>This creates opportunity in the secondary market as punters try to read these movements to make bets themselves. The task often becomes identifying which movements are caused by these sophisticated players or represent real signals of strength and which aren't.</p> <p>So do #theyknow generally? Before even looking at the data I can assure you that yes they do know pretty well. Strong movements in betting markets are usually pretty reliable indicators about what is about to happen. However, these moves can be overvalued by recreationals. When observing a horse plumet in from \\$3.50 to \\$2 you are usually suprised if the horse loses, but the general efficiency of late prices would suggest that this horse is going to still lose 50% of time. If you simply observe the large moves after the sharp players have corrected the market landscape you're in no better a position to bet than before the move happened. On the other hand what if would could reliably identify the move as it was happening or about to happen? That would be a recipe for successful trading of horse racing markets and no doubt this is what many players in this secondary market (analysis of Betfair markets rather than the races themselves) try to do.</p> <p>If you were to build up a manual qualitative strategy for this kind of market trading you need to understand: - Who the participants are - How sophisticated they are at the top end - What types of races do they bet on and for how much - When the different types of participants typically enter the market  - What do bet placement patterns look like for these participants - etc.</p> <p>This is the kind of task that takes a lot research, years of experience watching markets, a lot of trial and error, and a lot of industry know-how. Given I'm a lazy quantitative person I'll try to see if I can uncover any of these patterns in the data alone.</p> <p>Put simply the central question for the second half of this piece will be:</p> <p>&gt; How should you think about large price moves in Betfair markets and how could you look to build secondary betting strategies by quantitatively analysing historical data</p> <p>I'll just be analysing historical thoroughbred markets but the same approach could be applied to any sport or racing code of your interest.</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#03-this-example","title":"0.3 This example","text":"<p>Building market based trading strategies is a broad and fertile ground for many quantitative Betfair customers; too big to cover in a single article. I'll try to zero in on a small slice of thoroughbred markets and analyse how these markets form and how I'd start the process of trying to find the patterns in the market movements. Again hopefully this is some inspiration for you and you can pick up some of the ideas and build them out.</p> <p>Given volume of data (when analysing second by second slices of market data) I'll be looking at a year's worth of thoroughbred races from the 5 largest Victorian tracks: Flemington, Caulfield, Moonee Valley, Bendigo and Sandown.</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#10-data","title":"1.0 Data","text":"<p>Unlike in some of the previous tutorials we aren't going to collapse the stream data into a single row per runner. In those examples we were interested in analysing some discrete things about selections in Betfair markets like:</p> <ul> <li>Their final odds (bsp or last traded price)</li> <li>Their odds at some fixed time point or time points before the scheduled race start</li> <li>Other single number descriptors of the trading activity on a selection (eg total traded volume)</li> </ul> <p>In this analysis I want to analyse how markets form and prices move for selections as markets evolve. So we'll need to pull out multiple price points per runner - so we'll have multiple rows per runner in our parsed output dataset.</p> <p>To output a row for every stream update for every selection in every thoroughbred race over the last 12 months would produce a dataset far too big too analyse using normal data analysis tools - we're about 10s to 100s of billions of rows.</p> <p>To chop our sample down into a manageable slice I'm going to filter on some select tracks of interest (as mentioned above) and I'm also going to have 3 sections of data granularity:</p> <ul> <li>I won't log any of the odds or traded volumes &gt; 30mins before the scheduled off<ul> <li>In thoroughbreds there is non-trivial action before this point you may want to study, but it's not what I want to study here</li> </ul> </li> <li>Between 30 and 10 minutes before the scheduled off I'll log data every 60 seconds</li> <li>10 minutes or less to the scheuled off I'll log prices every second</li> </ul> <p>The code to manage this windowed granularity is in the below parsing code tweak as you wish if you want to tighten or broaden the analysis.</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#11-sourcing-data","title":"1.1 Sourcing data","text":"<p>First you'll need to source the Stream file TAR archive files. I'll be analysing 12 months of Australian thoroughbred Pro files. Ask automation@betfair.com.au for more info if you don't know how to do this. Once you've gotten access download them to your computer and store them together in a folder.</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#12-utility-functions","title":"1.2 Utility functions","text":""},{"location":"tutorials/analysingAndPredictingMarketMovements/#13-selection-metadata","title":"1.3 Selection metadata","text":"<p>Given that the detailed price data will have so many records we will split out the selection metadata (including the selection win outcome flag and the bsp) into it's own dataset much you would do in a relational database to manage data volumes.</p> <p>This means we'll have to parse over the data twice but our outputs will be much smaller than if we duplicated the selection name 800 times for example.</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#14-detailed-preplay-odds","title":"1.4 Detailed preplay odds","text":"<p>Like mentioned above there will be some time control logic injected to control the time granularity that odds are recorded in each step.</p> <p>Instead of widening the available to bet price ladder I'll extract the top 10 rungs of the available to back (atb) and available to lay (atl) ladders and write them both to the output file. That will give me more flexibility during the analysis to pull out things that interest me. So in total I'll extract:</p> <ul> <li>Top 10 ATB Ladder</li> <li>Top 10 ATL Ladder</li> <li>Total Traded Volume</li> <li>Volume weighted average traded price up till the current time</li> <li>Last Traded price</li> </ul>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#20-analysis","title":"2.0 Analysis","text":""},{"location":"tutorials/analysingAndPredictingMarketMovements/#21-load-and-assemble","title":"2.1 Load and Assemble","text":"<p>First let's load the raw datafiles we created in the previous step. </p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#211-load","title":"2.1.1 Load","text":"<p>We have the highlevel selection metadata (1 row per selection):</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#212-assemble","title":"2.1.2 Assemble","text":""},{"location":"tutorials/analysingAndPredictingMarketMovements/#213-transform","title":"2.1.3 Transform","text":"<p>Next we'll do some processing on the joined dataframe to add some columns that we can use in our analysis including calculating a numeric #seconds before the scheduled jump field that we'll use extensively.</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#22-market-formation","title":"2.2 Market formation","text":"<p>Before we analyse how prices for selections move let's understand some basic things about how thoroughbred markets form.</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#221-traded-volumes","title":"2.2.1 Traded Volumes","text":"<p>Let's look at how a typical market (at one of these 5 tracks) trades leading up to the scheduled race start.</p> <p>To make some of the analysis a little bit cleaner we need to pad out missing odds updates. For example for a given market we might have a market update 140 seconds before the jump but not another one till 132 seconds before the jump. We'll add rows for those interim 8 seconds by filling the values from the the previous row, this is required to iron out some idiosyncracies in the aggregations, it's not that important to follow if you don't understand it.</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#222-market-tightness","title":"2.2.2 Market Tightness","text":"<p>Understanding how the market tightens before the off is also another key conceptual component to market formation. I will consider two different variations on this concept:</p> <ul> <li>Market overround or Market Percentage<ul> <li>The sum of probabilities across all outcomes</li> <li>Back % are always above 1 (else there exists an arbitrage opportunity)</li> <li>Lay % are always below 1 (else there exists an arbitrage opportunity)</li> </ul> </li> <li>Market Spread<ul> <li>The # of ticks / rungs between the best available back price and the best available lay price</li> </ul> </li> </ul> <p>The market % is the value displayed on the Betfair website here:</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#23-market-moves","title":"2.3 Market Moves","text":"<p>Circling back to the motivation of this article: </p> <p>&gt; How should you think about large price moves in Betfair markets and how could you look to build secondary betting strategies by quantitatively analysing historical data</p> <p>Now that we have a good grasp on the data we've collected on how Betfair markets form let's try to analyse big price moves. Here's a sample of our largest price moves in our race sample:</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#231-reacting-to-moves-after-they-happen","title":"2.3.1 Reacting to moves after they happen","text":"<p>Bookies, race callers and other services will often tell you what selection has been strong in the market; who the \"money\" has come for. But once you hear this is it already too late to do anything about?</p> <p>Let's extend the sample in the previous section to see if we can draw any broader conclusions.</p> <ul> <li>I'll take a sample of the all selections 10 minutes before the scheduled jump</li> <li>I'll also take those same selections at the scheduled jump exactly</li> <li>I'll then calculate the number of ticks between the best back price at each time point to measure the movement in the last 10 minutes of trading</li> <li>I'll then calculate a back and lay profit using the top box at the scheduled off</li> </ul>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#232-visualising-the-moves","title":"2.3.2 Visualising the moves","text":"<p>In the final section I'll frame how I think you could go about identifying these moves as they're happening or about to happen. But first to illustrate some of dynamics let's visualise some of these large moves. How do selections firm and do they do it with different characteristics in different patterns? Candlestick charts are a good way to visualise the evolution of prices in markets and are often used in financial markets to do technical analysis.</p> <p>I'll first create a function to create a candlestick chart for a market / selection slice of our dataframe using plotly charts.</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#233-anticipating-a-move","title":"2.3.3 Anticipating a move","text":"<p>Our retrospective analysis was good to help us understand the dynamics of movements, the efficiency of markets, how they form, and what things we could put in a predictive analysis. The next step is to create forward looking estimates about where the market is headed next.</p> <p>You get setup your problem in lots of different ways including creating rules based strategies like discussed in a previous piece. Or you could go down a formalised machine learning approach. The first step to both is to build up some key features and test their predictive power.</p> <p>The factors I'll consider in this section will be:</p> <ul> <li>The movement over the last 30 seconds</li> <li>The weight of money on the back side</li> <li>The weight of money on the lay side</li> <li>The current best back / volume weighted average top 3 back (top box support on back side)</li> <li>The current best lay / volume weighted average top 3 lay (top box support on lay side)</li> <li>The current best back / The volume weighted traded price over the last increment</li> <li>The current best lay  / The volume weighted traded price over the last increment</li> </ul> <p>And im interested in correlating these variables with the number + direction of ticks moved over the next 30 seconds to see if I can find anything interesting.</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#234-next-steps","title":"2.3.4 Next Steps","text":"<p>This marks the end of this analysis however if you're interested in turning this kind of analysis / approach into something more I'd suggest there's two main paths you could go down from here.</p> <ol> <li> <p>Angles + Rules</p> <p>You could dig into the data a little more a find specific situations where some of these features (or ones like them) preceeded certain things happening in the market (movements or increases in trading volatility etc) You could then construct trading rules based on these findings an try to automate them or bake them into trading rules inside a 3rd party tool.</p> </li> <li> <p>Predictive Modelling</p> <p>If you were comfortable with statistical models or machine learning you could easily feed this data into a predictive modelling workflow. Once honed, the predictions from this workflow could be turned into recommended betting decisions which could form a part of an automated algorithmic betting framework. The work to fill out a project like this would be significant but so would the reward.</p> </li> </ol>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#30-conclusion","title":"3.0 Conclusion","text":"<p>Like previous articles this analysis is a sketch of what can be accomplished analysing the historical stream files. In this instance we focussed on the markets themselves, with a particular focussing on a small slice of markets: victorian thoroughbred markets. We analysed how these markets form, how to interpret the price movements on certain selections and the first steps to building out automated strategies based on these features alone. </p> <p>Building automated betting strategies based on the markets alone is path that plenty of quantitatively inclined Betfair customers go down as it minimises complexity in source data (there's only one!) and there's plenty of value in it left untapped for you to try to capture.</p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#complete-code","title":"Complete code","text":"<p>Run the code from your ide by using <code>py &lt;filename&gt;.py</code>, making sure you amend the path to point to your input data. </p> <p>Download from Github </p>"},{"location":"tutorials/analysingAndPredictingMarketMovements/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/","title":"Automated betting angles in Python","text":"<p>This tutorial was written by Tom Bishop and was originally published on Github. It is shared here with his permission. </p> <p>This tutorial follows on logically from the JSON to CSV tutorial and backteseting ratings in Python tutorial we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at those tutorials before diving into this one. </p> <p>As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! </p> <p>This article was written more than 2 years ago and some packages used here will have changed since the article was written. Continue at your peril</p> <pre><code>import requests\nimport pandas as pd\nfrom datetime import date, timedelta\nimport numpy as np\nimport os\nimport re\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\nimport logging\nimport yaml\nfrom unittest.mock import patch\nfrom typing import List, Set, Dict, Tuple, Optional\nfrom itertools import zip_longest\nimport betfairlightweight\nfrom betfairlightweight import StreamListener\nfrom betfairlightweight.resources.bettingresources import (\n    PriceSize,\n    MarketBook\n)\nfrom scipy.stats import t\nimport plotly.express as px\n</code></pre> <p></p> <p>For the purposes of this article I'm interested in backtesting some betting angles at the BSP, using some indication of price momentum/market support in some angles, and testing some back to lay strategies so we'll need to pull out some information about each runners in-play trading. </p> <p>So we'll extract the following for each runner: - BSP - Last Traded Price - Volume Weighted Avg Price (top 3 boxes) 5 mins before the scheduled jump time - Volume Weighted Avg Price (top 3 boxes) 30 seconds before the scheduled jump time - The volume traded on the selection - The minimum \"best available to lay\" price offered inplay (which is a measure of how low the selection traded during the race)</p> <p>First we'll establish some utility functions needed to parse the data. Most of these were discussed in the previous backtest your ratings tutorial.</p> <pre><code># Utility Functions For Stream Parsing\n# _________________________________\n\ndef as_str(v) -&amp;gt; str:\n    return '%.2f' % v if type(v) is float else v if type(v) is str else ''\n\ndef split_anz_horse_market_name(market_name: str) -&amp;gt; (str, str, str):\n    parts = market_name.split(' ')\n    race_no = parts[0] # return example R6\n    race_len = parts[1] # return example 1400m\n    race_type = parts[2].lower() # return example grp1, trot, pace\n    return (race_no, race_len, race_type)\n\ndef filter_market(market: MarketBook) -&amp;gt; bool: \n    d = market.market_definition\n    return (d.country_code == 'AU' \n        and d.market_type == 'WIN' \n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n\ndef load_markets(file_paths):\n    for file_path in file_paths:\n        print(file_path)\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n\n    return None\n\ndef slicePrice(l, n):\n    try:\n        x = l[n].price\n    except:\n        x = np.nan\n    return(x)\n\ndef sliceSize(l, n):\n    try:\n        x = l[n].size\n    except:\n        x = np.nan\n    return(x)\n\ndef wapPrice(l, n):\n    try:\n        x = round(sum( [rung.price * rung.size for rung in l[0:(n-1)] ] ) / sum( [rung.size for rung in l[0:(n-1)] ]),2)\n    except:\n        x = np.nan\n    return(x)\n\ndef ladder_traded_volume(ladder):\n    return(sum([rung.size for rung in ladder]))\n</code></pre> <p>Then we'll create our core execution functions that will scan over the historical stream files and use <code>betfairlightweight</code> to recreate the state of the exchange for each thoroughbred market and extract key information for each selection</p> <pre><code># Core Execution Fucntions\n# _________________________________\n\ndef extract_components_from_stream(s):\n\n    with patch(\"builtins.open\", lambda f, _: f):   \n\n        evaluate_market = None\n        prev_market = None\n        postplay = None\n        preplay = None\n        t5m = None\n        t30s = None\n        inplay_min_lay = None\n\n        gen = s.get_generator()\n\n        for market_books in gen():\n\n            for market_book in market_books:\n\n                # If markets don't meet filter return None's\n                if evaluate_market is None and ((evaluate_market := filter_market(market_book)) == False):\n                    return (None, None, None, None, None, None)\n\n                # final market view before market goes in play\n                if prev_market is not None and prev_market.inplay != market_book.inplay:\n                    preplay = market_book\n\n                # final market view before market goes is closed for settlement\n                if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n                    postplay = market_book\n\n                # Calculate Seconds Till Scheduled Market Start Time\n                seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n\n                # Market at 30 seconds before scheduled off\n                if t30s is None and seconds_to_start &amp;lt; 30:\n                    t30s = market_book\n\n                # Market at 5 mins before scheduled off\n                if t5m is None and seconds_to_start &amp;lt; 5*60:\n                    t5m = market_book\n\n                # Manage Inplay Vectors\n                if market_book.inplay:\n\n                    if inplay_min_lay is None:\n                        inplay_min_lay = [ slicePrice(runner.ex.available_to_lay,0) for runner in market_book.runners]\n                    else:\n                        inplay_min_lay = np.fmin(inplay_min_lay, [ slicePrice(runner.ex.available_to_lay,0) for runner in market_book.runners])\n\n                # update reference to previous market\n                prev_market = market_book\n\n        # If market didn't go inplay\n        if postplay is not None and preplay is None:\n            preplay = postplay\n            inplay_min_lay = [\"\" for runner in market_book.runners]\n\n        return (t5m, t30s, preplay, postplay, inplay_min_lay, prev_market) # Final market is last prev_market\n\ndef parse_stream(stream_files, output_file):\n\n    with open(output_file, \"w+\") as output:\n\n        output.write(\"market_id,selection_id,selection_name,wap_5m,wap_30s,bsp,ltp,traded_vol,inplay_min_lay\\n\")\n\n        for file_obj in load_markets(stream_files):\n\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            (t5m, t30s, preplay, postplay, inplayMin, final) = extract_components_from_stream(stream)\n\n            # If no price data for market don't write to file\n            if postplay is None or final is None or t30s is None:\n                continue; \n\n            # All runner removed\n            if all(runner.status == \"REMOVED\" for runner in final.runners):\n                continue\n\n            runnerMeta = [\n                {\n                    'selection_id': r.selection_id,\n                    'selection_name': next((rd.name for rd in final.market_definition.runners if rd.selection_id == r.selection_id), None),\n                    'selection_status': r.status,\n                    'sp': r.sp.actual_sp\n                }\n                for r in final.runners \n            ]\n\n            ltp = [runner.last_price_traded for runner in preplay.runners]\n\n            tradedVol = [ ladder_traded_volume(runner.ex.traded_volume) for runner in postplay.runners ]\n\n            wapBack30s = [ wapPrice(runner.ex.available_to_back, 3) for runner in t30s.runners]\n\n            wapBack5m = [ wapPrice(runner.ex.available_to_back, 3) for runner in t5m.runners]\n\n            # Writing To CSV\n            # ______________________\n\n            for (runnerMeta, ltp, tradedVol, inplayMin, wapBack5m, wapBack30s) in zip(runnerMeta, ltp, tradedVol, inplayMin, wapBack5m, wapBack30s):\n\n                if runnerMeta['selection_status'] != 'REMOVED':\n\n                    output.write(\n                        \"{},{},{},{},{},{},{},{},{}\\n\".format(\n                            str(final.market_id),\n                            runnerMeta['selection_id'],\n                            runnerMeta['selection_name'],\n                            wapBack5m,\n                            wapBack30s,\n                            runnerMeta['sp'],\n                            ltp,\n                            round(tradedVol),\n                            inplayMin\n                        )\n                    )\n</code></pre> <p>Finally, after sourcing and downloading 12 months of stream files (ask automation@betfair.com.au for more info if you don't know how to do this) we'll use the above code to parse each file and write to a single csv file to be used for analysis.</p> <pre><code># Description:\n#   Will loop through a set of stream data archive files and extract a few key pricing measures for each selection\n# Estimated Time:\n#   ~6 hours\n\n# Parameters\n# _________________________________\n\n# trading = betfairlightweight.APIClient(\"username\", \"password\")\n\n# listener = StreamListener(max_latency=None)\n\n# stream_files = glob.glob(\"[PATH TO LOCAL FOLDER STORING ARCHIVE FILES]*.tar\")\n# output_file = \"[SOME OUTPUT DIRECTORY]/thoroughbred-odds-2021.csv\"\n\n# Run\n# _________________________________\n\n# if __name__ == '__main__':\n#     parse_stream(stream_files, output_file)\n</code></pre> <p>If you're building a fundamental bottom-up model, finding and managing ETL from an appropriate data source is a large part of the exercise. If your needs are simpler (for these types of automated strategies for example) there's plenty of good information that's available right inside the betfair API itself. </p> <p>The <code>RUNNER_METADATA</code> slot inside the <code>listMarketCatalogue</code> response for example will return a pretty good slice of metadata about the horses racing in upcoming races including but not limited to: the trainer, the jockey, the horses age, and a class rating. The documentaion for this endpoint will give you the full extent of this what's inside this response.</p> <p>Our problem for this exercise is that the historical stream files don't include this <code>RUNNER_METADATA</code> so we weren't able to extract it in the previous step. However, a sneaky workaround is to use an unsuppoerted back-end endpoint, one which Betfair use for the Hub racing results page.  </p> <p>These API endpoints are:</p> <ul> <li>Market result data: https://apigateway.betfair.com.au/hub/raceevent/1.154620281</li> <li>Day\u2019s markets: https://apigateway.betfair.com.au/hub/racecard?date=2018-12-18</li> </ul> <pre><code>def getBfMarkets(dte):\n\n    url = 'https://apigateway.betfair.com.au/hub/racecard?date={}'.format(dte)\n\n    responseJson = requests.get(url).json()\n\n    marketList = []\n\n    for meeting in responseJson['MEETINGS']:\n        for markets in meeting['MARKETS']:\n            marketList.append(\n                {\n                    'date': dte,\n                    'track': meeting['VENUE_NAME'],\n                    'country': meeting['COUNTRY'],\n                    'race_type': meeting['RACE_TYPE'],\n                    'race_number': markets['RACE_NO'],\n                    'market_id': str('1.' + markets['MARKET_ID']),\n                    'start_time': markets['START_TIME']\n                }\n            )\n\n    marketDf = pd.DataFrame(marketList)\n\n    return(marketDf)\n</code></pre> <pre><code>def getBfRaceMeta(market_id):\n\n    url = 'https://apigateway.betfair.com.au/hub/raceevent/{}'.format(market_id)\n\n    responseJson = requests.get(url).json()\n\n    if 'error' in responseJson:\n        return(pd.DataFrame())\n\n    raceList = []\n\n    for runner in responseJson['runners']:\n\n        if 'isScratched' in runner and runner['isScratched']:\n            continue\n\n        # Jockey not always populated\n        try:\n            jockey = runner['jockeyName']\n        except:\n            jockey = \"\"\n\n        # Place not always populated\n        try:\n            placeResult = runner['placedResult']\n        except:\n            placeResult = \"\"\n\n        # Place not always populated\n        try:\n            trainer = runner['trainerName']\n        except:\n            trainer = \"\"\n\n        raceList.append(\n            {\n                'market_id': market_id,\n                'weather': responseJson['weather'],\n                'track_condition': responseJson['trackCondition'],\n                'race_distance': responseJson['raceLength'],\n                'selection_id': runner['selectionId'],\n                'selection_name': runner['runnerName'],\n                'barrier': runner['barrierNo'],\n                'place': placeResult,\n                'trainer': trainer,\n                'jockey': jockey,\n                'weight': runner['weight']\n            }\n        )\n\n    raceDf = pd.DataFrame(raceList)\n\n    return(raceDf)\n</code></pre> <pre><code>def scrapeThoroughbredBfDate(dte):\n\n    markets = getBfMarkets(dte)\n\n    if markets.shape[0] == 0:\n        return(pd.DataFrame())\n\n    thoMarkets = markets.query('country == \"AUS\" and race_type == \"R\"')\n\n    if thoMarkets.shape[0] == 0:\n        return(pd.DataFrame())\n\n    raceMetaList = []\n\n    for market in thoMarkets.market_id:\n        raceMetaList.append(getBfRaceMeta(market))\n\n    raceMeta = pd.concat(raceMetaList)\n\n    return(markets.merge(raceMeta, on = 'market_id'))\n</code></pre> <pre><code># Executing the wrapper for an example date\nscrapeThoroughbredBfDate(date(2021,2,10))\n</code></pre> date track country race_type race_number market_id start_time weather track_condition race_distance selection_id selection_name barrier place trainer jockey weight 0 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 38448397 Triple Missile 3 1 Todd Harvey Paul Harvey 60.0 1 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 28763768 Shock Result 5 4 P H Jordan Craig Staples 59.5 2 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 8772321 Secret Plan 6 3 G &amp; A Williams William Pike 59.0 3 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 9021011 Command Force 2 0 Daniel &amp; Ben Pearce J Azzopardi 58.0 4 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 38448398 Fish Hook 7 2 M P Allan Madi Derrick 57.5 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 458 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 133456 Sedition 12 2 Richard Litt Ms Rachel King 58.0 459 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 38447782 Amusez Moi 9 6 Richard Litt Josh Parr 57.0 460 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 25388274 Savoury 1 5 Bjorn Baker Jason Collett 57.0 461 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 38447783 Born A Warrior 7 3 Michael &amp; Wayne &amp; John Hawkes Tommy Berry 56.5 462 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 38447784 Newsreader 10 1 John O'shea James Mcdonald 55.5 <p>463 rows \u00d7 17 columns</p> <p>Then to produce a historical slice of all races between two dates we could just loop over a set of dates and append each results set</p> <pre><code># Description:\n#   Will loop through a set of dates (starting July 2020 in this instance) and return race metadata from betfair \n# Estimated Time:\n#   ~60 mins\n# \n# dataList = []\n# dateList = pd.date_range(date(2020,7,1),date.today()-timedelta(days=1),freq='d')\n# for dte in dateList:\n#     dte = dte.date()\n#     print(dte)\n#     races = scrapeThoroughbredBfDate(dte)\n#     dataList.append(races)\n# data = pd.concat(dataList)\n# data.to_csv(\"[LOCAL PATH SOMEWHERE]\", index=False)\n</code></pre> <pre><code>def bet_eval_metrics(d, side = False):\n\n    metrics = pd.DataFrame(d\n    .agg({\"npl\": \"sum\", \"stake\": \"sum\", \"win\": \"mean\"})\n    ).transpose().assign(pot=lambda x: x['npl'] / x['stake'])\n\n    return(metrics[metrics['stake'] != 0])\n</code></pre> <pre><code># Local Paths (will be different on your machine)\npath_odds_local = \"[PATH TO YOUR LOCAL FILES]/thoroughbred-odds-2021.csv\"\npath_race_local = \"[PATH TO YOUR LOCAL FILES]/thoroughbred-race-data.csv\"\n\nodds = pd.read_csv(path_odds_local, dtype={'market_id': object, 'selection_id': object})\nrace = pd.read_csv(path_race_local, dtype={'market_id': object, 'selection_id': object})\n</code></pre> <pre><code>odds.head(3)\n</code></pre> market_id selection_id selection_name wap_5m wap_30s bsp ltp traded_vol inplay_min_lay 0 1.179845158 23493550 1. Larmour 6.27 5.84 6.20 6.2 8277 1.19 1 1.179845158 16374800 3. Careering Away 3.31 3.67 3.60 3.65 18592 1.08 2 1.179845158 19740699 4. Bells N Bows 6.87 6.36 6.62 6.4 7413 1.42 <pre><code>race.head(3)\n</code></pre> date track country race_type race_number market_id start_time weather track_condition race_distance selection_id selection_name barrier place trainer jockey weight 0 2020-07-01 Balaklava AUS R 1 1.171091087 2020-07-01 02:40:00 FINE GOOD4 2200 19674744 Baldy 2 4.0 Peter Nolan Karl Zechner 59.0 1 2020-07-01 Balaklava AUS R 1 1.171091087 2020-07-01 02:40:00 FINE GOOD4 2200 401615 Nostrovia 4 7.0 Dennis O'leary Margaret Collett 59.0 2 2020-07-01 Balaklava AUS R 1 1.171091087 2020-07-01 02:40:00 FINE GOOD4 2200 26789410 Ammo Loco 5 1.0 John Hickmott Barend Vorster 58.5 <pre><code># Joining two datasets\ndf = race.merge(odds.loc[:, odds.columns != 'selection_name'], how = \"inner\", on = ['market_id', 'selection_id'])\n</code></pre> <pre><code># I'll also add columns for the net profit from backing and laying each selection to be picked up in subsequent sections\ndf['back_npl'] = np.where(df['place'] == 1, 0.95 * (df['bsp']-1), -1)\ndf['lay_npl'] = np.where(df['place'] == 1, -1 * (df['bsp']-1), 0.95)\n</code></pre> <pre><code>def distance_group(distance):\n\n    if distance is None:\n        return(\"missing\")\n    elif distance &amp;lt; 1100:\n        return(\"sprint\")\n    elif distance &amp;lt; 1400:\n        return(\"mid_short\")\n    elif distance &amp;lt; 1800:\n        return(\"mid_long\")\n    else:\n        return(\"long\")\n\ndef barrier_group(barrier):\n    if barrier is None:\n        return(\"missing\")\n    elif barrier &amp;lt; 4:\n        return(\"inside\")\n    elif barrier &amp;lt; 9:\n        return(\"mid_field\")\n    else:\n        return(\"outside\")\n\n\ndf['distance_group'] = pd.to_numeric(df.race_distance, errors = \"coerce\").apply(distance_group)\ndf['barrier_group'] = pd.to_numeric(df.barrier, errors = \"coerce\").apply(barrier_group)\n</code></pre> <pre><code>dfTrain = df.query('date &amp;lt; \"2021-04-01\"')\ndfTest = df.query('date &amp;gt;= \"2021-04-01\"')\n\n'{} rows in the \"training\" set and {} rows in the \"test\" data'.format(dfTrain.shape[0], dfTest.shape[0])\n</code></pre> <pre>\n<code>'119244 rows in the \"training\" set and 40783 rows in the \"test\" data'</code>\n</pre> <pre><code>(\n    dfTrain\n    .assign(stake=1)\n    .groupby('selection_name', as_index = False)\n    .agg({'back_npl': 'sum', 'stake': 'sum'})\n    .assign(pot=lambda x: x['back_npl'] / x['stake'])\n    .sort_values('pot', ascending=False)  \n    .head(3) \n)\n</code></pre> selection_name back_npl stake pot 12247 Little Vulpine 274.550 1 274.550000 15384 Not Tonight Dear 130.701 1 130.701000 9987 Im Cheeky 617.307 7 88.186714 <p>So back Little Vulpine whenever it races? We all know intuitively what's wrong with that betting angle - it's raced one time in our sample and happened to win at a bsp of 270. Sample size and variance are dominating this simple measure of historical POT.</p> <p>Instead what we can do is treat the historical betting outcomes as a random variable and apply some statistical tests of signifance to them. A more detailed discussion of this particular test can be found here as can an excel calculator you can input your stats into. I'll simply translate the test to python to enable it's use when formulating our betting angles.</p> <p>The TLDR version of this test is that; based on your bet sample size, your profit, and the average odds across that sample of bets, the calculation produces a p value which estimates the probability your profit (or loss) happened by pure chance (where chance would be an expectation of breakeven betting simply at fair odds).</p> <pre><code>def pl_pValue(number_bets, npl, stake, average_odds):\n\n    pot = npl / stake\n\n    tStatistic = (pot * np.sqrt(number_bets)) / np.sqrt( (1 + pot) * (average_odds - 1 - pot) )\n\n    pValue = 2 * t.cdf(-abs(tStatistic), number_bets-1)\n\n    return(np.where(np.logical_or(np.isnan(pValue), pValue == 0), 1, pValue))\n</code></pre> <p>That doesn't mean we can formulate our angles and use this metric (and this metric alone) to validate their profitability. You'll find that it will give you misleading results in some instances. As analysts we're also prone to finding infinite different ways to unintentionally overfit our analysis as you might have heard elsewhere described as the concept of p-hacking, but it does give us an extra filter to cast over our hypotheses before really validating them with out-of-sample testing.</p> <p>The first thing I'll test is whether or not there are any combinations of track/distance/barrier where backing or laying could produce robust long term profit. This probably fits within the types of betting angles people before you have already sucked all the value out of long before you started reading this article. That's not to say that you shouldn't test them though, as people have made livings on betting angles as simple as these.</p> <pre><code># Calculate the profit (back and lay) and average odds across all track / distance / barrier group combos\ntrackDistanceBarrier = (\n    dfTrain\n    .assign(stake = 1)\n    .assign(odds = lambda x: x['bsp'])\n    .groupby(['track', 'race_distance', 'barrier_group'], as_index=False)\n    .agg({'back_npl': 'sum', 'lay_npl': 'sum','stake': 'sum', 'odds': 'mean'})\n)\n\ntrackDistanceBarrier\n</code></pre> track race_distance barrier_group back_npl lay_npl stake odds 0 Albany 1000 inside 11.2550 -11.95 2 15.450000 1 Albany 1000 mid_field -5.0000 4.75 5 101.136000 2 Albany 1000 outside -5.0000 4.75 5 88.374000 3 Albany 1100 inside -3.0525 2.70 6 29.430000 4 Albany 1100 mid_field -6.4040 5.92 9 37.483333 ... ... ... ... ... ... ... ... 6325 York 1500 inside 1.8995 -2.41 6 41.195000 6326 York 1500 mid_field -7.0000 6.65 7 32.472857 6327 York 1920 inside -3.0000 2.85 3 21.883333 6328 York 1920 mid_field -0.3520 -0.04 5 20.978000 6329 York 1920 outside -2.0000 1.90 2 21.450000 <p>6330 rows \u00d7 7 columns</p> <p>So it looks like over 2 selections jumping from the inside 3 barriers at Albany 1000m you would have made a healthy profit if you'd decide to back them historically. </p> <p>Let's use our lense of statistical significance to view these profit figures</p> <pre><code>trackDistanceBarrier = (\n    trackDistanceBarrier\n    .assign(backPL_pValue = lambda x: pl_pValue(number_bets = x['stake'], npl = x['back_npl'], stake = x['stake'], average_odds = x['odds']))\n    .assign(layPL_pValue = lambda x: pl_pValue(number_bets = x['stake'], npl = x['lay_npl'], stake = x['stake'], average_odds = x['odds']))\n)\n\ntrackDistanceBarrier\n</code></pre> <pre>\n<code>/home/tmbish/.local/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in sqrt\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n</code>\n</pre> track race_distance barrier_group back_npl lay_npl stake odds backPL_pValue layPL_pValue 0 Albany 1000 inside 11.2550 -11.95 2 15.450000 0.487280 1.000000 1 Albany 1000 mid_field -5.0000 4.75 5 101.136000 1.000000 0.885995 2 Albany 1000 outside -5.0000 4.75 5 88.374000 1.000000 0.877954 3 Albany 1100 inside -3.0525 2.70 6 29.430000 0.754412 0.869397 4 Albany 1100 mid_field -6.4040 5.92 9 37.483333 0.532857 0.804366 ... ... ... ... ... ... ... ... ... ... 6325 York 1500 inside 1.8995 -2.41 6 41.195000 0.918934 0.849635 6326 York 1500 mid_field -7.0000 6.65 7 32.472857 1.000000 0.755643 6327 York 1920 inside -3.0000 2.85 3 21.883333 1.000000 0.816546 6328 York 1920 mid_field -0.3520 -0.04 5 20.978000 0.972659 0.996987 6329 York 1920 outside -2.0000 1.90 2 21.450000 1.000000 0.863432 <p>6330 rows \u00d7 9 columns</p> <p>So as you can see, whilst having a back POT of nearly 500% because the results were generated over 2 runners at quite high odds the p value (50%) suggest that it's quite likely we could have seen these exact results due to randomness, which is very intuitive.</p> <p>Let's have a look to see if there's any statistically significant edge to be gained on the lay side</p> <pre><code># Top 5 lay combos Track | Distance | Barrier (TDB)\nTDB_bestLay = trackDistanceBarrier.query('lay_npl&amp;gt;0').sort_values('layPL_pValue').head(5)\nTDB_bestLay\n</code></pre> track race_distance barrier_group back_npl lay_npl stake odds backPL_pValue layPL_pValue 188 Ascot 1000 inside -83.6395 77.06 115 24.616870 0.003054 0.248157 3619 Moonee Valley 1200 inside -52.2195 48.81 64 17.725313 0.000565 0.254399 6299 Yeppoon 1400 inside -11.0000 10.45 11 6.022727 1.000000 0.289686 959 Caulfield 1400 mid_field -74.3150 67.45 114 24.828772 0.018780 0.301137 1366 Darwin 1200 mid_field -47.3980 43.94 64 19.354531 0.009844 0.318178 <p>So despite high lay POT none of these angles suggest an irrefutablely profitable angle laying these combinations. However, that doesn't mean we shouldn't test them on our out of sample set of races. These are our statistically most promising examples, we'll just take the top 5 for now and see how we would have performed if we had of started betting them on April first 2021.</p> <p>Keep in mind this should give us a pretty good indication of what we could get over the next 3 months into the future if we started today because we haven't contaminated/leaked any data from the post April period into our angle formulation. </p> <pre><code># First let's test laying on the train set (by definition we know these will be profitable)\ntrain_TDB_bestLay = (\n    dfTrain\n    .merge(TDB_bestLay[['track', 'race_distance']])\n    .assign(npl=lambda x: x['lay_npl'])\n    .assign(stake=1)\n    .assign(win=lambda x: np.where(x['lay_npl'] &amp;gt; 0, 1, 0))\n)\n\n# This is the key test (non of the races has been part of analysis to this point)\ntest_TDB_bestLay = (\n    dfTest\n    .merge(TDB_bestLay[['track', 'race_distance']])\n    .assign(npl=lambda x: x['lay_npl'])\n    .assign(stake=1)\n    .assign(win=lambda x: np.where(x['lay_npl'] &amp;gt; 0, 1, 0))\n)\n\n# Peaking at the bets in the test set\ntest_TDB_bestLay[['track', 'race_distance', 'barrier', 'barrier_group', 'bsp', 'lay_npl', 'win', 'stake']]\n</code></pre> track race_distance barrier barrier_group bsp lay_npl win stake 0 Ascot 1000 4 mid_field 11.08 0.95 1 1 1 Ascot 1000 11 outside 5.41 0.95 1 1 2 Ascot 1000 1 inside 4.73 -3.73 0 1 3 Ascot 1000 5 mid_field 7.35 0.95 1 1 4 Ascot 1000 10 outside 4.97 0.95 1 1 ... ... ... ... ... ... ... ... ... 219 Darwin 1200 10 outside 18.31 0.95 1 1 220 Darwin 1200 8 mid_field 42.00 0.95 1 1 221 Darwin 1200 6 mid_field 29.91 0.95 1 1 222 Darwin 1200 11 outside 6.60 -5.60 0 1 223 Darwin 1200 7 mid_field 5.74 0.95 1 1 <p>224 rows \u00d7 8 columns</p> <pre><code># Let's run our evaluation on the training set\nbet_eval_metrics(train_TDB_bestLay)\n</code></pre> npl stake win pot 0 98.1 1047.0 0.892073 0.093696 <pre><code># And on the test set\nbet_eval_metrics(test_TDB_bestLay)\n</code></pre> npl stake win pot 0 18.44 224.0 0.870536 0.082321 <p>That's promising results. Our test set shows similar betting performance as our training set and we're still seeing a profitble trend. These are lay strategies so they aren't as robust as backing strategies as your profit distribution is lots of small wins and some large losses, but this is potentially a profitble betting angle!</p> <pre><code>(\n    dfTrain\n    .assign(market_support=lambda x: x['wap_5m'] / x['wap_30s'])\n    .assign(races=1)\n    .groupby('jockey')\n    .agg({'market_support': 'mean', 'races': 'count'})\n    .query('races &amp;gt; 10')\n    .sort_values('market_support', ascending = False)\n    .head()\n)\n</code></pre> market_support races jockey Scott Sheargold 1.133095 192 Lorelle Crow 1.056582 106 Chris Mc Carthy 1.051022 26 Anthony Darmanin 1.048931 142 James Winks 1.048893 12 Bob El-Issa 1.046756 196 Elyce Smith 1.043593 164 Jessica Gray 1.043376 108 Paul Francis Hamblin 1.042248 61 Alana Livesey 1.042188 32 <p>Next, let's split the sample of each jockey's races between two scenarios a) the market firmed for their horse b) their horse drifted in the market in the last 5 minutes of trading.</p> <p>We then calculate the same summary table of inputs (profit, average odds etc) for backing these jockeys at the BSP given some market move. We can then feed these metrics into our statistical significance test to get an idea of the profitability of each combination.</p> <pre><code># Group By Jockey and Market Support\njockeys = (\n    dfTrain\n    .assign(stake = 1)\n    .assign(odds = lambda x: x['bsp'])\n    .assign(npl=lambda x: np.where(x['place'] == 1, 0.95 * (x['odds']-1), -1))\n    .assign(market_support=lambda x: np.where(x['wap_5m'] &amp;gt; x['wap_30s'], \"Y\", \"N\"))\n    .groupby(['jockey', 'market_support'], as_index=False)\n    .agg({'odds': 'mean', 'stake': 'sum', 'npl': 'sum'})\n    .assign(pValue = lambda x: pl_pValue(number_bets = x['stake'], npl = x['npl'], stake = x['stake'], average_odds = x['odds']))\n)\n\njockeys.sort_values('pValue').query('npl &amp;gt; 0').head(10)\n</code></pre> jockey market_support odds stake npl pValue 624 K Jennings Y 18.106118 85 178.6955 0.005643 496 Jade Darose Y 87.343333 39 579.4265 0.008942 263 Clayton Gallagher Y 24.225338 148 226.7145 0.012994 906 Ms T Harrison Y 27.944125 160 241.3065 0.018095 615 Justin P Stanley N 13.084502 231 155.7305 0.019913 802 Michael Dee N 36.338213 263 299.7255 0.031634 753 Madeleine Wishart Y 25.249872 78 156.5065 0.033329 433 Hannah Fitzgerald Y 32.171944 72 170.1830 0.045334 937 Nick Heywood N 17.172857 98 111.8905 0.049176 745 M Pateman N 22.690808 260 189.2050 0.052283 <p>You can think of each of these scenarios representing different cases. If profitable: - Under market support this could indicate the jockey is being correctly favoured to maximise their horse's chances of winning the race or perhaps even some kind of insider knowledge coming out of certain stables - Under market drift this could indicate some incorrect skepticism about the jockeys ability and thus their horse has been overlayed</p> <p>Either way we're interested to see how these combinations would perform paper trading in our out of sample set</p> <pre><code># First evaluate on our training set\ntrain_jockeyMarket = (\n    dfTrain\n    .assign(market_support=lambda x: np.where(x['wap_5m'] &amp;gt; x['wap_30s'], \"Y\", \"N\"))\n    .merge(jockeys.sort_values('pValue').query('npl &amp;gt; 0').head(10)[['jockey', 'market_support']])\n    .assign(stake = 1)\n    .assign(odds = lambda x: x['bsp'])\n    .assign(npl=lambda x: np.where(x['place'] == 1, 0.95 * (x['odds']-1), -1))\n    .assign(win=lambda x: np.where(x['npl'] &amp;gt; 0, 1, 0))\n)\n\n# And on the test set\ntest_jockeyMarket = (\n    dfTest\n    .assign(market_support=lambda x: np.where(x['wap_5m'] &amp;gt; x['wap_30s'], \"Y\", \"N\"))\n    .merge(jockeys.sort_values('pValue').query('npl &amp;gt; 0').head(10)[['jockey', 'market_support']])\n    .assign(stake = 1)\n    .assign(odds = lambda x: x['bsp'])\n    .assign(npl=lambda x: np.where(x['place'] == 1, 0.95 * (x['odds']-1), -1))\n    .assign(win=lambda x: np.where(x['npl'] &amp;gt; 0, 1, 0))\n)\n</code></pre> <pre><code>bet_eval_metrics(train_jockeyMarket)\n</code></pre> npl stake win pot 0 2309.384 1434.0 0.154812 1.610449 <pre><code>bet_eval_metrics(test_jockeyMarket)\n</code></pre> npl stake win pot 0 36.329 375.0 0.109333 0.096877 <p>You can see overfitting in full effect here with the train set performance. However, our out-of-sample performance is still decently profitable. We might have found another profitable betting angle!</p> <p>It's worth noting that implementing this strategy would be slightly more complex than implementing our first strategy. Our code (or third party tool) would need to be able to check whether the market had firmed between 2 distinct time points before the jump of the race and cross reference that with the jockey name. Trivial for someone who is comfortable with bet placement and the betfair API but a little more involved for the uninitiated. It's important to formulate angles that you would know how and are capable of implementing.</p> <pre><code># First Investigate The Average Inplay Minimums And Loss Rates of Certain Jockeys\ntradeOutIndex = (\n    dfTrain\n    .query('distance_group in [\"long\", \"mid_long\"]')\n    .assign(inplay_odds_ratio=lambda x: x['inplay_min_lay'] / x['bsp'])\n    .assign(win=lambda x: np.where(x['place']==1,1,0))\n    .assign(races=lambda x: 1)\n    .groupby(['jockey'], as_index=False)\n    .agg({'inplay_odds_ratio': 'mean', 'win': 'mean', 'races': 'sum'})\n    .sort_values('inplay_odds_ratio')\n    .query('races &amp;gt;= 5')\n)\n\ntradeOutIndex\n</code></pre> jockey inplay_odds_ratio win races 291 John Rudd 0.352796 0.000000 8 457 Natalie M Morton 0.357216 0.142857 7 451 Murray Henderson 0.455943 0.166667 6 92 Bridget Grylls 0.474635 0.000000 11 431 Ms Heather Poland 0.478529 0.000000 5 ... ... ... ... ... 438 Ms K Stanley 0.898819 0.000000 21 619 Yasuhiro Nishitani 0.902459 0.043478 23 99 Cameron Quilty 0.907503 0.000000 20 87 Brett Fliedner 0.923814 0.000000 20 169 Desiree Stra 0.949329 0.000000 5 <p>558 rows \u00d7 4 columns</p> <p>Ok so what we have here is a list of all jockeys with over 5 races on long and mid-long race distance groups (over 1800m) ordered by their average ratio of inplay minimum traded price compared with their jump price.</p> <p>If this trend is predictive we could assume that these jockeys tend to have an agressive race style and like to get out and lead the race. We'd like to capitalise on that race style by backing the jockeys pre-play and putting in a lay order which we'll leave inplay hoping to get matched during the race.</p> <p>For simplicity let's just assume we're flat staking on both sides so that our payoff profile looks like this: - Horse never trades at &lt;50% of it's BSP our lay bet never get's matched and we lose 1 unit - Horse trades at &lt;50% of it's BSP but loses (our lay bet gets filled) we're breakeven for the market - Horse trades wins (our lay bet get's filled) and we profit on our back bet and lose our lay bet so our profit is: <code>(BSP-1) - (0.5*BSP-1)</code></p> <p>Let's run this backtest on the top 20 jockeys in our tradeOutIndex dataframe to see how we'd perform on the train and test set.</p> <pre><code>targetTradeoutFraction = 0.5\n\ntrain_JockeyBackToLay = (\n    dfTrain\n    .query('distance_group in [\"long\", \"mid_long\"]')\n    .merge(tradeOutIndex.head(20)['jockey'])\n    .assign(npl=lambda x: np.where(x['inplay_min_lay'] &amp;lt;= targetTradeoutFraction * x['bsp'], np.where(x['place'] == 1, 0.95 * (x['bsp']-1-(0.5*x['bsp']-1)), 0), -1))\n    .assign(stake=lambda x: np.where(x['npl'] != -1, 2, 1))\n    .assign(win=lambda x: np.where(x['npl'] &amp;gt;= 0, 1, 0))\n)\n\nbet_eval_metrics(train_JockeyBackToLay)\n</code></pre> npl stake win pot 0 23.797 671.0 0.5181 0.035465 <pre><code>test_JockeyBackToLay = (\n    dfTest\n    .query('distance_group in [\"long\", \"mid_long\"]')\n    .merge(tradeOutIndex.head(20)['jockey'])\n    .assign(npl=lambda x: np.where(x['inplay_min_lay'] &amp;lt;= targetTradeoutFraction * x['bsp'], np.where(x['place'] == 1, 0.95 * (x['bsp']-1-(0.5*x['bsp']-1)), 0), -1))\n    .assign(stake=lambda x: np.where(x['npl'] != -1, 2, 1))\n    .assign(win=lambda x: np.where(x['npl'] &amp;gt;= 0, 1, 0))\n\n)\n\nbet_eval_metrics(test_JockeyBackToLay)\n</code></pre> npl stake win pot 0 45.62475 255.0 0.342105 0.178921 <p>Not bad! Looks like we found another possibly promising lead.</p> <p>Again it's worth noting that this is probably another step up in implementation complexity again from previous angles. It's not very hard when you're familiar with betfair order types and placing them through the API but it requires some additional API savviness. But the documentation is quite good and there's plenty of resources available online to help you understand how to automate something like this.</p> <pre><code>import requests\nimport pandas as pd\nfrom datetime import date, timedelta\nimport numpy as np\nimport os\nimport re\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\nimport logging\nimport yaml\nfrom unittest.mock import patch\nfrom typing import List, Set, Dict, Tuple, Optional\nfrom itertools import zip_longest\nimport betfairlightweight\nfrom betfairlightweight import StreamListener\nfrom betfairlightweight.resources.bettingresources import (\n    PriceSize,\n    MarketBook\n)\nfrom scipy.stats import t\nimport plotly.express as px\n\n\n# Utility Functions\n#   + Stream Parsing\n#   + Betfair Race Data Scraping\n#   + Various utilities\n# _________________________________\n\ndef as_str(v) -&amp;gt; str:\n    return '%.2f' % v if type(v) is float else v if type(v) is str else ''\n\ndef split_anz_horse_market_name(market_name: str) -&amp;gt; (str, str, str):\n    parts = market_name.split(' ')\n    race_no = parts[0] # return example R6\n    race_len = parts[1] # return example 1400m\n    race_type = parts[2].lower() # return example grp1, trot, pace\n    return (race_no, race_len, race_type)\n\ndef filter_market(market: MarketBook) -&amp;gt; bool: \n    d = market.market_definition\n    return (d.country_code == 'AU' \n        and d.market_type == 'WIN' \n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n\ndef load_markets(file_paths):\n    for file_path in file_paths:\n        print(file_path)\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n\n    return None\n\ndef slicePrice(l, n):\n    try:\n        x = l[n].price\n    except:\n        x = np.nan\n    return(x)\n\ndef sliceSize(l, n):\n    try:\n        x = l[n].size\n    except:\n        x = np.nan\n    return(x)\n\ndef wapPrice(l, n):\n    try:\n        x = round(sum( [rung.price * rung.size for rung in l[0:(n-1)] ] ) / sum( [rung.size for rung in l[0:(n-1)] ]),2)\n    except:\n        x = np.nan\n    return(x)\n\ndef ladder_traded_volume(ladder):\n    return(sum([rung.size for rung in ladder]))\n\n# Core Execution Fucntions\n# _________________________________\n\ndef extract_components_from_stream(s):\n\n    with patch(\"builtins.open\", lambda f, _: f):   \n\n        evaluate_market = None\n        prev_market = None\n        postplay = None\n        preplay = None\n        t5m = None\n        t30s = None\n        inplay_min_lay = None\n\n        gen = s.get_generator()\n\n        for market_books in gen():\n\n            for market_book in market_books:\n\n                # If markets don't meet filter return None's\n                if evaluate_market is None and ((evaluate_market := filter_market(market_book)) == False):\n                    return (None, None, None, None, None, None)\n\n                # final market view before market goes in play\n                if prev_market is not None and prev_market.inplay != market_book.inplay:\n                    preplay = market_book\n\n                # final market view before market goes is closed for settlement\n                if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n                    postplay = market_book\n\n                # Calculate Seconds Till Scheduled Market Start Time\n                seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n\n                # Market at 30 seconds before scheduled off\n                if t30s is None and seconds_to_start &amp;lt; 30:\n                    t30s = market_book\n\n                # Market at 5 mins before scheduled off\n                if t5m is None and seconds_to_start &amp;lt; 5*60:\n                    t5m = market_book\n\n                # Manage Inplay Vectors\n                if market_book.inplay:\n\n                    if inplay_min_lay is None:\n                        inplay_min_lay = [ slicePrice(runner.ex.available_to_lay,0) for runner in market_book.runners]\n                    else:\n                        inplay_min_lay = np.fmin(inplay_min_lay, [ slicePrice(runner.ex.available_to_lay,0) for runner in market_book.runners])\n\n                # update reference to previous market\n                prev_market = market_book\n\n        # If market didn't go inplay\n        if postplay is not None and preplay is None:\n            preplay = postplay\n            inplay_min_lay = [\"\" for runner in market_book.runners]\n\n        return (t5m, t30s, preplay, postplay, inplay_min_lay, prev_market) # Final market is last prev_market\n\ndef parse_stream(stream_files, output_file):\n\n    with open(output_file, \"w+\") as output:\n\n        output.write(\"market_id,selection_id,selection_name,wap_5m,wap_30s,bsp,ltp,traded_vol,inplay_min_lay\\n\")\n\n        for file_obj in load_markets(stream_files):\n\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            (t5m, t30s, preplay, postplay, inplayMin, final) = extract_components_from_stream(stream)\n\n            # If no price data for market don't write to file\n            if postplay is None or final is None or t30s is None:\n                continue; \n\n            # All runner removed\n            if all(runner.status == \"REMOVED\" for runner in final.runners):\n                continue\n\n            runnerMeta = [\n                {\n                    'selection_id': r.selection_id,\n                    'selection_name': next((rd.name for rd in final.market_definition.runners if rd.selection_id == r.selection_id), None),\n                    'selection_status': r.status,\n                    'sp': r.sp.actual_sp\n                }\n                for r in final.runners \n            ]\n\n            ltp = [runner.last_price_traded for runner in preplay.runners]\n\n            tradedVol = [ ladder_traded_volume(runner.ex.traded_volume) for runner in postplay.runners ]\n\n            wapBack30s = [ wapPrice(runner.ex.available_to_back, 3) for runner in t30s.runners]\n\n            wapBack5m = [ wapPrice(runner.ex.available_to_back, 3) for runner in t5m.runners]\n\n            # Writing To CSV\n            # ______________________\n\n            for (runnerMeta, ltp, tradedVol, inplayMin, wapBack5m, wapBack30s) in zip(runnerMeta, ltp, tradedVol, inplayMin, wapBack5m, wapBack30s):\n\n                if runnerMeta['selection_status'] != 'REMOVED':\n\n                    output.write(\n                        \"{},{},{},{},{},{},{},{},{}\\n\".format(\n                            str(final.market_id),\n                            runnerMeta['selection_id'],\n                            runnerMeta['selection_name'],\n                            wapBack5m,\n                            wapBack30s,\n                            runnerMeta['sp'],\n                            ltp,\n                            round(tradedVol),\n                            inplayMin\n                        )\n                    )\n\ndef get_bf_markets(dte):\n\n    url = 'https://apigateway.betfair.com.au/hub/racecard?date={}'.format(dte)\n\n    responseJson = requests.get(url).json()\n\n    marketList = []\n\n    for meeting in responseJson['MEETINGS']:\n        for markets in meeting['MARKETS']:\n            marketList.append(\n                {\n                    'date': dte,\n                    'track': meeting['VENUE_NAME'],\n                    'country': meeting['COUNTRY'],\n                    'race_type': meeting['RACE_TYPE'],\n                    'race_number': markets['RACE_NO'],\n                    'market_id': str('1.' + markets['MARKET_ID']),\n                    'start_time': markets['START_TIME']\n                }\n            )\n\n    marketDf = pd.DataFrame(marketList)\n\n    return(marketDf)\n\ndef get_bf_race_meta(market_id):\n\n    url = 'https://apigateway.betfair.com.au/hub/raceevent/{}'.format(market_id)\n\n    responseJson = requests.get(url).json()\n\n    if 'error' in responseJson:\n        return(pd.DataFrame())\n\n    raceList = []\n\n    for runner in responseJson['runners']:\n\n        if 'isScratched' in runner and runner['isScratched']:\n            continue\n\n        # Jockey not always populated\n        try:\n            jockey = runner['jockeyName']\n        except:\n            jockey = \"\"\n\n        # Place not always populated\n        try:\n            placeResult = runner['placedResult']\n        except:\n            placeResult = \"\"\n\n        # Place not always populated\n        try:\n            trainer = runner['trainerName']\n        except:\n            trainer = \"\"\n\n        raceList.append(\n            {\n                'market_id': market_id,\n                'weather': responseJson['weather'],\n                'track_condition': responseJson['trackCondition'],\n                'race_distance': responseJson['raceLength'],\n                'selection_id': runner['selectionId'],\n                'selection_name': runner['runnerName'],\n                'barrier': runner['barrierNo'],\n                'place': placeResult,\n                'trainer': trainer,\n                'jockey': jockey,\n                'weight': runner['weight']\n            }\n        )\n\n    raceDf = pd.DataFrame(raceList)\n\n    return(raceDf)\n\ndef scrape_thoroughbred_bf_date(dte):\n\n    markets = get_bf_markets(dte)\n\n    if markets.shape[0] == 0:\n        return(pd.DataFrame())\n\n    thoMarkets = markets.query('country == \"AUS\" and race_type == \"R\"')\n\n    if thoMarkets.shape[0] == 0:\n        return(pd.DataFrame())\n\n    raceMetaList = []\n\n    for market in thoMarkets.market_id:\n        raceMetaList.append(get_bf_race_meta(market))\n\n    raceMeta = pd.concat(raceMetaList)\n\n    return(markets.merge(raceMeta, on = 'market_id'))\n\n\n# Execute Data Pipeline\n# _________________________________\n\n# Description:\n#   Will loop through a set of dates (starting July 2020 in this instance) and return race metadata from betfair \n# Estimated Time:\n#   ~60 mins\n# \n# if __name__ == '__main__':\n    # dataList = []\n    # dateList = pd.date_range(date(2020,7,1),date.today()-timedelta(days=1),freq='d')\n    # for dte in dateList:\n    #     dte = dte.date()\n    #     print(dte)\n    #     races = scrapeThoroughbredBfDate(dte)\n    #     dataList.append(races)\n    # data = pd.concat(dataList)\n    # data.to_csv(\"[LOCAL PATH SOMEWHERE]\", index=False)\n\n\n# Description:\n#   Will loop through a set of stream data archive files and extract a few key pricing measures for each selection\n# Estimated Time:\n#   ~6 hours\n#\n# trading = betfairlightweight.APIClient(\"username\", \"password\")\n# listener = StreamListener(max_latency=None)\n# stream_files = glob.glob(\"[PATH TO LOCAL FOLDER STORING ARCHIVE FILES]*.tar\")\n# output_file = \"[SOME OUTPUT DIRECTORY]/thoroughbred-odds-2021.csv\"\n# if __name__ == '__main__':\n#     parse_stream(stream_files, output_file)\n\n\n# Analysis\n# _________________________________\n\n\n# Functions ++++++++\n\ndef bet_eval_metrics(d, side = False):\n\n    metrics = pd.DataFrame(d\n    .agg({\"npl\": \"sum\", \"stake\": \"sum\", \"win\": \"mean\"})\n    ).transpose().assign(pot=lambda x: x['npl'] / x['stake'])\n\n    return(metrics[metrics['stake'] != 0])\n\ndef pl_pValue(number_bets, npl, stake, average_odds):\n\n    pot = npl / stake\n\n    tStatistic = (pot * np.sqrt(number_bets)) / np.sqrt( (1 + pot) * (average_odds - 1 - pot) )\n\n    pValue = 2 * t.cdf(-abs(tStatistic), number_bets-1)\n\n    return(np.where(np.logical_or(np.isnan(pValue), pValue == 0), 1, pValue))\n\ndef distance_group(distance):\n\n    if distance is None:\n        return(\"missing\")\n    elif distance &amp;lt; 1100:\n        return(\"sprint\")\n    elif distance &amp;lt; 1400:\n        return(\"mid_short\")\n    elif distance &amp;lt; 1800:\n        return(\"mid_long\")\n    else:\n        return(\"long\")\n\ndef barrier_group(barrier):\n    if barrier is None:\n        return(\"missing\")\n    elif barrier &amp;lt; 4:\n        return(\"inside\")\n    elif barrier &amp;lt; 9:\n        return(\"mid_field\")\n    else:\n        return(\"outside\")\n\n# Analysis ++++++++\n\n# Local Paths (will be different on your machine)\npath_odds_local = \"[PATH TO YOUR LOCAL FILES]/thoroughbred-odds-2021.csv\"\npath_race_local = \"[PATH TO YOUR LOCAL FILES]/thoroughbred-race-data.csv\"\n\nodds = pd.read_csv(path_odds_local, dtype={'market_id': object, 'selection_id': object})\nrace = pd.read_csv(path_race_local, dtype={'market_id': object, 'selection_id': object})\n\n# Joining two datasets\ndf = race.merge(odds.loc[:, odds.columns != 'selection_name'], how = \"inner\", on = ['market_id', 'selection_id'])\n\n# I'll also add columns for the net profit from backing and laying each selection to be picked up in subsequent sections\ndf['back_npl'] = np.where(df['place'] == 1, 0.95 * (df['bsp']-1), -1)\ndf['lay_npl'] = np.where(df['place'] == 1, -1 * (df['bsp']-1), 0.95)\n\n# Adding Variable Chunks\ndf['distance_group'] = pd.to_numeric(df.race_distance, errors = \"coerce\").apply(distance_group)\ndf['barrier_group'] = pd.to_numeric(df.barrier, errors = \"coerce\").apply(barrier_group)\n\n# Data Partitioning\ndfTrain = df.query('date &amp;lt; \"2021-04-01\"')\ndfTest = df.query('date &amp;gt;= \"2021-04-01\"')\n\n'{} rows in the \"training\" set and {} rows in the \"test\" data'.format(dfTrain.shape[0], dfTest.shape[0])\n\n# Angle 1 ++++++++++++++++++++++++++++++++++++++++++++++\n\n(\n    dfTrain\n    .assign(stake=1)\n    .groupby('selection_name', as_index = False)\n    .agg({'back_npl': 'sum', 'stake': 'sum'})\n    .assign(pot=lambda x: x['back_npl'] / x['stake'])\n    .sort_values('pot', ascending=False)  \n    .head(3) \n)\n\n# Calculate the profit (back and lay) and average odds across all track / distance / barrier group combos\ntrackDistanceBarrier = (\n    dfTrain\n    .assign(stake = 1)\n    .assign(odds = lambda x: x['bsp'])\n    .groupby(['track', 'race_distance', 'barrier_group'], as_index=False)\n    .agg({'back_npl': 'sum', 'lay_npl': 'sum','stake': 'sum', 'odds': 'mean'})\n)\n\ntrackDistanceBarrier\n\ntrackDistanceBarrier = (\n    trackDistanceBarrier\n    .assign(backPL_pValue = lambda x: pl_pValue(number_bets = x['stake'], npl = x['back_npl'], stake = x['stake'], average_odds = x['odds']))\n    .assign(layPL_pValue = lambda x: pl_pValue(number_bets = x['stake'], npl = x['lay_npl'], stake = x['stake'], average_odds = x['odds']))\n)\n\ntrackDistanceBarrier\n\n# Top 5 lay combos Track | Distance | Barrier (TDB)\nTDB_bestLay = trackDistanceBarrier.query('lay_npl&amp;gt;0').sort_values('layPL_pValue').head(5)\nTDB_bestLay\n\n# First let's test laying on the train set (by definition we know these will be profitable)\ntrain_TDB_bestLay = (\n    dfTrain\n    .merge(TDB_bestLay[['track', 'race_distance']])\n    .assign(npl=lambda x: x['lay_npl'])\n    .assign(stake=1)\n    .assign(win=lambda x: np.where(x['lay_npl'] &amp;gt; 0, 1, 0))\n)\n\n# This is the key test (non of the races has been part of analysis to this point)\ntest_TDB_bestLay = (\n    dfTest\n    .merge(TDB_bestLay[['track', 'race_distance']])\n    .assign(npl=lambda x: x['lay_npl'])\n    .assign(stake=1)\n    .assign(win=lambda x: np.where(x['lay_npl'] &amp;gt; 0, 1, 0))\n)\n\n# Peaking at the bets in the test set\ntest_TDB_bestLay[['track', 'race_distance', 'barrier', 'barrier_group', 'bsp', 'lay_npl', 'win', 'stake']]\n\n# Let's run our evaluation on the training set\nbet_eval_metrics(train_TDB_bestLay)\n\n# And on the test set\nbet_eval_metrics(test_TDB_bestLay)\n\n# Angle 2 ++++++++++++++++++++++++++++++++++++++++++++++\n\n(\n    dfTrain\n    .assign(market_support=lambda x: x['wap_5m'] / x['wap_30s'])\n    .assign(races=1)\n    .groupby('jockey')\n    .agg({'market_support': 'mean', 'races': 'count'})\n    .query('races &amp;gt; 10')\n    .sort_values('market_support', ascending = False)\n    .head()\n)\n\n# Group By Jockey and Market Support\njockeys = (\n    dfTrain\n    .assign(stake = 1)\n    .assign(odds = lambda x: x['bsp'])\n    .assign(npl=lambda x: np.where(x['place'] == 1, 0.95 * (x['odds']-1), -1))\n    .assign(market_support=lambda x: np.where(x['wap_5m'] &amp;gt; x['wap_30s'], \"Y\", \"N\"))\n    .groupby(['jockey', 'market_support'], as_index=False)\n    .agg({'odds': 'mean', 'stake': 'sum', 'npl': 'sum'})\n    .assign(pValue = lambda x: pl_pValue(number_bets = x['stake'], npl = x['npl'], stake = x['stake'], average_odds = x['odds']))\n)\n\njockeys.sort_values('pValue').query('npl &amp;gt; 0').head(10)\n\n# First evaluate on our training set\ntrain_jockeyMarket = (\n    dfTrain\n    .assign(market_support=lambda x: np.where(x['wap_5m'] &amp;gt; x['wap_30s'], \"Y\", \"N\"))\n    .merge(jockeys.sort_values('pValue').query('npl &amp;gt; 0').head(10)[['jockey', 'market_support']])\n    .assign(stake = 1)\n    .assign(odds = lambda x: x['bsp'])\n    .assign(npl=lambda x: np.where(x['place'] == 1, 0.95 * (x['odds']-1), -1))\n    .assign(win=lambda x: np.where(x['npl'] &amp;gt; 0, 1, 0))\n)\n\n# And on the test set\ntest_jockeyMarket = (\n    dfTest\n    .assign(market_support=lambda x: np.where(x['wap_5m'] &amp;gt; x['wap_30s'], \"Y\", \"N\"))\n    .merge(jockeys.sort_values('pValue').query('npl &amp;gt; 0').head(10)[['jockey', 'market_support']])\n    .assign(stake = 1)\n    .assign(odds = lambda x: x['bsp'])\n    .assign(npl=lambda x: np.where(x['place'] == 1, 0.95 * (x['odds']-1), -1))\n    .assign(win=lambda x: np.where(x['npl'] &amp;gt; 0, 1, 0))\n)\n\nbet_eval_metrics(train_jockeyMarket)\n\nbet_eval_metrics(test_jockeyMarket)\n\n# Angle 3 ++++++++++++++++++++++++++++++++++++++++++++++\n\n\n# First Investigate The Average Inplay Minimums And Loss Rates of Certain Jockeys\ntradeOutIndex = (\n    dfTrain\n    .query('distance_group in [\"long\", \"mid_long\"]')\n    .assign(inplay_odds_ratio=lambda x: x['inplay_min_lay'] / x['bsp'])\n    .assign(win=lambda x: np.where(x['place']==1,1,0))\n    .assign(races=lambda x: 1)\n    .groupby(['jockey'], as_index=False)\n    .agg({'inplay_odds_ratio': 'mean', 'win': 'mean', 'races': 'sum'})\n    .sort_values('inplay_odds_ratio')\n    .query('races &amp;gt;= 5')\n)\n\ntradeOutIndex\n\ntargetTradeoutFraction = 0.5\n\ntrain_JockeyBackToLay = (\n    dfTrain\n    .query('distance_group in [\"long\", \"mid_long\"]')\n    .merge(tradeOutIndex.head(20)['jockey'])\n    .assign(npl=lambda x: np.where(x['inplay_min_lay'] &amp;lt;= targetTradeoutFraction * x['bsp'], np.where(x['place'] == 1, 0.95 * (x['bsp']-1-(0.5*x['bsp']-1)), 0), -1))\n    .assign(stake=lambda x: np.where(x['npl'] != -1, 2, 1))\n    .assign(win=lambda x: np.where(x['npl'] &amp;gt;= 0, 1, 0))\n)\n\nbet_eval_metrics(train_JockeyBackToLay)\n\ntest_JockeyBackToLay = (\n    dfTest\n    .query('distance_group in [\"long\", \"mid_long\"]')\n    .merge(tradeOutIndex.head(20)['jockey'])\n    .assign(npl=lambda x: np.where(x['inplay_min_lay'] &amp;lt;= targetTradeoutFraction * x['bsp'], np.where(x['place'] == 1, 0.95 * (x['bsp']-1-(0.5*x['bsp']-1)), 0), -1))\n    .assign(stake=lambda x: np.where(x['npl'] != -1, 2, 1))\n    .assign(win=lambda x: np.where(x['npl'] &amp;gt;= 0, 1, 0))\n\n)\n\nbet_eval_metrics(test_JockeyBackToLay)\n</code></pre>"},{"location":"tutorials/automatedBettingAnglesTutorial/#automated-betting-angles-in-python","title":"Automated betting angles in Python","text":"<p>| Betting strategies based on your existing insights: no modelling required</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#workshop","title":"Workshop","text":""},{"location":"tutorials/automatedBettingAnglesTutorial/#cheat-sheet","title":"Cheat sheet","text":"<ul> <li>This is presented as a Jupyter notebook as this format is interactive and lets you run snippets of code from wihtin the notebook. To use this functionality you'll need to download a copy of the <code>ipynb</code> file locally and open it in a text editor (i.e. VS code).</li> <li>If you're looking for the complete code head to the bottom of the page or download the script from Github.</li> <li>To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code), make sure you've navigated in the terminal to the folder you've saved the script in and then type <code>py main.py</code> (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. </li> <li>Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site. We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. </li> <li>We're using the <code>betfairlightweight</code> package to do the heavy lifting</li> <li>We've also posted the completed code logic on the <code>betfair-downunder</code> Github repo.</li> </ul>"},{"location":"tutorials/automatedBettingAnglesTutorial/#01-setup","title":"0.1 Setup","text":"<p>Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language.</p> <p>Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer where you want to locally store the intermediate data files.</p> <p>You'll also need <code>betfairlightweight</code> which you can install with something like <code>pip install betfairlightweight</code>.</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#02-context","title":"0.2 Context","text":"<p>Formulating betting angles (or \"strategies\" as some call them) is quite a common pasttime for some. These angles can range all the way from very simple to quite sophisticated, and could include things like:</p> <ul> <li>Laying NBA teams playing on the second night of a back-to-back</li> <li>Laying AFL team coming off a bye when matched against a team who played last week</li> <li>Backing a greyhound in boxes 1 or 2 in short sprint style races</li> <li>Backing a horse pre-race who typically runs at the front of the field and placing an order to lay the same horse if it shortens to some lower price in-play, locking in a profit</li> </ul> <p>Beyond the complexity of the actual concept what really seperates these angles is evidence. You might have heard TV personalities and betting ads suggest a certain strategy (resembling one of the above) are real-world predictive trends but they rarely are. They are rarely derived from the right historical data or are concluded without the necessary statistical rigour. Most simply formulated their angles off intuition or observing a trend across a small sample of data.</p> <p>There are many users on betting exchanges who profit off these angles. In fact, when most people talk about automated or sophisticated exchange betting they are often talking about automating these kind of betting angles, as opposed to betting ratings produced from a sophisticated bottom-up fundemental modelling. That's because profitable fundemental modelling (where your model which arrives at some estimation of fair value from first principles) is very hard.</p> <p>The reason this approach is so much easier is that you assume the market odds are right except x and go from there, applying small top-down adjustments for factors that haven't historically been incorporated in the market opinion. The challenge lies in finding those factors and making sure you aren't tricking yourself in thinking you've found one that you can profit off in the future.</p> <p>Once again this is another example of the uses of the Betfair historical stream data. To get cracking - as always - we need historical odds and the best place to get that is to self serve the historical stream files.</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#03-examples","title":"0.3 Examples","text":"<p>I'll go through an end-to-end example of 3 different betting angles on Australian thoroughbred racing. Which will include: Which will include:</p> <ul> <li>Sourcing data</li> <li>Assembling data</li> <li>Formulating hypotheses</li> <li>Testing Hypotheses</li> <li>Discussion about implementation</li> </ul>"},{"location":"tutorials/automatedBettingAnglesTutorial/#10-data","title":"1.0 Data","text":""},{"location":"tutorials/automatedBettingAnglesTutorial/#11-betfair-odds-data","title":"1.1 Betfair Odds Data","text":"<p>We'll follow a very similar template as other tutorials extracting key information from the betfair stream data.</p> <p>It's important to note that given the volume of data you need to handle with these stream files, your workflow will probably involve choosing some methods of aggregation / summary that you'll reconsider after your first cut of analysis. Parsing and saving a dataset, using it to test some hypotheses which likely results in more questions that need to be examined by reparsing the stream files in a slightly different way. Your workflow will likely follow something like this diagram.</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#12-race-data","title":"1.2 Race Data","text":""},{"location":"tutorials/automatedBettingAnglesTutorial/#extract-betfair-racing-markets-for-a-given-date","title":"Extract Betfair Racing Markets for a Given Date","text":"<p>First we'll hit the <code>https://apigateway.betfair.com.au/hub/racecard</code> endpoint to get the racing markets available on Betfair for a given day in the past:</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#extract-key-race-metadata","title":"Extract Key Race Metadata","text":"<p>Then (for one of these <code>market_id</code>s) we'll hit the <code>https://apigateway.betfair.com.au/hub/raceevent/</code> enpdoint to get some key runner metadata for the runners in this race. It's important to note that this information is available through the Betfair API so we won't need to go to a secondary datasource to find it at the point of implementation, this would add a large layer of complexity to the project including things like string cleaning and matching.</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#wrapper-function","title":"Wrapper Function","text":"<p>Stiching these two functions together we can create a wrapper function that hits both endpoints for all the thoroughbred races in a given day and extract all the runner metadata and results.</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#20-analysis","title":"2.0 Analysis","text":"<p>I'll be running through 3 simple betting angles, one easy, one medium, and one hard to illustrate different types of angles you might want to try at home. The process I lay out is very similar (if not identical) but the implementation might be a bit trickier in each case and might take a little more programming skill to get up and running.</p> <p>We'll use a simple evaluation function (POT and strike rate) to evaluate each of these strategies.</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#21-assemble-data","title":"2.1 Assemble Data","text":"<p>Now that we have our 2 core datasets (odds + race / runner metadata) we can join them together and do some analysis</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#22-methodology","title":"2.2 Methodology","text":"<p>Looping back around to the context discussion in part 0.2 we need to decide on how to set up our analysis that will help us: find angles, formulate strategies, and test them with enough rigour that will give us a good estimate of our forward looking profitability on any that we choose to implement and automate.</p> <p>The 3 key tricks I'll lay out in this piece are: - Using a statistical estimate to quantify the robustness of historical profitibalility - Using out-of-sample validation (much like a you would in a model building exercise) to get an accurate view of forward looking profitability - Using domain knowledge to chunk selections to get broader sample for more stable estimate of profitability</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#231-chunking","title":"2.3.1 Chunking","text":"<p>This is a technique you can use to group together variables in conceptually similar groups. For example, thoroughbred races are run over many different exact distances (800m, 810m, 850m, 860m etc) which - using a domain overlay - are all very short sprint style races for a horse race. Similarly, barriers 1, 2 and 3 being on the very inside of the race field and closest to the rail all present similar early race challenges and advantages for horses jumping from those barriers.</p> <p>So formulating your betting angles you may want to overlay semantically similar variable groups to test your betting hypothesis.</p> <p>I'll add variable chunks for race distance and barrier for now but you may want to test more (for example horse experience, trainer stable size etc)</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#232-in-sample-vs-out-of-sample","title":"2.3.2 In Sample vs Out of Sample","text":"<p>The first thing I'm going to do is to split off a largish chunk of my data before even looking at it. I'll ultimately use it to paper trade some of my candidate angles but I want it to be as seperate from the idea generation process as possible. </p> <p>I'll use the model building nomenclature \"train\" and \"test\" even though I'm not really doing any \"training\". My data contains all AUS thoroughbred races from July 2020 until end of June 2021 so I'll cut off the period Apr-June 2021 as my \"test\" set.</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#233-statistically-measuring-profit","title":"2.3.3 Statistically Measuring Profit","text":"<p>Betting outcomes, and the randomness associated with them, at their core are the types of things the discipline of statistics was created to solve. Concepts like sample size, expected value, and variance are terms you might hear from sophisticated (and some novice) bettors and they are all drawn from the field of statistics. Though you don't need to become a PHD of statistics every little extra technique or concept you can glean from the field will help your betting if you want it to.</p> <p>To illustrate with an example, let's group by net backing profit on turnover for a horse to see which horses have the highest historical back POT:</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#24-angle-1-track-distance-barrier","title":"2.4 Angle 1: Track | Distance | Barrier","text":""},{"location":"tutorials/automatedBettingAnglesTutorial/#25-angle-2-jockeys-market-opinion","title":"2.5 Angle 2: Jockeys + Market Opinion","text":"<p>Moving up slightly in level of difficulty our angles could include different kinds of reference points. Jockeys seem to be a divisive form factor in thoroughbred racing, and their quality can be hard to isolate relative to the quality of the horse and its preperation etc.</p> <p>I'm going to look at isolating jockeys that are either favoured or unfavoured by the market to see if I can formulate a betting angle that could generate me expected profit.</p> <p>The metric I'm going to use to determine market favour will be the ratio between back price 5 minutes before the scheduled jump and 30 seconds before the scheduled jump. Plotting this ratio for jockeys in our training set we can see which jockeys tend to have high market support by a high ratio (horses they are riding tend to shorten before the off)</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#26-angle-3-backing-to-lay","title":"2.6 Angle 3: Backing To Lay","text":"<p>Now let's try to use some of our inplay price data we extracted from the stream files. I'm interested in testing some back-to-lay strategies where a horse is backed preplay with the intention to get some tradeout lay order filled during the race. The types of scenarios where this could be conceivably profitable would be on certain kinds of horses or jockeys that show promise or strength early in the race but generally fade late and might not convert those early advantages often.</p> <p>Things we could look at here are: - Horses that typically trade lower than their preplay odds but don't win often - Jockeys that typically trade lower than their preplay odds but don't win often - Certain combinations of jockey / trainer / horse / race distance that meet these criteria</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#30-conclusion","title":"3.0 Conclusion","text":"<p>This analysis is just a sketch. Hopefully it helps inspire you to think about what kinds of betting angles you could test for a sport or racing code you're interested in. It should give you a framework for thinking about this kind of automated betting, and how it differs from fundamental modelling. It should also give you a few tricks for coming up with your own angles and testing them with the rigour needed to have any realistic expectations of profit. Most of the betting angles you're sold are faulty or have long evaporated from the market by people long before you even knew the rules of the sport. You'll need to be creative and scientific to create your own profitable betting angles, but it's certainly worth it to try.</p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#complete-code","title":"Complete code","text":"<p>Run the code from your ide by using <code>py &lt;filename&gt;.py</code>, making sure you amend the path to point to your input data. </p> <p>Download from Github </p>"},{"location":"tutorials/automatedBettingAnglesTutorial/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"tutorials/backtestingRatingsTutorial/","title":"Backtesting wagering models with Betfair JSON stream data","text":""},{"location":"tutorials/backtestingRatingsTutorial/#workshop","title":"Workshop","text":"<p>This tutorial was written by Tom Bishop and was originally published on Github. It is shared here with his permission. </p> <p>This tutorial follows on logically from the JSON to CSV tutorial we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at that tutorial before diving into this one. </p> <p>As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! </p> <p>Cheat sheet</p> <ul> <li> <p>If you're looking for the complete code head to the bottom of the page or download the script from Github.</p> </li> <li> <p>To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code), make sure you've navigated in the terminal to the folder you've saved the script in and then type <code>py main.py</code> (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. </p> </li> <li> <p>Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site. We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. </p> </li> <li> <p>We're using the <code>betfairlightweight</code> package to do the heavy lifting</p> </li> <li> <p>We've also posted the completed code logic on the <code>betfair-downunder</code> Github repo.</p> </li> <li> <p>You can watch our workshop working through this tutorial on YouTube.</p> </li> </ul> <p>This article was written more than 2 years ago and some packages used here will have changed since the article was written. Continue at your peril</p>"},{"location":"tutorials/backtestingRatingsTutorial/#set-up","title":"Set up","text":"<p>I'm going to be using a jupyter notebook for this investigation which is a special type of data analysis output that is used to combine code, outputs and explanatory text in a readable single document. It's mostly closely associated with python data analysis code which is the language I'll be using here also. The entire body of python code used will be repeated at the bottom of the article where you can copy it and repurpose it for yourself.</p> <p>If you're not familiar with python, don't worry neither am I really! I'm inexperienced with python so if you have experience with some other programming language you should be able to follow along with the logic here too. If you don't have experience using another programming language this all might appear intimidating but it's heavy on the explanatory text so you should get something out of it.</p> <p>We need a few non standard python libraries so make sure to install <code>betfairlightweight</code> and <code>plotly</code> before you get started with something like <code>pip install betfairlightweight</code> &amp; <code>pip install plotly</code>. We'll load all our libraries and do some setup here.</p> <pre><code>import pandas as pd\nimport numpy as np\nimport requests\nfrom datetime import date, timedelta\nimport os\nimport re\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\nimport logging\nfrom unittest.mock import patch\nfrom typing import List, Set, Dict, Tuple, Optional\nfrom itertools import zip_longest\nimport plotly.express as px\n\nimport betfairlightweight\nfrom betfairlightweight import StreamListener\nfrom betfairlightweight.resources.bettingresources import (\n    PriceSize,\n    MarketBook\n)\n\n%config IPCompleter.greedy=True\n</code></pre>"},{"location":"tutorials/backtestingRatingsTutorial/#context","title":"Context","text":"<p>Backtesting is the life-blood of most successful wagering systems. In short it attempts to answer a single question for you:</p> <p>\ud835\udf0f : How much money will I win or lose if I started using this system to place bets with real money?</p> <p>Without a rigorous and quantitative backtesting approach it's really quite hard to estimate the answer to this question $ \\tau $ that will be even reliably on the right side of zero.</p> <p>You could live test your system with real bets at small stakes, however, this isn't the panacea it seems. It will take time (more than you think) for your results to converge to their long term expectation. How long? Answering this question will require some expertise with probability and statistics you might not have. Even more than that though is that depending on where you're betting your results at small stakes could be very different than at larger stakes. You might not be able get a good answer to $ \\tau $ until betting at full stakes at which point finding the answer might coincide with blowing up your gambling bankroll.</p> <p>Backtesting is also very hard. To perfectly backtest your own predicted probability on a historical race or sporting match you need to produce 2 things:</p> <p>(1) What would my predicted chance have been exactly for this selection in this market on this day in the past?</p> <p>(2) What would have I decided to bet at what odds (exactly) and for how much stake (exactly) based on this prediction?</p> <p>The devil in the detail of backtesting tends to be in those exactlys.</p> <p>The aim of the backtesting game is answering (2) as accurately as possible because it tells you exactly how much you would have made over your backtesting period, from there you can confidently project that rate of profitability forward.</p> <p>It's easy to make mistakes and small errors in the quantitative reasoning can lead you to extremely misguided projections downstream.</p> <p>Question (1) won't be in the scope of this notebook but it's equally (and probably more) important that (2) but it is the key challenge of all predictive modelling exercises so there's plenty of discussion about it elsewhere.</p>"},{"location":"tutorials/backtestingRatingsTutorial/#backtesting-on-betfair","title":"Backtesting on Betfair","text":"<p>Answering question (2) for betting on the Betfair Exchange is difficult. The Exchange is a dynamic system that changes from one micro second to the next.</p> <p>What number should you use for odds? How much could you assume to get down at those odds?</p> <p>The conventional and easiest approach is to backtest at the BSP. The BSP is simple because it's a single number (to use for both back and lay bets) and is a taken price (there's no uncertainty about getting matched). Depending on the liquidity of the market a reasonably sized stake might also not move the BSP very much. For some markets you may be able to safely assume you could be $10s of dollars at the BSP without moving it an inch. However, that's definitely not true of all BSP markets and you need to be generally aware that your Betfair orders in the future will change the state of the exchange, and large bets will move the BSP in an unfavourable direction.</p> <p>Aside from uncertainty around the liquidity and resilience of the BSP, many many markets don't have a BSP. So what do we do then?</p> <p>Typically what a lot of people (who have a relationship with Betfair Australia) do at this point is request a data dump. They might request an odds file for all Australian harness race win markets since June 2018 with results and 4 different price points: the BSP, the last traded price, the weighted average price (WAP) traded in 3 minutes before the race starts, and the WAP for all bets matched prior to 3 mins before the race.</p> <p>However, you will likely need to be an existing VIP customer to get this file and it's not a perfect solution: it might take 2 weeks to get, you can't refresh it, you can't test more hypothetical price points after your initial analysis amongst many other problems.</p> <p>What if you could produce this valuable data file yourself?</p>"},{"location":"tutorials/backtestingRatingsTutorial/#betfair-stream-data","title":"Betfair Stream Data","text":"<p>Betfair's historical stream data is an extremely rich source of data. However, in it's raw form it's difficult to handle for the uninitiated. It also might not be immediately obvious how many different things this dataset could be used for without seeing some examples. These guides will hopefully demystify how to turn this raw data into a familiar and usable format whilst also hopefully providing some inspiration for the kinds of value that can be excavated from it.</p>"},{"location":"tutorials/backtestingRatingsTutorial/#this-example-backtesting-betfair-hub-thoroughbred-model","title":"This example: backtesting Betfair Hub thoroughbred model","text":"<p>To illustrate how you can use the stream files to backtest the outputs of a rating system we'll use the Australian Thoroughbred Rating model available on the Betfair Hub. The most recent model iteration only goes back till Feb 28th 2021 however as an illustrative example this is fine. We'd normally want to backtest with a lot more historical data than this, which just means in this case our estimation of future performance will be unreliable.</p> <p>I'm interested to see how we would have fared betting all selections rated by this model according to a few different staking schemes and also at a few different times / price points.</p> <p>Old ratings</p> <p>If you want to pull in ratings from before Feb 2021 to add to your database for more complete backtesting these are available in a data dump here.</p>"},{"location":"tutorials/backtestingRatingsTutorial/#scrape-the-model-ratings","title":"Scrape The Model Ratings","text":"<p>If you travel to the Betfair hub ratings page you'll find that URL links behind the ratings download buttons have a consistent URL pattern that looks very scrape friendly.</p> <p></p> <p>We can take advantage of this consistency and use some simple python code to scrape all the ratings into a pandas dataframe.</p> <pre><code># Function to return Pandas DF of hub ratings for a particular date\ndef getHubRatings(dte):\n\n    # Substitute the date into the URL\n    url = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date={}presenter=RatingsPresenter&amp;json=true'.format(dte)\n\n    # Convert the response into JSON\n    responseJson = requests.get(url).json()\n\n    hubList = []\n\n    if not responseJson:\n        return(None)\n\n\n    # Want an normalised table (1 row per selection)\n    # Brute force / simple approach is to loop through meetings / races / runners and pull out the key fields\n    for meeting in responseJson['meetings']:\n        for race in meeting['races']:\n            for runner in race['runners']:\n                hubList.append(\n                    {\n                        'date': dte,\n                        'track': meeting['name'],\n                        'race_number': race['number'],\n                        'race_name': race['name'],\n                        'market_id': race['bfExchangeMarketId'],\n                        'selection_id':  str(runner['bfExchangeSelectionId']),\n                        'selection_name': runner['name'],\n                        'model_odds': runner['ratedPrice']\n                    }\n                )\n\n    out = pd.DataFrame(hubList)\n\n    return(out)\n</code></pre> <pre><code># See the response from a single day\ngetHubRatings(date(2021,3,1)).head(5)\n</code></pre> date track race_number race_name market_id selection_id selection_name model_odds 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38620052 1. Military Affair 6.44 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 5889703 3. Proverbial 21.11 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38177688 4. A Real Wag 9.97 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38620053 5. El Jay 44.12 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 37263264 6. Flying Honour 3.39 <pre><code>dateDFList = []\ndateList = pd.date_range(date(2021,2,18),date.today()-timedelta(days=1),freq='d')\n\nfor dte in dateList:\n    dateDFList.append(getHubRatings(dte))\n\n# Concatenate (add rows to rows) all the dataframes within the list\nhubRatings = pd.concat(dateDFList)\n</code></pre> <pre><code>hubRatings.shape\n</code></pre> <pre><code>(32519, 8)\n</code></pre>"},{"location":"tutorials/backtestingRatingsTutorial/#assembling-the-odds-file","title":"Assembling the odds file","text":"<p>So part 1 was very painless. This is how we like data: served by some API or available in a nice tabular format on a webpage ready to be scraped with standard tools available in popular languages.</p> <p>Unfortunately, it won't be so painless to assemble our odds file. We'll find out why it's tricky as we go.</p>"},{"location":"tutorials/backtestingRatingsTutorial/#the-data","title":"The Data","text":"<p>The data we'll be using is the historical Exchange data available from this website. The data available through this service is called streaming JSON data. There are a few options available relating to granularity (how many time points per second the data updates at) but we'll be using the most granular \"PRO\" set which has updates every 50 milliseconds.</p> <p>Essentially what the data allows us to do is, for a particular market, recreate the exact state of the Betfair Exchange at say: 150 milliseconds before the market closed. When people say the state of the Exchange they mean two things a) what are all the current open orders on all the selections b) what are the current traded volumes on each selection at each price point. We obviously don't have access to any information about which accounts are putting up which prices and other things Betfair has themselves. We're essentially getting a snapshot of everything you can see through the website by clicking on each selection manually and looking at the graphs, tables and ladders.</p> <p>However, with just these 2 sets of information we can build a rich view of the dynamics of exchange and also build out all of the summary metrics (WAP etc) we might have previously needed Betfair to help with.</p> <p>For our purposes 50 milli-second intervaled data is huge overkill. But you could imagine needing this kind of granularity for other kinds of wagering systems - eg a high frequency trading algorithm of some sort that needs to make many decisions and actions every second.</p> <p>Let's take a look at what the stream data looks like for a single market:</p> <p></p> <p>So it looks pretty intractable. For this particular market there's 14,384 lines of data where each line consists of a single JSON packet of data. If you're not a data engineer (neither am I) your head might explode thinking about how you could read this into your computer and transform it into something usable.</p> <p>The data looks like this because it is saved from a special Betfair API called the Stream API which which is used by high end Betfair API users and which delivers fast speeds other performance improvements over the normal \"polling\" API.</p> <p>Now what's good about that, for the purposes of our exercise, is that the very nice python package <code>betfairlightweight</code> has the functionality built to not only parse the Stream API when connected live but also these historical saved versions of the stream data. Without it we'd be very far away from the finish line, with <code>betfairlightweight</code> we're pretty close.</p>"},{"location":"tutorials/backtestingRatingsTutorial/#unpacking-flattening-the-data","title":"Unpacking / flattening the data","text":"<p>Because these files are so large and unprocessed this process won't look the same as your normal data ETL in python: where you can read a raw data file (csv, JSON, text etc.) into memory and use python functions to transform into usable format.</p> <p>I personally had no idea how to use python and <code>betfairlightweight</code> to parse these data until I saw Betfair's very instructive overview which you should read for a more detailed look at some of the below code.</p> <p>By my count there were 4 key conceptual components that I had to get my head around to understand and be able to re-purpose that code. So if you're like me (a bit confused by some of the steps in that piece) this explanation might help.</p> <p>I'll assume you don't do any decompression and keep the monthly PRO files as the .tar archives as they are.</p> <p>Conceptually the process looks something like this:</p> <ol> <li>Load the \"archives\" into a \"generator\"</li> <li>Scan across the generator (market_ids) and the market states within those markets to extract useful objects</li> <li>Process those useful objects to pull out some metadata + useful summary numbers derived from the available orders and traded volumes snapshot data</li> <li>Write this useful summarised data to a file that can be read and understood with normal data analysis workflows First we'll run a bunch of setup code setting up my libraries and creating some utility functions that will be used throughout the main parsing component. It'll also point to the two stream files I'll be parsing for this exercise.</li> </ol> <pre><code># create trading instance (don't need username/password)\ntrading = betfairlightweight.APIClient(\"username\", \"password\")\n\n# create listener\nlistener = StreamListener(max_latency=None)\n\n### Utility Functions\n\n# rounding to 2 decimal places or returning '' if blank\ndef as_str(v) -&gt; str:\n    return '%.2f' % v if type(v) is float else v if type(v) is str else ''\n\n# splitting race name and returning the parts \ndef split_anz_horse_market_name(market_name: str) -&gt; (str, str, str):\n    # return race no, length, race type\n    # input sample: R6 1400m Grp1\n    parts = market_name.split(' ')\n    race_no = parts[0] # return example R6\n    race_len = parts[1] # return example 1400m\n    race_type = parts[2].lower() # return example grp1, trot, pace\n\n    return (race_no, race_len, race_type)\n\n# creating a flag that is True when markets are australian thoroughbreds\ndef filter_market(market: MarketBook) -&gt; bool: \n    d = market.market_definition\n    return (d.country_code == 'AU' \n        and d.market_type == 'WIN' \n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n</code></pre>"},{"location":"tutorials/backtestingRatingsTutorial/#1-tar-load","title":"1: .tar load","text":"<ul> <li>This function I stole from Betfair's instructional article</li> <li>The stream files are downloaded as .tar archive files which are a special kind of file that we'll need to unpack</li> <li>Instead of loading each file into memory this function returns a \"generator\" which is a special python object that is to be iterated over</li> <li>This basically means it contains the instructions to unpack and scan over files on the fly</li> <li>This function also contains the logic to deal with if these files are zip archives or you've manually unpacked the archive and have the .bz2 zipped files</li> </ul> <pre><code># loading from tar and extracting files\ndef load_markets(file_paths):\n    for file_path in file_paths:\n        print(file_path)\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n\n    return None\n</code></pre>"},{"location":"tutorials/backtestingRatingsTutorial/#2-scan-across-market-states-and-extract-useful-objects","title":"2: Scan across market states and extract useful objects","text":"<ul> <li>So this function will take a special \"stream\" object which we'll create with <code>betfairlightweight</code></li> <li>The function takes a stream object input and returns 4 instances of the market state</li> <li>The market state 3 mins before the scheduled off</li> <li>The market state immediately before it goes inplay</li> <li>The market state immediately before it closes for settlement</li> <li>The final market state with outcomes</li> <li>It basically just loops over all the market states and has a few checks to determine if it should save the current market state as key variables and then returns those</li> </ul> <pre><code># Extract Components From Generated Stream\ndef extract_components_from_stream(s):\n\n    with patch(\"builtins.open\", lambda f, _: f):   \n\n        # Will return 3 market books t-3mins marketbook, the last preplay marketbook and the final market book\n        evaluate_market = None\n        prev_market = None\n        postplay_market = None\n        preplay_market = None\n        t3m_market = None\n\n        gen = stream.get_generator()\n\n        for market_books in gen():\n\n            for market_book in market_books:\n\n                # If markets don't meet filter return None's\n                if evaluate_market is None and ((evaluate_market := filter_market(market_book)) == False):\n                    return (None, None, None, None)\n\n                # final market view before market goes in play\n                if prev_market is not None and prev_market.inplay != market_book.inplay:\n                    preplay_market = market_book\n\n                # final market view before market goes is closed for settlement\n                if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n                    postplay_market = market_book\n\n                # Calculate Seconds Till Scheduled Market Start Time\n                seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n\n                # Market at 3 mins before scheduled off\n                if t3m_market is None and seconds_to_start &lt; 3*60:\n                    t3m_market = market_book\n\n                # update reference to previous market\n                prev_market = market_book\n\n        # If market didn't go inplay\n        if postplay_market is not None and preplay_market is None:\n            preplay_market = postplay_market\n\n        return (t3m_market, preplay_market, postplay_market, prev_market) # Final market is last prev_market\n</code></pre>"},{"location":"tutorials/backtestingRatingsTutorial/#3-4-summarise-those-useful-objects-and-write-to-csv","title":"3 &amp; 4: Summarise those useful objects and write to .csv","text":"<ul> <li>This next chunk contains a wrapper function that will do all the execution</li> <li>It will open a csv output file</li> <li>Use the load_markets utility to iterate over the .tar files</li> <li>Use <code>betfairlightweight</code> to instantiate the special stream object</li> <li>Pass that stream object to the extract_components_from_stream which will scan across the market states and pull out 4 key market books</li> <li>Convert those marketbooks into simple summary numbers or dictionaries that will be written to the output .csv file</li> </ul> <pre><code>def run_stream_parsing():\n\n    # Run Pipeline\n    with open(\"outputs/tho-odds.csv\", \"w+\") as output:\n\n        # Write Column Headers To File\n        output.write(\"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,ltp,matched_volume,atb_ladder_3m,atl_ladder_3m\\n\")\n\n        for file_obj in load_markets(data_path):\n\n            # Instantiate a \"stream\" object\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            # Extract key components according to the custom function above (outputs 4 objects)\n            (t3m_market, preplay_market, postplay_market, final_market) = extract_components_from_stream(stream)\n\n            # If no price data for market don't write to file\n            if postplay_market is None:\n                continue; \n\n            # Runner metadata and key fields available from final market book\n            runner_data = [\n                {\n                    'selection_id': r.selection_id,\n                    'selection_name': next((rd.name for rd in final_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n                    'selection_status': r.status,\n                    'sp': r.sp.actual_sp\n                }\n                for r in final_market.runners \n            ]\n\n            # Last Traded Price\n            # _____________________\n\n            # From the last marketbook before inplay or close\n            ltp = [runner.last_price_traded for runner in preplay_market.runners]\n\n            # Total Matched Volume  \n            # _____________________\n\n            # Calculates the traded volume across all traded price points for each selection\n            def ladder_traded_volume(ladder):\n                return(sum([rung.size for rung in ladder]))\n\n            selection_traded_volume = [ ladder_traded_volume(runner.ex.traded_volume) for runner in postplay_market.runners ]\n\n            # Top 3 Ladder\n            # ______________________\n\n            # Extracts the top 3 price / stakes in available orders on both back and lay sides. Returns python dictionaries\n\n            def top_3_ladder(availableLadder):\n                out = {}\n                price = []\n                volume = []\n                if len(availableLadder) == 0:\n                    return(out)        \n                else:\n                    for rung in availableLadder[0:3]:\n                        price.append(rung.price)\n                        volume.append(rung.size)\n                    out[\"price\"] = price\n                    out[\"volume\"] = volume\n                    return(out)\n\n            # Sometimes t-3 mins market book is empty\n            try:\n                atb_ladder_3m = [ top_3_ladder(runner.ex.available_to_back) for runner in t3m_market.runners]\n                atl_ladder_3m = [ top_3_ladder(runner.ex.available_to_lay) for runner in t3m_market.runners]\n            except:\n                atb_ladder_3m = {}\n                atl_ladder_3m = {}\n\n            # Writing To CSV\n            # ______________________\n\n            for (runnerMeta, ltp, selection_traded_volume, atb_ladder_3m, atl_ladder_3m) in zip(runner_data, ltp, selection_traded_volume, atb_ladder_3m, atl_ladder_3m):\n\n                if runnerMeta['selection_status'] != 'REMOVED':\n\n                    output.write(\n                        \"{},{},{},{},{},{},{},{},{},{},{},{},{} \\n\".format(\n                            str(final_market.market_id),\n                            final_market.market_definition.market_time,\n                            final_market.market_definition.country_code,\n                            final_market.market_definition.venue,\n                            final_market.market_definition.name,\n                            runnerMeta['selection_id'],\n                            runnerMeta['selection_name'],\n                            runnerMeta['selection_status'],\n                            runnerMeta['sp'],\n                            ltp,\n                            selection_traded_volume,\n                            '\"' + str(atb_ladder_3m) + '\"', # Forcing the dictionaries to strings so we don't run into issues loading the csvs with the dictionary commas\n                            '\"' + str(atl_ladder_3m) + '\"'\n                        )\n                    )\n</code></pre> <pre><code># This will execute the files (it took me ~2 hours for 2 months of data)\n#run_stream_parsing()\n</code></pre>"},{"location":"tutorials/backtestingRatingsTutorial/#extending-this-code","title":"Extending this code","text":"<ul> <li>Because this process is very slow you might want to save much more information than you think you need</li> <li>For example I currently think I only want the best back and lay prices at t-3 mins before the off but I've saved the top 3 boxes in the available to back and lay ladders as dictionary strings</li> <li>From these ladders I can retroactively calculate not only just the best back and lay prices but also WAP prices and also sizes at those boxes which I could use for much more accurate backtesting if I wanted to later without having to scan across the entire stream files again</li> <li>I could easily save the entire open and traded orders ladders in the same way amongst many other ways of retaining more of the data for post-processing analysis</li> </ul>"},{"location":"tutorials/backtestingRatingsTutorial/#backtesting-analysis","title":"Backtesting Analysis","text":"<p>Let's take stock of where we are. We currently have model ratings (about 1.5 months worth) and Betfair Odds (2 months worth).</p> <p>Circling back to the original backtesting context we needed to solve for 2 key questions:</p> <ol> <li>What would my predicted chance have been exactly for this selection in this market on this day in the past?</li> <li>What would have I decided to bet at what odds (exactly) and for how much stake (exactly) based on this prediction?</li> </ol> <p>Backtesting with someone else's publicly available and historically logged ratings solves question 1. With these particular ratings we're fine but generally we should just be aware there are some sketchy services that might make retroactive adjustments to historical ratings to juice their performance which obviously violates 1.</p> <p>For the second part we now have several real Betfair odds values to combine with the ratings and some chosen staking formula to simulate actual bets. I won't dwell too much on the stake size component but it's important.</p> <p>Similarly we aren't out of the woods with the \"what odds exactly\" question either. I'll show performance of backtesting at the \"Last Traded Price\" however, there's literally no way of actually being the last bet matched order on every exchange market so there's some uncertainty in a few of these prices.</p> <p>Further, and from experience, if you placing bets at the BSP and you're using some form of proportional staking (like Kelly) then you're calculated stake size will need to include a quantity (the BSP) which you will literally never be 100% sure of. You'll need to estimate the BSP as close to market suspension as you can and place your BSP bets with a stake sized derived from that estimation. This imprecision in stake calculation WILL cost you some profit relative to your backtested expectation.</p> <p>These might seem like minor considerations but you should be aware of some of the gory details of the many ways becoming successful on Betfair is really difficult. To be reliably profitable on Betfair you don't just need a good model, you'll likely need to spend hours and hours thinking about these things: testing things, ironing out all these little kinks and trying to account for all your uncertainties. I'll just be running over the skeleton of what you should do.</p>"},{"location":"tutorials/backtestingRatingsTutorial/#setting-up-your-master-data","title":"Setting up your master data","text":"<pre><code># First we'll load and tidy our odds data\n\n# Load in odds file we created above\nbfOdds = pd.read_csv(\"outputs/tho-odds.csv\", dtype={'market_id': object, 'selection_id': object, 'atb_ladder_3m': object, 'atl_ladder_3m': object})\n\n# Convert dictionary columns\nimport ast\nbfOdds['atb_ladder_3m'] = [ast.literal_eval(x) for x in bfOdds['atb_ladder_3m']]\nbfOdds['atl_ladder_3m'] = [ast.literal_eval(x) for x in bfOdds['atl_ladder_3m']]\n\n# Convert LTP to Numeric\nbfOdds['ltp'] = pd.to_numeric(bfOdds['ltp'], errors='coerce')\n\n# Filter after 18th Feb\nbfOdds = bfOdds.query('event_date &gt;= \"2021-02-18\"')\n\nbfOdds.head(5)\n</code></pre> market_id event_date country track market_name selection_id selection_name result bsp ltp matched_volume atb_ladder_3m atl_ladder_3m 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 31552374 2. Chubascos LOSER 5.84 5.9 7390.59 {'price': [6, 5.9, 5.8], 'volume': [30.99, 82.... {'price': [6.2, 6.4, 6.6], 'volume': [4.99, 22... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620171 3. Love You More LOSER 65.00 70.0 1297.27 {'price': [65, 60, 55], 'volume': [2, 2.9, 15.... {'price': [75, 80, 85], 'volume': [0.66, 3.24,... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620172 4. Splashing Rossa LOSER 10.98 10.5 2665.94 {'price': [9, 8.8, 8.6], 'volume': [21.92, 10.... {'price': [9.6, 9.8, 10], 'volume': [13.43, 7.... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620173 5. The Fairytale LOSER 54.56 50.0 221.13 {'price': [55, 50, 48], 'volume': [4.85, 2.85,... {'price': [65, 70, 75], 'volume': [2.1, 7.18, ... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620174 6. My Boy Dragon LOSER 166.90 160.0 199.00 {'price': [140, 120, 110], 'volume': [0.36, 1.... {'price': [260, 270, 340], 'volume': [1.29, 2.... <p>When backtesting, and developing wagering systems more generally, I've found it really helpful to have a set of standard patterns or ways of representing common datasets. For a task like this it's really helpful to keep everything joined and together in a wide table.</p> <p>So we want a dataframe with everything we need to conduct the backtest: your model ratings, the odds you're betting at, the results on the bets, and ultimately betting logic will all become columns in a dataframe.</p> <p>It's helpful to have consistent column names so that the code for any new test you run looks much like previous tests and you can leverage custom functions that can be reused across tests and other projects. I like to have the following columns in my backtesting dataframe:</p> <ul> <li>date</li> <li>market_id (can be a surrogate id if dealing with fixed odds markets)</li> <li>selection_id (could be selection name)</li> <li>win (a binary win loss)</li> <li>model_odds</li> <li>model_prob</li> <li>market_odds</li> <li>market_prob</li> <li>bet_side</li> <li>stake</li> <li>gpl</li> <li>commission</li> <li>npl</li> </ul> <p>This analysis will be a little more complex as we're considering different price points so I'll leave out the <code>market_odds</code> and <code>market_prob</code> columns.</p> <pre><code># Joining the ratings data and odds data and combining\nrawDF = pd.merge(\n        hubRatings[hubRatings['market_id'].isin(bfOdds.market_id.unique())], \n        bfOdds[['market_name', 'market_id', 'selection_id', 'result', 'matched_volume', 'bsp', 'ltp', 'atb_ladder_3m', 'atl_ladder_3m']],\n        on = ['market_id', 'selection_id'],\n        how = 'inner'\n    )\n\nrawDF\n</code></pre> date track race_number race_name market_id selection_id selection_name model_odds market_name result matched_volume bsp ltp atb_ladder_3m atl_ladder_3m 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523320 11. Vast Kama 34.28 R1 1200m 3yo LOSER 1934.49 42.00 42.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523319 10. Triptonic 21.22 R1 1200m 3yo LOSER 1710.76 23.87 23.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 35773035 9. Right Reason 10.23 R1 1200m 3yo LOSER 5524.11 12.50 11.5 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523318 8. Off Road 40.75 R1 1200m 3yo LOSER 1506.51 35.31 34.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523317 7. More Than Value 77.49 R1 1200m 3yo LOSER 617.18 55.00 55.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 28092381 11. Born A Warrior 10.67 R8 1300m Hcap LOSER 905.55 6.97 6.2 {'price': [6.2, 5.8, 5.1], 'volume': [7.98, 40... {'price': [6.8, 7, 7.8], 'volume': [6.26, 41.5... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 38698010 12. Diva Bella 25.77 R8 1300m Hcap LOSER 11.06 23.60 18.5 {'price': [23, 22, 18], 'volume': [0.31, 24.91... {'price': [70, 75, 95], 'volume': [0.61, 3.5, ... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 28224034 13. Twice As Special 51.23 R8 1300m Hcap LOSER 52.49 36.37 26.0 {'price': [30, 29, 26], 'volume': [13.84, 5.92... {'price': [44, 50, 95], 'volume': [2.76, 1.66,... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 38913296 15. Rosie Riveter 24.92 R8 1300m Hcap LOSER 58.65 9.72 11.0 {'price': [10.5, 10, 9.6], 'volume': [0.69, 28... {'price': [11, 12.5, 19], 'volume': [3.87, 2.7... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 4973624 8. Celer 26.23 R8 1300m Hcap LOSER 22.14 21.73 28.0 {'price': [24, 23, 20], 'volume': [1.55, 18.26... {'price': [30, 65, 70], 'volume': [0.55, 1.55,... <pre><code>df = (\n    rawDF\n    # Extra Best Back + Lay 3 mins before of\n    .assign(best_back_3m = lambda x: [np.nan if d.get('price') is None else d.get('price')[0] for d in x['atb_ladder_3m']])\n    .assign(best_lay_3m = lambda x: [np.nan if d.get('price') is None else d.get('price')[0] for d in x['atl_ladder_3m']])\n    # Coalesce LTP to BSP (about 60 rows)\n    .assign(ltp = lambda x: np.where(x[\"ltp\"].isnull(), x[\"bsp\"], x[\"ltp\"]))\n    # Add a binary win / loss column\n    .assign(win=lambda x: np.where(x['result'] == \"WINNER\", 1, 0))\n    # Extra columns\n    .assign(model_prob=lambda x: 1 / x['model_odds'])\n    # Reorder Columns\n    .reindex(columns = ['date', 'track', 'race_number', 'market_id', 'selection_id', 'bsp', 'ltp','best_back_3m','best_lay_3m','atb_ladder_3m', 'atl_ladder_3m', 'model_prob', 'model_odds', 'win'])\n\n)\n\ndf.head(5)\n</code></pre> date track race_number market_id selection_id bsp ltp best_back_3m best_lay_3m atb_ladder_3m atl_ladder_3m model_prob model_odds win 2021-02-18 DOOMBEN 1 1.179418181 38523320 42.00 42.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 0.029172 34.28 0 2021-02-18 DOOMBEN 1 1.179418181 38523319 23.87 23.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 0.047125 21.22 0 2021-02-18 DOOMBEN 1 1.179418181 35773035 12.50 11.5 9.4 9.6 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 0.097752 10.23 0 2021-02-18 DOOMBEN 1 1.179418181 38523318 35.31 34.0 30.0 36.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 0.024540 40.75 0 2021-02-18 DOOMBEN 1 1.179418181 38523317 55.00 55.0 65.0 70.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... 0.012905 77.49 0"},{"location":"tutorials/backtestingRatingsTutorial/#staking-outcome-functions","title":"Staking + Outcome Functions","text":"<p>Now we can create a set of standard staking functions that a dataframe with an expected set of columns and add staking and bet outcome fields.</p> <p>We'll also add the ability of these functions to reference a different odds column so that we can backtest against our different price points.</p> <p>For simplicity we'll assume you're paying 5% commission on winnings however it could be higher or lower and depends on the MBR of the market.</p> <pre><code>def bet_apply_commission(df, com = 0.05):\n\n    # Total Market GPL\n    df['market_gpl'] = df.groupby('market_id')['gpl'].transform(sum)\n\n    # Apply 5% commission\n    df['market_commission'] = np.where(df['market_gpl'] &lt;= 0, 0, 0.05 * df['market_gpl'])\n\n    # Sum of Market Winning Bets\n    df['floored_gpl'] = np.where(df['gpl'] &lt;= 0, 0, df['gpl'])\n    df['market_netwinnings'] = df.groupby('market_id')['floored_gpl'].transform(sum)\n\n    # Partition Commission According to Selection GPL\n    df['commission'] = np.where(df['market_netwinnings'] == 0, 0, (df['market_commission'] * df['floored_gpl']) / (df['market_netwinnings']))\n\n    # Calculate Selection NPL\n    df['npl'] = df['gpl'] - df['commission']\n\n    # Drop excess columns\n    df = df.drop(columns = ['floored_gpl', 'market_netwinnings', 'market_commission', 'market_gpl'])\n\n    return(df)\n\n\ndef bet_flat(df, stake = 1, back_odds = 'market_odds', lay_odds = 'market_odds'):\n\n    \"\"\"\n    Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns\n    \"\"\"\n\n    df['bet_side'] = np.where((df[\"model_odds\"] &gt;= df[back_odds]) &amp; (df[\"model_odds\"] &lt;= df[lay_odds]),\n                        \"P\", # PUSH\n                       np.where(\n                            df[\"model_odds\"] &lt; df[back_odds],\n                            \"B\",\n                            \"L\"\n                        )\n                       )\n\n    df['stake'] = np.where(df['bet_side'] == \"P\", 0, stake)\n\n    df['gpl'] = np.where(df['bet_side'] == \"B\", \n                         np.where(df['win'] == 1, df['stake'] * (df[back_odds]-1), -df['stake']), # PL for back bets\n                         np.where(df['win'] == 1, -df['stake'] * (df[lay_odds]-1), df['stake']) # PL for lay bets\n                        )   \n\n    # Apply commission and NPL\n    df = bet_apply_commission(df)\n\n    return(df)\n\ndef bet_kelly(df, stake = 1, back_odds = 'market_odds', lay_odds = 'market_odds'):\n\n    \"\"\"\n    Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns\n    \"\"\"\n\n    df['bet_side'] = np.where((df[\"model_odds\"] &gt;= df[back_odds]) &amp; (df[\"model_odds\"] &lt;= df[lay_odds]),\n                            \"P\", # PUSH\n                            np.where(\n                                df[\"model_odds\"] &lt; df[back_odds],\n                                \"B\",\n                                \"L\"\n                            )\n                       )\n\n    df['stake'] = np.where(df['bet_side'] == \"P\", # PUSH\n                           0,\n                           np.where(\n                             df['bet_side'] == \"B\",\n                             ( (1 / df['model_odds']) - (1 / df[back_odds]) ) / (1 - (1 / df[back_odds])),\n                             ( (1 / df[lay_odds]) - (1 / df['model_odds']) ) / (1 - (1 / df[lay_odds])),\n                           )\n                          )\n\n    df['gpl'] = np.where(df['bet_side'] == \"B\", \n                         np.where(df['win'] == 1, df['stake'] * (df[back_odds]-1), -df['stake']), # PL for back bets\n                         np.where(df['win'] == 1, -df['stake'] * (df[lay_odds]-1), df['stake']) # PL for lay bets\n                        )\n\n    # Apply commission and NPL\n    df = bet_apply_commission(df)\n\n    return(df)\n</code></pre> <pre><code># Testing one of these functions\nflat_bets_bsp = bet_flat(df, stake = 1, back_odds = 'bsp', lay_odds = 'bsp')\nflat_bets_bsp.head(5)\n</code></pre> date track race_number market_id selection_id bsp ltp best_back_3m best_lay_3m atb_ladder_3m atl_ladder_3m model_prob model_odds win bet_side stake gpl commission npl 2021-02-18 DOOMBEN 1 1.179418181 38523320 42.00 42.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 0.029172 34.28 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 38523319 23.87 23.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 0.047125 21.22 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 35773035 12.50 11.5 9.4 9.6 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 0.097752 10.23 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 38523318 35.31 34.0 30.0 36.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 0.024540 40.75 0 L 1 1.0 0.0 1.0 2021-02-18 DOOMBEN 1 1.179418181 38523317 55.00 55.0 65.0 70.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... 0.012905 77.49 0 L 1 1.0 0.0 1.0"},{"location":"tutorials/backtestingRatingsTutorial/#evaluation-functions","title":"Evaluation Functions","text":"<p>In my experience it's great to develop a suite of functions and analytical tools that really dig into every aspect of your simulated betting performance. You want to be as thorough and critical as possible, even when you're results are good.</p> <p>Another tip to guide this process is to have a reasonable benchmark. Essentially no one wins at 10% POT on thoroughbreds at the BSP so if your analysis suggests you can... there's a bug. Similarly you almost certainly won't lose at more than &lt;-10%. Different sports and codes will have different realistic profitability ranges depending on the efficiency of the markets (will be roughly correlated to matched volume). Ruling out unreasonable results can save you a lot of time and delusion.</p> <p>I'm keeping it pretty simple here but you might also want to create functions to analyse:</p> <ul> <li>Track / distance based performance</li> <li>Performance across odds ranges</li> <li>Profit volatility (maybe using sharpe ratio to optimise volatility - adjusted profit)</li> <li>Date ranges (weeks / months etc)</li> </ul> <pre><code># Create simple PL and POT table\ndef bet_eval_metrics(d, side = False):\n\n    if side:\n        metrics = (d\n         .groupby('bet_side', as_index=False)\n         .agg({\"npl\": \"sum\", \"stake\": \"sum\"})\n         .assign(pot=lambda x: x['npl'] / x['stake'])\n        )\n    else:\n        metrics = pd.DataFrame(d\n         .agg({\"npl\": \"sum\", \"stake\": \"sum\"})\n        ).transpose().assign(pot=lambda x: x['npl'] / x['stake'])\n\n    return(metrics[metrics['stake'] != 0])\n\n# Cumulative PL by market to visually see trend and consistency\ndef bet_eval_chart_cPl(d):\n\n    d = (\n        d\n        .groupby('market_id')\n        .agg({'npl': 'sum'})\n    )\n\n    d['market_number'] = np.arange(len(d))\n    d['cNpl'] = d.npl.cumsum()\n\n    chart = px.line(d, x=\"market_number\", y=\"cNpl\", title='Cumulative Net Profit', template='simple_white')\n\n    return(chart)\n</code></pre> <p>To illustrate these evaluation functions let's analyse flat staking at the BSP.</p> <pre><code>bets = bet_flat(df, stake = 1, back_odds = 'bsp', lay_odds = 'bsp')\n\nbet_eval_metrics(bets, side = True)\n</code></pre> bet_side npl stake pot B -749.493788 8356 -0.089695 L -268.499212 8592 -0.031250 <pre><code>bet_eval_chart_cPl(bets)\n</code></pre> <p>So this isn't gonna build us an art gallery! This is to be expected though, it's not easy to make consistent profit certainly from free ratings sources available online.</p> <p></p>"},{"location":"tutorials/backtestingRatingsTutorial/#testing-different-approaches","title":"Testing different approaches","text":"<p>We pulled those extra price points for a reason. Let's set up a little test harness that enables us to use different price points and bet using different staking functions.</p> <pre><code># We'll test a 2 different staking schemes on 3 different price points\ngrid = {\n        \"flat_bsp\": (bet_flat, \"bsp\", \"bsp\"),\n        \"flat_ltp\": (bet_flat, \"ltp\", \"ltp\"),\n        \"flat_3m\": (bet_flat, \"best_back_3m\", \"best_lay_3m\"),\n        \"kelly_bsp\": (bet_kelly, \"bsp\", \"bsp\"),\n        \"kelly_ltp\": (bet_kelly, \"ltp\", \"ltp\"),\n        \"kelly_3m\": (bet_kelly, \"best_back_3m\", \"best_lay_3m\")\n    }\n</code></pre> <pre><code>metricSummary = None\nfor strategy, objects in grid.items():\n\n    # Assemble bets based on staking function and odds column\n    # objects[0] is the staking function itself\n    bets = objects[0](df, back_odds = objects[1], lay_odds = objects[2])\n\n    # Calculate the metrics and tag with strategy label\n    betMetrics = (\n        bet_eval_metrics(bets)\n        .assign(strategy=lambda x: strategy)\n        .reindex(columns = ['strategy', 'stake', 'npl', 'pot'])\n    )\n\n    # Init the betMetrics df or append if already exists\n    try:\n        metricSummary = pd.concat([metricSummary, betMetrics], ignore_index=True)\n    except:\n        metricSummary = betMetrics\n\nmetricSummary.sort_values(by=['pot'], ascending=False)\n</code></pre> strategy stake npl pot kelly_ltp 754.496453 -31.814150 -0.042166 kelly_bsp 732.165110 -34.613773 -0.047276 flat_bsp 16948.000000 -1017.993000 -0.060066 flat_ltp 16949.000000 -1184.546000 -0.069889 flat_3m 15712.000000 -1225.123000 -0.077974 kelly_3m 614.135601 -50.469295 -0.082179 <pre><code># Compare Cumulative PL Charts\ncumulativePLs = None\nfor strategy, objects in grid.items():\n\n    # Assemble bets based on staking function and odds column\n    bets = objects[0](df, back_odds = objects[1], lay_odds = objects[2])\n\n    d = (\n           bets\n           .groupby('market_id')\n           .agg({'npl': 'sum', 'stake': 'sum'})\n    )\n\n    d['market_number'] = np.arange(len(d))\n    # Normalise to $10,000 stake for visual comparison\n    d['npl'] = d['npl'] / (d.stake.sum() / 10000)\n    d['cNpl'] = d.npl.cumsum()\n    d['strategy'] = strategy\n\n    # Init the cumulativePLs df or append if already exists\n    try:\n        cumulativePLs = pd.concat([cumulativePLs, d], ignore_index=True)\n    except:\n        cumulativePLs = d\n\npx.line(cumulativePLs, x=\"market_number\", y=\"cNpl\", color=\"strategy\", title='Cumulative Net Profit', template='simple_white')\n</code></pre> <p></p>"},{"location":"tutorials/backtestingRatingsTutorial/#searching-for-profit","title":"Searching For Profit","text":"<p>So this is often where you're going to arrive developing many wagering models: there's no indication of reliable long term profit. Where do you go from here?</p> <p>TBH I think most people give up here. Because you're not a quitter though you've got 3 main option categories:</p> <ol> <li>Make the underlying model better</li> <li>Search for better prices via detailed price analysis and clever bet placement</li> <li>Try to find a subset of these selections with these ratings and these price points that are sustainably profitable</li> </ol> <p>Obviously each situation is different but I think option 3 isn't a bad way to go initially because it will definitely help you understand your model better. For a racing model you might want to split your performance by:</p> <ul> <li>tracks or states</li> <li>track conditions or weather</li> <li>barriers</li> <li>race quality or grade</li> <li>odds ranges</li> <li>selection sample size (you likely perform worse on horses with little form for eg)</li> <li>perceived model value</li> </ul> <p>Finding a big enough slice across those dimensions that's either really profitable or really losing might reveal to you a bug in the data or workflow in your model development that you can go back and fix.</p> <p>As an example of a simple approach to selectiveness I'll quickly run through how being more selective about your perceived value might make a difference in final profitability.</p> <p>So our best performing strategy using our simple analysis above was Kelly staking at the last traded price. We'll start with that but be aware of that there's no way of implementing a LTP bet placement engine, you could imagine a proxy being placing limit bets \"just before\" the race jumps which is a whole other kettle of fish.</p> <p>Anyway, let's plot our profitability under this strategy at different perceived \"edges\". If we are more selective of only large overlays according to the hub's rated chance you can see we can increase the profitability.</p> <pre><code>bets = bet_kelly(df, back_odds = 'ltp', lay_odds = 'ltp')\n\nmetricSummary = None\n\nfor bVal in [0.05, 0.1, 0.15, 0.2, 0.3]:\n    for lVal in [0.05, 0.1, 0.15, 0.2, 0.3]:\n        x = bets.query('((ltp-model_odds) / ltp) &gt; {}  | ((model_odds-ltp) / ltp) &gt; {}'.format(bVal, lVal))\n\n        betMetrics = bet_eval_metrics(x, side = False)\n        betMetrics['bVal'] = bVal\n        betMetrics['lVal'] = lVal\n\n        try:\n            metricSummary = pd.concat([metricSummary, betMetrics], ignore_index=True)\n        except:\n            metricSummary = betMetrics\n\nmetricSummary.sort_values(by=['pot'], ascending=False).head(4)\n</code></pre> npl stake pot bVal lVal -18.059813 574.944431 -0.031411 0.3 0.30 -22.887791 628.302349 -0.036428 0.3 0.20 -24.509182 669.482514 -0.036609 0.3 0.05 -22.997908 614.528386 -0.037424 0.2 0.30 <pre><code>betsFilters = bets.query('((ltp-model_odds) / ltp) &gt; {}  | ((model_odds-ltp) / ltp) &gt; {}'.format(0.3, 0.3))\nbet_eval_chart_cPl(betsFilters)\n</code></pre> <p></p> <p>We were doing ok till the last 200 market nightmare! Might be one to test with more data.</p> <ul> <li>So we still haven't found a clear profitable edge with these ratings, however we got a bit closer to break even which is positive.</li> <li>This step also indicates that this rating system performs better for large overlays which is a good model indicator (if you can't improve by selecting for larger overlays it's usually a sign you need to go back to the drawing board)</li> <li>You could imagine a few more iterations of analysis you might be able to eek out a slight edge</li> <li>However, be wary as these steps optimisation steps are very prone to overfitting so you need to be careful.</li> </ul>"},{"location":"tutorials/backtestingRatingsTutorial/#conclusion-and-next-steps","title":"Conclusion and Next Steps","text":"<p>While using someone else's model is easy it's also not likely to end in personal riches. Developing your own model with your own tools and on a sport or racing code you know about is probably where you should start. However, hopefully this short guide helps you think about what to do when you finish the modelling component:</p> <p>How much money will I win or lose if I started using this system to place bets with real money?</p> <p>If you want to expand this backtesting analysis, here's a list (in no particular order) of things that I've omitted or angles I might look at next:</p> <ul> <li>Get more data -- more rating data and odds data is needed for draw a good conclusion about long term expectation</li> <li>Cross reference performance against race or selection metadata (track, # races run etc.) to improve performance with betting selectivity</li> <li>Extract more price points from the stream data to try to gain an pricing edge on these ratings</li> </ul>"},{"location":"tutorials/backtestingRatingsTutorial/#over-to-you","title":"Over to you","text":"<p>We're planning on writing some more tutorials to help make it easier to work with the JSON data sets. If there are particular examples or data sets you'd like to see us walk through please reach out.</p> <p>Community support</p> <ul> <li>There's a really active Betcode (formerly Betfairlightweight) slack group that's a great place to go to ask questions about the library and get support from other people who are also working in the space</li> </ul>"},{"location":"tutorials/backtestingRatingsTutorial/#complete-code","title":"Complete code","text":"<p>Run the code from your ide by using <code>py &lt;filename&gt;.py</code>, making sure you amend the path to point to your input data. </p> <p>Download from Github</p> <pre><code>import pandas as pd\nimport numpy as np\nimport requests\nfrom datetime import date, timedelta\nimport os\nimport re\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\nimport logging\nfrom unittest.mock import patch\nfrom typing import List, Set, Dict, Tuple, Optional\nfrom itertools import zip_longest\nimport plotly.express as px\n\nimport betfairlightweight\nfrom betfairlightweight import StreamListener\nfrom betfairlightweight.resources.bettingresources import (\n    PriceSize,\n    MarketBook\n)\n#Comment out the below line if not using a Jupyter Notebook (.ipynb)\n%config IPCompleter.greedy=True\n\n#### --------------------------\n#### FUNCTIONS\n#### --------------------------\n\n# Function to return Pandas DF of hub ratings for a particular date\ndef getHubRatings(dte):\n\n    # Substitute the date into the URL\n    url = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date=20{}&amp;presenter=RatingsPresenter&amp;json=true'.format(dte)\n\n    # Convert the response into JSON\n    responseJson = requests.get(url).json()\n    # if SSL.Certificate errors are encountered, replace the above line with: responseJson = requests.get(url,verify=False).json()\n\n    hubList = []\n\n    if not responseJson:\n        return(None)\n\n\n    # Want an normalised table (1 row per selection)\n    # Brute force / simple approach is to loop through meetings / races / runners and pull out the key fields\n    for meeting in responseJson['meetings']:\n        for race in meeting['races']:\n            for runner in race['runners']:\n                hubList.append(\n                    {\n                        'date': dte,\n                        'track': meeting['name'],\n                        'race_number': race['number'],\n                        'race_name': race['name'],\n                        'market_id': race['bfExchangeMarketId'],\n                        'selection_id':  str(runner['bfExchangeSelectionId']),\n                        'selection_name': runner['name'],\n                        'model_odds': runner['ratedPrice']\n                    }\n                )\n\n    out = pd.DataFrame(hubList)\n\n    return(out)\n\n# rounding to 2 decimal places or returning '' if blank\ndef as_str(v) -&gt; str:\n    return '%.2f' % v if type(v) is float else v if type(v) is str else ''\n\n# splitting race name and returning the parts \ndef split_anz_horse_market_name(market_name: str) -&gt; (str, str, str):\n    # return race no, length, race type\n    # input sample: R6 1400m Grp1\n    parts = market_name.split(' ')\n    race_no = parts[0] # return example R6\n    race_len = parts[1] # return example 1400m\n    race_type = parts[2].lower() # return example grp1, trot, pace\n\n    return (race_no, race_len, race_type)\n\n# creating a flag that is True when markets are australian thoroughbreds\ndef filter_market(market: MarketBook) -&gt; bool: \n    d = market.market_definition\n    return (d.country_code == 'AU' \n        and d.market_type == 'WIN' \n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n\n# loading from tar and extracting files\ndef load_markets(file_paths):\n    for file_path in file_paths:\n        print(file_path)\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n\n    return None\n\n# Extract Components From Generated Stream\ndef extract_components_from_stream(stream):\n\n    with patch(\"builtins.open\", lambda f, _: f):   \n\n        # Will return 3 market books t-3mins marketbook, the last preplay marketbook and the final market book\n        evaluate_market = None\n        prev_market = None\n        postplay_market = None\n        preplay_market = None\n        t3m_market = None\n\n        gen = stream.get_generator()\n\n        for market_books in gen():\n\n            for market_book in market_books:\n\n                # If markets don't meet filter return None's\n                if evaluate_market is None and ((evaluate_market := filter_market(market_book)) == False):\n                    return (None, None, None, None)\n\n                # final market view before market goes in play\n                if prev_market is not None and prev_market.inplay != market_book.inplay:\n                    preplay_market = market_book\n\n                # final market view before market goes is closed for settlement\n                if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n                    postplay_market = market_book\n\n                # Calculate Seconds Till Scheduled Market Start Time\n                seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n\n                # Market at 3 mins before scheduled off\n                if t3m_market is None and seconds_to_start &lt; 3*60:\n                    t3m_market = market_book\n\n                # update reference to previous market\n                prev_market = market_book\n\n        # If market didn't go inplay\n        if postplay_market is not None and preplay_market is None:\n            preplay_market = postplay_market\n\n        return (t3m_market, preplay_market, postplay_market, prev_market) # Final market is last prev_market\n\ndef run_stream_parsing():\n\n    # Run Pipeline\n    with open(\"outputs/tho-odds.csv\", \"w\") as output:\n\n        # Write Column Headers To File\n        output.write(\"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,ltp,matched_volume,atb_ladder_3m,atl_ladder_3m\\n\")\n\n        for file_obj in load_markets('[INPUT LOCATION OF DATA FILES HERE]'):\n\n            # Instantiate a \"stream\" object\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            # Extract key components according to the custom function above (outputs 4 objects)\n            (t3m_market, preplay_market, postplay_market, final_market) = extract_components_from_stream(stream)\n\n            # If no price data for market don't write to file\n            if postplay_market is None:\n                continue; \n\n            # Runner metadata and key fields available from final market book\n            runner_data = [\n                {\n                    'selection_id': r.selection_id,\n                    'selection_name': next((rd.name for rd in final_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n                    'selection_status': r.status,\n                    'sp': r.sp.actual_sp\n                }\n                for r in final_market.runners \n            ]\n\n            # Last Traded Price\n            # _____________________\n\n            # From the last marketbook before inplay or close\n            ltp = [runner.last_price_traded for runner in preplay_market.runners]\n\n            # Total Matched Volume  \n            # _____________________\n\n            # Calculates the traded volume across all traded price points for each selection\n            def ladder_traded_volume(ladder):\n                return(sum([rung.size for rung in ladder]))\n\n            selection_traded_volume = [ ladder_traded_volume(runner.ex.traded_volume) for runner in postplay_market.runners ]\n\n            # Top 3 Ladder\n            # ______________________\n\n            # Extracts the top 3 price / stakes in available orders on both back and lay sides. Returns python dictionaries\n\n            def top_3_ladder(availableLadder):\n                out = {}\n                price = []\n                volume = []\n                if len(availableLadder) == 0:\n                    return(out)        \n                else:\n                    for rung in availableLadder[0:3]:\n                        price.append(rung.price)\n                        volume.append(rung.size)\n                    out[\"price\"] = price\n                    out[\"volume\"] = volume\n                    return(out)\n\n            # Sometimes t-3 mins market book is empty\n            try:\n                atb_ladder_3m = [ top_3_ladder(runner.ex.available_to_back) for runner in t3m_market.runners]\n                atl_ladder_3m = [ top_3_ladder(runner.ex.available_to_lay) for runner in t3m_market.runners]\n            except:\n                atb_ladder_3m = {}\n                atl_ladder_3m = {}\n\n            # Writing To CSV\n            # ______________________\n\n            for (runnerMeta, ltp, selection_traded_volume, atb_ladder_3m, atl_ladder_3m) in zip(runner_data, ltp, selection_traded_volume, atb_ladder_3m, atl_ladder_3m):\n\n                if runnerMeta['selection_status'] != 'REMOVED':\n\n                    output.write(\n                        \"{},{},{},{},{},{},{},{},{},{},{},{},{} \\n\".format(\n                            str(final_market.market_id),\n                            final_market.market_definition.market_time,\n                            final_market.market_definition.country_code,\n                            final_market.market_definition.venue,\n                            final_market.market_definition.name,\n                            runnerMeta['selection_id'],\n                            runnerMeta['selection_name'],\n                            runnerMeta['selection_status'],\n                            runnerMeta['sp'],\n                            ltp,\n                            selection_traded_volume,\n                            '\"' + str(atb_ladder_3m) + '\"', # Forcing the dictionaries to strings so we don't run into issues loading the csvs with the dictionary commas\n                            '\"' + str(atl_ladder_3m) + '\"'\n                        )\n                    )\n\n\ndef bet_apply_commission(df, com = 0.05):\n\n    # Total Market GPL\n    df['market_gpl'] = df.groupby('market_id')['gpl'].transform(sum)\n\n    # Apply 5% commission\n    df['market_commission'] = np.where(df['market_gpl'] &lt;= 0, 0, 0.05 * df['market_gpl'])\n\n    # Sum of Market Winning Bets\n    df['floored_gpl'] = np.where(df['gpl'] &lt;= 0, 0, df['gpl'])\n    df['market_netwinnings'] = df.groupby('market_id')['floored_gpl'].transform(sum)\n\n    # Partition Commission According to Selection GPL\n    df['commission'] = np.where(df['market_netwinnings'] == 0, 0, (df['market_commission'] * df['floored_gpl']) / (df['market_netwinnings']))\n\n    # Calculate Selection NPL\n    df['npl'] = df['gpl'] - df['commission']\n\n    # Drop excess columns\n    df = df.drop(columns = ['floored_gpl', 'market_netwinnings', 'market_commission', 'market_gpl'])\n\n    return(df)\n\n\ndef bet_flat(df, stake = 1, back_odds = 'market_odds', lay_odds = 'market_odds'):\n\n    \"\"\"\n    Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns\n    \"\"\"\n\n    df['bet_side'] = np.where((df[\"model_odds\"] &gt;= df[back_odds]) &amp; (df[\"model_odds\"] &lt;= df[lay_odds]),\n                        \"P\",\n                       np.where(\n                            df[\"model_odds\"] &lt; df[back_odds],\n                            \"B\",\n                            \"L\"\n                        )\n                       )\n\n    df['stake'] = np.where(df['bet_side'] == \"P\", 0, stake)\n\n    df['gpl'] = np.where(df['bet_side'] == \"B\", \n                         np.where(df['win'] == 1, df['stake'] * (df[back_odds]-1), -df['stake']), # PL for back bets\n                         np.where(df['win'] == 1, -df['stake'] * (df[lay_odds]-1), df['stake']) # PL for lay bets\n                        )   \n\n    # Apply commission and NPL\n    df = bet_apply_commission(df)\n\n    return(df)\n\ndef bet_kelly(df, stake = 1, back_odds = 'market_odds', lay_odds = 'market_odds'):\n\n    \"\"\"\n    Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns\n    \"\"\"\n\n    df['bet_side'] = np.where((df[\"model_odds\"] &gt;= df[back_odds]) &amp; (df[\"model_odds\"] &lt;= df[lay_odds]),\n                            \"P\",\n                            np.where(\n                                df[\"model_odds\"] &lt; df[back_odds],\n                                \"B\",\n                                \"L\"\n                            )\n                       )\n\n    df['stake'] = np.where(df['bet_side'] == \"P\",\n                           0,\n                           np.where(\n                             df['bet_side'] == \"B\",\n                             ( (1 / df['model_odds']) - (1 / df[back_odds]) ) / (1 - (1 / df[back_odds])),\n                             ( (1 / df[lay_odds]) - (1 / df['model_odds']) ) / (1 - (1 / df[lay_odds])),\n                           )\n                          )\n\n    df['gpl'] = np.where(df['bet_side'] == \"B\", \n                         np.where(df['win'] == 1, df['stake'] * (df[back_odds]-1), -df['stake']), # PL for back bets\n                         np.where(df['win'] == 1, -df['stake'] * (df[lay_odds]-1), df['stake']) # PL for lay bets\n                        )\n\n    # Apply commission and NPL\n    df = bet_apply_commission(df)\n\n    return(df)\n\n\n# Create simple PL and POT table\ndef bet_eval_metrics(d, side = False):\n\n    if side:\n        metrics = (d\n         .groupby('bet_side', as_index=False)\n         .agg({\"npl\": \"sum\", \"stake\": \"sum\"})\n         .assign(pot=lambda x: x['npl'] / x['stake'])\n        )\n    else:\n        metrics = pd.DataFrame(d\n         .agg({\"npl\": \"sum\", \"stake\": \"sum\"})\n        ).transpose().assign(pot=lambda x: x['npl'] / x['stake'])\n\n    return(metrics[metrics['stake'] != 0])\n\n# Cumulative PL by market to visually see trend and consistency\ndef bet_eval_chart_cPl(d):\n\n    d = (\n        d\n        .groupby('market_id')\n        .agg({'npl': 'sum'})\n    )\n\n    d['market_number'] = np.arange(len(d))\n    d['cNpl'] = d.npl.cumsum()\n\n    chart = px.line(d, x=\"market_number\", y=\"cNpl\", title='Cumulative Net Profit', template='simple_white')\n\n    return(chart)\n\n\n#### --------------------------\n#### EXECUTION\n#### --------------------------\n\n\n# Loop through all recent history\ndateDFList = []\ndateList = pd.date_range(date(2021,2,18),date.today()-timedelta(days=1),freq='d')\n\nfor dte in dateList:\n    dte = dte.strftime(\"%y-%m-%d\")\n    dateDFList.append(getHubRatings(dte))\n\n# Concatenate (add rows to rows) all the dataframes within the list\nhubRatings = pd.concat(dateDFList)\n\n# create trading instance (don't need username/password)\ntrading = betfairlightweight.APIClient(\"username\", \"password\")\n\n# create listener\nlistener = StreamListener(max_latency=None)\n\n# This will execute the files (it took me ~2 hours for 2 months of data)\nrun_stream_parsing()\n\n# Load in odds file we created above\nbfOdds = pd.read_csv(\"outputs/tho-odds.csv\", dtype={'market_id': object, 'selection_id': object, 'atb_ladder_3m': object, 'atl_ladder_3m': object})\n\n# Convert dictionary columns\nimport ast\nbfOdds['atb_ladder_3m'] = [ast.literal_eval(x) for x in bfOdds['atb_ladder_3m']]\nbfOdds['atl_ladder_3m'] = [ast.literal_eval(x) for x in bfOdds['atl_ladder_3m']]\n\n# Convert LTP to Numeric\nbfOdds['ltp'] = pd.to_numeric(bfOdds['ltp'], errors='coerce')\n\n# Filter after 18th Feb\nbfOdds = bfOdds.query('event_date &gt;= \"2021-02-18\"')\n\n# Joining the ratings data and odds data and combining\nrawDF = pd.merge(\n        hubRatings[hubRatings['market_id'].isin(bfOdds.market_id.unique())], \n        bfOdds[['market_name', 'market_id', 'selection_id', 'result', 'matched_volume', 'bsp', 'ltp', 'atb_ladder_3m', 'atl_ladder_3m']],\n        on = ['market_id', 'selection_id'],\n        how = 'inner'\n    )\n\n# Join and clean up columns\ndf = (\n    rawDF\n    # Extra Best Back + Lay 3 mins before of\n    .assign(best_back_3m = lambda x: [np.nan if d.get('price') is None else d.get('price')[0] for d in x['atb_ladder_3m']])\n    .assign(best_lay_3m = lambda x: [np.nan if d.get('price') is None else d.get('price')[0] for d in x['atl_ladder_3m']])\n    # Coalesce LTP to BSP (about 60 rows)\n    .assign(ltp = lambda x: np.where(x[\"ltp\"].isnull(), x[\"bsp\"], x[\"ltp\"]))\n    # Add a binary win / loss column\n    .assign(win=lambda x: np.where(x['result'] == \"WINNER\", 1, 0))\n    # Extra columns\n    .assign(model_prob=lambda x: 1 / x['model_odds'])\n    # Reorder Columns\n    .reindex(columns = ['date', 'track', 'race_number', 'market_id', 'selection_id', 'bsp', 'ltp','best_back_3m','best_lay_3m','atb_ladder_3m', 'atl_ladder_3m', 'model_prob', 'model_odds', 'win'])\n\n)\n\n\nbets = bet_flat(df, stake = 1, back_odds = 'bsp', lay_odds = 'bsp')\n\nbet_eval_metrics(bets, side = True)\n\nbet_eval_chart_cPl(bets)\n\n\n# We'll test a 2 different staking schemes on 3 different price points\ngrid = {\n        \"flat_bsp\": (bet_flat, \"bsp\", \"bsp\"),\n        \"flat_ltp\": (bet_flat, \"ltp\", \"ltp\"),\n        \"flat_3m\": (bet_flat, \"best_back_3m\", \"best_lay_3m\"),\n        \"kelly_bsp\": (bet_kelly, \"bsp\", \"bsp\"),\n        \"kelly_ltp\": (bet_kelly, \"ltp\", \"ltp\"),\n        \"kelly_3m\": (bet_kelly, \"best_back_3m\", \"best_lay_3m\")\n    }\n\n\n# Evaluate Metrics For Strategy Grid\nmetricSummary = None\nfor strategy, objects in grid.items():\n\n    # Assemble bets based on staking function and odds column\n    # objects[0] is the staking function itself\n    bets = objects[0](df, back_odds = objects[1], lay_odds = objects[2])\n\n    betMetrics = (\n        bet_eval_metrics(bets)\n        .assign(strategy=lambda x: strategy)\n        .reindex(columns = ['strategy', 'stake', 'npl', 'pot'])\n    )\n\n    try:\n        metricSummary = pd.concat([metricSummary, betMetrics], ignore_index=True)\n    except:\n        metricSummary = betMetrics\n\nmetricSummary.sort_values(by=['pot'], ascending=False)\n\n\n# Compare Cumulative PL Charts\ncumulativePLs = None\nfor strategy, objects in grid.items():\n\n    # Assemble bets based on staking function and odds column\n    bets = objects[0](df, back_odds = objects[1], lay_odds = objects[2])\n\n    d = (\n           bets\n           .groupby('market_id')\n           .agg({'npl': 'sum', 'stake': 'sum'})\n    )\n\n    d['market_number'] = np.arange(len(d))\n    # Normalise to $10,000 stake for visual comparison\n    d['npl'] = d['npl'] / (d.stake.sum() / 10000)\n    d['cNpl'] = d.npl.cumsum()\n    d['strategy'] = strategy\n\n    try:\n        cumulativePLs = pd.concat([cumulativePLs, d], ignore_index=True)\n    except:\n        cumulativePLs = d\n\npx.line(cumulativePLs, x=\"market_number\", y=\"cNpl\", color=\"strategy\", title='Cumulative Net Profit', template='simple_white')\n\n\nbets = bet_kelly(df, back_odds = 'ltp', lay_odds = 'ltp')\n\nmetricSummary = None\n\nfor bVal in [0.05, 0.1, 0.15, 0.2, 0.3]:\n    for lVal in [0.05, 0.1, 0.15, 0.2, 0.3]:\n        x = bets.query('((ltp-model_odds) / ltp) &gt; {}  | ((model_odds-ltp) / ltp) &gt; {}'.format(bVal, lVal))\n\n        betMetrics = bet_eval_metrics(x, side = False)\n        betMetrics['bVal'] = bVal\n        betMetrics['lVal'] = lVal\n\n        try:\n            metricSummary = pd.concat([metricSummary, betMetrics], ignore_index=True)\n        except:\n            metricSummary = betMetrics\n\nmetricSummary.sort_values(by=['pot'], ascending=False).head(4)\n\nbetsFilters = bets.query('((ltp-model_odds) / ltp) &gt; {}  | ((model_odds-ltp) / ltp) &gt; {}'.format(0.3, 0.3))\nbet_eval_chart_cPl(betsFilters)\n</code></pre>"},{"location":"tutorials/backtestingRatingsTutorial/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"tutorials/jsonToCsvTutorial/","title":"JSON to CSV tutorial: making a market summary","text":"<p>Before you start</p> <p>This tutorial was original shared in 2021. Since then a new library has been created that allows you to run the same logic included here with about a 97% reduction in run time, which makes a significant difference in usability. To learn about these changes and how to implement them to speed up your code take a look at our JSON to CSV revisited article.</p> <p>The historic pricing data available on the Betfair Historic Data site is an excellent resource, including almost every market offered on the Exchange back to 2016. We do appreciate though that the JSON format of the data sets can make it challenging to find value in the data, especially if you're not confident in working with large data sets. </p> <p>In this tutorial we're going to step through the process of using the Python <code>betfairlightweight</code> library to take in a compressed tar folder, process the historic JSON files, and convert the data into a simple csv output, including basic market summary data for each runner split into pre play and in play values. We're also going to include a filter function, to allow us to filter out markets we're not interested in. </p> <p>The idea of this tutorial is to share a way of using existing libraries to make working with the JSON data sets easier, and hopefully the provide a foundation that you can build your own code base and data sets from. We'll be focusing on horse racing data; what we want to produce is a csv output that includes one row per runner for each market we're interested in, along with summary pre-play and in-play data for the runner. We'll step through the issues we encountered and how we went about solving the various challenges, including sharing relevant code snips along the way. </p> <p>We're not Python natives and acknowledge that there are probably more efficient and neater ways of achieving the same end goal! As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! </p> <p>Cheat sheet</p> <ul> <li> <p>If you're looking for the complete code head to the bottom of the page or download the script from Github.</p> </li> <li> <p>To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code), make sure you've navigated in the terminal to the folder you've saved the script in and then type <code>py main.py</code> (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. </p> </li> <li> <p>The script will take some time before it starts outputting to the output.csv file, so let it run for a few minutes before getting worried that it's not working!</p> </li> <li> <p>Make sure you amend your data path to point to your data file (instructions below). We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site. We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. </p> </li> <li> <p>We're using the <code>betfairlightweight</code> package to do the heavy lifting</p> </li> <li> <p>We've also posted the completed code logic on the <code>betfair-downunder</code> Github repo.</p> </li> </ul>"},{"location":"tutorials/jsonToCsvTutorial/#setting-up-your-environment","title":"Setting up your environment","text":"<p>You're going to need to make sure you have Python and pip installed to get this code to run. If you're just starting out with Python, you may have to add Python to your environment variables. The is generally easiest to do by checking the box when you're installing Python choosing to 'add to PATH'. </p> <p>The alternative approach to the above is to use a Jupyter notebook which has the environment already set up - this might be the easier option for people new to programming. </p> <p>We're using some pretty new Python features, so it might be worth checking your version and updating if you're keen to follow along. </p> <p>To install <code>betfairlightweight</code> open a command prompt, or a terminal in your text editor of choice and input <code>pip install betfairlightweight</code> then return. </p>"},{"location":"tutorials/jsonToCsvTutorial/#data-input","title":"Data input","text":"<p>We started with the historic data parsing example from liampauling's Github repo. </p> <p>Our first issue was that the example provided was expecting to take in an individual market file. We wanted to be able to accept data in a tar archive, a zipped folder, or a directory of individual bz2 files.</p> <p>Here's the code we used for handling the different file formats. </p> <pre><code># loading from tar and extracting files\ndef load_markets(file_paths: List[str]):\n    for file_path in file_paths:\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n\n    return None\n</code></pre> <p>and then used it like this:</p> <pre><code># the path directories to the data sets\n# accepts tar files, zipped files or \n# directory with bz2 file(s)\nmarket_paths = [\n    './2020_12_DecRacingPro.zip',\n    './PRO',\n    './2021_01_JanRacingPro.tar'\n]\n\n... \n\nfor file_obj in load_markets(market_paths):\n    stream = trading.streaming.create_historical_generator_stream(\n        file_path=file_obj,\n        listener=listener,\n    )\n\n    def get_pre_post_final(s):\n        with patch(\"builtins.open\", lambda f, _: f):  \n</code></pre> <p>This means we can pass in the tar and/or zipped file in its compressed form and/or directory with individual bz2 files in it and not worry about extracting the file contents, or having to handle the logic of iterating over the inner nested file structure. </p> <p>File paths</p> <p>The program will look at the file path you pass in relative to the location of the script you're running. So it will start by looking in the same folder it's saved in and then follow your navigation instructions from there, using <code>/</code> to indicate a folder and <code>../</code> to navigate up a level in the folder structure. </p> <p>If our example the data files sit in the same folder as the script (<code>./PRO</code>).</p> <p>If it were in a folder at the same level as the folder that our script is in then we'd need to navigate 'up' a level (using <code>../</code>) and then into the folder housing the data, i.e. <code>'../dataFolder/PRO'</code> and if the data were in a different folder within the same folder as our script file we'd use <code>'./dataFolder/PRO'</code> etc.</p>"},{"location":"tutorials/jsonToCsvTutorial/#type-definitions","title":"Type definitions","text":"<p>If you're used to working in strongly typed languages, especially those with type definitions, you might find it a bit frustrating to try and figure out where you can access the different data types, for example market name or runner BSP. There are some things you can do to make this a bit easier, other than digging into the <code>betfairlightweight</code> source code, which was where we started. </p> <p>If you want to look at the definitions from the source code:</p> <ul> <li><code>MarketBook, RunnerBook</code></li> <li><code>MarketDefinitionRunner, MarketDefinition</code></li> </ul> <p>There are some Python extensions you can use in your ide that go some way to helping here. </p> <pre><code># importing data types\nimport betfairlightweight\nfrom betfairlightweight.resources.bettingresources import (\n    PriceSize,\n    MarketBook\n)\n</code></pre>"},{"location":"tutorials/jsonToCsvTutorial/#market-summary-data","title":"Market summary data","text":"<p>The raw files show the data at 50ms (PRO) or 1 second (ADVANCED) intervals. Too produce our csv we will need to look at the state of the market before the market goes in play, and then the state at the end of the market, and calculate from that what the pre play and in play figures are. </p> <p>This is the data we're going to include in our output csv.</p> Column Definition market_id unique market identifier event_date scheduled start date/time (UTC) country event country code track track name market_name market name selection_id unique runner identifier selection_name runner name result win/loss/removed bsp Betfair starting price pp_min pre play min price traded pp_max pre play max price traded pp_wap pre play weighted average price pp_ltp pre play last traded price pp_volume pre play matched volume ip_min in play min price traded ip_max in play max price traded ip_wap in play weighted average price ip_ltp in play last traded price ip_volume in play matched volume <p><code>betfairlightweight</code> exposes snapshots of the market that include all the price data we need. To allow us to compute pre play and in play figures there are three market snapshots we need to find. These are the final view before the market turns in play, the market at the end of the race once it's no longer open but the price ladder hasn't yet been cleared, and the final closed snapshot that shows winner/loser status etc. We can then use the deltas between these market views to calculate the pre play and in play summary statistics. </p> <p>We iterate over these market snapshots and when we find the first market showing as in play we go back to the previous update, and use this as our pre play view. After this we keep iterating until we find the last time that the market status shows as 'open' and then use the data from the following update for the final pricing data (i.e. the first market view once the market was suspended at the end of the race). The winner/loser statuses come from the final market view. </p> <pre><code>def get_pre_post_final(s):\n    with patch(\"builtins.open\", lambda f, _: f):   \n        eval_market = None\n        prev_market = None\n        preplay_market = None\n        postplay_market = None       \n\n        gen = stream.get_generator()\n\n        for market_books in gen():\n            for market_book in market_books:\n\n                # if market doesn't meet filter return out\n                if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n                    return (None, None, None)\n\n                # final market view before market goes in play\n                if prev_market is not None and prev_market.inplay != market_book.inplay:\n                    preplay_market = prev_market\n\n                # final market view at the conclusion of the market\n                if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n                    postplay_market = market_book\n\n                # update reference to previous market\n                prev_market = market_book\n\n        return (preplay_market, postplay_market, prev_market) # prev is now final\n\n(preplay_market, postplay_market, final_market) = get_pre_post_final(stream)\n</code></pre> <p>We needed to write a function to parse the price data (pre play and in play) and pull out the values we're interested in. We used a reduce function to go over each matched price point, and calculate the four necessary values. </p> <p>To calculate weighted average price we multiplied price by size for each price point, and added them together. Once they're summed, we divided that figure by the total matched value. </p> <p>The matched volume is simply the sum of all matched stakes. </p> <p>The min price and max price are the lowest and highest values where money has matched on the runner.</p> <p>Reduce functions</p> <p>I gather from some actual Python gurus in our community that while reduce functions are very common in other languages (i.e. the ones I normally work in!), apparently they're not very Pythonic... if you're super keen, feel free to rewrite this section into a list/dict comprehension or another more Pythonic solution!</p> <pre><code># parsing price data and pulling out weighted avg price, matched, min price and max price\ndef parse_traded(traded: List[PriceSize]) -&gt; (float, float, float, float):\n    if len(traded) == 0: \n        return (None, None, None, None)\n\n    (wavg_sum, matched, min_price, max_price) = functools.reduce(\n        lambda total, ps: (\n            total[0] + (ps.price * ps.size), # wavg_sum before we divide by total matched\n            total[1] + ps.size, # total matched\n            min(total[2], ps.price), # min price matched\n            max(total[3], ps.price), # max price matched\n        ),\n        traded,\n        (0, 0, 1001, 0) # starting default values\n    )\n\n    wavg_sum = (wavg_sum / matched) if matched &gt; 0 else None # dividing sum of wavg by total matched\n    matched = matched if matched &gt; 0 else None \n    min_price = min_price if min_price != 1001 else None\n    max_price = max_price if max_price != 0 else None\n\n    return (wavg_sum, matched, min_price, max_price)\n</code></pre> <p>Our volume figures don't include BSP bets yet, so to account for that we're looking at the <code>back_stake_taken</code> and <code>lay_liability_taken</code> values on the SP object from the post play market snapshot, then finding whichever the smaller of those two values is and saving it that so we can add it to the <code>traded_volume</code> field in a later step. We use the smaller value of <code>back_stake_taken</code> or (<code>lay_liability_taken</code>/(BSP - 1)) (i.e. backer's stake for SP lay bets) as any difference between the two values will have matched against non-BSP money and therefore is already accounted for in our matched volume. </p> <pre><code>preplay_traded = [ (r.last_price_traded, r.ex.traded_volume) for r in preplay_market.runners ] if preplay_market is not None else None\npostplay_traded = [ (\n    r.last_price_traded,\n    r.ex.traded_volume,\n    # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1))        \n    min_gr0(\n        next((pv.size for pv in r.sp.back_stake_taken if pv.size &gt; 0), 0),\n        next((pv.size for pv in r.sp.lay_liability_taken if pv.size &gt; 0), 0)  / ((r.sp.actual_sp if (type(r.sp.actual_sp) is float) or (type(r.sp.actual_sp) is int) else 0) - 1)\n    )\n) for r in postplay_market.runners ]\n</code></pre> <p>For our csv, we have columns for runner id, runner name, winning status and BSP, so we'll store these values too. The runner name is a bit harder to get, as we need to match up the runner definition with the same <code>selection_id</code> as the <code>market_book</code> object we're currently looking at.</p> <pre><code># generic runner data\n    runner_data = [\n        {\n            'selection_id': r.selection_id,\n            'selection_name': next((rd.name for rd in final_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n            'selection_status': r.status,\n            'sp': as_str(r.sp.actual_sp),\n        }\n        for r in final_market.runners \n    ]\n</code></pre> <p>Not all markets go in play, and therefore won't have any values for the in play portion of the csv, so we need to make sure we can handle this case.</p> <p>We don't have in play figures separate to pre play; we have a snapshot before the market went in play, and then the view at the end of the market, so we need to use the difference between these two sets of figures to figure out what happened in play.</p> <p>We have two ladders, one post play and one pre play. We go through every price point in the post play ladder, and remove any volume that's showing in the pre play ladder at the corresponding price point. This leaves us with the volumes matched while the market was in play. </p> <p>One corner case we had to catch is that our resulting list might have prices with 0 volume, which trip up our min and max values, which doesn't use volume in its calculations. To catch this we filter out any items from the ladder with a volume of 0.</p> <p>Note: there are some markets included in the data files that are effective empty and don't contain any price data. We're disregarding these markets and printing out an error message to the log (<code>market has no price data</code>).</p> <pre><code># runner price data for markets that go in play\nif preplay_traded is not None:\n    def runner_vals(r):\n        (pre_ltp, pre_traded), (post_ltp, post_traded, sp_traded) = r\n\n        inplay_only = list(filter(lambda ps: ps.size &gt; 0, [\n            PriceSize(\n                price=post_ps.price, \n                size=post_ps.size - next((pre_ps.size for pre_ps in pre_traded if pre_ps.price == post_ps.price), 0)\n            )\n            for post_ps in post_traded \n        ]))\n\n        (ip_wavg, ip_matched, ip_min, ip_max) = parse_traded(inplay_only)\n        (pre_wavg, pre_matched, pre_min, pre_max) = parse_traded(pre_traded)\n\n        return {\n            'preplay_ltp': as_str(pre_ltp),\n            'preplay_min': as_str(pre_min),\n            'preplay_max': as_str(pre_max),\n            'preplay_wavg': as_str(pre_wavg),\n            'preplay_matched': as_str((pre_matched or 0) + (sp_traded or 0)),\n            'inplay_ltp': as_str(post_ltp),\n            'inplay_min': as_str(ip_min),\n            'inplay_max': as_str(ip_max),\n            'inplay_wavg': as_str(ip_wavg),\n            'inplay_matched': as_str(ip_matched),\n        }\n\n    runner_traded = [ runner_vals(r) for r in zip_longest(preplay_traded, postplay_traded, fillvalue=PriceSize(0, 0)) ]\n\n# runner price data for markets that don't go in play\nelse:\n    def runner_vals(r):\n        (ltp, traded, sp_traded) = r\n        (wavg, matched, min_price, max_price) = parse_traded(traded)\n\n        return {\n            'preplay_ltp': as_str(ltp),\n            'preplay_min': as_str(min_price),\n            'preplay_max': as_str(max_price),\n            'preplay_wavg': as_str(wavg),\n            'preplay_matched': as_str((matched or 0) + (sp_traded or 0)),\n            'inplay_ltp': '',\n            'inplay_min': '',\n            'inplay_max': '',\n            'inplay_wavg': '',\n            'inplay_matched': '',\n        }\n\n    runner_traded = [ runner_vals(r) for r in postplay_traded ]\n</code></pre>"},{"location":"tutorials/jsonToCsvTutorial/#writing-to-csv","title":"Writing to CSV","text":"<p>We defined the columns we want for our csv pretty early in the code. </p> <pre><code># record prices to a file\nwith open(\"output.csv\", \"w\") as output:\n    # defining column headers\n    output.write(\"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume\\n\")\n</code></pre> <p>We then assign the values for each column.</p> <pre><code># printing to csv for each runner\nfor (rdata, rprices) in zip(runner_data, runner_traded):\n    # defining data to go in each column\n    output.write(\n        \"{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n            postplay_market.market_id,\n            postplay_market.market_definition.market_time,\n            postplay_market.market_definition.country_code,\n            postplay_market.market_definition.venue,\n            postplay_market.market_definition.name,\n            rdata['selection_id'],\n            rdata['selection_name'],\n            rdata['selection_status'],\n            rdata['sp'],\n            rprices['preplay_min'],\n            rprices['preplay_max'],\n            rprices['preplay_wavg'],\n            rprices['preplay_ltp'],\n            rprices['preplay_matched'],\n            rprices['inplay_min'],\n            rprices['inplay_max'],\n            rprices['inplay_wavg'],\n            rprices['inplay_ltp'],\n            rprices['inplay_matched'],\n        )\n    )\n</code></pre>"},{"location":"tutorials/jsonToCsvTutorial/#filtering-markets","title":"Filtering markets","text":"<p>Currently we're going through every file provided in the raw data folders, which in our case included markets from different countries, all different market types and both gallops and harness races. To save filtering these markets manually later in Excel, and also to avoid processing additional data we don't need and slowing the process down further, we decided to add a market filter so we only kept the markets we were interested in.</p> <p>We filtered on three things:</p> <ul> <li>event country code (i.e. AU, NZ, GB etc)</li> <li>market type (i.e. win, place etc)</li> <li>race type (i.e. gallops or harness)</li> </ul> <p>Using this logic, we are only keeping Australian win markets for gallops races.</p> <pre><code># filtering markets to those that fit the following criteria\ndef filter_market(market: MarketBook) -&gt; bool: \n    d = market.market_definition\n    return (d.country_code == 'AU' \n        and d.market_type == 'WIN' \n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n</code></pre> <p>Filtering out harness markets was the trickiest part of the process, as there's no neat way of separating harness meetings from gallops. To do this we had to parse the market name and look for the words 'trot' and 'pace', and treat the market as harness if we found either. To make it a little tidier we wrote a function to split the market name into its component parts. </p> <pre><code># splitting race name and returning the parts \ndef split_anz_horse_market_name(market_name: str) -&gt; (str, str, str):\n    # return race no, length, race type\n    # input samples: \n    # 'R6 1400m Grp1' -&gt; ('R6','1400m','grp1')\n    # 'R1 1609m Trot M' -&gt; ('R1', '1609m', 'trot')\n    # 'R4 1660m Pace M' -&gt; ('R4', '1660m', 'pace')\n    parts = market_name.split(' ')\n    race_no = parts[0] \n    race_len = parts[1] \n    race_type = parts[2].lower()\n\n    return (race_no, race_len, race_type)\n</code></pre> <p>We declare an <code>evaluate_market</code> flag and set it to none, and then in our loop the first time we evaluate the market we run the filter and skip any markets that don't meet our criteria.</p> <pre><code>eval_market = None\n\ngen = stream.get_generator()\n\nfor market_books in gen():\n    for market_book in market_books:\n\n        # if market doesn't meet filter return out\n        if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n            return (None, None, None)\n</code></pre>"},{"location":"tutorials/jsonToCsvTutorial/#helper-functions","title":"Helper functions","text":"<p>There are a couple of helper functions we wrote along the way to make the rest of the code easier to handle. </p> <p>As string</p> <p>Takes in a number and returns a text representation of it, rounding to two decimal places. </p> <pre><code># rounding to 2 decimal places or returning '' if blank\ndef as_str(v) -&gt; str:\n    return '%.2f' % v if (type(v) is float) or (type(v) is int) else v if type(v) is str else ''\n</code></pre> <p>Min value greater than 0</p> <p>Returns the smaller of two numbers, where the smaller isn't 0. </p> <pre><code># returning smaller of two numbers where min not 0\ndef min_gr0(a: float, b: float) -&gt; float:\n    if a &lt;= 0:\n        return b\n    if b &lt;= 0:\n        return a\n\n    return min(a, b)\n</code></pre>"},{"location":"tutorials/jsonToCsvTutorial/#final-thoughts","title":"Final thoughts","text":"<p><code>betfairlightweight</code> provides a ready made package that makes it easier to work with the JSON data and a pretty easy way to convert the data into a csv format, allowing you to then do your data wrangling in Excel if that's where you're more comfortable. </p> <p>Our intention is that you don't need a heap of Python experience to be able to work through this tutorial; as long as you're prepared to get the Python environment set up and learn some basic programming skills, the hope is that you'll be able to customise your own csv file and maybe even extend on what we've covered and produced here.</p> <p>We're planning on writing some more tutorials to help make it easier to work with the JSON data sets. If there are particular examples or data sets you'd like to see us walk through please reach out.</p> <p>Community support</p> <ul> <li>There's a really active Betcode (formerly Betfairlightweight) slack group that's a great place to go to ask questions about the library and get support from other people who are also working in the space</li> </ul>"},{"location":"tutorials/jsonToCsvTutorial/#complete-code","title":"Complete code","text":"<p>Run the code from your ide by using <code>py &lt;filename&gt;.py</code>, making sure you amend the path to point to your input data. Please note: the script will take some time before it starts outputting to the output.csv file, so let it run for a few minutes before getting worried that it's not working! You'll also see errors logged to the out file or terminal screen depending on your set up. </p> <p>Download from Github</p> <pre><code>import logging\nfrom typing import List, Tuple\n\nfrom unittest.mock import patch\nfrom itertools import zip_longest\nimport functools\n\nimport os\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\n\n# importing data types\nimport betfairlightweight\nfrom betfairlightweight.resources.bettingresources import (\n    PriceSize,\n    MarketBook \n)\n\nfile_output = \"output_bflw.csv\"\n\nmarket_paths = [\n    \"data/2021_10_OctRacingAUPro.tar\",\n    \"data/2021_11_NovRacingAUPro.tar\",\n    \"data/2021_12_DecRacingAUPro.tar\",\n]\n\n# setup logging\nlogging.basicConfig(level=logging.FATAL)\n\n# create trading instance (don't need username/password)\ntrading = betfairlightweight.APIClient(\"username\", \"password\", \"appkey\")\n\n# create listener\nlistener = betfairlightweight.StreamListener(\n    max_latency=None,   # ignore latency errors\n    output_queue=None,  # use generator rather than a queue (faster)\n    lightweight=False,  # lightweight mode is faster\n    update_clk=False,   # do not update clk on updates (not required when backtesting)\n\n    cumulative_runner_tv=True, \n    calculate_market_tv=True\n)\n\n# loading from tar and extracting files\ndef load_markets(file_paths: List[str]):\n    for file_path in file_paths:\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n    return None\n\n# rounding to 2 decimal places or returning '' if blank\ndef as_str(v) -&gt; str:\n    return '%.2f' % v if (type(v) is float) or (type(v) is int) else v if type(v) is str else ''\n\n# returning smaller of two numbers where min not 0\ndef min_gr0(a: float, b: float) -&gt; float:\n    if a &lt;= 0:\n        return b\n    if b &lt;= 0:\n        return a\n\n    return min(a, b)\n\n# parsing price data and pulling out weighted avg price, matched, min price and max price\ndef parse_traded(traded: List[PriceSize]) -&gt; Tuple[float, float, float, float]:\n    if len(traded) == 0: \n        return (None, None, None, None)\n\n    (wavg_sum, matched, min_price, max_price) = functools.reduce(\n        lambda total, ps: (\n            total[0] + (ps.price * ps.size), # wavg_sum before we divide by total matched\n            total[1] + ps.size, # total matched\n            min(total[2], ps.price), # min price matched\n            max(total[3], ps.price), # max price matched\n        ),\n        traded,\n        (0, 0, 1001, 0) # starting default values\n    )\n\n    wavg_sum = (wavg_sum / matched) if matched &gt; 0 else None # dividing sum of wavg by total matched\n    matched = matched if matched &gt; 0 else None \n    min_price = min_price if min_price != 1001 else None\n    max_price = max_price if max_price != 0 else None\n\n    return (wavg_sum, matched, min_price, max_price)\n\n# splitting race name and returning the parts \ndef split_anz_horse_market_name(market_name: str) -&gt; Tuple[str, str, str]:\n    # return race no, length, race type\n    # input samples: \n    # 'R6 1400m Grp1' -&gt; ('R6','1400m','grp1')\n    # 'R1 1609m Trot M' -&gt; ('R1', '1609m', 'trot')\n    # 'R4 1660m Pace M' -&gt; ('R4', '1660m', 'pace')\n    parts = market_name.split(' ')\n    race_no = parts[0] \n    race_len = parts[1] \n    race_type = parts[2].lower() \n\n    return (race_no, race_len, race_type)\n\n# filtering markets to those that fit the following criteria\ndef filter_market(market: MarketBook) -&gt; bool: \n    d = market.market_definition\n    return (d != None\n        and d.country_code == 'AU' \n        and d.market_type == 'WIN' \n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n\n# record prices to a file\nwith open(file_output, \"w\") as output:\n    # defining column headers\n    output.write(\"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume\\n\")\n\n    for i, file_obj in enumerate(load_markets(market_paths)):\n        print(\"Market {}\".format(i), end='\\r')\n\n        stream = trading.streaming.create_historical_generator_stream(\n            file_path=file_obj,\n            listener=listener,\n        )\n\n        def get_pre_post_final(s):\n            with patch(\"builtins.open\", lambda f, _: f):   \n                eval_market = None\n                prev_market = None\n                preplay_market = None\n                postplay_market = None       \n\n                gen = stream.get_generator()\n\n                for market_books in gen():\n                    for market_book in market_books:\n                        # if market doesn't meet filter return out\n                        if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n                            return (None, None, None)\n\n                        # final market view before market goes in play\n                        if prev_market is not None and prev_market.inplay != market_book.inplay:\n                            preplay_market = prev_market\n\n                        # final market view at the conclusion of the market\n                        if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n                            postplay_market = market_book\n\n                        # update reference to previous market\n                        prev_market = market_book\n\n                return (preplay_market, postplay_market, prev_market) # prev is now final\n\n        (preplay_market, postplay_market, final_market) = get_pre_post_final(stream)\n\n        # no price data for market\n        if postplay_market is None:\n            continue; \n\n        preplay_traded = [ (r.last_price_traded, r.ex.traded_volume) for r in preplay_market.runners ] if preplay_market is not None else None\n        postplay_traded = [ (\n            r.last_price_traded,\n            r.ex.traded_volume,\n            # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1))        \n            min_gr0(\n                next((pv.size for pv in r.sp.back_stake_taken if pv.size &gt; 0), 0),\n                next((pv.size for pv in r.sp.lay_liability_taken if pv.size &gt; 0), 0)  / ((r.sp.actual_sp if (type(r.sp.actual_sp) is float) or (type(r.sp.actual_sp) is int) else 0) - 1)\n            ) if r.sp.actual_sp is not None else 0,\n        ) for r in postplay_market.runners ]\n\n        # generic runner data\n        runner_data = [\n            {\n                'selection_id': r.selection_id,\n                'selection_name': next((rd.name for rd in final_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n                'selection_status': r.status,\n                'sp': as_str(r.sp.actual_sp),\n            }\n            for r in final_market.runners \n        ]\n\n        # runner price data for markets that go in play\n        if preplay_traded is not None:\n            def runner_vals(r):\n                (pre_ltp, pre_traded), (post_ltp, post_traded, sp_traded) = r\n\n                inplay_only = list(filter(lambda ps: ps.size &gt; 0, [\n                    PriceSize(\n                        price=post_ps.price, \n                        size=post_ps.size - next((pre_ps.size for pre_ps in pre_traded if pre_ps.price == post_ps.price), 0)\n                    )\n                    for post_ps in post_traded \n                ]))\n\n                (ip_wavg, ip_matched, ip_min, ip_max) = parse_traded(inplay_only)\n                (pre_wavg, pre_matched, pre_min, pre_max) = parse_traded(pre_traded)\n\n                return {\n                    'preplay_ltp': as_str(pre_ltp),\n                    'preplay_min': as_str(pre_min),\n                    'preplay_max': as_str(pre_max),\n                    'preplay_wavg': as_str(pre_wavg),\n                    'preplay_matched': as_str((pre_matched or 0) + (sp_traded or 0)),\n                    'inplay_ltp': as_str(post_ltp),\n                    'inplay_min': as_str(ip_min),\n                    'inplay_max': as_str(ip_max),\n                    'inplay_wavg': as_str(ip_wavg),\n                    'inplay_matched': as_str(ip_matched),\n                }\n\n            runner_traded = [ runner_vals(r) for r in zip_longest(preplay_traded, postplay_traded, fillvalue=PriceSize(0, 0)) ]\n\n        # runner price data for markets that don't go in play\n        else:\n            def runner_vals(r):\n                (ltp, traded, sp_traded) = r\n                (wavg, matched, min_price, max_price) = parse_traded(traded)\n\n                return {\n                    'preplay_ltp': as_str(ltp),\n                    'preplay_min': as_str(min_price),\n                    'preplay_max': as_str(max_price),\n                    'preplay_wavg': as_str(wavg),\n                    'preplay_matched': as_str((matched or 0) + (sp_traded or 0)),\n                    'inplay_ltp': '',\n                    'inplay_min': '',\n                    'inplay_max': '',\n                    'inplay_wavg': '',\n                    'inplay_matched': '',\n                }\n\n            runner_traded = [ runner_vals(r) for r in postplay_traded ]\n\n        # printing to csv for each runner\n        for (rdata, rprices) in zip(runner_data, runner_traded):\n            # defining data to go in each column\n            output.write(\n                \"{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n                    postplay_market.market_id,\n                    postplay_market.market_definition.market_time,\n                    postplay_market.market_definition.country_code,\n                    postplay_market.market_definition.venue,\n                    postplay_market.market_definition.name,\n                    rdata['selection_id'],\n                    rdata['selection_name'],\n                    rdata['selection_status'],\n                    rdata['sp'],\n                    rprices['preplay_min'],\n                    rprices['preplay_max'],\n                    rprices['preplay_wavg'],\n                    rprices['preplay_ltp'],\n                    rprices['preplay_matched'],\n                    rprices['inplay_min'],\n                    rprices['inplay_max'],\n                    rprices['inplay_wavg'],\n                    rprices['inplay_ltp'],\n                    rprices['inplay_matched'],\n                )\n            )\n</code></pre>"},{"location":"tutorials/jsonToCsvTutorial/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"tutorials/processingTarFiles101/","title":"Take away the pain! Processing TAR Files 101","text":"<p>So even though we've got some great tutorials up here on our Automation Hub, a common wish from many an automated customer, is for a simple plug-and-play tutorial and script for parsing the PRO TAR files into CSVs. So here it is, an easy-to-follow tutorial that makes use of existing tutorials by Tom Bishop and others to easily and painlessly process these JSON files into CSVs ready to be modelled.</p> <p>And don't worry, this tutorial has plenty of error-handling built in so (hopefully) you won't wake up in the morning to find the script failed at 1AM while you were dreaming of the final straight at Moonee Valley. </p> <p>This tutorial will concern itself primarily with racing, however, at the bottom of the page we have also included complete code suited for parsing sports TAR files including handling for line/handicap markets.</p> <p>So without further ado, let's jump into it!</p> <pre><code>import pandas as pd\nimport numpy as np\nimport os\nimport csv\nimport csv\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\nimport ast\nfrom unittest.mock import patch\nimport betfairlightweight\nfrom betfairlightweight import StreamListener\nfrom betfair_data import bflw \nimport pandas as pd\nfrom betfair_data import PriceSize\nimport functools\nfrom typing import List, Tuple\nfrom pandas.errors import SettingWithCopyWarning\nimport warnings\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\nfrom itertools import zip_longest\nfrom currency_converter import CurrencyConverter\n</code></pre> <pre><code>file_directory= '' #INSERT FILE DIRECTORY WHERE TAR FILES ARE STORED\n\nlog1_Start = 60 * 10 # Seconds before scheduled off to start recording data for data segment one\nlog1_Step = 30       # Seconds between log steps for first data segment\nlog2_Start = 60 * 1  # Seconds before scheduled off to start recording data for data segment two\nlog2_Step = 5    # Seconds between log steps for second data segment\n</code></pre> <pre><code># splitting race name and returning the parts \ndef split_anz_horse_market_name(market_name: str) -&amp;gt; Tuple[str, int, str]:\n    # return race no, length, race type\n    # input samples: \n    # 'R6 1400m Grp1' -&amp;gt; ('R6','1400m','grp1')\n    # 'R1 1609m Trot M' -&amp;gt; ('R1', '1609m', 'trot')\n    # 'R4 1660m Pace M' -&amp;gt; ('R4', '1660m', 'pace')\n    parts = market_name.split(' ')\n    race_no = parts[0] \n    race_len = parts[1].split('m')\n    race_len = race_len[0]\n    race_type = parts[2].lower() \n    return (race_no, race_len, race_type)\n\n# filtering markets to those that fit the following criteria\ndef filter_market(market: bflw.MarketBook) -&amp;gt; bool: \n    d = market.market_definition\n    return (d != None\n        and d.country_code == 'AU' \n        and d.market_type == 'WIN'\n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace' #strips out Harness Races\n        #and (c := split_anz_horse_market_name(d.name)[2]) == 'hcap'#can use this to filter by race type\n        #and (c := split_anz_horse_market_name(d.name)[1]) &amp;gt;= '1200' #can use this to filter by race length\n        )\n\n# Simply add the below variable name to the market filter function above with the filter value\n# Equals (== 'Value in Quotation' or True/False/None), Does Not Equal (!= 'Value in Quotation' or True/False/None) - FOR ALL TYPES\n# Greater than (&amp;gt;), Greater than or equal to (&amp;gt;=), Less than (&amp;lt;), Less than or equal to (&amp;lt;=) - FOR INT/FLOAT\n# For list of value 'in'\n\n# and d.betting_type: str - ODDS, ASIAN_HANDICAP_SINGLES, ASIAN_HANDICAP_DOUBLES or LINE\n# and d.bsp_market: bool - True, False\n# and d.country_code: str - list of codes can be found here: https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2 - Default value is 'GB' - Australia = 'AU', New Zealand = 'NZ'\n# and d.event_id: str - PARENT_EVENT_ID\n# and d.event_name: Optional[str] - Usually the name of the Match-Up (e.g. Bangladesh v Sri Lanka) or Race Meeting Name (e.g. Wangaratta (AUS) 1st Dec) - Note: Dictionaries don't support wildcard searches\n# and d.event_type_id: str - SportID [Horse Racing - 7, Greyhounds - 4339]\n# and d.market_base_rate: float - Market Commission Rate\n# and d.market_type: str - e.g. \"WIN\"\n# and d.name: Optional[str] - market name (e.g. R1 1170m Mdn)\n# and d.number_of_active_runners: int - number of horses/dogs in the race\n# and d.number_of_winners: int - Win market 1, Place markets 2+\n# and d.turn_in_play_enabled: bool - True, False\n# and d.venue: Optional[str] - Racing Only - Track\n</code></pre> <pre><code>trading = betfairlightweight.APIClient(username = \"username\", password = \"password\", app_key=\"app_key\")\nlistener = StreamListener(max_latency=None)\n\nstream_files = glob.glob(file_directory+\"*.tar\") \nselection_meta = file_directory+\"metadata.csv\"\nprices_path =  file_directory+\"preplay.csv\"\n\n# rounding to 2 decimal places or returning '' if blank\ndef as_str(v) -&amp;gt; str:\n    return '%.2f' % v if (type(v) is float) or (type(v) is int) else v if type(v) is str else ''\n\n# returning smaller of two numbers where min not 0\ndef min_gr0(a: float, b: float) -&amp;gt; float:\n    if a &amp;lt;= 0:\n        return b\n    if b &amp;lt;= 0:\n        return a\n\n    return min(a, b)\n\n# parsing price data and pulling out weighted avg price, matched, min price and max price\ndef parse_traded(traded: List[PriceSize]) -&amp;gt; Tuple[float, float, float, float]:\n    if len(traded) == 0: \n        return (None, None, None, None)\n\n    (wavg_sum, matched, min_price, max_price) = functools.reduce(\n        lambda total, ps: (\n            total[0] + (ps.price * ps.size), # wavg_sum before we divide by total matched\n            total[1] + ps.size, # total matched\n            min(total[2], ps.price), # min price matched\n            max(total[3], ps.price), # max price matched\n        ),\n        traded,\n        (0, 0, 1001, 0) # starting default values\n    )\n\n    wavg_sum = (wavg_sum / matched) if matched &amp;gt; 0 else None # dividing sum of wavg by total matched\n    matched = matched if matched &amp;gt; 0 else None \n    min_price = min_price if min_price != 1001 else None\n    max_price = max_price if max_price != 0 else None\n\n    return (wavg_sum, matched, min_price, max_price)\n\n\ndef load_markets(file_paths):\n    for file_path in file_paths:\n        print(file_path)\n        print(\"__ Parsing Detailed Prices ___ \")\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n\n    return None\n\ndef slicePrice(l, n):\n    try:\n        x = l[n].price\n    except:\n        x = \"\"\n    return(x)\n\ndef sliceSize(l, n):\n    try:\n        x = l[n].size\n    except:\n        x = \"\"\n    return(x)\n\ndef pull_ladder(availableLadder, n = 5):\n        out = {}\n        price = []\n        volume = []\n        if len(availableLadder) == 0:\n            return(out)        \n        else:\n            for rung in availableLadder[0:n]:\n                price.append(rung.price)\n                volume.append(rung.size)\n\n            out[\"p\"] = price\n            out[\"v\"] = volume\n            return(out)\n\ndef final_market_book(s):\n\n    with patch(\"builtins.open\", lambda f, _: f):\n\n        gen = s.get_generator()\n\n        for market_books in gen():\n\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    return(None)\n\n            for market_book in market_books:\n\n                last_market_book = market_book\n\n        return(last_market_book)\n</code></pre> <pre><code>def loop_prices(s, o):\n\n    with patch(\"builtins.open\", lambda f, _: f):\n\n        gen = s.get_generator()\n\n        marketID = None\n        tradeVols = None\n        time = None\n\n        for market_books in gen():\n\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    break\n\n            for market_book in market_books:\n\n                # Time Step Management ++++++++++++++++++++++++++++++++++\n\n                if marketID is None:\n\n                    # No market initialised\n                    marketID = market_book.market_id\n                    time =  market_book.publish_time\n\n                elif market_book.inplay:\n\n                    # Stop once market inplay\n                    break\n\n                else:\n\n                    seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n\n                    if seconds_to_start &amp;gt; log1_Start:\n\n                        # Too early before off to start logging prices\n                        continue\n\n                    else:\n\n                        # Update data at different time steps depending on seconds to off\n                        wait = np.where(seconds_to_start &amp;lt;= log2_Start, log2_Step, log1_Step)\n\n                        # New Market\n                        if market_book.market_id != marketID:\n                            marketID = market_book.market_id\n                            time =  market_book.publish_time\n                        # (wait) seconds elapsed since last write\n                        elif (market_book.publish_time - time).total_seconds() &amp;gt; wait:\n                            time = market_book.publish_time\n                        # fewer than (wait) seconds elapsed continue to next loop\n                        else:\n                            continue\n\n                # Execute Data Logging ++++++++++++++++++++++++++++++++++\n\n                for runner in market_book.runners:\n\n                    try:\n                        selection_status = runner.status\n                        reduction_factor = runner.adjustment_factor\n                        atb_ladder = pull_ladder(runner.ex.available_to_back, n = 5)\n                        atl_ladder = pull_ladder(runner.ex.available_to_lay, n = 5)\n                        spn = runner.sp.near_price\n                        spf = runner.sp.far_price\n                    except:\n                        selection_status = None\n                        reduction_factor = None\n                        atb_ladder = {}\n                        atl_ladder = {}\n                        spn = None\n                        spf = None\n\n                    # Calculate Current Traded Volume + Traded WAP\n                    limitTradedVol = sum([rung.size for rung in runner.ex.traded_volume])\n                    if limitTradedVol == 0:\n                        limitWAP = \"\"\n                    else:\n                        limitWAP = sum([rung.size * rung.price for rung in runner.ex.traded_volume]) / limitTradedVol\n                        limitWAP = round(limitWAP, 2)\n\n                    o.writerow(\n                        (\n                            market_book.market_id,\n                            market_book.number_of_active_runners,\n                            runner.selection_id,\n                            market_book.publish_time,\n                            limitTradedVol,\n                            limitWAP,\n                            runner.last_price_traded or \"\",\n                            selection_status,\n                            reduction_factor,\n                            str(atb_ladder).replace(' ',''), \n                            str(atl_ladder).replace(' ',''),\n                            str(spn),\n                            str(spf)\n                        )\n                    )\n\n\ndef parse_prices(dir, out_file):\n\n    with open(out_file, \"w+\") as output:\n\n        writer = csv.writer(\n            output, \n            delimiter=',',\n            lineterminator='\\r\\n',\n            quoting=csv.QUOTE_ALL\n        )\n\n        writer.writerow((\"market_id\",\"active_runners\",\"selection_id\",\"time\",\"traded_volume\",\"wap\",\"ltp\",\"selection_status\",'reduction_factor',\"atb_ladder\",\"atl_ladder\",\"sp_near\",\"sp_far\"))\n\n        for file_obj in load_markets(dir):\n\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            loop_prices(stream, writer)\n</code></pre> <pre><code>#loop over each TAR file\nfor tar in stream_files:\n    parse_prices([tar], prices_path) #This is where the timestamped prices are generated\n    print(\"__ Parsing Market and Selection Data ___ \")\n\n    # record prices to a file\n    with open(selection_meta, \"w\") as output:\n    # defining column headers\n        output.write(\"market_id,event_date,country,track,event_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,bsp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume\\n\")\n\n        for i, g in enumerate(bflw.Files([tar])):\n            print(\"Market {}\".format(i), end='\\r')\n\n            def get_pre_post_final():\n                eval_market = None\n                prev_market = None\n                preplay_market = None\n                postplay_market = None       \n\n                for market_books in g:\n                    for market_book in market_books:\n                        # if market doesn't meet filter return out\n                        if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n                            return (None, None, None)\n\n                        # final market view before market goes in play\n                        if prev_market is not None and prev_market.inplay != market_book.inplay:\n                            preplay_market = prev_market\n\n                        # final market view at the conclusion of the market\n                        if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n                            postplay_market = market_book\n\n                        # update reference to previous market\n                        prev_market = market_book\n\n                return (preplay_market, postplay_market, prev_market) # prev is now final\n\n            (preplay_market, postplay_market, final_market) = get_pre_post_final()\n\n            # no price data for market\n            if postplay_market is None:\n                continue; \n\n            preplay_traded = [ (r.last_price_traded, r.ex.traded_volume) for r in preplay_market.runners ] if preplay_market is not None else None\n\n            postplay_traded = [ (\n                r.last_price_traded,\n                r.ex.traded_volume,\n                # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (bsp - 1))        \n                min_gr0(\n                    next((pv.size for pv in r.sp.back_stake_taken if pv.size &amp;gt; 0), 0),\n                    next((pv.size for pv in r.sp.lay_liability_taken if pv.size &amp;gt; 0), 0)  / ((r.sp.actual_sp if (type(r.sp.actual_sp) is float) or (type(r.sp.actual_sp) is int) else 0) - 1)\n                ) if r.sp.actual_sp is not None else 0,\n            ) for r in postplay_market.runners ]\n\n            runner_data = [\n            {\n                'selection_id': r.selection_id,\n                'selection_name': next((rd.name for rd in final_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n                'selection_status': r.status,\n                'sp': as_str(r.sp.actual_sp),\n            }\n            for r in final_market.runners \n            ]\n\n            # runner price data for markets that go in play\n            if preplay_traded is not None:\n                def runner_vals(r):\n                    (pre_ltp, pre_traded), (post_ltp, post_traded, sp_traded) = r\n\n                    inplay_only = list(filter(lambda ps: ps.size &amp;gt; 0, [\n                        PriceSize(\n                            price=post_ps.price, \n                            size=post_ps.size - next((pre_ps.size for pre_ps in pre_traded if pre_ps.price == post_ps.price), 0)\n                        )\n                        for post_ps in post_traded \n                    ]))\n\n                    (ip_wavg, ip_matched, ip_min, ip_max) = parse_traded(inplay_only)\n                    (pre_wavg, pre_matched, pre_min, pre_max) = parse_traded(pre_traded)\n\n                    return {\n                        'preplay_ltp': as_str(pre_ltp),\n                        'preplay_min': as_str(pre_min),\n                        'preplay_max': as_str(pre_max),\n                        'preplay_wavg': as_str(pre_wavg),\n                        'preplay_matched': as_str(pre_matched or 0),\n                        'bsp_matched': as_str(sp_traded or 0),\n                        'inplay_ltp': as_str(post_ltp),\n                        'inplay_min': as_str(ip_min),\n                        'inplay_max': as_str(ip_max),\n                        'inplay_wavg': as_str(ip_wavg),\n                        'inplay_matched': as_str(ip_matched),\n                    }\n\n                runner_traded = [ runner_vals(r) for r in zip_longest(preplay_traded, postplay_traded, fillvalue=PriceSize(0, 0)) ]\n\n            # runner price data for markets that don't go in play\n            else:\n                def runner_vals(r):\n                    (ltp, traded, sp_traded) = r\n                    (wavg, matched, min_price, max_price) = parse_traded(traded)\n\n                    return {\n                        'preplay_ltp': as_str(ltp),\n                        'preplay_min': as_str(min_price),\n                        'preplay_max': as_str(max_price),\n                        'preplay_wavg': as_str(wavg),\n                        'preplay_matched': as_str(matched or 0),\n                        'bsp_matched': as_str(sp_traded or 0),\n                        'inplay_ltp': '',\n                        'inplay_min': '',\n                        'inplay_max': '',\n                        'inplay_wavg': '',\n                        'inplay_matched': '',\n                    }\n\n                runner_traded = [ runner_vals(r) for r in postplay_traded ]\n\n            # printing to csv for each runner\n            for (rdata, rprices) in zip(runner_data, runner_traded):\n                # defining data to go in each column\n                output.write(\n                    \"{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n                        postplay_market.market_id,\n                        postplay_market.market_definition.market_time,\n                        postplay_market.market_definition.country_code,\n                        postplay_market.market_definition.venue,\n                        postplay_market.market_definition.name,\n                        rdata['selection_id'],\n                        rdata['selection_name'],\n                        rdata['selection_status'],\n                        rdata['sp'],\n                        rprices['preplay_min'],\n                        rprices['preplay_max'],\n                        rprices['preplay_wavg'],\n                        rprices['preplay_ltp'],\n                        rprices['preplay_matched'],\n                        rprices['bsp_matched'],\n                        rprices['inplay_min'],\n                        rprices['inplay_max'],\n                        rprices['inplay_wavg'],\n                        rprices['inplay_ltp'],\n                        rprices['inplay_matched'],\n                    )\n                )\n\n\n    #loading selection file and parsing dates\n    selection = pd.read_csv(selection_meta, dtype={'market_id': object, 'selection_id': object}, parse_dates = ['event_date'])\n\n    #loading price file and parsing dates\n    prices = pd.read_csv(\n        prices_path, \n        quoting=csv.QUOTE_ALL,\n        dtype={'market_id': 'string', 'selection_id': 'string', 'atb_ladder': 'string', 'atl_ladder': 'string'},\n        parse_dates=['time']\n    )\n\n    #creating the ladder as a dictionary\n    prices['atb_ladder'] = [ast.literal_eval(x) for x in prices['atb_ladder']]\n    prices['atl_ladder'] = [ast.literal_eval(x) for x in prices['atl_ladder']]\n\n    #merging the price and selection files\n    df = selection.merge(prices, on = ['market_id', 'selection_id'])\n    #assigning best prices available and calculating time relative to market start time\n    df = (\n        df\n        .assign(back_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atb_ladder']])\n        .assign(lay_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atl_ladder']])\n        .assign(seconds_before_scheduled_off = lambda x: round((x['event_date'] - x['time']).dt.total_seconds()))\n        .query('seconds_before_scheduled_off &amp;lt; @log1_Start')\n    )\n\n    #creating a unique list of market ids\n    marketids = df['market_id'].unique().tolist()\n\n    #writing each market to its own csv file\n    for market in marketids:\n        #create a dataframe and a naming convention for this market\n        pricing_data=df[(df['market_id']==market)]\n        if pricing_data.empty:\n            continue\n        race_track=pricing_data['track'].iloc[0]\n        market_name=pricing_data['event_name'].iloc[0]\n        market_time=pricing_data['event_date'].iloc[0]\n        off=market_time.strftime('%Y-%m-%d')\n        #write race details to the dataframe\n        pricing_data['race']=pricing_data['event_name'].str.split('R').str[1]\n        pricing_data['race']=pricing_data['race'].str.split(' ').str[0]\n        pricing_data['distance']=pricing_data['event_name'].str.split(' ').str[1]\n        pricing_data['distance']=pricing_data['distance'].str.split('m').str[0]\n        pricing_data['race_type']=pricing_data['event_name'].str.split('m ').str[1]\n        pricing_data['selection_name']=pricing_data['selection_name'].str.split('\\. ').str[1]\n        #convert GMT timezone to AEST/AEDT\n        pricing_data['event_date']=pricing_data['event_date'].astype('datetime64[ns]')\n        pricing_data['event_date']=pricing_data['event_date'].dt.tz_localize('UTC',ambiguous=False)\n        pricing_data['event_date']=pricing_data['event_date'].dt.tz_convert('Australia/Melbourne')\n        pricing_data['event_date']=pricing_data['event_date'].dt.tz_localize(None)\n        pricing_data['time']=pricing_data['time'].astype('datetime64[ns]')\n        pricing_data['time']=pricing_data['time'].dt.tz_localize('UTC',ambiguous=False)\n        pricing_data['time']=pricing_data['time'].dt.tz_convert('Australia/Melbourne')\n        pricing_data['time']=pricing_data['time'].dt.tz_localize(None)\n        #covert GBP to AUD\n        event_date=(pd.to_datetime(pricing_data['event_date']).dt.date).iloc[0]\n        conversion_rate=CurrencyConverter(fallback_on_missing_rate=True).convert(1,'GBP','AUD',date=event_date)\n        pricing_data['traded_volume']=pricing_data['traded_volume']*conversion_rate\n        pricing_data.loc[(pricing_data['traded_volume'] &amp;lt; 0), 'traded_volume'] = 0\n        pricing_data['traded_volume'] = pricing_data['traded_volume'].round(decimals=2)\n        #reorder the dataframe and write to csv\n        pricing_data=pricing_data[['event_date','country','track','race','distance','race_type','market_id','selection_id','selection_name',\"selection_status\",'reduction_factor','result','bsp','time','traded_volume','wap','ltp','atb_ladder','atl_ladder','back_best','lay_best','seconds_before_scheduled_off','sp_near','sp_far','pp_min','pp_max','pp_wap','pp_ltp','pp_volume','bsp_volume','ip_min','ip_max','ip_wap','ip_ltp','ip_volume']]\n        pricing_data.to_csv(file_directory+off+' - '+race_track+' - '+market_name+'.csv',index=False)\n</code></pre> <pre><code>#removing intermediate working documents to clean up\nos.remove(selection_meta)\nos.remove(prices_path)\n</code></pre> <pre><code>import pandas as pd\nimport numpy as np\nimport os\nimport csv\nimport csv\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\nimport ast\nfrom unittest.mock import patch\nimport betfairlightweight\nfrom betfairlightweight import StreamListener\nfrom betfair_data import bflw\nimport pandas as pd\nfrom betfair_data import PriceSize\nimport functools\nfrom typing import List, Tuple\nfrom pandas.errors import SettingWithCopyWarning\nimport warnings\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\nfrom itertools import zip_longest\nfrom currency_converter import CurrencyConverter\n\nfile_directory= 'C:/Users/motykam/Documents/Thoroughbreds/' #INSERT FILE DIRECTORY WHERE TAR FILES ARE STORED\n\nlog1_Start = 60 * 10 # Seconds before scheduled off to start recording data for data segment one\nlog1_Step = 30       # Seconds between log steps for first data segment\nlog2_Start = 60 * 1  # Seconds before scheduled off to start recording data for data segment two\nlog2_Step = 1    # Seconds between log steps for second data segment\n\n# splitting race name and returning the parts \ndef split_anz_horse_market_name(market_name: str) -&amp;gt; Tuple[str, int, str]:\n    # return race no, length, race type\n    # input samples: \n    # 'R6 1400m Grp1' -&amp;gt; ('R6','1400m','grp1')\n    # 'R1 1609m Trot M' -&amp;gt; ('R1', '1609m', 'trot')\n    # 'R4 1660m Pace M' -&amp;gt; ('R4', '1660m', 'pace')\n    parts = market_name.split(' ')\n    race_no = parts[0] \n    race_len = parts[1].split('m')\n    race_len = race_len[0]\n    race_type = parts[2].lower() \n    return (race_no, race_len, race_type)\n\n# filtering markets to those that fit the following criteria\ndef filter_market(market: bflw.MarketBook) -&amp;gt; bool: \n    d = market.market_definition\n    return (d != None\n        and d.country_code == 'AU' \n        and d.market_type == 'WIN'\n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace' #strips out Harness Races\n        #and (c := split_anz_horse_market_name(d.name)[2]) == 'hcap'\n        and (c := split_anz_horse_market_name(d.name)[1]) &amp;gt;= '1200'\n        )\n\n# Simply add the below variable name to the market filter function above with the filter value\n# Equals (== 'Value in Quotation' or True/False/None), Does Not Equal (!= 'Value in Quotation' or True/False/None) - FOR ALL TYPES\n# Greater than (&amp;gt;), Greater than or equal to (&amp;gt;=), Less than (&amp;lt;), Less than or equal to (&amp;lt;=) - FOR INT/FLOAT\n# For list of value 'in'\n\n# and d.betting_type: str - ODDS, ASIAN_HANDICAP_SINGLES, ASIAN_HANDICAP_DOUBLES or LINE\n# and d.bsp_market: bool - True, False\n# and d.country_code: str - list of codes can be found here: https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2 - Default value is 'GB' - Australia = 'AU', New Zealand = 'NZ'\n# and d.event_id: str - PARENT_EVENT_ID\n# and d.event_name: Optional[str] - Usually the name of the Match-Up (e.g. Bangladesh v Sri Lanka) or Race Meeting Name (e.g. Wangaratta (AUS) 1st Dec) - Note: Dictionaries don't support wildcard searches\n# and d.event_type_id: str - SportID [Horse Racing - 7, Greyhounds - 4339]\n# and d.market_base_rate: float - Market Commission Rate\n# and d.market_type: str - e.g. \"WIN\"\n# and d.name: Optional[str] - market name (e.g. R1 1170m Mdn)\n# and d.number_of_active_runners: int - number of horses/dogs in the race\n# and d.number_of_winners: int - Win market 1, Place markets 2+\n# and d.turn_in_play_enabled: bool - True, False\n# and d.venue: Optional[str] - Racing Only - Track\n\ntrading = betfairlightweight.APIClient(username = \"username\", password = \"password\", app_key=\"app_key\")\nlistener = StreamListener(max_latency=None)\n\nstream_files = glob.glob(file_directory+\"*.tar\") \nselection_meta = file_directory+\"metadata.csv\"\nprices_path =  file_directory+\"preplay.csv\"\n\n# rounding to 2 decimal places or returning '' if blank\ndef as_str(v) -&amp;gt; str:\n    return '%.2f' % v if (type(v) is float) or (type(v) is int) else v if type(v) is str else ''\n\n# returning smaller of two numbers where min not 0\ndef min_gr0(a: float, b: float) -&amp;gt; float:\n    if a &amp;lt;= 0:\n        return b\n    if b &amp;lt;= 0:\n        return a\n\n    return min(a, b)\n\n# parsing price data and pulling out weighted avg price, matched, min price and max price\ndef parse_traded(traded: List[PriceSize]) -&amp;gt; Tuple[float, float, float, float]:\n    if len(traded) == 0: \n        return (None, None, None, None)\n\n    (wavg_sum, matched, min_price, max_price) = functools.reduce(\n        lambda total, ps: (\n            total[0] + (ps.price * ps.size), # wavg_sum before we divide by total matched\n            total[1] + ps.size, # total matched\n            min(total[2], ps.price), # min price matched\n            max(total[3], ps.price), # max price matched\n        ),\n        traded,\n        (0, 0, 1001, 0) # starting default values\n    )\n\n    wavg_sum = (wavg_sum / matched) if matched &amp;gt; 0 else None # dividing sum of wavg by total matched\n    matched = matched if matched &amp;gt; 0 else None \n    min_price = min_price if min_price != 1001 else None\n    max_price = max_price if max_price != 0 else None\n\n    return (wavg_sum, matched, min_price, max_price)\n\n\ndef load_markets(file_paths):\n    for file_path in file_paths:\n        print(file_path)\n        print(\"__ Parsing Detailed Prices ___ \")\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n\n    return None\n\ndef slicePrice(l, n):\n    try:\n        x = l[n].price\n    except:\n        x = \"\"\n    return(x)\n\ndef sliceSize(l, n):\n    try:\n        x = l[n].size\n    except:\n        x = \"\"\n    return(x)\n\ndef pull_ladder(availableLadder, n = 5):\n        out = {}\n        price = []\n        volume = []\n        if len(availableLadder) == 0:\n            return(out)        \n        else:\n            for rung in availableLadder[0:n]:\n                price.append(rung.price)\n                volume.append(rung.size)\n\n            out[\"p\"] = price\n            out[\"v\"] = volume\n            return(out)\n\ndef final_market_book(s):\n\n    with patch(\"builtins.open\", lambda f, _: f):\n\n        gen = s.get_generator()\n\n        for market_books in gen():\n\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    return(None)\n\n            for market_book in market_books:\n\n                last_market_book = market_book\n\n        return(last_market_book)\n\ndef loop_prices(s, o):\n\n    with patch(\"builtins.open\", lambda f, _: f):\n\n        gen = s.get_generator()\n\n        marketID = None\n        tradeVols = None\n        time = None\n\n        for market_books in gen():\n\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    break\n\n            for market_book in market_books:\n\n                # Time Step Management ++++++++++++++++++++++++++++++++++\n\n                if marketID is None:\n\n                    # No market initialised\n                    marketID = market_book.market_id\n                    time =  market_book.publish_time\n\n                elif market_book.inplay:\n\n                    # Stop once market inplay\n                    break\n\n                else:\n\n                    seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n\n                    if seconds_to_start &amp;gt; log1_Start:\n\n                        # Too early before off to start logging prices\n                        continue\n\n                    else:\n\n                        # Update data at different time steps depending on seconds to off\n                        wait = np.where(seconds_to_start &amp;lt;= log2_Start, log2_Step, log1_Step)\n\n                        # New Market\n                        if market_book.market_id != marketID:\n                            marketID = market_book.market_id\n                            time =  market_book.publish_time\n                        # (wait) seconds elapsed since last write\n                        elif (market_book.publish_time - time).total_seconds() &amp;gt; wait:\n                            time = market_book.publish_time\n                        # fewer than (wait) seconds elapsed continue to next loop\n                        else:\n                            continue\n\n                # Execute Data Logging ++++++++++++++++++++++++++++++++++\n\n                for runner in market_book.runners:\n\n                    try:\n                        selection_status = runner.status\n                        reduction_factor = runner.adjustment_factor\n                        atb_ladder = pull_ladder(runner.ex.available_to_back, n = 5)\n                        atl_ladder = pull_ladder(runner.ex.available_to_lay, n = 5)\n                        spn = runner.sp.near_price\n                        spf = runner.sp.far_price\n                    except:\n                        selection_status = None\n                        reduction_factor = None\n                        atb_ladder = {}\n                        atl_ladder = {}\n                        spn = None\n                        spf = None\n\n                    # Calculate Current Traded Volume + Traded WAP\n                    limitTradedVol = sum([rung.size for rung in runner.ex.traded_volume])\n                    if limitTradedVol == 0:\n                        limitWAP = \"\"\n                    else:\n                        limitWAP = sum([rung.size * rung.price for rung in runner.ex.traded_volume]) / limitTradedVol\n                        limitWAP = round(limitWAP, 2)\n\n                    o.writerow(\n                        (\n                            market_book.market_id,\n                            market_book.number_of_active_runners,\n                            runner.selection_id,\n                            market_book.publish_time,\n                            limitTradedVol,\n                            limitWAP,\n                            runner.last_price_traded or \"\",\n                            selection_status,\n                            reduction_factor,\n                            str(atb_ladder).replace(' ',''), \n                            str(atl_ladder).replace(' ',''),\n                            str(spn),\n                            str(spf)\n                        )\n                    )\n\n\ndef parse_prices(dir, out_file):\n\n    with open(out_file, \"w+\") as output:\n\n        writer = csv.writer(\n            output, \n            delimiter=',',\n            lineterminator='\\r\\n',\n            quoting=csv.QUOTE_ALL\n        )\n\n        writer.writerow((\"market_id\",\"active_runners\",\"selection_id\",\"time\",\"traded_volume\",\"wap\",\"ltp\",\"selection_status\",'reduction_factor',\"atb_ladder\",\"atl_ladder\",\"sp_near\",\"sp_far\"))\n\n        for file_obj in load_markets(dir):\n\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            loop_prices(stream, writer)\n\n\n\n#loop over each TAR file\nfor tar in stream_files:\n    parse_prices([tar], prices_path)\n    print(\"__ Parsing Market and Selection Data ___ \")\n\n    # record prices to a file\n    with open(selection_meta, \"w\") as output:\n    # defining column headers\n        output.write(\"market_id,event_date,country,track,event_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,bsp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume\\n\")\n\n        for i, g in enumerate(bflw.Files([tar])):\n            print(\"Market {}\".format(i), end='\\r')\n\n            def get_pre_post_final():\n                eval_market = None\n                prev_market = None\n                preplay_market = None\n                postplay_market = None       \n\n                for market_books in g:\n                    for market_book in market_books:\n                        # if market doesn't meet filter return out\n                        if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n                            return (None, None, None)\n\n                        # final market view before market goes in play\n                        if prev_market is not None and prev_market.inplay != market_book.inplay:\n                            preplay_market = prev_market\n\n                        # final market view at the conclusion of the market\n                        if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n                            postplay_market = market_book\n\n                        # update reference to previous market\n                        prev_market = market_book\n\n                return (preplay_market, postplay_market, prev_market) # prev is now final\n\n            (preplay_market, postplay_market, final_market) = get_pre_post_final()\n\n            # no price data for market\n            if postplay_market is None:\n                continue; \n\n            preplay_traded = [ (r.last_price_traded, r.ex.traded_volume) for r in preplay_market.runners ] if preplay_market is not None else None\n\n            postplay_traded = [ (\n                r.last_price_traded,\n                r.ex.traded_volume,\n                # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (bsp - 1))        \n                min_gr0(\n                    next((pv.size for pv in r.sp.back_stake_taken if pv.size &amp;gt; 0), 0),\n                    next((pv.size for pv in r.sp.lay_liability_taken if pv.size &amp;gt; 0), 0)  / ((r.sp.actual_sp if (type(r.sp.actual_sp) is float) or (type(r.sp.actual_sp) is int) else 0) - 1)\n                ) if r.sp.actual_sp is not None else 0,\n            ) for r in postplay_market.runners ]\n\n            runner_data = [\n            {\n                'selection_id': r.selection_id,\n                'selection_name': next((rd.name for rd in final_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n                'selection_status': r.status,\n                'sp': as_str(r.sp.actual_sp),\n            }\n            for r in final_market.runners \n            ]\n\n            # runner price data for markets that go in play\n            if preplay_traded is not None:\n                def runner_vals(r):\n                    (pre_ltp, pre_traded), (post_ltp, post_traded, sp_traded) = r\n\n                    inplay_only = list(filter(lambda ps: ps.size &amp;gt; 0, [\n                        PriceSize(\n                            price=post_ps.price, \n                            size=post_ps.size - next((pre_ps.size for pre_ps in pre_traded if pre_ps.price == post_ps.price), 0)\n                        )\n                        for post_ps in post_traded \n                    ]))\n\n                    (ip_wavg, ip_matched, ip_min, ip_max) = parse_traded(inplay_only)\n                    (pre_wavg, pre_matched, pre_min, pre_max) = parse_traded(pre_traded)\n\n                    return {\n                        'preplay_ltp': as_str(pre_ltp),\n                        'preplay_min': as_str(pre_min),\n                        'preplay_max': as_str(pre_max),\n                        'preplay_wavg': as_str(pre_wavg),\n                        'preplay_matched': as_str(pre_matched or 0),\n                        'bsp_matched': as_str(sp_traded or 0),\n                        'inplay_ltp': as_str(post_ltp),\n                        'inplay_min': as_str(ip_min),\n                        'inplay_max': as_str(ip_max),\n                        'inplay_wavg': as_str(ip_wavg),\n                        'inplay_matched': as_str(ip_matched),\n                    }\n\n                runner_traded = [ runner_vals(r) for r in zip_longest(preplay_traded, postplay_traded, fillvalue=PriceSize(0, 0)) ]\n\n            # runner price data for markets that don't go in play\n            else:\n                def runner_vals(r):\n                    (ltp, traded, sp_traded) = r\n                    (wavg, matched, min_price, max_price) = parse_traded(traded)\n\n                    return {\n                        'preplay_ltp': as_str(ltp),\n                        'preplay_min': as_str(min_price),\n                        'preplay_max': as_str(max_price),\n                        'preplay_wavg': as_str(wavg),\n                        'preplay_matched': as_str(matched or 0),\n                        'bsp_matched': as_str(sp_traded or 0),\n                        'inplay_ltp': '',\n                        'inplay_min': '',\n                        'inplay_max': '',\n                        'inplay_wavg': '',\n                        'inplay_matched': '',\n                    }\n\n                runner_traded = [ runner_vals(r) for r in postplay_traded ]\n\n            # printing to csv for each runner\n            for (rdata, rprices) in zip(runner_data, runner_traded):\n                # defining data to go in each column\n                output.write(\n                    \"{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n                        postplay_market.market_id,\n                        postplay_market.market_definition.market_time,\n                        postplay_market.market_definition.country_code,\n                        postplay_market.market_definition.venue,\n                        postplay_market.market_definition.name,\n                        rdata['selection_id'],\n                        rdata['selection_name'],\n                        rdata['selection_status'],\n                        rdata['sp'],\n                        rprices['preplay_min'],\n                        rprices['preplay_max'],\n                        rprices['preplay_wavg'],\n                        rprices['preplay_ltp'],\n                        rprices['preplay_matched'],\n                        rprices['bsp_matched'],\n                        rprices['inplay_min'],\n                        rprices['inplay_max'],\n                        rprices['inplay_wavg'],\n                        rprices['inplay_ltp'],\n                        rprices['inplay_matched'],\n                    )\n                )\n\n\n    #loading selection file and parsing dates\n    selection = pd.read_csv(selection_meta, dtype={'market_id': object, 'selection_id': object}, parse_dates = ['event_date'])\n\n    #loading price file and parsing dates\n    prices = pd.read_csv(\n        prices_path, \n        quoting=csv.QUOTE_ALL,\n        dtype={'market_id': 'string', 'selection_id': 'string', 'atb_ladder': 'string', 'atl_ladder': 'string'},\n        parse_dates=['time']\n    )\n\n    #creating the ladder as a dictionary\n    prices['atb_ladder'] = [ast.literal_eval(x) for x in prices['atb_ladder']]\n    prices['atl_ladder'] = [ast.literal_eval(x) for x in prices['atl_ladder']]\n\n    #merging the price and selection files\n    df = selection.merge(prices, on = ['market_id', 'selection_id'])\n    #assigning best prices available and calculating time relative to market start time\n    df = (\n        df\n        .assign(back_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atb_ladder']])\n        .assign(lay_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atl_ladder']])\n        .assign(seconds_before_scheduled_off = lambda x: round((x['event_date'] - x['time']).dt.total_seconds()))\n        .query('seconds_before_scheduled_off &amp;lt; @log1_Start')\n    )\n\n    #creating a unique list of market ids\n    marketids = df['market_id'].unique().tolist()\n\n    #writing each market to its own csv file\n    for market in marketids:\n        #create a dataframe and a naming convention for this market\n        pricing_data=df[(df['market_id']==market)]\n        if pricing_data.empty:\n            continue\n        race_track=pricing_data['track'].iloc[0]\n        market_name=pricing_data['event_name'].iloc[0]\n        market_time=pricing_data['event_date'].iloc[0]\n        off=market_time.strftime('%Y-%m-%d')\n        #write race details to the dataframe\n        pricing_data['race']=pricing_data['event_name'].str.split('R').str[1]\n        pricing_data['race']=pricing_data['race'].str.split(' ').str[0]\n        pricing_data['distance']=pricing_data['event_name'].str.split(' ').str[1]\n        pricing_data['distance']=pricing_data['distance'].str.split('m').str[0]\n        pricing_data['race_type']=pricing_data['event_name'].str.split('m ').str[1]\n        pricing_data['selection_name']=pricing_data['selection_name'].str.split('\\. ').str[1]\n        #convert GMT timezone to AEST/AEDT\n        pricing_data['event_date']=pricing_data['event_date'].astype('datetime64[ns]')\n        pricing_data['event_date']=pricing_data['event_date'].dt.tz_localize('UTC',ambiguous=False)\n        pricing_data['event_date']=pricing_data['event_date'].dt.tz_convert('Australia/Melbourne')\n        pricing_data['event_date']=pricing_data['event_date'].dt.tz_localize(None)\n        pricing_data['time']=pricing_data['time'].astype('datetime64[ns]')\n        pricing_data['time']=pricing_data['time'].dt.tz_localize('UTC',ambiguous=False)\n        pricing_data['time']=pricing_data['time'].dt.tz_convert('Australia/Melbourne')\n        pricing_data['time']=pricing_data['time'].dt.tz_localize(None)\n        #covert GBP to AUD\n        event_date=(pd.to_datetime(pricing_data['event_date']).dt.date).iloc[0]\n        conversion_rate=CurrencyConverter(fallback_on_missing_rate=True).convert(1,'GBP','AUD',date=event_date)\n        pricing_data['traded_volume']=pricing_data['traded_volume']*conversion_rate\n        pricing_data.loc[(pricing_data['traded_volume'] &amp;lt; 0), 'traded_volume'] = 0\n        pricing_data['traded_volume'] = pricing_data['traded_volume'].round(decimals=2)\n        #reorder the dataframe and write to csv\n        pricing_data=pricing_data[['event_date','country','track','race','distance','race_type','market_id','selection_id','selection_name',\"selection_status\",'reduction_factor','result','bsp','time','traded_volume','wap','ltp','atb_ladder','atl_ladder','back_best','lay_best','seconds_before_scheduled_off','sp_near','sp_far','pp_min','pp_max','pp_wap','pp_ltp','pp_volume','bsp_volume','ip_min','ip_max','ip_wap','ip_ltp','ip_volume']]\n        pricing_data.to_csv(file_directory+off+' - '+race_track+' - '+market_name+'.csv',index=False)\n\n\n#removing intermediate working documents to clean up\nos.remove(selection_meta)\nos.remove(prices_path)\n</code></pre> <pre><code>import pandas as pd\nimport numpy as np\nimport os\nimport csv\nimport csv\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\nimport ast\nfrom unittest.mock import patch\nimport betfairlightweight\nfrom betfairlightweight import StreamListener\nfrom betfair_data import bflw #\"Import \"betfair_data.bflw\" could not be resolved from source\" - This is a known issue, the script should still run\nimport pandas as pd\nfrom currency_converter import CurrencyConverter\nfrom pandas.errors import SettingWithCopyWarning\nimport warnings\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n\nfile_directory = ''# INSERT FILE DIRECTORY WHERE TAR FILES ARE STORED\n\nlog1_Start = 60 * 60  # seconds before scheduled off to start recording data for data segment one\nlog1_Step = 60  # seconds between log steps for first data segment\nlog2_Start = 60 * 10  # seconds before scheduled off to start recording data for data segment two\nlog2_Step = 10  # seconds between log steps for second data segment\n\n\ndef filter_market(market: bflw.MarketBook) -&amp;gt; bool:\n    d = market.market_definition\n    return (\n        d is not None\n        # and d.country_code in ['ES']\n        # and d.market_type == 'MATCH_ODDS'\n        and d.name in ['Match Odds', '1st Innings 20 Overs Line']\n        # and d.betting_type == 'ODDS'\n    )\n\n# Simply add the below variable name to the market filter function above with the filter value\n# Equals (== 'Value in Quotation' or True/False/None), Does Not Equal (!= 'Value in Quotation' or True/False/None) - FOR ALL TYPES\n# Greater than (&amp;gt;), Greater than or equal to (&amp;gt;=), Less than (&amp;lt;), Less than or equal to (&amp;lt;=) - FOR INT/FLOAT\n# For list of value 'in'\n\n# and d.betting_type: str - ODDS, ASIAN_HANDICAP_SINGLES, ASIAN_HANDICAP_DOUBLES or LINE\n# and d.bsp_market: bool - True, False\n# and d.country_code: str - list of codes can be found here: https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2 - Default value is 'GB' - Australia = 'AU', New Zealand = 'NZ'\n# and d.event_id: str - PARENT_EVENT_ID\n# and d.event_name: Optional[str] - Usually the name of the Match-Up (e.g. Bangladesh v Sri Lanka) or Race Name (e.g. R6 1400m Grp1) - Note: Dictionaries don't support wildcard searches\n# and d.event_type_id: str - SportID [Soccer - 1, Tennis - 2, Golf - 3, Cricket - 4, AFL - 61420]\n# and d.market_base_rate: float - Market Commission Rate\n# and d.market_type: str - e.g. \"MATCH_ODDS\", \"1ST_INNINGS_RUNS\",\"TO_QUALIFY\" - always all caps with \"_\" replacing spaces\n# and d.name: Optional[str] - market name (e.g. Sri Lanka 1st Inns Runs)\n# and d.number_of_active_runners: int - Head-To-Heads markets will be 2\n# and d.number_of_winners: int - Odds Markets usually 1, Line/Handicap markets usually 0\n# and d.regulators: str - 'MR_INT' to remove ring-fenced exchange markets \n# and d.turn_in_play_enabled: bool - True, False\n\n\ntrading = betfairlightweight.APIClient(username = \"username\", password = \"password\", app_key=\"app_key\")\nlistener = StreamListener(max_latency=None)\nstream_files = glob.glob(file_directory+\"*.tar\") \nselection_meta = file_directory+\"metadata.csv\"\nprices_path =  file_directory+\"preplay.csv\"\n\n# rounding to 2 decimal places or returning '' if blank\ndef as_str(v) -&amp;gt; str:\n    return '%.2f' % v if (type(v) is float) or (type(v) is int) else v if type(v) is str else ''\n\n# returning smaller of two numbers where min not 0\ndef min_gr0(a: float, b: float) -&amp;gt; float:\n    if a &amp;lt;= 0:\n        return b\n    if b &amp;lt;= 0:\n        return a\n\n    return min(a, b)\n\ndef load_markets(file_paths):\n    for file_path in file_paths:\n        print(file_path)\n        print(\"__ Parsing Detailed Prices ___ \")\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n\n    return None\n\ndef slicePrice(l, n):\n    try:\n        x = l[n].price\n    except:\n        x = \"\"\n    return(x)\n\ndef sliceSize(l, n):\n    try:\n        x = l[n].size\n    except:\n        x = \"\"\n    return(x)\n\ndef pull_ladder(availableLadder, n = 3):\n        out = {}\n        price = []\n        volume = []\n        if len(availableLadder) == 0:\n            return(out)        \n        else:\n            for rung in availableLadder[0:n]:\n                price.append(rung.price)\n                volume.append(rung.size)\n\n            out[\"p\"] = price\n            out[\"v\"] = volume\n            return(out)\n\ndef final_market_book(s):\n\n    with patch(\"builtins.open\", lambda f, _: f):\n\n        gen = s.get_generator()\n\n        for market_books in gen():\n\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    return(None)\n\n            for market_book in market_books:\n\n                last_market_book = market_book\n\n        return(last_market_book)\n\ndef loop_prices(s, o):\n\n    with patch(\"builtins.open\", lambda f, _: f):\n\n        gen = s.get_generator()\n\n        marketID = None\n        tradeVols = None\n        time = None\n\n        for market_books in gen():\n\n            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n\n            if ((evaluate_market := filter_market(market_books[0])) == False):\n                    break\n\n            for market_book in market_books:\n\n                # Time Step Management ++++++++++++++++++++++++++++++++++\n\n                if marketID is None:\n\n                    # No market initialised\n                    marketID = market_book.market_id\n                    time =  market_book.publish_time\n\n                elif market_book.status == \"CLOSED\":\n\n                    # Stop once market settled\n                    break\n\n                else:\n\n                    seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n\n                    if seconds_to_start &amp;gt; log1_Start:\n\n                        # Too early before off to start logging prices\n                        continue\n\n                    else:\n\n                        # Update data at different time steps depending on seconds to off\n                        wait = np.where(seconds_to_start &amp;lt;= log2_Start, log2_Step, log1_Step)\n\n                        # New Market\n                        if market_book.market_id != marketID:\n                            marketID = market_book.market_id\n                            time =  market_book.publish_time\n                        # (wait) seconds elapsed since last write\n                        elif (market_book.publish_time - time).total_seconds() &amp;gt; wait:\n                            time = market_book.publish_time\n                        # fewer than (wait) seconds elapsed continue to next loop\n                        else:\n                            continue\n\n                # Execute Data Logging ++++++++++++++++++++++++++++++++++\n\n                for runner in market_book.runners:\n\n                    try:\n                        atb_ladder = pull_ladder(runner.ex.available_to_back, n = 3)\n                        atl_ladder = pull_ladder(runner.ex.available_to_lay, n = 3)\n                    except:\n                        atb_ladder = {}\n                        atl_ladder = {}\n\n                    # Calculate Current Traded Volume + Traded WAP\n                    limitTradedVol = sum([rung.size for rung in runner.ex.traded_volume])\n                    if limitTradedVol == 0:\n                        limitWAP = \"\"\n                    else:\n                        limitWAP = sum([rung.size * rung.price for rung in runner.ex.traded_volume]) / limitTradedVol\n                        limitWAP = round(limitWAP, 2)\n\n                    #Use this section to write rows that are required to join the metadata OR that will change in time\n                    o.writerow(\n                        (\n                            market_book.market_id,\n                            runner.selection_id,\n                            market_book.publish_time,\n                            market_book.inplay,\n                            limitTradedVol,\n                            limitWAP,\n                            runner.last_price_traded or \"\",\n                            str(atb_ladder).replace(' ',''), \n                            str(atl_ladder).replace(' ','')\n                        )\n                    )\n\n\ndef parse_prices(dir, out_file):\n\n    with open(out_file, \"w+\") as output:\n\n        writer = csv.writer(\n            output, \n            delimiter=',',\n            lineterminator='\\r\\n',\n            quoting=csv.QUOTE_ALL\n        )\n\n        writer.writerow((\"market_id\",\"selection_id\",\"time\",\"inplay\",\"traded_volume\",\"wap\",\"ltp\",\"atb_ladder\",\"atl_ladder\"))\n\n        for file_obj in load_markets(dir):\n\n            stream = trading.streaming.create_historical_generator_stream(\n                file_path=file_obj,\n                listener=listener,\n            )\n\n            loop_prices(stream, writer)\n\n#loop over each TAR file\nfor tar in stream_files:\n    parse_prices([tar], prices_path)\n    print(\"__ Parsing Market and Selection Data ___ \")\n\n    # record prices to a file\n    with open(selection_meta, \"w\") as output:\n        # defining column headers\n        output.write(\"market_id,market_time,market_type,event_name,market_name,selection_id,x,selection_name,y,result\\n\")\n        #loop over each market in the TAR file\n        for i, g in enumerate(bflw.Files([tar])):\n            print(\"Market {}\".format(i), end='\\r')\n\n            def get_pre_post_final():\n                eval_market = None\n                prev_market = None\n                preplay_market = None\n                postplay_market = None       \n\n                for market_books in g:\n                    for market_book in market_books:\n                        # if market doesn't meet filter return out\n                        if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n                            return (None, None, None)\n\n                        # final market view before market goes in play\n                        if prev_market is not None and prev_market.inplay != market_book.inplay:\n                            preplay_market = prev_market\n\n                        # final market view at the conclusion of the market\n                        if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n                            postplay_market = market_book\n\n                        # update reference to previous market\n                        prev_market = market_book\n\n                return (preplay_market, postplay_market, prev_market) # prev is now final\n\n            (preplay_market, postplay_market, final_market) = get_pre_post_final()\n\n            # no price data for market\n            if postplay_market is None:\n                continue; \n\n            preplay_traded = [ (r.last_price_traded, r.ex.traded_volume) for r in preplay_market.runners ] if preplay_market is not None else None\n\n            postplay_traded = [ (\n                r.last_price_traded,\n                r.ex.traded_volume,\n                # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1))        \n                min_gr0(\n                    next((pv.size for pv in r.sp.back_stake_taken if pv.size &amp;gt; 0), 0),\n                    next((pv.size for pv in r.sp.lay_liability_taken if pv.size &amp;gt; 0), 0)  / ((r.sp.actual_sp if (type(r.sp.actual_sp) is float) or (type(r.sp.actual_sp) is int) else 0) - 1)\n                ) if r.sp.actual_sp is not None else 0,\n            ) for r in postplay_market.runners ]\n\n            # generic selection data\n            for r in final_market.runners:\n                selection_id=r.selection_id,\n                selection_name=next((rd.name for rd in final_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n                selection_status=r.status\n\n            # printing to csv for each selection\n                output.write(\n                    \"{},{},{},{},{},{},{},{}\\n\".format(\n                        postplay_market.market_id,\n                        postplay_market.market_definition.market_time,\n                        postplay_market.market_definition.market_type,\n                        postplay_market.market_definition.event_name,\n                        postplay_market.market_definition.name,\n                        selection_id,\n                        selection_name.encode('utf-8'),\n                        selection_status\n                    )\n                )\n\n    #loading selection file, parsing dates and cleaning the table\n    # loading selection file, parsing dates and cleaning the table\n    selection = pd.read_csv(\n        selection_meta, dtype={'market_id': object, 'selection_id': object}, parse_dates=['market_time']\n    )\n    selection.set_axis(\n        [\n            'market_id',\n            'market_time',\n            'market_type',\n            'event_name',\n            'market_name',\n            'selection_id',\n            'x',\n            'selection_name',\n            'y',\n            'result'\n        ],\n        axis=1\n    )\n\n    selection = selection[['market_id','market_time','market_type','event_name','market_name','selection_id','selection_name','result']]\n    selection['selection_id'] = selection['selection_id'].str.split('\\(').str[1]\n    selection['selection_name'] = selection['selection_name'].str.split(\"\\('\").str[1]\n    selection['selection_name'] = selection['selection_name'].str.split(\"'\").str[0]\n\n    # loading price file and parsing dates\n    prices = pd.read_csv(\n        prices_path,\n        quoting=csv.QUOTE_ALL,\n        dtype={'market_id': 'string', 'selection_id': 'string', 'atb_ladder': 'string', 'atl_ladder': 'string'},\n        parse_dates=['time']\n    )\n\n    # creating the ladder as a dictionary\n    prices['atb_ladder'] = [ast.literal_eval(x) for x in prices['atb_ladder']]\n    prices['atl_ladder'] = [ast.literal_eval(x) for x in prices['atl_ladder']]\n\n    # merging the price and selection files\n    df = selection.merge(prices, on=['market_id', 'selection_id'])\n\n    # assigning best prices available and calculating time relative to market start time\n    df = (\n        df\n        .assign(back_best=lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atb_ladder']])\n        .assign(lay_best=lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atl_ladder']])\n        .assign(\n            seconds_before_scheduled_off=lambda x: round((x['market_time'] - x['time']).dt.total_seconds())\n        )\n        .query('seconds_before_scheduled_off &amp;lt; @log1_start')\n    )\n\n    # Writing each processed market to its own csv file\n    market_ids = df['market_id'].unique().tolist()\n    for market in market_ids:\n        # Create a dataframe and a naming convention for this market\n        pricing_data = df[df['market_id'] == market]\n        fixture = pricing_data['event_name'].iloc[0].replace(r\"/\",\"\")\n        market_name = pricing_data['market_name'].iloc[0]\n        market_time = pricing_data['market_time'].iloc[0]\n        off = market_time.strftime('%Y-%m-%d')\n        # Convert GBP to AUD\n        event_date = (pd.to_datetime(pricing_data['market_time']).dt.date).iloc[0]\n        conversion_rate = CurrencyConverter(fallback_on_missing_rate=True).convert(1, 'GBP', 'AUD', date=event_date)\n        pricing_data['traded_volume'] = pricing_data['traded_volume'] * conversion_rate\n        pricing_data.loc[pricing_data['traded_volume'] &amp;lt; 0, 'traded_volume'] = 0\n        pricing_data['traded_volume'] = pricing_data['traded_volume'].round(decimals=2)\n        pricing_data.to_csv(file_directory + off + ' - ' + fixture + ' - ' + market_name + '.csv', index=False)\n\n#removing intermediate working documents to clean up\nos.remove(selection_meta)\nos.remove(prices_path)\n</code></pre>"},{"location":"tutorials/processingTarFiles101/#take-away-the-pain-processing-tar-files-101","title":"Take away the pain! Processing TAR Files 101","text":""},{"location":"tutorials/processingTarFiles101/#cheat-sheet","title":"Cheat sheet","text":"<ul> <li>This is presented as a Jupyter notebook as this format is interactive and lets you run snippets of code from within the notebook. To use this functionality you'll need to download a copy of the <code>ipynb</code> file locally and open it in a text editor (i.e. VS code).</li> <li>If you're looking for the complete code head to the bottom of the page or download the script from Github.</li> <li>To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code), make sure you've navigated in the terminal to the folder you've saved the script in and then type <code>py main.py</code> (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. </li> <li>Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site. We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. For more information on these files please reach out to us at automation</li> <li>We're using the betfairlightweight and betfair_data package to do the heavy lifting</li> <li>We've also posted the completed code logic on the <code>betfair-downunder</code> Github repo.</li> </ul>"},{"location":"tutorials/processingTarFiles101/#10-setup","title":"1.0 Setup","text":""},{"location":"tutorials/processingTarFiles101/#11-importing-libraries","title":"1.1 Importing libraries","text":"<p>Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language.</p> <p>You'll need <code>betfairlightweight</code> and <code>betfair_data</code> which you can install with something like <code>pip install betfairlightweight</code>.</p> <p>NOTE: Import 'betfair_data.bflw' could not be resolved from source - This is a known issue, the script should still run</p>"},{"location":"tutorials/processingTarFiles101/#12-picking-our-intervals","title":"1.2 Picking our intervals","text":"<p>Here we'll define our working folder and the timestamps for which we want to pull all of our data.  We've set the script to pull prices from 10 minutes out from the jump at 30 second intervals and then at 5 second intervals from 1 minute out. Feel free to change as you need.</p> <p>PRO TIP: copying your file path from windows explorer works really well as long as you replace the back-slashes with forward-slashes</p>"},{"location":"tutorials/processingTarFiles101/#13-filtering-the-markets","title":"1.3 Filtering the markets","text":"<p>We'll then define a market filter as well as a way to filter out the harness racing markets. The different filter variables are listed below and can be added or removed as required. Some of these aren't that useful but have been listed for the sake of completeness.</p>"},{"location":"tutorials/processingTarFiles101/#14-utility-functions","title":"1.4 Utility Functions","text":"<p>Here we will define our trading and listener functions which will be able to handle the json file format as well as defining some more working directories. We've also defined a few functions to generate the required output.</p> <p>NOTE: Input credentials are not required here</p>"},{"location":"tutorials/processingTarFiles101/#15-parsing-timestamped-prices","title":"1.5 Parsing Timestamped Prices","text":"<p>Next up are the functions that enable us to read the timestamped prices from the json files and output them ready to be joined on the metadata.  This will provide:</p> <ul> <li>Market ID</li> <li>Number of Active Runners in the market (this can help in the event of late scratchings)</li> <li>Timestamp (will help us later to calculate time relative to scheduled off time)</li> <li>Traded Volume on the selection so far</li> <li>Weighted Average Price</li> <li>Last Traded Price</li> <li>Selection Status (this can help in the event of late scratchings)</li> <li>Reduction Factor (effect on the market if the runner were to be scratched)</li> <li>Back Ladder (pulls 5 best prices and volume available)</li> <li>Lay Ladder (pulls 5 best prices and volume available)</li> <li>BSP Near Price</li> <li>BSP Far Price</li> </ul> <p>NOTE: Projected BSP displayed on the app/website is the Near Price - more information about Near/Far Prices and their accuracy can be found here</p> <p>NOTE: The function is defined to stop pulling the prices once the market goes in-play. If in-play prices are required, then simply remove the condition for market_book.inplay. However, you will need to insert an inplay flag in order to distinguish when the market goes inplay in the csv file.</p>"},{"location":"tutorials/processingTarFiles101/#16-the-good-stuff","title":"1.6 The Good Stuff","text":"<p>Now comes the really meaty chunk of code that will do the grunt work for us. We'll generate the timestamped prices for each TAR file separately and then loop over them again to generate the selection metadata. (This is using a chunk of code from our JSON to CSV Revisited tutorial using the Betfair_Data package. This is super speedy because its mainly written in Rust rather than python) In our metadata files we will provide:</p> <ul> <li>Market ID</li> <li>Market Time</li> <li>Country Code</li> <li>Track/Venue</li> <li>Market Name</li> <li>Selection ID</li> <li>Horse Name</li> <li>Result</li> <li>BSP</li> <li>Preplay Minimum Matched Price</li> <li>Preplay Maximum Matched Price</li> <li>Preplay Weighted Average Matched Price</li> <li>Preplay Last Traded Price</li> <li>Preplay Matched Volume (not including SP volume)</li> <li>BSP Matched Volume (Back Stake)</li> <li>Inplay Minimum Matched Price</li> <li>Inplay Maximum Matched Price</li> <li>Inplay Weighted Average Matched Price</li> <li>Inplay Last Traded Price</li> <li>Inplay Matched Volume</li> </ul> <p>Note: For markets that don't go in play (like Greyhounds/Place markets), the 'Inplay' fields will be empty.</p> <p>Following this metadata generation, we'll then join the dataframes together and add the \"seconds to scheduled off\" field, as well as converting the GMT time in the TAR Files to Melbourne time (this is useful for when markets are timed for early morning before 10am (or 11am during daylight savings) which causes the local event date to be the previous day in GMT) and converting the currency to Australian Dollars using a historical currency conversion.</p> <p>The final piece will be outputting the files to CSV. The code is setup to output a separate file for each race which reduces the number of rows per race and enables output to be easily opened and checked for completeness. Changing this to output one CSV file per TAR file or one CSV file per day is fairly straightforward.</p>"},{"location":"tutorials/processingTarFiles101/#17-clean-up","title":"1.7 Clean-Up","text":"<p>The final piece is just to clean up and remove the intermediate files. If you're encountering errors with the final output, comment this code out so these intermediate files can be viewed.</p>"},{"location":"tutorials/processingTarFiles101/#18-conclusion-and-next-steps","title":"1.8 Conclusion and Next Steps","text":"<p>If you'd like to expand your datasets, here's a list of other accessible data sources that can add additional datapoints and are fairly simple to join:</p> <ul> <li>Unsupported API endpoint for a day's racecard - <code>https://apigateway.betfair.com.au/hub/racecard?date=YYYY-MM-DD</code> (simply substitute your day at the end)</li> <li>Unsupported API endpoint for market and runner metadata including Barrier, Trainer, Jockey and Best Tote Price - <code>https://apigateway.betfair.com.au/hub/raceevent/market_id</code> (simply substitute your market_id at the end including \"1.\")</li> <li>Carrot Cruncher Horse Racing Model (url=<code>https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date='+YYYY-MM-DD+'&amp;presenter=RatingsPresenter&amp;csv=true</code>) (simply substitute your date in the middle). These ratings are available back to 18/2/21.</li> <li>Iggy Joey Greyhound Model (url=<code>https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date='+YYYY-MM-DD+'&amp;presenter=RatingsPresenter&amp;csv=true</code>) (simply substitute your date in the middle)</li> </ul> <p>Hopefully this tutorial has helped and provides a great resource to be able to process these powerful, if cumbersome, TAR files and assists in your data gathering!</p> <p>For further tutorials on how to handle these new CSV files to create a model - check out our suite of other tutorials in the modelling section </p>"},{"location":"tutorials/processingTarFiles101/#20-complete-code","title":"2.0 Complete Code","text":""},{"location":"tutorials/processingTarFiles101/#21-complete-code-racing-processor","title":"2.1 Complete Code Racing Processor","text":""},{"location":"tutorials/processingTarFiles101/#22-complete-code-sports-processor","title":"2.2 Complete Code Sports Processor","text":""},{"location":"tutorials/processingTarFiles101/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"tutorials/usingHistoricDataSite/","title":"Using the Historic Data site","text":"<p>The Betfair Historic Data site includes complete historic data for nearly all markets offered on the Exchange since 2016, when the new APING was launched. The data available includes prices, volume traded, winning status, average weighted price, BSP, and a variety of other details that are valuable for modelling and strategy development. </p> <p>We know that the process of downloading and extracting these data files can be a bit intimidating the first time round, so here's a walk through of one way to go about it to help make it more accessible. </p> <p>Data tiers</p> <p>There are three tiers of historic data available on this site. You can download samples of each tier of data here.</p> <p>The biggest difference is between the free and paid data. The free data includes a lot of information about the market, but no volume, and only last traded price per minute, not a full price ladder. The two paid tiers include the same data, just at different frequencies. If your strategy isn't particularly price sensitive and doesn't need volume as a variable then you'll probably be fine with the free tier, however if you need to see a more granular view of the market then you should probably consider the paid advanced or pro tiers. </p> <p>A full catalogue of the values included in each data tier is available here.</p> Basic Advanced Pro <li>1 minute intervals</li><li>last traded price</li><li>no volume</li> <li>1 second intervals</li><li>price ladder (top 3)</li><li>volume</li> <li>API tick intervals (50ms)</li><li>price ladder (full)</li><li>volume</li>"},{"location":"tutorials/usingHistoricDataSite/#purchasing-the-data","title":"Purchasing the data","text":"<p>Start by going to the Betfair Historic Data site and log in using your Betfair account. </p> <p>On the Home page select the data set you want to download.</p> <p>Free data</p> <ul> <li>You need to 'purchase' the data set you want to download, even if it's from the free tier</li> <li>You can only 'purchase' each time period of data once. For example, if you had previously 'purchased' all Greyhound data for January 2018, then tried to download Greyhound data for January to March 2018 you would receive an error, and would need to purchase the data for February to March instead. </li> </ul> <p>Once you 'purchase' your choice of data it's recommended that you go to the My Data page, and choose the subset of data to then download.</p> <p></p>"},{"location":"tutorials/usingHistoricDataSite/#downloading-the-data","title":"Downloading the data","text":"<p>On the My Data page you can filter the purchased data to the actual markets you're interested in. You can filter by Sport, Date range, EventId, Event Name, Market Type, Country &amp; File Type (M = market, E = Event), which will cut down the size of the data you need to download.</p> <p>For example, if you wanted the win market for Australian and New Zealand greyhound races you'd use these filters. </p> <p></p> <p>File type</p> <p>The file type filter has two options that you can choose from:</p> <ul> <li>E = Event - includes event level data, i.e. Geelong greyhounds on x date</li> <li>M = Market - includes market level data, i.e. the win market for Geelong greyhounds race 3 on x date</li> </ul> <p>The site can be pretty slow to download from, and you'll generally have a better experience if you download the data a bit at a time, say month by month. Alternatively if you're going to download a lot of data it might be worth having a look at the historic data API, that can automate the download process and speed it up significantly. There's a guide available here, and some sample code the help get you started.</p>"},{"location":"tutorials/usingHistoricDataSite/#unzipping-the-files","title":"Unzipping the files","text":"<p>You'll need to download a program to unzip the TAR files. Here we'll be using 7Zip, which is free, open source and generally well respected. Once you've downloaded it make sure you also install it onto the computer you'll be using to open the data files.</p> <p>Locate the data.tar file in your computer's file explorer program. Right click on the file, select '7-Zip' from the menu then choose 'Extract files...'.</p> <p></p> <p>In the model that pops up change the path mode to 'No pathnames'. You can also change the name and/or path of the folder you want the files extracted to if you want to.</p> <p></p> <p>You now have a collection of .bz2 files. The final step is to select all the files, right click, select '7-Zip' from the menu then choose 'Extract here'. This will then extract all the individual zipped files which you can then either open in a text editor - you can use something basic like Notepad (installed on basically all computers by default) or a more complete program like Visual Studio Code (my go to), Vim or Notepad++ - or you can parse over the using a program to do the work for you. We'll explore how to parse the data another time. If you're opening the files with a text editor you might need to right click, choose 'open with' and select your preferred program. </p> <p></p>"},{"location":"tutorials/usingHistoricDataSite/#whats-it-for","title":"What's it for?","text":"<p>The data available on the Historic Data site is extensive, and can be a really valuable tool or input. For example, you can include some of the columns as variables in a predictive model, compare BSP odds against win rates, or determine the average length of time it takes for 2 year old horses to run 1200m at Geelong. Quality data underpins the vast majority of successful betting strategies, so becoming comfortable working with the data available to you is a really important part of both the modelling and automation processes. </p>"},{"location":"tutorials/usingHistoricDataSite/#extra-resources","title":"Extra resources","text":"<p>Here are some other useful resources that can help you work with this Historic Data:</p> <ul> <li>Historic Data FAQs</li> <li>Data Specification</li> <li>API for downloading historic data files (quicker than manually downloading)</li> <li>Sample code for using the historic data download API</li> </ul>"},{"location":"tutorials/jsonToCsvRevisited/","title":"JSON to CSV | Revisited","text":"<p>Before you start</p> <p>This tutorial follows on from our previous JSON to CSV tutorial where we stepped through how to work with the historic recordings of Betfair's price data. You should make sure you read that first before continuing here! </p> <p>So we're nearly a year on from our original JSON to CSV tutorial, and while it was generally well received a very common (and fair) complaint was how long it took the script to run. Many people said that it could take all day, or they had to leave their PC on over night to finish, which obviously makes the data processing and back testing process a lot less accessible, which was the whole point of the original article. So we're circling back around to see what we can do to decrease the running time and to make working with larger samples of data less of an all-day affair. </p> <p>You might want to stick around because the final results are really something. </p> <p>Cheat sheet</p> <ul> <li> <p>If you're looking for the complete code head to the bottom of the page or download the script from Github.</p> </li> <li> <p>To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code), make sure you've navigated in the terminal to the folder you've saved the script in and then type <code>py main.py</code> (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. </p> </li> <li> <p>Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site. We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. </p> </li> <li> <p>We're using the  <code>betfair_data</code> package to do the heavy lifting; it's a Python library with a Rust backend, which makes it incredibly fast (spoiler alert!)</p> </li> </ul>"},{"location":"tutorials/jsonToCsvRevisited/#lets-get-a-baseline","title":"Let's get a baseline","text":"<p>So how slow are we talking? Let's pick some data and run our script as it is from our last tutorial and see how long it takes. We'll be using 3 months worth of PRO Australian Racing data, from October to December 2021, as the basis for this benchmark and our other tests going forward.</p> Files used through this article<pre><code>market_paths = [\n    \"data/2021_10_OctRacingAUPro.tar\",\n    \"data/2021_11_NovRacingAUPro.tar\",\n    \"data/2021_12_DecRacingAUPro.tar\",\n]\n</code></pre> <p>Now let's run the OG script and time how long it takes with the terminal command below:</p> <p>Terminal<pre><code>gtime python json2csv.py\n\n9665.79user 18.27system 2:41:26elapsed 99%CPU (0avgtext+0avgdata 96624maxresident)k\n0inputs+0outputs (405major+10862minor)pagefaults 0swaps\n</code></pre> ... oof. This took a while: <code>2h 41m 26s</code> to be precise. </p> <p>This is less 'run to the kitchen and grab a cup of coffee' and more 'head to the pub and settle in for the night' territory, and that's on a new, very fast, MacBook Pro... if you have an older pc you should expect this to take significantly longer again.</p> <p>But is this actually slow? Let's count up how many files there are in our TAR archives, and then how many lines in each of the files, with each line being an update to a market, to give us an ideal of the scale of data we're dealing with:</p> <p><code>Markets: 22,001 | Updates: 166,677,569</code></p> <p>Each update can effect any or all of the runners in a market, and, to be fair, we are keeping track of a sizeable amount of data for each runner, including all the prices and volumes available in both back and lay ladders, all the money matched at every price point, the money in the BSP pools at each limit, plus a lot more... how much faster can we really expect this to run?</p>"},{"location":"tutorials/jsonToCsvRevisited/#read-the-documentation","title":"READ. THE. DOCUMENTATION.","text":"<p>The <code>betfairlightweight</code> library is doing all the computationally heavy lifting in our script, and speeding it up even slightly could save us a lot of total run time, so we figured we should probably take a look online to see if there were known ways of speeding it up. </p> <p>Let's implement these changes and see if that's enough to let us keep this article to just a couple of paragraphs!</p> <p>Firstly, let's install the speed version of betfairlightweight; it says we should have this by default on Mac but there's no harm in checking anyway.</p> Terminal<pre><code>pip install 'betfairlightweight[speed]'\n</code></pre> <p>There's some quick and easy code changes to throw in as well while we're at it:</p> Performance parameters<pre><code># create trading instance (don't need username/password)\ntrading = betfairlightweight.APIClient(\"username\", \"password\", \"appkey\")\n\n# create listener\nlistener = betfairlightweight.StreamListener(\n    max_latency=None,  # ignore latency errors\n    output_queue=None,  # use generator rather than a queue (faster)\n    lightweight=True,  # lightweight mode is faster\n    update_clk=False,  # do not update clk on updates (not required when backtesting)\n\n    # We need these as were using historic files\n    cumulative_runner_tv=True,  \n    calculate_market_tv=True\n)\n</code></pre> <p>Okay, so let's give it a go with these changes:</p> <p>Terminal<pre><code>python json2csv.py      \nTraceback (most recent call last):\n  File \"xxx/json2csv.py\", line 162, in &lt;module&gt;\n    (preplay_market, postplay_market, final_market) = get_pre_post_final(stream)\n  File \"xxx/json2csv.py\", line 146, in get_pre_post_final\n    if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n  File \"xxx/json2csv.py\", line 104, in filter_market\n    d = market.market_definition\nAttributeError: 'dict' object has no attribute 'market_definition'\n</code></pre> ... and now it's broken. Taking another look at the docs reveals the culprit to be <code>lightweight=True</code> which makes the library skip parsing into objects and returns the raw JSON in dicts instead. This reportedly has a decent speed increase, but will come with some pretty significant usability constraints which undermines the point of the tutorial, and we would also need to rewrite our whole script. Let's just set that back to false <code>lightweight=False</code>, and see how much performance we gain from the other changes.</p> <p>Terminal<pre><code>gtime python json2csv.py  \n\n9716.23user 39.08system 2:42:44elapsed 99%CPU (0avgtext+0avgdata 106784maxresident)k\n0inputs+0outputs (80major+11965minor)pagefaults 0swaps\n</code></pre> ... so that's basically the same. Without switching to the slightly faster, but significantly harder to use <code>lightweight</code> mode, this may be the best we are going to get from <code>betfairlightweight</code> in this context. </p> <p>It's worth noting here that <code>betfairlightweight</code> is a Python library, written predominantly in Python (more on this in a minute). Looking at this execution through a profiler, we can see the vast majority of our time (&gt; 90%) is spent in parsing and creating objects. The only other significant time was a relatively small (~4%) amount spent both reading the file from disk and decompressing the data.</p> Betfairlightweight <p></p> <p>Python sits in a peculiar space as both a very fast and simultaneously very slow programming language. When your program is spending most of its time running slow interpreted Python code it can be one of the slowest languages around - much slower then many other popular languages. Having said that though, there is a good chance if you're working in Python that a lot of your program isn't actually written in Python, but is instead written in a very fast systems language like C, C++ or Rust; much of the Python standard library, and many popular libraries like numpy, scipy and tensorflow are all examples of this. In these cases when Python is just acting as glue between code written in lower, faster languages a Python program can actually be really fast. </p> <p>This leaves us with two paths: stay with <code>betfairlightweight</code> and try and optimise the performance up to a sufficiently faster level, or rewrite the portion of <code>betfairlightweight</code> that handles parsing the JSON stream files in a much faster language and expose that back to Python as a new library. It is worth noting that <code>betfairlightweight</code> is not a new library - it's been around for a good period of time and has been actively developed enough that the chance of us finding some ground breaking optimisations that could dramatically speed up performance is low, and we would ideally like to see a very large performance increase to move the dial on our original numbers. </p> <p>So we took the path less travelled. And that made all the difference. </p> <p></p>"},{"location":"tutorials/jsonToCsvRevisited/#riir-rewrite-it-in-rust","title":"RIIR (Rewrite it in Rust)","text":"<p>So we've chosen path two: write a new, faster library. I've been playing round in Rust for the last few years - it's a great, modern, and extremely fast language, with a well-loved mascot (hi Ferris!), but most importantly we know it has really great support for writing fast Python libraries. If this was a movie this is where they would show a montage of us staying up late, chugging coffee, and tapping away at our keyboards, but I'll save you that and instead just give you a link to our new library, betfair_data. Quite a bit of effort has gone into making this library as fast as we can, and if there's interest we might write a follow up article that discusses the techniques we used to increase it's speed - let us know if you're keen for that - but for the purpose of this discussion the internals of the library don't really matter.</p> <p>To use our library we'll only need to make a few small changes to our script. You can jump ahead and see the completed file for this stage, but we'll walk through the main changes step by step below.</p> <p>We need to start by importing the new library - it's available in pypi and can be easily installed with pip.</p> Terminal<pre><code>pip install betfair_data\n</code></pre> Import the new library<pre><code>from betfair_data import PriceSize\nfrom betfair_data import bflw\n</code></pre> <p>We'll also need to update our <code>load-markets</code> function to return the bytes of the file, instead of the file object itself. This saves us the complexity of having to make our library know how to read Python files.</p> Change load_markets function<pre><code># loading from tar and extracting files\ndef load_markets(file_paths: List[str]):\n    for file_path in file_paths:\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                with bz2.BZ2File(path, 'rb') as f:\n                    bytes = f.read()\n                    yield bflw.File(path, bytes)\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        name = file.name\n                        bytes = bz2.open(archive.extractfile(file)).read()\n                        yield bflw.File(name, bytes)\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for name in archive.namelist():\n                        bytes = bz2.open(archive.open(name)).read()\n                        yield bflw.File(name, bytes)\n    return None\n</code></pre> <p>Then we just need to use these values, passing them to our new library in a similar way to how we previously passed the data to <code>betfairlightweight</code>.</p> iterate over bflw.File<pre><code>for file in load_markets(market_paths):\n\n    def get_pre_post_final():\n        eval_market = None\n        prev_market = None\n        preplay_market = None\n        postplay_market = None       \n\n        for market_books in file:\n            for market_book in market_books:\n                # if market doesn't meet filter return out\n                if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n                    return (None, None, None)\n\n                # final market view before market goes in play\n                if prev_market is not None and prev_market.inplay != market_book.inplay:\n                    preplay_market = prev_market\n\n                # final market view at the conclusion of the market\n                if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n                    postplay_market = market_book\n\n                # update reference to previous market\n                prev_market = market_book\n\n        return (preplay_market, postplay_market, prev_market) \n</code></pre>"},{"location":"tutorials/jsonToCsvRevisited/#testing-it-out","title":"Testing it out","text":"Terminal<pre><code>gtime python json2csv_bfd.py\n\n618.00user 20.53system 10:38.74elapsed 99%CPU (0avgtext+0avgdata 767712maxresident)k\n0inputs+0outputs (0major+51396minor)pagefaults 0swaps\n</code></pre> <p>Not bad! <code>10m 38s</code> - that's roughly a 93% reduction in run time, and nicely in the range we hoped to achieve. We could stop here, having happily achieved our goals. But can we go even faster?</p> <p>Loading up the profiler again shows us some interesting results.</p> Rust LibraryBetfairlightweight <p></p> <p></p> <p>The work seems to be divided into two main areas. On the left you can see <code>load_markets</code> making up about ~70% of the computation time, and the parsing (<code>get_pre_post_final</code>) comprising the majority of the remaining.</p> <p>Interestingly, we've now sped up our parsing and object creation so much that it no longer comprises the majority of the workload of our script. Looking back at the <code>betfairlightweight</code> example from our original implementation we can see that the parsing took up a touch over 90% of the run time, while reading and decompressing the files only took only ~4%. Now because our parsing and object creation is so much faster, that same decompressing work that was previously only a small fraction is the vast majority of our workload! </p> <p>This split work loads highlights another opportunity. Unlike the JSON parsing, which needs to interact closely with the Python environment to create new objects and merge data, the process of reading and decompressing the files can be completed completely isolated and can therefor take place concurrently in another thread. </p> <p>A simple visualisation of our current workload can be seen in the chart below, where between each parsing workload we need to load and decompress the file. </p> <p></p> <p>However if we were to move the file loading and decompression process to another thread we could free up the main Python thread to just spend its time on the JSON parsing, object creation, merging values and execution of our script. Essentially our worker thread would load and decompress the file and free up the main Python thread to complete the tasks that can only run on there.</p> <p></p>"},{"location":"tutorials/jsonToCsvRevisited/#gotta-go-fast","title":"Gotta go fast","text":"<p>This version only has a few additional code changes to the previous <code>betfair_data</code> example - we'll be removing the <code>load_markets</code> generator function from the previous script, and will replace it with a call to a new Rust function that we have added to our library. Now the <code>betfair_data.Files</code> function is doing all the heavy lifting. It creates its own thread, loads the files, decompresses them (if needed) and buffers them for the main Python thread to take as needed.</p> Loading the files in a separate thread<pre><code>for file in betfair_data.Files(market_paths).bflw():\n    def get_pre_post_final():\n        eval_market = None\n        prev_market = None\n        preplay_market = None\n        postplay_market = None       \n\n        for market_books in file:\n            for market_book in market_books:\n</code></pre> <p>Now we can run it again as usual.</p> Terminal<pre><code>gtime python json2csv_bfd_Rustsrc.py\n673.28user 18.31system 4:25.71elapsed 260%CPU (0avgtext+0avgdata 878720maxresident)k\n0inputs+0outputs (0major+72667minor)pagefaults 0swaps\n</code></pre> <p>Down to <code>4m 25s</code>, an over 100% improvement again! Realistically though it's probably better to think of this change as a reduction of a fixed amount of work, as opposed to a doubling of performance. We are removing the loading and decompressing workload from the main thread, and in all fairness, we could take this exact same approach to the original <code>betfairlightweight</code> solution and expect to save roughly the same amount of time, <code>5mins</code> (from <code>2h 41m</code> to <code>2h 36m</code>). </p> <p>When this workload is the majority of the computation time, this optimisation feels necessary, but much less so when it represents only a small fraction of the total run time.</p> <p>Loading up the profiler, we can see that our flamegraph has gotten pretty bare. There's not much Python code left anymore, and what is left is truly just acting as glue between fast systems libraries to join them together and retrieve our result. Future optimisations will probably need to happen inside our new library, but whilst there are probably some performance gains to be found, we'll be unlikely to achieve anywhere near the scale of the speed increase we've already made.</p> Rust Library, threaded loadingRust Library, Python loading<code>Betfairlightweight</code> <p></p> <p></p> <p></p>"},{"location":"tutorials/jsonToCsvRevisited/#validating-our-results","title":"Validating our results","text":"<p>All of this effort measuring speed, and we haven't actually stopped to look at our output! Getting an answer quickly is only useful if we actually get the right answer, so let's run some comparisons between our original output and the output of the new library.</p> <p>All of the outputted CSV files are <code>52,483</code> lines long, which is definitely a good start. We do need to go deeper though to make sure the values in every row are the same - we have essentially rewritten all the complex data handling logic so small differences in the output could be expected.</p> <p>Luckily as CSV files are just plain text we can check the outputs of each line using <code>diff</code>:</p> Terminal<pre><code>diff output_bfd.csv output_bflw.csv  \n\n&lt;no output&gt;\n</code></pre> <p><code>diff</code> found no differences at all between the generated files! This confirms we have produced the exact same CSV.</p>"},{"location":"tutorials/jsonToCsvRevisited/#conclusion","title":"Conclusion","text":"<p>These comparisons leave us confident that we're getting the same outputs from our new library as we were getting from our original implementation, but now we can run a year's worth of data in ~20 minutes, instead of close to 11 hours... </p> <p>Also, with our new library being able to be a drop in replacement for the <code>betfairlightweights</code> object structure, we only needed to make a small number of changes to our script, and most of those changes were deleting lines (always a good feeling). </p> <p>We'll clock that up as a win.</p>"},{"location":"tutorials/jsonToCsvRevisited/#completed-code","title":"Completed code","text":"Rust Library, threaded loadingRust Library, Python loading<code>Betfairlightweight</code> implementation <p>Download from Github</p> <pre><code>import logging\nimport functools\nfrom typing import List, Tuple\nfrom itertools import zip_longest\nfrom betfair_data import PriceSize\nfrom betfair_data import bflw\nimport betfair_data\n\nfile_output = \"output_rust_source.csv\"\n\nmarket_paths = [\n    \"data/2021_10_OctRacingAUPro.tar\",\n    \"data/2021_11_NovRacingAUPro.tar\",\n    \"data/2021_12_DecRacingAUPro.tar\",\n]\n\n# setup logging\nlogging.basicConfig(level=logging.FATAL)\n\n# rounding to 2 decimal places or returning '' if blank\ndef as_str(v) -&gt; str:\n    return '%.2f' % v if (type(v) is float) or (type(v) is int) else v if type(v) is str else ''\n\n# returning smaller of two numbers where min not 0\ndef min_gr0(a: float, b: float) -&gt; float:\n    if a &lt;= 0:\n        return b\n    if b &lt;= 0:\n        return a\n\n    return min(a, b)\n\n# parsing price data and pulling out weighted avg price, matched, min price and max price\ndef parse_traded(traded: List[PriceSize]) -&gt; Tuple[float, float, float, float]:\n    if len(traded) == 0: \n        return (None, None, None, None)\n\n    (wavg_sum, matched, min_price, max_price) = functools.reduce(\n        lambda total, ps: (\n            total[0] + (ps.price * ps.size), # wavg_sum before we divide by total matched\n            total[1] + ps.size, # total matched\n            min(total[2], ps.price), # min price matched\n            max(total[3], ps.price), # max price matched\n        ),\n        traded,\n        (0, 0, 1001, 0) # starting default values\n    )\n\n    wavg_sum = (wavg_sum / matched) if matched &gt; 0 else None # dividing sum of wavg by total matched\n    matched = matched if matched &gt; 0 else None \n    min_price = min_price if min_price != 1001 else None\n    max_price = max_price if max_price != 0 else None\n\n    return (wavg_sum, matched, min_price, max_price)\n\n# splitting race name and returning the parts \ndef split_anz_horse_market_name(market_name: str) -&gt; Tuple[str, str, str]:\n    # return race no, length, race type\n    # input samples: \n    # 'R6 1400m Grp1' -&gt; ('R6','1400m','grp1')\n    # 'R1 1609m Trot M' -&gt; ('R1', '1609m', 'trot')\n    # 'R4 1660m Pace M' -&gt; ('R4', '1660m', 'pace')\n    parts = market_name.split(' ')\n    race_no = parts[0] \n    race_len = parts[1] \n    race_type = parts[2].lower() \n\n    return (race_no, race_len, race_type)\n\n# filtering markets to those that fit the following criteria\ndef filter_market(market: bflw.MarketBook) -&gt; bool: \n    d = market.market_definition\n    return (d != None\n        and d.country_code == 'AU' \n        and d.market_type == 'WIN' \n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n\n\n\n# record prices to a file\nwith open(file_output, \"w\") as output:\n    # defining column headers\n    output.write(\"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume\\n\")\n\n    for i, g in enumerate(bflw.Files(market_paths)):\n        print(\"Market {}\".format(i), end='\\r')\n\n        def get_pre_post_final():\n            eval_market = None\n            prev_market = None\n            preplay_market = None\n            postplay_market = None       \n\n            for market_books in g:\n                for market_book in market_books:\n                    # if market doesn't meet filter return out\n                    if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n                        return (None, None, None)\n\n                    # final market view before market goes in play\n                    if prev_market is not None and prev_market.inplay != market_book.inplay:\n                        preplay_market = prev_market\n\n                    # final market view at the conclusion of the market\n                    if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n                        postplay_market = market_book\n\n                    # update reference to previous market\n                    prev_market = market_book\n\n            return (preplay_market, postplay_market, prev_market) # prev is now final\n\n        (preplay_market, postplay_market, final_market) = get_pre_post_final()\n\n        # no price data for market\n        if postplay_market is None:\n            continue; \n\n        preplay_traded = [ (r.last_price_traded, r.ex.traded_volume) for r in preplay_market.runners ] if preplay_market is not None else None\n        postplay_traded = [ (\n            r.last_price_traded,\n            r.ex.traded_volume,\n            # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1))        \n            min_gr0(\n                next((pv.size for pv in r.sp.back_stake_taken if pv.size &gt; 0), 0),\n                next((pv.size for pv in r.sp.lay_liability_taken if pv.size &gt; 0), 0)  / ((r.sp.actual_sp if (type(r.sp.actual_sp) is float) or (type(r.sp.actual_sp) is int) else 0) - 1)\n            ) if r.sp.actual_sp is not None else 0,\n        ) for r in postplay_market.runners ]\n\n        # generic runner data\n        runner_data = [\n            {\n                'selection_id': r.selection_id,\n                'selection_name': next((rd.name for rd in final_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n                'selection_status': r.status,\n                'sp': as_str(r.sp.actual_sp),\n            }\n            for r in final_market.runners \n        ]\n\n        # runner price data for markets that go in play\n        if preplay_traded is not None:\n            def runner_vals(r):\n                (pre_ltp, pre_traded), (post_ltp, post_traded, sp_traded) = r\n\n                inplay_only = list(filter(lambda ps: ps.size &gt; 0, [\n                    PriceSize(\n                        price=post_ps.price, \n                        size=post_ps.size - next((pre_ps.size for pre_ps in pre_traded if pre_ps.price == post_ps.price), 0)\n                    )\n                    for post_ps in post_traded \n                ]))\n\n                (ip_wavg, ip_matched, ip_min, ip_max) = parse_traded(inplay_only)\n                (pre_wavg, pre_matched, pre_min, pre_max) = parse_traded(pre_traded)\n\n                return {\n                    'preplay_ltp': as_str(pre_ltp),\n                    'preplay_min': as_str(pre_min),\n                    'preplay_max': as_str(pre_max),\n                    'preplay_wavg': as_str(pre_wavg),\n                    'preplay_matched': as_str((pre_matched or 0) + (sp_traded or 0)),\n                    'inplay_ltp': as_str(post_ltp),\n                    'inplay_min': as_str(ip_min),\n                    'inplay_max': as_str(ip_max),\n                    'inplay_wavg': as_str(ip_wavg),\n                    'inplay_matched': as_str(ip_matched),\n                }\n\n            runner_traded = [ runner_vals(r) for r in zip_longest(preplay_traded, postplay_traded, fillvalue=PriceSize(0, 0)) ]\n\n        # runner price data for markets that don't go in play\n        else:\n            def runner_vals(r):\n                (ltp, traded, sp_traded) = r\n                (wavg, matched, min_price, max_price) = parse_traded(traded)\n\n                return {\n                    'preplay_ltp': as_str(ltp),\n                    'preplay_min': as_str(min_price),\n                    'preplay_max': as_str(max_price),\n                    'preplay_wavg': as_str(wavg),\n                    'preplay_matched': as_str((matched or 0) + (sp_traded or 0)),\n                    'inplay_ltp': '',\n                    'inplay_min': '',\n                    'inplay_max': '',\n                    'inplay_wavg': '',\n                    'inplay_matched': '',\n                }\n\n            runner_traded = [ runner_vals(r) for r in postplay_traded ]\n\n        # printing to csv for each runner\n        for (rdata, rprices) in zip(runner_data, runner_traded):\n            # defining data to go in each column\n            output.write(\n                \"{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n                    postplay_market.market_id,\n                    postplay_market.market_definition.market_time,\n                    postplay_market.market_definition.country_code,\n                    postplay_market.market_definition.venue,\n                    postplay_market.market_definition.name,\n                    rdata['selection_id'],\n                    rdata['selection_name'],\n                    rdata['selection_status'],\n                    rdata['sp'],\n                    rprices['preplay_min'],\n                    rprices['preplay_max'],\n                    rprices['preplay_wavg'],\n                    rprices['preplay_ltp'],\n                    rprices['preplay_matched'],\n                    rprices['inplay_min'],\n                    rprices['inplay_max'],\n                    rprices['inplay_wavg'],\n                    rprices['inplay_ltp'],\n                    rprices['inplay_matched'],\n                )\n            )\n</code></pre> <p>Download from Github</p> <pre><code>import logging\nfrom typing import List, Tuple\n\nfrom itertools import zip_longest\nimport functools\n\nimport os\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\nfrom betfair_data import PriceSize\nfrom betfair_data import bflw\n\nfile_output = \"output_py_source.csv\"\n\nmarket_paths = [\n    \"data/2021_10_OctRacingAUPro.tar\",\n    \"data/2021_11_NovRacingAUPro.tar\",\n    \"data/2021_12_DecRacingAUPro.tar\",\n]\n\n# setup logging\nlogging.basicConfig(level=logging.FATAL)\n\n# loading from tar and extracting files\ndef load_markets(file_paths: List[str]):\n    for file_path in file_paths:\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                with bz2.BZ2File(path, 'rb') as f:\n                    bytes = f.read()\n                    yield bflw.File(path, bytes)\n\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        name = file.name\n                        bytes = bz2.open(archive.extractfile(file)).read()\n                        yield bflw.File(name, bytes)\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for name in archive.namelist():\n                        bytes = bz2.open(archive.open(name)).read()\n                        yield bflw.File(name, bytes)\n    return None\n\n# rounding to 2 decimal places or returning '' if blank\ndef as_str(v) -&gt; str:\n    return '%.2f' % v if (type(v) is float) or (type(v) is int) else v if type(v) is str else ''\n\n# returning smaller of two numbers where min not 0\ndef min_gr0(a: float, b: float) -&gt; float:\n    if a &lt;= 0:\n        return b\n    if b &lt;= 0:\n        return a\n\n    return min(a, b)\n\n# parsing price data and pulling out weighted avg price, matched, min price and max price\ndef parse_traded(traded: List[PriceSize]) -&gt; Tuple[float, float, float, float]:\n    if len(traded) == 0: \n        return (None, None, None, None)\n\n    (wavg_sum, matched, min_price, max_price) = functools.reduce(\n        lambda total, ps: (\n            total[0] + (ps.price * ps.size), # wavg_sum before we divide by total matched\n            total[1] + ps.size, # total matched\n            min(total[2], ps.price), # min price matched\n            max(total[3], ps.price), # max price matched\n        ),\n        traded,\n        (0, 0, 1001, 0) # starting default values\n    )\n\n    wavg_sum = (wavg_sum / matched) if matched &gt; 0 else None # dividing sum of wavg by total matched\n    matched = matched if matched &gt; 0 else None \n    min_price = min_price if min_price != 1001 else None\n    max_price = max_price if max_price != 0 else None\n\n    return (wavg_sum, matched, min_price, max_price)\n\n# splitting race name and returning the parts \ndef split_anz_horse_market_name(market_name: str) -&gt; Tuple[str, str, str]:\n    # return race no, length, race type\n    # input samples: \n    # 'R6 1400m Grp1' -&gt; ('R6','1400m','grp1')\n    # 'R1 1609m Trot M' -&gt; ('R1', '1609m', 'trot')\n    # 'R4 1660m Pace M' -&gt; ('R4', '1660m', 'pace')\n    parts = market_name.split(' ')\n    race_no = parts[0] \n    race_len = parts[1] \n    race_type = parts[2].lower() \n\n    return (race_no, race_len, race_type)\n\n# filtering markets to those that fit the following criteria\ndef filter_market(market: bflw.MarketBook) -&gt; bool: \n    d = market.market_definition\n    return (d != None\n        and d.country_code == 'AU' \n        and d.market_type == 'WIN' \n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n\n# record prices to a file\nwith open(file_output, \"w\") as output:\n    # defining column headers\n    output.write(\"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume\\n\")\n\n    for i, file in enumerate(load_markets(market_paths)):\n        print(\"Market {}\".format(i), end='\\r')\n\n        def get_pre_post_final():\n            eval_market = None\n            prev_market = None\n            preplay_market = None\n            postplay_market = None       \n\n            for market_books in file:\n                for market_book in market_books:\n                    # if market doesn't meet filter return out\n                    if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n                        return (None, None, None)\n\n                    # final market view before market goes in play\n                    if prev_market is not None and prev_market.inplay != market_book.inplay:\n                        preplay_market = prev_market\n\n                    # final market view at the conclusion of the market\n                    if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n                        postplay_market = market_book\n\n                    # update reference to previous market\n                    prev_market = market_book\n\n            return (preplay_market, postplay_market, prev_market) # prev is now final\n\n        (preplay_market, postplay_market, final_market) = get_pre_post_final()\n\n        # no price data for market\n        if postplay_market is None:\n            continue; \n\n        preplay_traded = [ (r.last_price_traded, r.ex.traded_volume) for r in preplay_market.runners ] if preplay_market is not None else None\n        postplay_traded = [ (\n            r.last_price_traded,\n            r.ex.traded_volume,\n            # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1))        \n            min_gr0(\n                next((pv.size for pv in r.sp.back_stake_taken if pv.size &gt; 0), 0),\n                next((pv.size for pv in r.sp.lay_liability_taken if pv.size &gt; 0), 0)  / ((r.sp.actual_sp if (type(r.sp.actual_sp) is float) or (type(r.sp.actual_sp) is int) else 0) - 1)\n            ) if r.sp.actual_sp is not None else 0,\n        ) for r in postplay_market.runners ]\n\n        # generic runner data\n        runner_data = [\n            {\n                'selection_id': r.selection_id,\n                'selection_name': next((rd.name for rd in final_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n                'selection_status': r.status,\n                'sp': as_str(r.sp.actual_sp),\n            }\n            for r in final_market.runners \n        ]\n\n        # runner price data for markets that go in play\n        if preplay_traded is not None:\n            def runner_vals(r):\n                (pre_ltp, pre_traded), (post_ltp, post_traded, sp_traded) = r\n\n                inplay_only = list(filter(lambda ps: ps.size &gt; 0, [\n                    PriceSize(\n                        price=post_ps.price, \n                        size=post_ps.size - next((pre_ps.size for pre_ps in pre_traded if pre_ps.price == post_ps.price), 0)\n                    )\n                    for post_ps in post_traded \n                ]))\n\n                (ip_wavg, ip_matched, ip_min, ip_max) = parse_traded(inplay_only)\n                (pre_wavg, pre_matched, pre_min, pre_max) = parse_traded(pre_traded)\n\n                return {\n                    'preplay_ltp': as_str(pre_ltp),\n                    'preplay_min': as_str(pre_min),\n                    'preplay_max': as_str(pre_max),\n                    'preplay_wavg': as_str(pre_wavg),\n                    'preplay_matched': as_str((pre_matched or 0) + (sp_traded or 0)),\n                    'inplay_ltp': as_str(post_ltp),\n                    'inplay_min': as_str(ip_min),\n                    'inplay_max': as_str(ip_max),\n                    'inplay_wavg': as_str(ip_wavg),\n                    'inplay_matched': as_str(ip_matched),\n                }\n\n            runner_traded = [ runner_vals(r) for r in zip_longest(preplay_traded, postplay_traded, fillvalue=PriceSize(0, 0)) ]\n\n        # runner price data for markets that don't go in play\n        else:\n            def runner_vals(r):\n                (ltp, traded, sp_traded) = r\n                (wavg, matched, min_price, max_price) = parse_traded(traded)\n\n                return {\n                    'preplay_ltp': as_str(ltp),\n                    'preplay_min': as_str(min_price),\n                    'preplay_max': as_str(max_price),\n                    'preplay_wavg': as_str(wavg),\n                    'preplay_matched': as_str((matched or 0) + (sp_traded or 0)),\n                    'inplay_ltp': '',\n                    'inplay_min': '',\n                    'inplay_max': '',\n                    'inplay_wavg': '',\n                    'inplay_matched': '',\n                }\n\n            runner_traded = [ runner_vals(r) for r in postplay_traded ]\n\n        # printing to csv for each runner\n        for (rdata, rprices) in zip(runner_data, runner_traded):\n            # defining data to go in each column\n            output.write(\n                \"{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n                    postplay_market.market_id,\n                    postplay_market.market_definition.market_time,\n                    postplay_market.market_definition.country_code,\n                    postplay_market.market_definition.venue,\n                    postplay_market.market_definition.name,\n                    rdata['selection_id'],\n                    rdata['selection_name'],\n                    rdata['selection_status'],\n                    rdata['sp'],\n                    rprices['preplay_min'],\n                    rprices['preplay_max'],\n                    rprices['preplay_wavg'],\n                    rprices['preplay_ltp'],\n                    rprices['preplay_matched'],\n                    rprices['inplay_min'],\n                    rprices['inplay_max'],\n                    rprices['inplay_wavg'],\n                    rprices['inplay_ltp'],\n                    rprices['inplay_matched'],\n                )\n            )\n</code></pre> <p>Download from Github</p> <pre><code>import logging\nfrom typing import List, Tuple\n\nfrom unittest.mock import patch\nfrom itertools import zip_longest\nimport functools\n\nimport os\nimport tarfile\nimport zipfile\nimport bz2\nimport glob\n\n# importing data types\nimport betfairlightweight\nfrom betfairlightweight.resources.bettingresources import (\n    PriceSize,\n    MarketBook \n)\n\nfile_output = \"output_bflw.csv\"\n\nmarket_paths = [\n    \"data/2021_10_OctRacingAUPro.tar\",\n    \"data/2021_11_NovRacingAUPro.tar\",\n    \"data/2021_12_DecRacingAUPro.tar\",\n]\n\n# setup logging\nlogging.basicConfig(level=logging.FATAL)\n\n# create trading instance (don't need username/password)\ntrading = betfairlightweight.APIClient(\"username\", \"password\", \"appkey\")\n\n# create listener\nlistener = betfairlightweight.StreamListener(\n    max_latency=None,   # ignore latency errors\n    output_queue=None,  # use generator rather than a queue (faster)\n    lightweight=False,  # lightweight mode is faster\n    update_clk=False,   # do not update clk on updates (not required when backtesting)\n\n    cumulative_runner_tv=True, \n    calculate_market_tv=True\n)\n\n# loading from tar and extracting files\ndef load_markets(file_paths: List[str]):\n    for file_path in file_paths:\n        if os.path.isdir(file_path):\n            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n                f = bz2.BZ2File(path, 'rb')\n                yield f\n                f.close()\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1]\n            # iterate through a tar archive\n            if ext == '.tar':\n                with tarfile.TarFile(file_path) as archive:\n                    for file in archive:\n                        yield bz2.open(archive.extractfile(file))\n            # or a zip archive\n            elif ext == '.zip':\n                with zipfile.ZipFile(file_path) as archive:\n                    for file in archive.namelist():\n                        yield bz2.open(archive.open(file))\n    return None\n\n# rounding to 2 decimal places or returning '' if blank\ndef as_str(v) -&gt; str:\n    return '%.2f' % v if (type(v) is float) or (type(v) is int) else v if type(v) is str else ''\n\n# returning smaller of two numbers where min not 0\ndef min_gr0(a: float, b: float) -&gt; float:\n    if a &lt;= 0:\n        return b\n    if b &lt;= 0:\n        return a\n\n    return min(a, b)\n\n# parsing price data and pulling out weighted avg price, matched, min price and max price\ndef parse_traded(traded: List[PriceSize]) -&gt; Tuple[float, float, float, float]:\n    if len(traded) == 0: \n        return (None, None, None, None)\n\n    (wavg_sum, matched, min_price, max_price) = functools.reduce(\n        lambda total, ps: (\n            total[0] + (ps.price * ps.size), # wavg_sum before we divide by total matched\n            total[1] + ps.size, # total matched\n            min(total[2], ps.price), # min price matched\n            max(total[3], ps.price), # max price matched\n        ),\n        traded,\n        (0, 0, 1001, 0) # starting default values\n    )\n\n    wavg_sum = (wavg_sum / matched) if matched &gt; 0 else None # dividing sum of wavg by total matched\n    matched = matched if matched &gt; 0 else None \n    min_price = min_price if min_price != 1001 else None\n    max_price = max_price if max_price != 0 else None\n\n    return (wavg_sum, matched, min_price, max_price)\n\n# splitting race name and returning the parts \ndef split_anz_horse_market_name(market_name: str) -&gt; Tuple[str, str, str]:\n    # return race no, length, race type\n    # input samples: \n    # 'R6 1400m Grp1' -&gt; ('R6','1400m','grp1')\n    # 'R1 1609m Trot M' -&gt; ('R1', '1609m', 'trot')\n    # 'R4 1660m Pace M' -&gt; ('R4', '1660m', 'pace')\n    parts = market_name.split(' ')\n    race_no = parts[0] \n    race_len = parts[1] \n    race_type = parts[2].lower() \n\n    return (race_no, race_len, race_type)\n\n# filtering markets to those that fit the following criteria\ndef filter_market(market: MarketBook) -&gt; bool: \n    d = market.market_definition\n    return (d != None\n        and d.country_code == 'AU' \n        and d.market_type == 'WIN' \n        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n\n# record prices to a file\nwith open(file_output, \"w\") as output:\n    # defining column headers\n    output.write(\"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume\\n\")\n\n    for i, file_obj in enumerate(load_markets(market_paths)):\n        print(\"Market {}\".format(i), end='\\r')\n\n        stream = trading.streaming.create_historical_generator_stream(\n            file_path=file_obj,\n            listener=listener,\n        )\n\n        def get_pre_post_final(s):\n            with patch(\"builtins.open\", lambda f, _: f):   \n                eval_market = None\n                prev_market = None\n                preplay_market = None\n                postplay_market = None       \n\n                gen = stream.get_generator()\n\n                for market_books in gen():\n                    for market_book in market_books:\n                        # if market doesn't meet filter return out\n                        if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n                            return (None, None, None)\n\n                        # final market view before market goes in play\n                        if prev_market is not None and prev_market.inplay != market_book.inplay:\n                            preplay_market = prev_market\n\n                        # final market view at the conclusion of the market\n                        if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n                            postplay_market = market_book\n\n                        # update reference to previous market\n                        prev_market = market_book\n\n                return (preplay_market, postplay_market, prev_market) # prev is now final\n\n        (preplay_market, postplay_market, final_market) = get_pre_post_final(stream)\n\n        # no price data for market\n        if postplay_market is None:\n            continue; \n\n        preplay_traded = [ (r.last_price_traded, r.ex.traded_volume) for r in preplay_market.runners ] if preplay_market is not None else None\n        postplay_traded = [ (\n            r.last_price_traded,\n            r.ex.traded_volume,\n            # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1))        \n            min_gr0(\n                next((pv.size for pv in r.sp.back_stake_taken if pv.size &gt; 0), 0),\n                next((pv.size for pv in r.sp.lay_liability_taken if pv.size &gt; 0), 0)  / ((r.sp.actual_sp if (type(r.sp.actual_sp) is float) or (type(r.sp.actual_sp) is int) else 0) - 1)\n            ) if r.sp.actual_sp is not None else 0,\n        ) for r in postplay_market.runners ]\n\n        # generic runner data\n        runner_data = [\n            {\n                'selection_id': r.selection_id,\n                'selection_name': next((rd.name for rd in final_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n                'selection_status': r.status,\n                'sp': as_str(r.sp.actual_sp),\n            }\n            for r in final_market.runners \n        ]\n\n        # runner price data for markets that go in play\n        if preplay_traded is not None:\n            def runner_vals(r):\n                (pre_ltp, pre_traded), (post_ltp, post_traded, sp_traded) = r\n\n                inplay_only = list(filter(lambda ps: ps.size &gt; 0, [\n                    PriceSize(\n                        price=post_ps.price, \n                        size=post_ps.size - next((pre_ps.size for pre_ps in pre_traded if pre_ps.price == post_ps.price), 0)\n                    )\n                    for post_ps in post_traded \n                ]))\n\n                (ip_wavg, ip_matched, ip_min, ip_max) = parse_traded(inplay_only)\n                (pre_wavg, pre_matched, pre_min, pre_max) = parse_traded(pre_traded)\n\n                return {\n                    'preplay_ltp': as_str(pre_ltp),\n                    'preplay_min': as_str(pre_min),\n                    'preplay_max': as_str(pre_max),\n                    'preplay_wavg': as_str(pre_wavg),\n                    'preplay_matched': as_str((pre_matched or 0) + (sp_traded or 0)),\n                    'inplay_ltp': as_str(post_ltp),\n                    'inplay_min': as_str(ip_min),\n                    'inplay_max': as_str(ip_max),\n                    'inplay_wavg': as_str(ip_wavg),\n                    'inplay_matched': as_str(ip_matched),\n                }\n\n            runner_traded = [ runner_vals(r) for r in zip_longest(preplay_traded, postplay_traded, fillvalue=PriceSize(0, 0)) ]\n\n        # runner price data for markets that don't go in play\n        else:\n            def runner_vals(r):\n                (ltp, traded, sp_traded) = r\n                (wavg, matched, min_price, max_price) = parse_traded(traded)\n\n                return {\n                    'preplay_ltp': as_str(ltp),\n                    'preplay_min': as_str(min_price),\n                    'preplay_max': as_str(max_price),\n                    'preplay_wavg': as_str(wavg),\n                    'preplay_matched': as_str((matched or 0) + (sp_traded or 0)),\n                    'inplay_ltp': '',\n                    'inplay_min': '',\n                    'inplay_max': '',\n                    'inplay_wavg': '',\n                    'inplay_matched': '',\n                }\n\n            runner_traded = [ runner_vals(r) for r in postplay_traded ]\n\n        # printing to csv for each runner\n        for (rdata, rprices) in zip(runner_data, runner_traded):\n            # defining data to go in each column\n            output.write(\n                \"{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n                    postplay_market.market_id,\n                    postplay_market.market_definition.market_time,\n                    postplay_market.market_definition.country_code,\n                    postplay_market.market_definition.venue,\n                    postplay_market.market_definition.name,\n                    rdata['selection_id'],\n                    rdata['selection_name'],\n                    rdata['selection_status'],\n                    rdata['sp'],\n                    rprices['preplay_min'],\n                    rprices['preplay_max'],\n                    rprices['preplay_wavg'],\n                    rprices['preplay_ltp'],\n                    rprices['preplay_matched'],\n                    rprices['inplay_min'],\n                    rprices['inplay_max'],\n                    rprices['inplay_wavg'],\n                    rprices['inplay_ltp'],\n                    rprices['inplay_matched'],\n                )\n            )\n</code></pre>"},{"location":"tutorials/jsonToCsvRevisited/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"wagering/2ndPlaceVoid/","title":"Cashback 2nd - New Markets","text":"<p>In February 2024, Betfair Australia launched a new product on Australian Greyhound and Thoroughbred racing markets called 'Cashback 2nd' with a market type of 'MONEY_BACK_2ND' In these markets the runner who finishes in second place will have all bets voided before the market is settled (this includes lay bets)</p> <p>How could you go about calculating a fair price for a runner in this market? Well, lets discuss how you can do that just by the using the price available for the runner in the regular 'WIN' market. This approach uses the data found here as an example</p>"},{"location":"wagering/2ndPlaceVoid/#the-code","title":"The Code","text":"<p>Loading the data<pre><code>import pandas as pd\n\n# Read the data\ncashbacksecond = pd.read_csv('ANZ_Greyhounds_2024_02.csv')\n\n# Select relevant columns\ncashbacksecond = cashbacksecond[['LOCAL_MEETING_DATE','TRACK','STATE_CODE','RACE_NO','WIN_MARKET_ID','SELECTION_ID','TAB_NUMBER','SELECTION_NAME','BEST_AVAIL_BACK_AT_SCHEDULED_OFF']]\n\n# Calculate IMPLIED_WIN_PERCENTAGE\ncashbacksecond['IMPLIED_WIN_PERCENTAGE'] = 1/cashbacksecond['BEST_AVAIL_BACK_AT_SCHEDULED_OFF']\n\n# Calculate BMP (Back Market Percentage) (i.e. Overround)\ncashbacksecond['BMP'] = cashbacksecond.groupby('WIN_MARKET_ID')['IMPLIED_WIN_PERCENTAGE'].transform('sum')\n\n# Calculate SCALED_WIN_PROBABILITY\ncashbacksecond['SCALED_WIN_PROBABILITY'] = cashbacksecond['IMPLIED_WIN_PERCENTAGE']/cashbacksecond['BMP']\n</code></pre> Here we've calculated our efficient win price based on the best available price at the scheduled off by removing the effect of the overround.</p> Assign the win probability of all other runners in the race<pre><code># Get unique tab numbers\nunique_tab_numbers = cashbacksecond['TAB_NUMBER'].unique()\n\n# Create sub-dataframes for each unique tab_number\nsub_dataframes = []\nfor tab_number in unique_tab_numbers:\n    # Filter the main DataFrame based on 'TAB_NUMBER' == X\n    sub_df = cashbacksecond[cashbacksecond['TAB_NUMBER'] == tab_number][['WIN_MARKET_ID', 'SCALED_WIN_PROBABILITY']]\n    # Rename the 'IMPLIED_WIN_PERCENTAGE' column to 'IMPLIED_WIN_PERCENTAGE_{X}'\n    sub_df = sub_df.rename(columns={'SCALED_WIN_PROBABILITY': f'SCALED_WIN_PROBABILITY_{tab_number}'})\n    # Append the sub-dataframe to the list\n    sub_dataframes.append(sub_df)\n\n# Merge sub-dataframes back to the main dataframe\nfor sub_df in sub_dataframes:\n    cashbacksecond = pd.merge(cashbacksecond, sub_df, on=['WIN_MARKET_ID'], how='left')\n\ncashbacksecond = cashbacksecond.fillna(0)\ncashbacksecond = cashbacksecond.drop_duplicates()\n</code></pre> <p>Now to assign the proper cashback price we need to subtract the percentage of the market that represents the runner that will be voided and then renormalise the probabilities for the remaining outcomes</p> Calculate probability of winning given that a certain dog comes 2nd<pre><code># Calculate probability of winning given that a certain dog comes 2nd\nfor tab_number in unique_tab_numbers:\n    # Calculate the denominator (1 - Probability_Second_{X})\n    # This is the line where we remove the cashback2nd runner from our market\n    denominator = 1 - cashbacksecond[f'SCALED_WIN_PROBABILITY_{tab_number}']\n    # Create the new column Prob_Win_With_2nd_Dog_Being_{X}\n    cashbacksecond[f'Prob_Win_With_2nd_Dog_Being_{tab_number}'] = cashbacksecond['SCALED_WIN_PROBABILITY'] / denominator\n    # If Probability_Second_{tab_number} is 0, set Prob_Win_With_2nd_Dog_Being_{tab_number} to 0\n    cashbacksecond.loc[cashbacksecond[f'SCALED_WIN_PROBABILITY_{tab_number}'] == 0, f'Prob_Win_With_2nd_Dog_Being_{tab_number}'] = 0\n    # If 'TAB_NUMBER' equals tab_number, set Prob_Win_With_2nd_Dog_Being_{tab_number} to 0\n    cashbacksecond.loc[cashbacksecond['TAB_NUMBER'] == tab_number, f'Prob_Win_With_2nd_Dog_Being_{tab_number}'] = 0\n</code></pre> <p>Finally lets assign the sum the probabilities across all outcomes, renormalise and assign the cashback prices.  It is expected that the efficient market overround will be over 100% to deal with the runner being voided. This is normal for back markets but not usually for lay markets.</p> <p>If you wanted to calculate a price where you wanted to include an edge for yourself, you would multiply the column 'CASHBACK_WIN_PERCENTAGE' by a number greater than 1 for lay bets and less than 1 for back bets. This implies that should those bets get matched, you have positive expected value on these bets.</p> Calculate and assign the cashback prices<pre><code># Select the relevant columns\nprob_columns = [f'Prob_Win_With_2nd_Dog_Being_{tab_number}' for tab_number in unique_tab_numbers]\n# Sum the selected columns row-wise\ntotal_probability = cashbacksecond[prob_columns].sum(axis=1)\n# Count the number of non-zero entries in the selected columns\nnon_zero_count = (cashbacksecond[prob_columns] != 0).sum(axis=1)\n# Divide the total probability by the count of non-zero entries to get the average\ncashback_percentage = total_probability / non_zero_count\n# Assign the calculated average to the 'CASHBACK_WIN_PERCENTAGE' column\ncashbacksecond['CASHBACK_WIN_PERCENTAGE'] = cashback_percentage\n# Assign the cashback price\ncashbacksecond['CASHBACK_PRICE'] = 1 / cashbacksecond['CASHBACK_WIN_PERCENTAGE']\n# Remove unnecessary columns\ncashbacksecond=cashbacksecond[['LOCAL_MEETING_DATE','TRACK','STATE_CODE','RACE_NO','WIN_MARKET_ID','SELECTION_ID','TAB_NUMBER','SELECTION_NAME','BEST_AVAIL_BACK_AT_SCHEDULED_OFF','CASHBACK_PRICE']]\n# Print the first few rows of the dataframe\nprint(cashbacksecond.head)\n# Export to csv\ncashbacksecond.to_csv('Cashback2nd_Probabilities.csv',index=False)\n</code></pre> LOCAL_MEETING_DATE TRACK RACE_NO WIN_MARKET_ID SELECTION_ID TAB_NUMBER SELECTION_NAME WIN_PRICE CASHBACK_PRICE 1/02/2024 Albion Park 1 224245223 62990490 2 Cleopatra Hayze $2.22 $2.07 1/02/2024 Albion Park 1 224245223 65520447 3 Bounding Over $6.40 $5.54 1/02/2024 Albion Park 1 224245223 65328340 1 Whistle Away $7.80 $6.72 1/02/2024 Albion Park 1 224245223 65520448 4 Gone On Green $9.80 $8.41 1/02/2024 Albion Park 1 224245223 65520449 8 Serrai $10.00 $8.58 1/02/2024 Albion Park 1 224245223 64876290 7 Snowy Waugh $18.00 $15.34 1/02/2024 Albion Park 1 224245223 54266223 6 Pocket Say Itch $50.00 $42.42 1/02/2024 Albion Park 1 224245223 59147471 5 City Steamer $170.00 $143.99"},{"location":"wagering/2ndPlaceVoid/#conclusion","title":"Conclusion","text":"<p>This approach can also be used to calculate place market probabilities (though be aware that the complexity will be much higher for a 3rd or 4th place market than just a 2nd place market) as well as exotics like quinellas and exactas. Here we've just used a win market price to calculate a price for the cashback 2nd market, but this approach can easily be applied to a model's rated place like the Betfair Hub Thoroughbred and Greyhound predictions models!</p> <p>If you have any questions or want to learn more, join the Discord server or Australian/New Zealand customers can email us at automation@betfair.com.au. </p> <p>We've also done this in a simple excel sheet to use for a single race - Download Excel</p>"},{"location":"wagering/betfairHowTo/","title":"How To: Betfair Exchange","text":"<p>As Australia\u2019s only online betting exchange, Betfair offers a real alternative to betting with a bookmaker. The platform provides a unique range of non-traditional betting methods that can help punters increase payout potential, uncover opportunities and minimise risk.</p>"},{"location":"wagering/betfairHowTo/#so-how-does-betfair-work","title":"So how does Betfair work?","text":"<p>You know how traditional betting works? A licensed bookmaker puts up the odds and you take them. They accept your bet and pay out if you win.</p> <p>Betfair is different. The Betfair Exchange connects you with punters all over the world to put up your own odds and place bets. With no interference from a bookmaker, the Betfair Exchange invites a real community to interact directly with the odds and control the market, meaning better odds and better value for you.</p>"},{"location":"wagering/betfairHowTo/#how-does-betfair-make-money","title":"How does Betfair make money?","text":"<p>You might be wondering: if a bookmaker makes money by beating punters and claiming their losing stake, how does Betfair make theirs?</p> <p>The answer is simple \u2013 we take a small cut from the punter\u2019s winnings instead.</p> <p>Better known as commission, Betfair will charge a small percentage fee or Market Base Rate (or MBR) on all customer winnings. The MBR will vary depending on region or sporting code and is dictated by how much a sporting governing body taxes Betfair for allowing punters to bet on their product on the Betfair Exchange.</p> <p>So while bookmakers attempt to make profit off losing punters, the Betfair Exchange encourages you to find an edge to win \u2013 and win more! We don\u2019t care how good you are or the rate of money you make. Everyone is welcome to bet at Betfair. You can find out more about commission and other charges here</p>"},{"location":"wagering/betfairHowTo/#how-do-i-place-a-bet","title":"How do I place a bet?","text":"<p>There are two ways to bet at Betfair: you can either back the odds or you can lay them. To \u201cback\u201d the odds means you are betting that something will happen \u2013 a traditional way to bet. By \u201claying\u201d the odds you are doing the reverse: you bet that something will not happen \u2013 and you want to be proven right.</p> <p>To place your bet, go to your selected market and choose the odds available in either the blue (back) or pink (lay) boxes. Decide how much you want to bet and confirm.</p> <p>But what if you don\u2019t like the odds you see? Ask for something better! Simply adjust the odds on your betting slip and place your bet. Your request will be submitted to the exchange platform, waiting for the Betfair community to decide if they will accept (or \u201cmatch\u201d) your offer.</p> <p>Perhaps you are growing tired of being returned with unmatched bets? This is why we introduced the Betfair Starting Price (or Betfair SP). It guarantees that a punter will have their bet locked in at a fixed price, which is based on back and lay bets placed by the community on horse racing, greyhound racing and harness racing markets.</p>"},{"location":"wagering/betfairHowTo/#back-betting","title":"Back Betting","text":"<p>Placing a back bet is the standard play for all punters who used a bookmaker. So we\u2019ll keep this brief: in placing a back bet, you are betting on the odds of something to happen. To place your back bet, choose the market selection box in blue. The odds in the blue column are the biggest prices currently available to back in the market. (Those odds are in the selection box on top, while the amount currently available to back at those odds is underneath.)</p> <p>When you have chosen a selection, the betting slip will appear. Time to outlay.</p> <p>You\u2019ve chosen the best odds available, but maybe you\u2019re not satisfied? You can request even better odds following a few simple steps</p> <ol> <li>Toggle the odds on your betting slip to your desired price</li> <li>Enter your stake amount</li> <li>Place your bet to confirm. But be warned: your bet will be unmatched until another customer (the layer) agrees to your odds and staked amount. So don\u2019t go setting the odds of a $3.00 chance at $10.00 and expect to get matched instantly \u2013 or at all!</li> </ol>"},{"location":"wagering/betfairHowTo/#lay-betting","title":"Lay Betting","text":"<p>When you place a lay bet, you are betting on something not to happen. Before we dive into a market, let\u2019s walk you through the steps for placing a lay bet.</p> <ol> <li>Choose your market selection: just like if you\u2019re back betting \u2013 but this time, pick the market selection box in pink. The odds here are the smallest price currently available to lay in the market.</li> <li>Take the odds available \u2013 or set your own: again, you can adjust the odds box to an even shorter price. But remember: those odds will only be in locked in if another punter (the \u2018backer\u2019) matches them.</li> <li>Enter your stake: unlike a back bet, your stake is the money you want to profit from the predicted result (before commission plus any additional fees and charges). It\u2019s also called the \u2018backer\u2019s stake\u2019 because you essentially want to keep the money of someone who\u2019s wagering that the same result will happen. (This is the basic idea of bookmaking.) It is not, however, the full amount that will be deducted from your account when you place your lay bet. That number is your \u2018liability\u2019.</li> <li>Liability explained: this is the amount you could lose if the result does not fall your way. It is calculated as follows: Liability = [lay odds x backer\u2019s stake] \u2013 backer\u2019s stake</li> </ol> <p>If you wanted to lay the Sydney Swans in the AFL Premiership market at the odds of $8.00 and entered a $10.00 stake, that means there is someone else putting $10.00 on (backing) for the same result to happen at the same odds.</p> <p>For the backer it\u2019s $10.00 x $8.00 = $80.00 payout \u2013 a $70.00 profit. And that backer\u2019s profit becomes your liability.</p> <ol> <li>Place your bet: now you know what liability means, hit that \u2018place bet/s\u2019 button and start the lay bet experience.</li> </ol>"},{"location":"wagering/betfairHowTo/#why-lay","title":"Why lay?","text":"<p>Consider the Melbourne Cup. You\u2019ve got 24 horses in the race. There\u2019s only one winner. Finding that winner can be extremely hard when you\u2019re pouring over different form lines from different states, regions and hemispheres. How do you land on one good back bet?</p> <p>Turn to a lay bet and you\u2019re essentially backing the other horses in the race to beat the one you\u2019re laying. On sheer head count alone (23 against one), the odds are stacked in your favour.</p> <p>Similar logic works for sport. Let\u2019s say the odds for Jordan Spieth to win the Masters Tournament are too short, but you don\u2019t know who else to back. Place a lay bet on Spieth and you are backing any of his opponents to win instead. Sounds easier, right?</p> <p>Just remember though: the bigger the odds for laying, the greater the liability you face. Know your odds and how to minimise risk. Bet wisely and bet with care.</p>"},{"location":"wagering/betfairHowTo/#lay-betting-trading-the-odds","title":"Lay Betting = Trading the Odds","text":"<p>The chance to lay opens the door to betting opportunities you won\u2019t find with a traditional bookmaker. We will explore these in more detail later, but here\u2019s a quick snapshot</p> <ul> <li>Trading on racing and sport: lay something not to happen at lower odds, then back it at a higher price to secure a profit. You\u2019re playing the betting game here and the result of the event becomes meaningless.</li> <li>Lay betting on racing in-play: consider yourself a form student? Find an edge through video form and more by laying lead horses you know have a weakness at the finish.</li> <li>Arbitrage betting: or \u2018arbing\u2019 for short, where you back a selection with a traditional bookmaker, then lay the same selection on the exchange at odds which can secure a profit. Like trading, you\u2019re now letting the result just play out.</li> </ul>"},{"location":"wagering/betfairHowTo/#persistence-types-and-limit-bets","title":"Persistence Types and Limit Bets","text":"<p>Limit bets on the Exchange are simple back and lay bets where a price limit has been set. This is the bet type created when you either back or lay a selection using the instructions above. </p> <p>A persistence type is simply the behaviour of a bet if it is not matched when it is first placed. There are three types:</p> <ul> <li>Lapse: This is the default setting and means that when the event starts, any unmatched bets will automatically be cancelled</li> <li>Keep: This keeps unmatched bets alive when the market goes in-play. These can be cancelled at any time, but in Australia where in-play sports betting is restricted, they cannot be changed. For in-play racing betting on thoroughbreds and harness, these can be changed at any time. For markets that do not go in-play like Greyhounds, this persistence type is invalid</li> <li>Take SP/Market On Close: This causes unmatched bets to accept the BSP where the unmatched portion of the bet is greater than the minimum required bet. For bets smaller than this, they will lapse. For markets that don't support BSP, this persistence type is invalid. These bets cannot be cancelled and no odds limit can be specified.</li> </ul>"},{"location":"wagering/betfairHowTo/#betfair-starting-price-bsp","title":"Betfair Starting Price (BSP)","text":"<p>The Betfair Staring Price (BSP) is similar to the tote in that it allows you to take a fair price that's based on the backers and layers in the market.</p>"},{"location":"wagering/betfairHowTo/#how-is-bsp-different-to-the-traditional-starting-price","title":"How is BSP different to the traditional starting price?","text":"<p>Traditionally in Australia, the Starting Price for any horse in any race has been determined by the most common prevailing price available in the bookmakers ring at the meet in question. Betfair\u2019s Starting Price is different.</p> <p>It\u2019s based on bets placed by both backers and layers in any market. At the start of an event it looks at the relationship between the amount of money requested at SP by opposing customers and any unmatched Exchange bets.</p> <p>There\u2019s no margin for profit built in \u2013 and that\u2019s why we think it will give you much better value, even after taking account of commission paid on your net winnings.</p>"},{"location":"wagering/betfairHowTo/#where-can-i-bet-on-the-bsp","title":"Where can I bet on the BSP?","text":"<p>Australia \u2013 All thoroughbred win markets and all place markets. All harness win markets and all place markets, except for NSW meetings and any meetings predominantly on Sky 2. Sky 2 meetings are win market only. All greyhound win markets and all place markets.</p> <p>New Zealand \u2013 All thoroughbred win markets. All greyhound win markets.</p> <p>International \u2013 All thoroughbred win markets across selected racing around the world.</p>"},{"location":"wagering/betfairHowTo/#how-to-place-an-sp-bet","title":"How to place an SP bet?","text":"<ul> <li>Outright BSP betting - you can place the full stake of your wager on the BSP, either regardless of the odds (this is a \"market_on_close\" bet) or you can specify an odds limit (the worst odds you are willing to accept). You can specify a minimum odds for a back bet and a maximum odds for a lay bet (this is a \"limit_on_close\" bet).</li> <li>Unmatched SP Betting - you can take a price at the Back or Lay odds and if your bet is partially matched, you are prompted to \u2018Take SP\u2019 for the unmatched amount. By clicking \u2018Take SP\u2019 and updating your bet, when the event turns in-play, any unmatched amounts will automatically be matched at the SP price. This also applies to bets that may be fully unmatched when the event turns in-play \u2013 the SP provides convenience ensuring you get your stake matched. Note that this only applies to markets which support BSP betting</li> </ul>"},{"location":"wagering/betfairHowTo/#in-play-betting","title":"In-Play Betting","text":""},{"location":"wagering/betfairHowTo/#fortune-favours-fast-fingers","title":"Fortune Favours Fast Fingers","text":"<p>Sport is the greatest theatre. It can bring a level of uncertainty and unpredictability you won\u2019t find anywhere else. Where the momentum swings like a pendulum, between heartbreak and glory, in a matter of seconds. Think of the ball that escaped Saints player Stephen Milne in the epic 2010 AFL Grand Final. Or the thunderous finish by Chautauqua in the 2017 T J Smith Stakes.</p> <p>Moments like these have you wondering, \u2018what are the odds of that happening?\u2019</p> <p>At Betfair, you can bet in-play on the outcome of a market as the event in question is happening \u2013 in real time. That means, you could\u2019ve backed the \u201cThe Draw\u201d to occur in the St Kilda v Collingwood Grand Final and Chautauqua to win at Randwick in the split-second that neither seemed possible. Conversely, you could\u2019ve layed South Africa to win the 1999 Cricket World Cup semi final against Australia in the final over.</p> <p>There are racing and sports markets with in-play odds you might never find in a pre-play market. All you need is speed and determination!</p>"},{"location":"wagering/betfairHowTo/#a-note-on-in-play-sports-betting","title":"A Note on In-Play Sports Betting","text":"<p>In 2015 the Federal Government prohibited Australian customers from betting online on any aspect of a sporting event after that event has begun. Therefore, customers cannot place bets in-play on sporting events via the Betfair App or on desktop. If customers wish to place an in-play bet on a sports market, they must call our telephone betting service on 132 BET (132 238).</p> <p>There are two notable exceptions to this \u2013 online customers can bet in-play on the AFL Brownlow Medal and politics markets.</p>"},{"location":"wagering/betfairHowTo/#wait-for-the-green-light","title":"Wait for the Green Light","text":"<p>In-play betting is no different to backing or laying on a market before the start time. Find the market you\u2019d like to wager on and wait for the in-play status to appear. A market is turned in-play at either the event\u2019s start time or the scheduled start time, depending on the rules specific to the sport. Look out for the green tick above the market selections on desktop, while on the App a green \u201cIn Play\u201d tag will appear next to the race number in question when betting is turned live.</p> <p>You can place back or lay bets at odds ranging from $1.01 to $1000.00 in-play. Trust us: those prices get matched, and they have occurred before! Where there is a sporting collapse or victory snatched from the jaws of defeat, the big (and small) odds are getting matched on the Betfair Exchange.</p>"},{"location":"wagering/betfairHowTo/#why-wait","title":"Why Wait?","text":"<p>In-play betting provides an opportunity for punters to observe initial momentum and tactical arrangements of a race or sporting match before getting involved. The fast-paced nature of horse and harness racing appeals to punters looking for better odds and utilising their judgement of form and speed maps to claim unlikely sources of value.</p> <p>For example, if you know a short-priced favourite is a backmarker who will finish strongly, you might find an opportunity to back at better odds in-play versus pre-play. It depends on the prevailing circumstances midrace, but it could lead to opportunities you will never find in relatively static pre-play markets.</p> <p>While in-play sports betting is generally less volatile due to matches taking place over a longer period, punters may prefer it as they can weigh up their options with more confidence. It still can lend itself to abrupt changes in odds, which you\u2019ll find in Twenty20 cricket, AFL or soccer, where a wicket or turnover can lead to major changes on the scoreboard, and markets will reflect those differing circumstances in a match.</p> <p>A lesser known but key attraction of in-play betting is the opportunity for punters to combine back and lay bets in the pursuit of profit. With markets active right up to the final siren or winning post, punters can place multiple bets at any time on the same selection and improve their position.</p>"},{"location":"wagering/betfairHowTo/#bet-type-summary","title":"Bet Type Summary","text":"<p>Here's a summary of the different bet-types you'll encounter on the Betfair Exchange. While all this talk about persistence types, limit bets, market_on_close and limit_on_close may seem confusing at first, it will all become clear in time. </p> <p></p>"},{"location":"wagering/bettingGlossary/","title":"Betting Glossary","text":"<p>New to Betfair or simply looking for a basic explanation for a bunch of keywords that you\u2019ve come across along your wagering journey? Our betting glossary is here to help.</p> <p>Below you\u2019ll find what we believe are essential terms and phrases for mastering The Game Within The Game. Please reach out to us at automation if there are other terms you'd like to see included.</p> Wagering Term Definition Arbitrage Is a process through which gamblers aim to make a profit by betting on all possible outcomes of an event, at different odds.\u00a0Arbitrage\u00a0betting involves taking advantage of the variation in odds offered on the Exchange or by different bookmakers and calculating an appropriate stake level that will ensure any losses are covered by winnings in all potential outcomes. Asian Handicap A type of football market where two teams are given a handicap \u2013 positive or negative \u2013 before the game starts. The handicap is a figure such as +0.5 or +1.0 or +1.5, or -0.5 or -1.0 or -1.5. This figure represents a goals head-start or a goals deficit awarded to the teams before kick-off. Back Betting Placing a\u00a0back bet\u00a0is the traditional form of gambling \u2013 you believe a result will happen and bet on it happening, at odds you want to take. On the Betfair Exchange, bettors placing these bets are called \u2018backers\u2019. Bankroll The total amount of money that a player has set aside for the purpose of gambling Bankroll management The practice of managing one's gambling funds in order to minimize losses and maximize profits over time Betfair Starting Price The Betfair Starting Price\u00a0(BSP)\u00a0is the odds you get on a horse at the start of a race based on bets placed by backers and layers in the race market. Bookmaker An individual or company that sets and offers odds on the outcome of events and accepts bets on those outcomes Commission Commission\u00a0is the amount you are charged by Betfair on your net winnings on an Exchange market. Also called \"vig\". Drift If we say a selection in an Exchange market is on the drift or drifting, it means the odds on the selection are getting longer. Dutching The process of backing a number of outcomes for a particular event, in order to ensure the same amount of profit if any of your selections win. Edge The built-in advantage that the bookmaker or casino has in a particular game or bet or the advantage that a bettor's model has over the rest of the market Exacta An exotic bet in which the bettor aims to pick the first two horses in a race including the correct order Expected Value (EV) A mathematical concept used to calculate the average outcome of a particular bet or strategy over a large number of trials.  A positive EV indicates that the bet or strategy is expected to be profitable in the long run, while a negative EV indicates that it is expected to be unprofitable Favourite The selection that the markets sees as the most probable winner of a given event. The quoted odds reflect the extent to which the choice is favoured First Four An exotic bet in which the bettor aims to pick the four three horses in a race including the correct order Futures Betting Odds for the winner of a specific future event often posted far in advance of it occurring. Examples include the AFL Grand Final winner, the Super Bowl winner or the Melbourne Cup winner. Green Book Achieving a\u00a0green\u00a0book\u00a0means creating a position on a market where you eliminate any risk and put yourself in the position to profit regardless of the outcome of the event. Handicapping The process of analyzing and predicting the outcome of a particular event, often used in horse racing to determine which horses have the best chance of winning. Hedging A betting strategy used to reduce the potential losses from a bet by placing an opposing bet on the opposite outcome In-Play Betting In-play betting \u2013 or \u2018in-running betting\u2019 on racing \u2013 involves placing a bet on a particular outcome after the event has started. Kelly Criterion A mathematical formula used to determine the optimal size of a bet based on the probability of the outcome and the potential payout Lay Betting Lay betting is betting on something not to happen. For example, if you\u00a0Lay\u00a0Manchester United to win, your bet will be settled as a winner if they lose or if the game ends in a draw \u2013 so two outcomes are playing in your favour. In racing terms, if you Lay Nature Strip, you are betting on any other runner in the race to win. Liability When you place a Lay bet your \u2018liability\u2019 is the amount you are risking if the event you are laying wins. Line/Spread A handicap applied by bookmakers to balance the odds of a particular event. The favorite must win by a certain number of points or goals, while the underdog can lose by a certain number of points or goals and still win the bet. Liquidity The liquidity is the amount of money available for you to bet (back/lay) at the relevant odds on a selection on the Exchange. You can bet all or part of that amount. Machine Learning Model A machine learning model is a type of algorithm that is trained on data to make predictions or decisions. The model is designed to learn patterns and relationships from a dataset, and then generalize that knowledge to new, unseen data Market Base Rate The base level of commission charged by Betfair on winning bets on that market Market sentiment The overall sentiment of the market, reflected by the activity and sentiment of the backers and layers in the market Matched Betting Taking advantage of odds variation and promotions across bookmakers to make lock in potentially risk free profit. Also called \"Promotional Arbitrage\" or \"Promo Arbing\". Mental Game The mental game of wagering refers to the psychological factors that can influence a person's ability to make effective betting decisions. Wagering is not just about analyzing data and making predictions; it also requires discipline, emotional control, and the ability to manage risk. Minimum Bet Laws Laws in Australia that dictate the minimum amount that bettors are allowed to bet to win on Australian Horse Racing that bookmakers must accept Monte Carlo Simulation In a Monte Carlo simulation, a model is built based on assumptions and rules that describe the behavior of a system or process. The simulation then generates random numbers to simulate the uncertain or stochastic elements of the system. By running the simulation many times, using different sets of random numbers each time, the model produces a distribution of possible outcomes or results. This distribution can be used to estimate the likelihood of different outcomes and to calculate statistical measures such as means, variances, and confidence intervals. Multi/Parlay A type of bet where you combine two or more individual bets into a single wager, with higher potential payouts but greater risk. Odds The probability of a particular outcome occurring, expressed as a ratio of the total payout to the original stake. Over/Under A type of bet where you predict whether the total score or number of goals in a particular event will be higher or lower than a specified amount. Overrounds Term to describe when the\u00a0betting percentage\u00a0of a market is above 100% and therefore providing an advantage to the bookmaker and the opposite to the bettor Probability The likelihood of a particular outcome occurring, typically expressed as a decimal or percentage. The implied probability of a selection is calculated by finding the reciprocal of the odds Promo Banned Customers who are no longer able to take advantage of bookmaker promotions. Also known as \"gubbed\" Promotions Incentives offered by bookmakers to attract and encourage bettors to register, deposit funds and/or place bets Proposition bet A type of bet that does not directly relate to the outcome of a particular event, such as the number of disposals a player will have in a football game Quadrella An exotic bet in which the bettor aims to pick the winner of four consecutive races - usually the last four of a meeting. This is also known as a \"Quaddie\" and if it refers to races earlier in the meeting, it may be referred to as an \"Early Quaddie\" Quinella An exotic bet in which the bettor aims to pick the first two horses in a race not including the correct order Reduction Factor The Reduction Factor (RF) is a percentage assigned to an individual runner and is based on its likely chance of winning the race. The reduction factor that is assigned to a non-runner (scratching / late scratching), is applied on all bets that are matched on the market at the time the non-runner was removed to both backers and layers. Corporate bookies calculate these as cent deductions. Running cold A term used to describe a player or team that is currently having a streak of bad luck or poor performance Running Hot A term used to describe a player or team that is currently having a streak of good luck or success Sharpe Ratio A measure used to evaluate the performance of a gambling strategy or system, comparing the expected returns to the level of risk taken Stake The amount of money wagered on a particular bet. Steam If we say a selection in an Exchange market is steaming, it means the odds on the selection are getting shorter. Sure thing A term used to describe a bet that is considered to be a safe and likely to win Third-Party Tools Betting applications\u00a0such as\u00a0Bet Angel\u00a0and\u00a0Gruss\u00a0that allow Betfair users to bet faster and with more information than other Betfair platforms. Popular with traders, these programs access the Betfair API directly. Tote A system used in horse racing where all bets of a particular type are placed in a pool and the payout is divided among the winning bets based on the total amount of money in the pool. In Australia there are three main pools: VIC operated by TAB, NSW operated by NSWTAB and QLD/SA/TAS/NT operated by UBET. Often bookmakers will advertise \"Best Tote\" which means that they will pay out at the highest odds of the three tote pools. Trading Similar to what you would see in the stock market, but on the Exchange.\u00a0Trading\u00a0involves placing multiple Back and Lay bets on the same market, with the hope of profiting from the fluctuations in odds. Trifecta An exotic bet in which the bettor aims to pick the first three horses in a race including the correct order Underdog The team, player, or horse that is expected to lose a particular event. Variance The degree of deviation of a set of results from the expected value.  A high variance means that results are likely to be more spread out and unpredictable, while a low variance means that results are more consistent and predictable"},{"location":"wagering/commission/","title":"Betfair Commission and Other Charges","text":""},{"location":"wagering/commission/#commission","title":"Commission","text":"<p>Betfair Commission. We know it\u2019s scary. We\u2019d rather spend time talking about the great features of the Exchange. We\u2019d rather spend time talking up better odds, which have been proven time and again. We\u2019d rather spend our time talking about anything else, really.</p> <p>But like any traditional bookmaker, there needs to be some way for Betfair to make money. To keep the Exchange running, help us in improving our products and services \u2013 and pay the handsome bloke writing this article. Our pricing model, however, looks a little different to a bookie. Whilst they must interfere with your odds with built-in margins, we don\u2019t. Remember: odds creation is completely decided by you.</p>"},{"location":"wagering/commission/#what-are-the-different-market-base-rates","title":"What are the different market base rates?","text":"<p>Betfair\u2019s pricing model comes into the picture after the race has been won, when commission is taken from a customer\u2019s net winnings. This is called a Market Base Rate. This rate is the maximum percentage of your winnings that you will pay in commission. To find out what the Market Base Rate is for a market, simply click on the \u201cRules\u201d section of the Market you wish to place a bet on to find it. The percentage of commission will depend on what type of sport you are betting into, as well as which governing body is overseeing the sporting event. And that can vary from state to state, code to code. </p> <p>Market Base Rates for Sport and International Racing Markets are currently 5%, except for NRL at 10%. </p> <p>On Australian thoroughbred, harness and greyhound racing, the Market Base Rate ranges from 7% \u2013 10% depending on the state and racing code. Betfair is charged different fees to provide the betting exchange from state to state. These variable Market Base Rates are reflective of the fees each racing body charges Betfair. </p> <p>Betfair Commission is automatically removed from your winnings when the market is settled. </p> <p>View this easy-to-read chart for a guide to MBR across racing and sport on the Exchange.</p> <p></p>"},{"location":"wagering/commission/#example","title":"Example","text":"<p>For example, Victorian thoroughbred racing (i.e. the Melbourne Cup market) has a 7% Market Base Rate. </p> <p>You place a $50 BACK bet on Deauville Legend in the Melbourne Cup at odds of $4.00. If Deauville Legend wins, this means that your winnings are $150 ($200 return \u2013 $50 stake). </p> <p>With the Market Base Rate at 7%, the commission you will pay is $150 x 7% = $10.50. </p> <p>The commission is cumulative across a single market and is calculated only on the total profit for the market. If you also placed a $50 back bet on Smokin Romans at $15, but it lost, your total winnings on Deauville Legend would now be $100 on which your commission would be $7.00 </p> <p>It's important to note that commission is charged per market not per event. We\u2019ll talk about the difference between markets and events a bit later. This means if you were to place a bet in the Place market as well in the same race, any losses you incur there will not offset commission in the win market. </p>"},{"location":"wagering/commission/#why-commission","title":"Why Commission?","text":"<p>Taxes set by state and territory governments on individual customer winnings are imposed on Australian licensed wagering providers. In addition, these providers must also pay product fees to Australian sporting and racing bodies so they can put up betting markets on their sporting matches and races.</p> <p>Traditional bookmakers offset any fees and taxes by having a built-in margin on all betting markets. This is made possible because the bookies are setting the odds and can dictate odds movement and creation. And once again, the Betfair Exchange does not set the odds \u2013 you do!</p>"},{"location":"wagering/commission/#turnover-charge","title":"Turnover Charge","text":"<p>One of the unique benefits of the Betfair Exchange is the ability for customers to trade in and out (back and lay) as prices on a market fluctuate in the build up to the jump and then in-play, as is famous on the stock market, buy low sell high. </p> <p>Unfortunately, Racing NSW have a race field fee approach (the Turnover Model) where every \u201cback bet\u201d attracts a race field fee, even if a customer is trading and therefore generating very high levels of back bet volume relative to their eventual net customer winning position. </p> <p>The Turnover Charge is aimed at customer activity which generates large volumes of back bets without providing to the exchange the corresponding revenue to cover the race field fees imposed. Therefore, without the Turnover Charge, Betfair would be left with a turnover fee payable to Racing NSW\u202fbut no revenue with which to pay it. </p>"},{"location":"wagering/commission/#who-will-pay-the-turnover-charge","title":"Who will pay the Turnover Charge?","text":"<p>This charge will only apply to customers who during a week (Monday to Sunday GMT time) meet all three of the below criteria: </p> <ul> <li>have matched back bets on 1\u202for more NSW thoroughbred markets; </li> <li>have matched back bets with an aggregate value of $1,000 or greater on NSW thoroughbred markets; and </li> <li>the total commission generated by the customer on NSW thoroughbred markets is less than 1.25% of the aggregate value of matched back bets placed by the customer on those markets. </li> </ul> <p>Total commission generated is half of the commission paid plus half of the implied commission, calculated by the formula = (commission paid + implied commission) \u00f7 2 where implied commission = market losses x 3%</p>"},{"location":"wagering/commission/#what-is-the-charge","title":"What is the charge?","text":"<p>The charge will be 3.0% of the aggregate value of matched back bets on NSW thoroughbred markets. </p> <p>The charge will be calculated for the period from each Monday to Sunday and will be payable on a retrospective basis. </p> <p>The charge will only be incurred for any week when a customer\u2019s betting activity meets the criteria. Please note the charge will not be applicable to: (a) any lay bets; or (b) bets on products other than NSW thoroughbreds. </p>"},{"location":"wagering/commission/#how-will-i-be-notified-if-i-am-required-to-pay","title":"How will I be notified if I am required to pay?","text":"<p>Customers will be contacted directly by Betfair and informed on the first occasion on which this Turnover Charge would have been payable. </p> <p>Customers will also be afforded a one-week \u2018grace period\u2019 for the first instance they hit Turnover Charge, this is designed to give them an opportunity to refine their betting habits should they wish to do so. </p> <p>However, if a customer has previously hit Turnover Charge and then hits the revised turnover charge, the charge will be payable and no one week \u2018grace period\u2019 will be afforded as this is not the first time the charge has been hit. </p>"},{"location":"wagering/commission/#transaction-charge","title":"Transaction Charge","text":"<p>High frequency trading and betting is a wagering proposition unique to the Betfair Exchange and is often an introductory point for many new traders looking to automate their activity using an API or third-party betting tool. To ensure that the Exchange remains accessible and not overloaded with requests from ineffecient transactional behaviour or \"rogue\" bots, a transaction charge exists for any customer placing over 5000 bets per hour (which is more than 1 per second!)</p>"},{"location":"wagering/commission/#who-will-pay-the-transaction-charge","title":"Who will pay the Transaction Charge?","text":"<p>Only customers who log more than 5000 transactions in one hour from XX:00:00 to XX:59:59 will be required to pay this charge which includes both successful and failed transactions including:</p> <ul> <li>Any bet that is placed, cancelled or modified.</li> <li>Any transaction that fails due to an error.</li> </ul>"},{"location":"wagering/commission/#what-is-a-failed-transaction","title":"What is a failed transaction?","text":"<p>An action that leads to an API error, resulting in a failed bet placement, failed cancellation or other failed transaction. A summary of common errors, the reasons for them and how to avoid them is detailed below. This list is not exhaustive.</p> <p></p>"},{"location":"wagering/commission/#what-is-the-transaction-charge","title":"What is the Transaction Charge?","text":"<p>At the end of every day, we add up all the transactions you have recorded. If this number is in excess of 5000 in any hour of the day then (based on the 24-hour clock system) then we will multiply the exceeded amount by 0.2p.:</p> <p>We will offset any transaction fee against total comission. Total commission generated is half of the commission paid plus half of the implied commission, calculated by the formula = (commission paid + implied commission) \u00f7 2 where implied commission = market losses x 3%</p> <p>Any remaining amount will be charged to your account on a daily basis. Should your commission exceed this amount, you will not be charged a transaction fee. Accounts that relate to one person, entity, API subscription or a Master account (Trading version only) with related Sub accounts are treated as one customer for the purposes of transaction charging. Note transaction charges will not be offset against any Premium charges.</p>"},{"location":"wagering/commission/#how-will-i-be-notified-if-i-am-required-to-pay_1","title":"How will I be notified if I am required to pay?","text":"<p>Customers will be contacted directly by Betfair and informed when this Transaction Charge is payable. We may be able to waive these charges in the first instance, but this will be assessed on an individual basis. We can provide assistance and guidance on best practices and how to avoid these charges in the future based on your specific use case for the Betfair Exchange.</p>"},{"location":"wagering/commission/#premium-charge","title":"Premium Charge","text":"<p>The Betfair Premium charge is only payable by the top 0.5% of customers, and while most exchange accounts will never encounter it, it is important to become informed about how and when it applies. In its essence, premium charge is a profit-sharing arrangement, whereby successful punters and traders share profit with Betfair to help us grow the exchange to ensure a strong and vibrant future.</p>"},{"location":"wagering/commission/#who-will-pay-the-premium-charge","title":"Who will pay the Premium Charge?","text":"<p>Each week Betfair will calculate your \u2018gross profits\u2019 made, and your \u2018total charges\u2019 generated over the lifetime of your account. The details of these calculations are explained below.</p> <p>You\u2019ll only be considered for the Premium Charge if your account is in profit and only if the total charges generated since joining Betfair are less than 20% of your gross profits.</p> <p>The vast majority of customers, and even the majority of those whose betting on Betfair is profitable since they joined, do not meet both these conditions and will not incur the Premium Charge.</p> <p>While those conditions accurately describe our most successful customers, they might also apply to new customers who have only bet in a few markets, or those whose accounts are in profit because of a significant big win. To ensure that those accounts are not inadvertently charged, we\u2019ve added two further conditions: </p> <ul> <li>any single win that constitutes more than 50% of lifetime gross profits will be excluded from the calculation</li> <li>customers will only be considered for the Premium Charge after they have bet in more than 250 markets.</li> </ul>"},{"location":"wagering/commission/#what-is-the-premium-charge","title":"What is the Premium Charge?","text":"<p>Each week the customers who meet all the conditions set out above will be charged the lesser of:</p> <ul> <li>The difference between 20% of the previous week\u2019s gross profits and the total charges generated during the week; and</li> <li>The difference between 20% of gross profits and the total charges generated during the lifetime of the account.</li> </ul>"},{"location":"wagering/commission/#closing-thoughts","title":"Closing thoughts","text":"<p>While ultimately we would prefer to not charge fees, it is necessary to pay the sporting and racing bodies to be able to offer markets on the exchange as well as to invest in our people and infrastructure to ensure a bright future for the exchange. </p> <p>Australian and New Zealand customers are welcome to reach out to us to discuss fees and what we can do to help you at automation@betfair.com.au</p>"},{"location":"wagering/exactaQuinella/","title":"Racing Exotics Markets","text":"<p>A common market type used by traditional bookmakers is the Exacta or Quinella where a punter will bet on the combination of runners coming in first and second place in a given race. In the case of the exacta, this is the first and second runner in order, whereas the quinella disregards the order. This is traditionally done utilising a tote pool instead of fixed odds, however Betfair offers these markets in a traditional format where punters can back and lay these combinations at fixed odds prices. </p> <p>How could you go about calculating a fair price for a runner in these markets? Well, lets discuss how you can do that just by the using the price available for the runner in the regular 'WIN' market. This approach uses the data found here as an example</p>"},{"location":"wagering/exactaQuinella/#the-code","title":"The Code","text":"<p>Loading the data<pre><code>import pandas as pd\n\n# Read the data\nexacta_quinella = pd.read_csv('ANZ_Greyhounds_2024_02.csv')\n\n# Select relevant columns\nexacta_quinella = exacta_quinella[['LOCAL_MEETING_DATE','TRACK','STATE_CODE','RACE_NO','WIN_MARKET_ID','SELECTION_ID','TAB_NUMBER','SELECTION_NAME','BEST_AVAIL_BACK_AT_SCHEDULED_OFF']]\n\n# Calculate IMPLIED_WIN_PERCENTAGE\nexacta_quinella['IMPLIED_WIN_PERCENTAGE'] = 1/exacta_quinella['BEST_AVAIL_BACK_AT_SCHEDULED_OFF']\n\n# Calculate BMP (Back Market Percentage) (i.e. Overround)\nexacta_quinella['BMP'] = exacta_quinella.groupby('WIN_MARKET_ID')['IMPLIED_WIN_PERCENTAGE'].transform('sum')\n\n# Calculate SCALED_WIN_PROBABILITY\nexacta_quinella['SCALED_WIN_PROBABILITY'] = exacta_quinella['IMPLIED_WIN_PERCENTAGE']/exacta_quinella['BMP']\n</code></pre> Here we've calculated our efficient win price based on the best available price at the scheduled off by removing the effect of the overround.</p> <p>Assign the win probability of all other runners in the race<pre><code># Get unique values from the 'TAB_NUMBER' column\nunique_tab_numbers = exacta_quinella['TAB_NUMBER'].unique()\n\n# Create a list to store sub-dataframes\nsub_dataframes = []\n\n# Create sub-dataframes for each unique value of 'TAB_NUMBER'\nfor tab_number in unique_tab_numbers:\n    # Filter the main DataFrame based on 'TAB_NUMBER' == X\n    sub_df = exacta_quinella[exacta_quinella['TAB_NUMBER'] == tab_number][['WIN_MARKET_ID', 'SCALED_WIN_PROBABILITY']]\n\n    # Rename the 'IMPLIED_WIN_PERCENTAGE' column to 'IMPLIED_WIN_PERCENTAGE_{X}'\n    sub_df = sub_df.rename(columns={'SCALED_WIN_PROBABILITY': f'SCALED_WIN_PROBABILITY_{tab_number}'})\n\n    # Append the sub-dataframe to the list\n    sub_dataframes.append(sub_df)\n\n# Merge sub-dataframes back to the main dataframe on 'WIN_MARKET_ID'\nfor sub_df in sub_dataframes:\n    exacta_quinella = pd.merge(exacta_quinella, sub_df, on=['WIN_MARKET_ID'], how='left')\n\n# Now exacta_quinella DataFrame contains the new columns with the 'IMPLIED_WIN_PERCENTAGE_{X}' for each TAB_NUMBER\nexacta_quinella = exacta_quinella.fillna(0)\nexacta_quinella = exacta_quinella.drop_duplicates()\n</code></pre> Now lets calculate the probability of each runner finishing second to every other runner.</p> Calculate probability of winning given that a certain dog comes 2nd<pre><code># Calculate probability of winning given that a certain dog comes 2nd\nfor tab_number in unique_tab_numbers:\n    # Calculate the denominator (1 - IMPLIED_WIN_PERCENTAGE_{X})\n    denominator = 1 - exacta_quinella[f'SCALED_WIN_PROBABILITY_{tab_number}']\n\n    # Calculate the 'Probability_Second_Given_{X}_Wins' column, handling the case where 'IMPLIED_WIN_PERCENTAGE_{X}' is 0\n    exacta_quinella[f'Probability_Second_Given_{tab_number}_Wins'] = exacta_quinella['SCALED_WIN_PROBABILITY'] / denominator\n    exacta_quinella.loc[exacta_quinella[f'SCALED_WIN_PROBABILITY_{tab_number}'] == 0, f'Probability_Second_Given_{tab_number}_Wins'] = 0\n\n    # If 'TAB_NUMBER' equals tab_number, set Prob_Win_With_2nd_Dog_Being_{tab_number} to 0\n    exacta_quinella.loc[exacta_quinella['TAB_NUMBER'] == tab_number, f'Probability_Second_Given_{tab_number}_Wins'] = 0\n</code></pre> <p>Now lets create two dataframes containing each runner finishing in first and second positions</p> Setting up for creating the exotic combinations<pre><code># Define the columns to include in the new DataFrame\ncolumns_to_include = ['WIN_MARKET_ID','TAB_NUMBER']\n\n# Add the scaled win probability and probability second columns for each tab_number\nfor tab_number in unique_tab_numbers:\n    columns_to_include.extend([f'Probability_Second_Given_{tab_number}_Wins'])\n\n# Create the new DataFrame 'exacta' by selecting the specified columns from 'greyhounds'\nexacta_second = exacta_quinella[columns_to_include].copy()\nexacta_second.rename(columns={'TAB_NUMBER': 'exacta_second'}, inplace=True)\n\ncolumns_to_include = ['WIN_MARKET_ID','TAB_NUMBER','SCALED_WIN_PROBABILITY']\n\n# Create the new DataFrame 'exacta' by selecting the specified columns from 'greyhounds'\nexacta_first = exacta_quinella[columns_to_include].copy()\nexacta_first = exacta_first.drop_duplicates()\n\nexacta_first.rename(columns={'TAB_NUMBER': 'exacta_first'}, inplace=True)\n# Now 'exacta_seconds' DataFrame contains the required columns\nexacta_first.head\n</code></pre> <p>Next up is to assign every other runner to finish second to one runner with the runner's win probability in the exacta first table and to assign every other runner to beat to one runner with the runner's probability to finish second given that the other runner has won in the exacta second table. We'll then merge the tables at the end.</p> Assigning probability of first and second given first outcome probabilities to every outcome<pre><code>exacta_first_list = []\nexacta_second_list = []\n\n# Step 1: Iterate over unique tab_numbers in exacta_first\nfor tab_number in exacta_first['exacta_first'].unique():\n    # Step 2: Create a sub-dataframe from exacta_first\n    sub_exacta_first = exacta_first[exacta_first['exacta_first'] == tab_number].copy()\n    sub_exacta_first = exacta_first[['WIN_MARKET_ID', 'exacta_first', 'SCALED_WIN_PROBABILITY']].copy()\n    sub_exacta_first['exacta_second'] = tab_number\n    # Append sub-dataframe to the list\n    exacta_first_list.append(sub_exacta_first)\n\nexacta_first_combined = pd.DataFrame()\n# Concatenate all sub-dataframes into one dataframe called 'exacta_first_combined'\nfor i in exacta_first_list:\n    exacta_first_combined = pd.concat([exacta_first_combined,i], ignore_index=True)\n\nexacta_first_combined = exacta_first_combined.reset_index(drop=True)\nexacta_first_combined = exacta_first_combined[exacta_first_combined['SCALED_WIN_PROBABILITY'] != 0]\nexacta_first_combined = exacta_first_combined.dropna(subset=['SCALED_WIN_PROBABILITY'])\n\nexacta_second_combined = pd.DataFrame()\n\n# Step 3: Iterate over unique tab_numbers in exacta_second\nfor tab_number_2 in exacta_second['exacta_second'].unique():\n    # Step 4: Create a sub-dataframe from exacta_second\n    sub_exacta_second = exacta_second[exacta_second['exacta_second'] != tab_number_2].copy()\n    sub_exacta_second = sub_exacta_second[['WIN_MARKET_ID', 'exacta_second', f'Probability_Second_Given_{tab_number_2}_Wins']].copy()\n    sub_exacta_second.rename(columns={f'Probability_Second_Given_{tab_number_2}_Wins': 'Probability_Second_Given_Wins'}, inplace=True)\n    sub_exacta_second['exacta_first'] = tab_number_2\n    # Append sub-dataframe to the list\n    exacta_second_list.append(sub_exacta_second)\n\nfor i in exacta_second_list:\n    # Concatenate all sub-dataframes into one dataframe called 'exacta_second_combined'\n    exacta_second_combined = pd.concat([exacta_second_combined,i], ignore_index=True)\n\nexacta_second_combined = exacta_second_combined.reset_index(drop=True)\nexacta_second_combined = exacta_second_combined[exacta_second_combined['Probability_Second_Given_Wins'] != 0]\nexacta_second_combined = exacta_second_combined.dropna(subset=['Probability_Second_Given_Wins'])\n\nexacta_combinations = pd.merge(exacta_first_combined, exacta_second_combined, how='left', on=['WIN_MARKET_ID','exacta_first','exacta_second'])\n</code></pre> <p>Finally we'll calculate the exacta probability by multiplying these two probabilites together. The quinella probability is found by simply adding the probability of the exacta combination with reverse selections.</p> <p>E.g. Quinella Prob (1-2) = Exacta Prob (1-2) + Exacta Prob (2-1)</p> What are the odds?<pre><code># Create a new column for exacta combinations\nexacta_combinations['exacta_combination'] = exacta_combinations['exacta_first'].astype(str) + ' | ' + exacta_combinations['exacta_second'].astype(str)\n\n# Create a column for exacta odds\nexacta_combinations['exacta_probability'] = exacta_combinations['SCALED_WIN_PROBABILITY'] * exacta_combinations['Probability_Second_Given_Wins']\n# Assuming exacta_combinations is your DataFrame containing the combinations\nexacta_combinations = exacta_combinations[exacta_combinations['exacta_first'] != exacta_combinations['exacta_second']]\nexacta_combinations = exacta_combinations.dropna(subset=['exacta_probability'])\n\n# Create a copy of the DataFrame with columns 'WIN_MARKET_ID', 'exacta_second', 'exacta_first', 'exacta_odds'\nexacta_combinations_reverse = exacta_combinations[['WIN_MARKET_ID', 'exacta_second', 'exacta_first', 'exacta_probability']].copy()\nexacta_combinations_reverse.rename(columns={'exacta_probability':'reverse_exacta','exacta_first':'exacta_first_reverse','exacta_second':'exacta_second_reverse'},inplace=True)\n\n# Merge the reverse dataframe together with the original dataframe to find the exacta probability for the reverse combination\nexacta_combinations=pd.merge(exacta_combinations,exacta_combinations_reverse,how='left',left_on = ['WIN_MARKET_ID', 'exacta_first', 'exacta_second'],right_on = ['WIN_MARKET_ID', 'exacta_second_reverse', 'exacta_first_reverse'])\nexacta_combinations['exacta_odds'] = 1/exacta_combinations['exacta_probability']\n\n# Calculating quinella_odds by summing the exacta_odds for each combination\nexacta_combinations['quinella_probability'] = exacta_combinations['exacta_probability']+exacta_combinations['reverse_exacta']\nexacta_combinations['quinella_odds'] = 1/exacta_combinations['quinella_probability']\n\n# Discard extra columns\nexacta_combinations=exacta_combinations[['WIN_MARKET_ID','exacta_first','exacta_second','exacta_combination','exacta_probability','exacta_odds','quinella_probability','quinella_odds']]\n\n# Display the result\nprint(exacta_combinations)\nexacta_combinations.to_csv('exacta_combinations.csv',index=False)\n</code></pre>"},{"location":"wagering/exactaQuinella/#conclusion","title":"Conclusion","text":"<p>This approach can also be used to calculate place market probabilities by simply finding the average probability of the runner finishing second (though be aware that the complexity will be much higher for a 3rd or 4th place market than just a 2nd place market). Here we've just used a win market price to calculate a price for the exacta and quinella markets, but this approach can easily be applied to a model's rated place like the Betfair Hub Thoroughbred and Greyhound predictions models!</p> <p>If you have any questions or want to learn more, join the Discord server or Australian/New Zealand customers can email us at automation@betfair.com.au. </p> <p>We've also done this in a simple excel sheet to use for a single race - Download Excel</p>"},{"location":"wagering/hubPredictionsModel/","title":"Staking Strategy - Betfair Hub Thoroughbred Predictions Model","text":""},{"location":"wagering/hubPredictionsModel/#what-is-the-ratings-model","title":"What is the ratings model?","text":"<p>The Ratings Model provides rated prices for every runner at selected thoroughbred meetings across Australia, offering data driven horse racing predictions.</p> <p>Using Punting Form data, the model is able to calculate its prediction of each runner\u2019s percentage chance of winning that race, before then converting it to what it believes that runner\u2019s most accurate odds should be, a.k.a. that runner\u2019s rated price.</p> <p>Using these rated prices, Betfair customers can then compare the model\u2019s price to that of the price on offer on the Exchange to see which horses the horse racing predictor model believes is good and bad value.</p> <p>The model can help you identify value, outlining the value percentage of each runner in the race where applicable.</p>"},{"location":"wagering/hubPredictionsModel/#how-do-i-read-the-ratings","title":"How do I read the ratings?","text":"<p>Ratings are numerical measures that are an expected rating for each horse for the upcoming meeting. Prices are formulated by multiple factors including past performance, upcoming race conditions, trainer and jockey statistics and a range of other variables considered by our Data Scientists.</p> <p>NB: The Data Scientists\u2019 best-profiled runner is highlighted in grey. The \u2018Value\u2019 % is calculated looking at the difference in win probability based on the Analysts rated price v current Exchange market price.</p> <p>A 20% win rated chance vs 12.5% win chance in the market is a 7.5% difference in winning chance. This is calculated by: (1 / model odds) \u2013 (1 / price).</p>"},{"location":"wagering/hubPredictionsModel/#how-can-i-approach-betting-this-model","title":"How can I approach betting this model?","text":"<p>There are several ways in which you could slice and dice the historical ratings to try to come up with an edge that you could transform into a potentially profitable betting strategy. </p> <p>Some of the potential features you could include in such an analysis are:</p> <ul> <li>Race Type (e.g. Handicaps, Maidens, Group Races etc.)</li> <li>Race Distance (segmented into buckets like Sprint, Middle, Staying)</li> <li>Model Rank (How a horse has been rated compared to other horses in the race)</li> <li>Number of Runners (Does too many runners introduce too much variance?)</li> <li>Implied Value (The difference in implied winning probability between the rated price and the BSP)</li> <li>Track / State (Is the 10% MBR in NSW too difficult to find an edge with? Or are TAS races are bit weird?)</li> </ul> <p>All of these features you can implement using the data on our data listing page.</p> <p>Using an external data source, it is possible to further segment this information.</p>"},{"location":"wagering/hubPredictionsModel/#staking-strategies","title":"Staking Strategies","text":"<p>Once you've done some segmentation, then it's time to look at different staking strategies. There are a significant number of different ways in which you could bet on a selection and we'll explore a couple here.</p> <p>If you decide that a selection is worth backing, either for the confidence the model has in it or the under confidence the market has in it (corresponding to a higher available price than the rated price), then you could use a flat staking model, where you simply decide to put the same amount on every selection regardless of price.</p> <p>Proportional staking is also another option. This means taking a fixed stake for a whole market, say $100, and then assigning a proportion of this stake to the implied winning probability given by the rated price. If a horse has a rated price of $5, this implies a winning probability of 20% (1/5). So the proportional stake for a $100 total stake would be 20% of $100 or $20.</p> <p>If, on the other hand, you believe the selection is worth laying, because the market over values it, then you could use a fixed or proportional liability at BSP to lay it. There are others too like Kelly and the dreaded Martingale though they are beyond the scope of this article</p>"},{"location":"wagering/hubPredictionsModel/#analysis-of-the-horse-racing-predictions-model","title":"Analysis of the Horse Racing Predictions Model","text":"<p>All of the below graphs showcase the 2023 ratings and results from the predictions model for the top 2 horses in each race using implied value between the rated price and the BSP with negative value (good lays) on the left and positive value (good backs) on the right.</p> <p>A profitable strategy could include using a value range between a trough on the left and a peak on the right which shows the highest profitability. However, these results are before commission so may not be genuinely profitable. They are simply an illustration of how different variables can affect performance of a model in betting markets.</p> <p>We will showcase a flat back, proportional back and flat lay strategy.</p> <p>For the purposes of this analysis we have segmented data on the below definitions:</p> <ul> <li>Sprint &lt;=1200m</li> <li>Middle 1201m-2040m</li> <li>Staying &gt;2041m</li> <li>Win Restricted (Maiden, CL1-CL6)</li> <li>Handicap</li> <li>Other (WFA, Qlty, Grp1-3, Hrd, Stpl, 2-5yo, Listed, Cup)</li> </ul> <p>In these graphs, going from left to right is the cumulative profit associated with betting the top 2 horses in the race as chosen by the model by the implied value of the price against the BSP. We have previously done another tutorial where we analysed this model using python and the graphs there display profit against time (or number of bets)</p>"},{"location":"wagering/hubPredictionsModel/#all-races","title":"All Races","text":""},{"location":"wagering/hubPredictionsModel/#distance-buckets","title":"Distance Buckets","text":""},{"location":"wagering/hubPredictionsModel/#race-types","title":"Race Types","text":""},{"location":"wagering/hubPredictionsModel/#field-size","title":"Field Size","text":""},{"location":"wagering/hubPredictionsModel/#state-based","title":"State Based","text":""},{"location":"wagering/hubPredictionsModel/#practically-betting-a-value-range","title":"Practically betting a value range","text":"<p>If you were to bet a value range, say laying the Top 2 Runners in WA races where the value of the best available price is between -7% and 0%, how would you do that?</p> <p>Well, you would need to first calculate what those boundary prices would be. The upper bound of 0% is easy, that's simply the rated price. The lower bound of -7% requires a bit of calculation but comes out as:</p> <p> ('7' being your value target)</p> <p>For example, if a horse at Ascot had a rated price of $3.85 and the upper and lower boundaries for your betting price would be $3.03 and $3.85 (noting that the prices must be converted to valid Betfair ticks). Now to bet this practically, you can easily place a limit BSP lay bet with the 0% price as your upper bound. However, you cannot specify a minimum lay price, so how can you do that?</p> <p>Using a third party tool like Bet Angel or a bot directly using the Betfair API, the program can read the available prices and then only bet if the available price is within your range. We have some more information here on third party tools and using the API </p>"},{"location":"wagering/hubPredictionsModel/#conclusion","title":"Conclusion","text":"<p>We hope you found this introduction to model staking strategies interesting and informative. If you'd like to know more, please reach out to us at automation@betfair.com.au</p>"},{"location":"wagering/hubPredictionsModel/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"wagering/stakingMethods/","title":"Staking Methods and Bankroll Management","text":""},{"location":"wagering/stakingMethods/#workshop","title":"Workshop","text":"<p>This tutorial was written by Jason and was originally published on Github. It is shared here with his permission. </p> <p>Good staking systems should be used in combination with betting models. In this article, we\u2019ll go over the basics of staking and analyse some of the more popular staking strategies, outlining their pros and cons.</p>"},{"location":"wagering/stakingMethods/#why-is-staking-important","title":"Why is staking important?","text":"<p>Many bettors with models don\u2019t have sound bankroll management, to their detriment. A good staking system is primarily about balancing risk vs reward:</p> <ul> <li>If we are too risk inclined and bet aggressively, there is a good chance we will suffer losses</li> <li>We want to maximise our returns within our risk constraints</li> </ul> <p>Despite many resources on the web claiming that there is an optimal way to stake, the reality is that there is no one right answer and how you stake will be a personal one which will be affected by:</p> <ol> <li>Your return objectives</li> <li>Your risk tolerance</li> <li>Your emotional well-being (we\u2019re not robots)</li> </ol> <p></p> <p>Your return objectives and risk tolerance are usually the primary considerations when staking and will likely be in conflict with one another and hence will need to be balanced. Generally, chasing higher returns will mean greater levels of risk. Finding the right balance between risk and return that works for you is not an easy task and there will likely be a lot of iterating before getting it right.</p> <p>The third factor that influences your staking that most people overlook is the impact the size of your bets has on your emotional well-being. Remember that betting should not be stressful and if it is, it probably means that you\u2019re staking too big. Even if your bankroll can theoretically accommodate an increase in your stake sizes, be wary of whether your emotions can.</p>"},{"location":"wagering/stakingMethods/#common-staking-systems","title":"Common staking systems","text":"<p>Here, we\u2019ll go through some of the more common staking systems. However, rather than just describing each of the staking systems, we\u2019ll attempt to understand the risk/reward profile of each by simulating each staking system 10,000 times based on the following parameters:</p> <ul> <li>Starting bank of $1,000</li> <li>Ruin defined as bank going down to $5 or less (No longer can place a minimum back BSP bet)</li> <li>Objective is to quadruple our money</li> <li>Assume we have an edge of 5% on all bets</li> <li>Each bet is assumed to be between odds of 1.5 and 5, with bets being drawn in this range uniformly</li> </ul> <p>What we\u2019re most interested in seeing is how likely it is that the staking system will succeed on the objective and on average how many bets it takes for success/failure.</p>"},{"location":"wagering/stakingMethods/#overview-of-staking-methods","title":"Overview of staking methods","text":"Staking method Description 1. Fixed Staking <p>Fixed staking is probably the most simplistic betting system where we bet the same $ amount on every bet no matter what happens to our bankroll in the future</p><p></p><p>Stake = x where x is some fixed $ amount</p><p></p> 2a. Proportional Staking - % of bankroll <p>Proportional staking defines the bet stake size based on a percentage of your current bank. As your bank increases, the absolute value of your stakes increase and vice versa for when your bank decreases.</p><p></p><p>Stake = Bh where B is the current bank size<p>h is some chosen fixed %</p><p></p> 2b. Proportional Staking - bet to win certain amount <p>A variant of proportional staking, where for each bet we stake an amount such that if we win, the winning amount is a set % of our bankroll</p><p></p><p>Stake = Bk / (o-1) where B is the current bank size<p>k is some chosen fixed %</p><p>o is the odds in decimal form</p><p></p><p></p><p></p> <p>3. Martingale Staking</p><p></p> <p>The Martingale betting system is a progressive betting system whereby after every loss, we stake an amount that (if successful) will recoup all previous consecutive losses and win the original desired amount. After every win, the betting stakes are reset to the initial desired win amount.</p><p></p> <p>4. Kelly Staking</p><p></p> <p>Kelly staking or some variant is probably the most popular staking method amongst serious bettors. The Kelly staking formula determines bet size based on the odds of the bet and the assumed edge.</p><p></p><p>Stake = B[(o-1)p - (1-p)] / (o-1) where B is the current bank size<p>k is some chosen fixed %</p><p>o is the odds in decimal form</p><p></p>"},{"location":"wagering/stakingMethods/#proscons","title":"Pros/Cons","text":"Staking method Pros Cons 1. Fixed Staking <p>- Simple to use</p><p></p> <p>- Does not account for the size of the current bank</p><p>- Can take a long time to reach desired objective</p><p></p> 2a. Proportional Staking - % of bankroll <p>- Reduces the risk of ruin by reducing stakes when bank decreases</p><p>- Maintains the same relative risk-reward profile no matter the size of the bank</p><p></p> <p>- If you start on a bad run, it can take a long time to recoup your losses</p><p></p> 2b. Proportional Staking - bet to win certain amount <p>- Reduces the risk of ruin by reducing stakes when bank decreases</p><p>- Compared to proportionally staking a % of bankroll, this method tends to have lower odds slippage</p><p></p> <p>- If you start on a bad run, it can take a long time to recoup your losses</p><p></p> <p>5. Martingale Staking</p><p></p> <p>- Each betting \u201crun\u201d has a high probability of winning</p><p>- If on a string of good luck, your bank can increase quickly</p><p></p> <p>- It takes one bad run to wipe out your entire bank .. they do happen!</p><p>- Highest probability of ruin out of all the staking methods</p><p></p> <p>6. Kelly Staking</p><p></p> <p>- Theoretically, it gives the \u201cbest\u201d balance of risk vs reward</p><p>- Varies bet size based on assumed edge</p><p></p> <p>- Variance is high, bank can be very erratic</p><p>- Kelly assumes full knowledge of what our edge is. Given we will never know this with certainty, it may lead us to stake too much on low or negative edge bets.</p><p></p>"},{"location":"wagering/stakingMethods/#simulation-results","title":"Simulation results","text":"<p>The following simulation is based on artificial parameters set by Betfair to illustrate the effects of different staking strategies. Of course, gambling is not a reasonable strategy for financial betterment.</p> Staking method Simulation results 1. Fixed Staking <p>Bet amount: $20</p><p>% chance to achieve objective: 89% (2449 bets on average)</p><p>% chance of ruin: 11% (679 bets on average)</p><p></p><p>Example of simulation success</p><p></p><p></p> 2a. Proportional Staking <p>Bet amount: 2% of bankroll</p><p>% chance to achieve objective: 97% (1563 bets on average)</p><p>% chance of ruin: 3% (2893 bets on average)</p><p></p><p>Example of simulation success</p><p></p><p></p> 2b. Proportional Staking <p>Bet amount: to win 5% of bankroll</p><p>% chance to achieve objective: 96% (1074 bets on average)</p><p>% chance of ruin: 4% (1776 bets on average)</p><p></p><p>Example of simulation success</p><p></p><p></p> <p>3. Martingale Staking</p><p></p> <p>Bet amount: to win 3% of bankroll</p><p>% chance to achieve objective: 46% (408 bets on average)</p><p>% chance of ruin: 54% (1776 bets on average)</p><p></p><p>Example of simulation success</p><p></p><p></p> <p>4. Kelly Staking</p><p></p> <p>Bet amount: Kelly formula</p><p>% chance to achieve objective: 96% (1071  bets on average)</p><p>% chance of ruin: 4% (2040 bets on average)</p><p></p><p>Example of simulation success</p><p></p><p></p>"},{"location":"wagering/stakingMethods/#things-to-watch-out-for-when-choosing-a-staking-system","title":"Things to watch out for when choosing a staking system","text":""},{"location":"wagering/stakingMethods/#edge-cliff","title":"Edge Cliff","text":"<p>A common mistake when deploying a model is to be overly confident in your model\u2019s edge on the market. This is especially problematic when using Kelly staking where edge is a required input into the staking decision.</p> <p>One thing to be especially wary of when using a model\u2019s theoretical edge in staking decisions is the \u201cedge\u201d cliff where assumed edge and realised edge increase together up to a certain point before dropping markedly. The rationale behind why this happens is because we are unlikely to have perfect information, especially in mature markets where edges beyond a certain amount are unrealistic. Bets that are highlighted from a strategy as having unrealistic edges most likely arise due to the market knowing a key piece of information that we don\u2019t know (e.g. trial information in a race). Hence these bets could actually be our worst performing bets where edge is zero or negative. A Kelly betting system without acknowledging that this can occur can lead to very large decreases in our bank.</p> <p></p>"},{"location":"wagering/stakingMethods/#odds-slippage","title":"Odds Slippage","text":"<p>Before you productionise a model, it\u2019s always good to back-test the model against historic odds if possible. However, be careful about putting too much faith in the Return on Investment (ROI) of your back-test as the prices you\u2019ll actually get will unlikely be the same as the ones in your back-test (odds slippage). This is mainly due to:</p> <ol> <li>An exchange is dynamic, actions you take will affect the behaviours of other participants on the exchange influencing prices and volumes</li> <li>The back-tested ROI will unlikely scale linearly with increased bet sizes, as you stake more you will be forced to take worse prices, especially at larger odds</li> </ol> <p>That is not to say that you shouldn\u2019t back-test your models but you should build in buffers into the back-tests and make assumptions of reasonable but conservative slippage prices. Also, remember to test at small stakes initially, a model may be profitable but unprofitable at larger stakes .. don\u2019t jump to the wrong conclusion!</p>"},{"location":"wagering/stakingMethods/#complete-code","title":"Complete code","text":"<p>Run the code from your ide by using <code>py &lt;filename&gt;.py</code>, making sure you amend the path to point to your input data. </p> <p>Download from Github</p> <pre><code>import random\nimport numpy as np\n\ndef FixedStakesSim(stakeSize, bankroll, ruin, minBet, maxBet, edge, bankObj):\n    \"\"\"\n    Fixed Stakes Simulation\n\n    Parameters\n    ----------\n    stakeSize : float\n        stake size for each bet\n    bankroll : float\n        starting bankroll\n    ruin : float\n        amount where if bankroll drops below, you are ruined\n    minBet : float\n        the minimum bet odds which you will bet at\n    maxBet : float\n        the maximum bet odds which you will bet at\n    edge : float\n        assumed edge for each bet\n    bankObj : float\n        your bank objective expressed as a multiple of your starting bank\n    \"\"\"\n\n    betRange = maxBet - minBet\n    dynamicBank = bankroll\n\n    # Simulate bets until either objective is achieved or ruined, cap at 50,000 bets\n    for i in range(50000):\n        betOdds = round(random.uniform(0,1) * betRange + minBet,2)\n        winChance = (1 + edge)/betOdds\n        rand = random.uniform(0,1)\n        outcome = \"Exhausted Bets\"\n        if rand &lt; winChance:\n            betPnl = stakeSize * (betOdds - 1)\n        else:\n            betPnl = -stakeSize\n        dynamicBank = dynamicBank + betPnl\n        if dynamicBank &gt; bankObj * bankroll:\n            outcome = \"Objective Achieved\"\n            break\n        if dynamicBank &lt; ruin:\n            outcome = \"Ruined\"\n            break\n        if i == 49999:\n            outcome  = \"Bets exhausted\"\n    return [outcome, i]\n\nsimStore = []\n\n# Simulate 10,000 times\nfor i in range(10000):\n    simStore.append(FixedStakesSim(20, 1000, 5, 1.5, 5, 0.05, 4))\n\nprobSuccess = len([i[0] for i in simStore if i[0] == 'Objective Achieved']) / len(simStore)\nprobRuined = len([i[0] for i in simStore if i[0] == 'Ruined']) / len(simStore)\nnumbetsSuccess = np.median([i[1] for i in simStore if i[0] == 'Objective Achieved'])\nnumbetsRuined = np.median([i[1] for i in simStore if i[0] == 'Ruined'])\n\n\ndef ProportionalStakesSimA(stakepct, bankroll, ruin, minBet, maxBet, edge, bankObj):\n    \"\"\"\n    Proportional stakes simulation (staking a % of bankroll)\n\n    Parameters\n    ----------\n    stakepct : float\n        the % of your dynamic bankroll that you're staking\n    bankroll : float\n        starting bankroll\n    ruin : float\n        amount where if bankroll drops below, you are ruined\n    minBet : float\n        the minimum bet odds which you will bet at\n    maxBet : float\n        the maximum bet odds which you will bet at\n    edge : float\n        assumed edge for each bet\n    bankObj : float\n        your bank objective expressed as a multiple of your starting bank\n    \"\"\"\n\n    betRange = maxBet - minBet\n    dynamicBank = bankroll\n    for i in range(50000):\n        stakeSize = max(stakepct * dynamicBank, ruin)\n        betOdds = round(random.uniform(0,1) * betRange + minBet,2)\n        winChance = (1 + edge)/betOdds\n        rand = random.uniform(0,1)\n        if rand &lt; winChance:\n            betPnl = stakeSize * (betOdds - 1)\n        else:\n            betPnl = -stakeSize\n        dynamicBank = dynamicBank + betPnl\n        if dynamicBank &gt; bankObj * bankroll:\n            outcome = \"Objective Achieved\"\n            break\n        if dynamicBank &lt; ruin:\n            outcome = \"Ruined\"\n            break\n        if i == 49999:\n            outcome  = \"Bets exhausted\"\n    return [outcome, i]\n\nsimStore = []\n\n# Simulate 10,000 times\nfor i in range(10000):\n    simStore.append(ProportionalStakesSimA(0.02, 1000, 5, 1.5, 5, 0.05, 4))\n\nprobSuccess = len([i[0] for i in simStore if i[0] == 'Objective Achieved']) / len(simStore)\nprobRuined = len([i[0] for i in simStore if i[0] == 'Ruined']) / len(simStore)\nnumbetsSuccess = np.median([i[1] for i in simStore if i[0] == 'Objective Achieved'])\nnumbetsRuined = np.median([i[1] for i in simStore if i[0] == 'Ruined'])\n\n\ndef ProportionalStakesSimB(winpct, bankroll, ruin, minBet, maxBet, edge, bankObj):\n    \"\"\"\n    Proportional stakes simulation (staking to win a certain % of bankroll)\n\n    Parameters\n    ----------\n    winpct : float\n        the % of your dynamic bankroll that you're staking to win\n    bankroll : float\n        starting bankroll\n    ruin : float\n        amount where if bankroll drops below, you are ruined\n    minBet : float\n        the minimum bet odds which you will bet at\n    maxBet : float\n        the maximum bet odds which you will bet at\n    edge : float\n        assumed edge for each bet\n    bankObj : float\n        your bank objective expressed as a multiple of your starting bank\n    \"\"\"\n\n    betRange = maxBet - minBet\n    dynamicBank = bankroll\n    for i in range(50000):\n        betOdds = round(random.uniform(0,1) * betRange + minBet,2)\n        stakeSize = max((dynamicBank * winpct) / (betOdds - 1), ruin)\n        winChance = (1 + edge)/betOdds\n        rand = random.uniform(0,1)\n        if rand &lt; winChance:\n            betPnl = stakeSize * (betOdds - 1)\n        else:\n            betPnl = -stakeSize\n        dynamicBank = dynamicBank + betPnl\n        if dynamicBank &gt; bankObj * bankroll:\n            outcome = \"Objective Achieved\"\n            break\n        if dynamicBank &lt; ruin:\n            outcome = \"Ruined\"\n            break\n        if i == 49999:\n            outcome  = \"Bets exhausted\"\n    return [outcome, i]\n\nsimStore = []\n\n# Simulate 10,000 times\nfor i in range(10000):\n    simStore.append(ProportionalStakesSimB(0.05, 1000, 5, 1.3, 5, 0.05, 4))\n\nprobSuccess = len([i[0] for i in simStore if i[0] == 'Objective Achieved']) / len(simStore)\nprobRuined = len([i[0] for i in simStore if i[0] == 'Ruined']) / len(simStore)\nnumbetsSuccess = np.median([i[1] for i in simStore if i[0] == 'Objective Achieved'])\nnumbetsRuined = np.median([i[1] for i in simStore if i[0] == 'Ruined'])\n\n\n\ndef Martingale(winamt, bankroll, ruin, minBet, maxBet, edge, bankObj):\n    \"\"\"\n    Martingale staking simulation\n\n    Parameters\n    ----------\n    winamt : float\n        the desired win amount for each betting run\n    bankroll : float\n        starting bankroll\n    ruin : float\n        amount where if bankroll drops below, you are ruined\n    minBet : float\n        the minimum bet odds which you will bet at\n    maxBet : float\n        the maximum bet odds which you will bet at\n    edge : float\n        assumed edge for each bet\n    bankObj : float\n        your bank objective expressed as a multiple of your starting bank\n    \"\"\"\n\n    betRange = maxBet - minBet\n    dynamicBank = bankroll\n    martingale_win = 1\n    martingale_progressive_loss = 0\n    for i in range(50000):\n        betOdds = round(random.uniform(0,1) * betRange + minBet,2)\n        stakeSize = max((winamt - martingale_progressive_loss) / (betOdds - 1), ruin)\n        winChance = (1 + edge)/betOdds\n        rand = random.uniform(0,1)\n        outcome = \"Exhausted Bets\"\n        if rand &lt; winChance:\n            betPnl = stakeSize * (betOdds - 1)\n            martingale_win = 1\n            martingale_progressive_loss = 0\n        else:\n            betPnl = -stakeSize\n            martingale_win = 0\n            martingale_progressive_loss =  martingale_progressive_loss - stakeSize\n        dynamicBank = dynamicBank + betPnl\n        if dynamicBank &gt; bankObj * bankroll:\n            outcome = \"Objective Achieved\"\n            break\n        if dynamicBank &lt; ruin:\n            outcome = \"Ruined\"\n            break\n    return [outcome, i]\n\nsimStore = []\n\n# Simulate 10,000 times\nfor i in range(10000):\n    simStore.append(Martingale(20, 1000, 5, 1.5, 5, 0.05, 4))\n\nprobSuccess = len([i[0] for i in simStore if i[0] == 'Objective Achieved']) / len(simStore)\nprobRuined = len([i[0] for i in simStore if i[0] == 'Ruined']) / len(simStore)\nnumbetsSuccess = np.median([i[1] for i in simStore if i[0] == 'Objective Achieved'])\nnumbetsRuined = np.median([i[1] for i in simStore if i[0] == 'Ruined'])\n\n\ndef KellyStake(bankroll, ruin, minBet, maxBet, edge, bankObj, partialKelly):\n    \"\"\"\n    Kelly staking simulation\n\n    Parameters\n    ----------\n\n    bankroll : float\n        starting bankroll\n    ruin : float\n        amount where if bankroll drops below, you are ruined\n    minBet : float\n        the minimum bet odds which you will bet at\n    maxBet : float\n        the maximum bet odds which you will bet at\n    edge : float\n        assumed edge for each bet\n    bankObj : float\n        your bank objective expressed as a multiple of your starting bank\n    partialKelly: float\n        proportion of kelly staking to bet\n    \"\"\"\n\n    betRange = maxBet - minBet\n    dynamicBank = bankroll\n    for i in range(50000):\n        betOdds = round(random.uniform(0,1) * betRange + minBet,2)\n        winChance = (1 + edge)/betOdds\n        stakeSize = max(((((betOdds - 1) * winChance) - (1-winChance)) / (betOdds - 1)) * dynamicBank * partialKelly,ruin) \n        rand = random.uniform(0,1)\n        outcome = \"Exhausted Bets\"\n        if rand &lt; winChance:\n            betPnl = stakeSize * (betOdds - 1)\n        else:\n            betPnl = -stakeSize\n        dynamicBank = dynamicBank + betPnl\n        if dynamicBank &gt; bankObj * bankroll:\n            outcome = \"Objective Achieved\"\n            break\n        if dynamicBank &lt; ruin:\n            outcome = \"Ruined\"\n            break\n    return [outcome, i]\n\nsimStore = []\n\n# Simulate 10,000 times\nfor i in range(10000):\n    simStore.append(KellyStake(1000, 5, 1.5, 5 ,0.05, 4, 1))\n\nprobSuccess = len([i[0] for i in simStore if i[0] == 'Objective Achieved']) / len(simStore)\nprobRuined = len([i[0] for i in simStore if i[0] == 'Ruined']) / len(simStore)\nnumbetsSuccess = np.median([i[1] for i in simStore if i[0] == 'Objective Achieved'])\nnumbetsRuined = np.median([i[1] for i in simStore if i[0] == 'Ruined'])\n</code></pre>"},{"location":"wagering/stakingMethods/#disclaimer","title":"Disclaimer","text":"<p>Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.</p>"},{"location":"wagering/valueAndOdds/","title":"Value and Odds","text":"<p>Article</p> <p>This article has been copied from the Betfair Hub and was written by Daniel O'Sullivan from the Ratings Bureau</p>"},{"location":"wagering/valueAndOdds/#market-price-and-winning-chance","title":"Market Price and Winning Chance","text":"<p>All punters are familiar with the concept that the price of a horse determines how much money you get back per $1 bet. For example, if you have a $20 win bet on a horse at a fixed price of $5 and the horse wins, you are returned $100 ($20 x $5)\u2026 for a profit of $80.</p> <p>However the price of a horse also has another very important meaning. It reflects the market\u2019s opinion on the winning chance of that horse in the race.</p> <p>THE EASY WAY TO APPROXIMATE THE WINNING CHANCE OF A HORSE IS TO TAKE 100 AND DIVIDE IT BY THE MARKET PRICE</p> <p>For Example:</p> <ul> <li>A horse that is $1.80 in the market has a 100 / 1.8 = 55.55% chance of winning</li> <li>A horse that is $2.20 in the market has a 100 / 2.2 = 45.45% chance of winning</li> <li>A horse that is $5.00 in the market has a 100 / 5 = 20% chance of winning</li> <li>A horse that is $11 in the market has a 100 / 11 = 9.1% chance of winning</li> <li>A horse that is $26 in the market has a 100 / 26 = 3.85% chance of winning</li> </ul> <p>Close to jump time the total BF Exchange market is very close to 100% (see below for an explanation of market percentage) so the prices provide a direct indication of the markets opinion on winning chances.</p> <p>Bookmaker fixed odds market add up to much greater than 100% (usually 115% or more) so a direct calculation using the method above will slightly overestimate the winning chance of each horse (the longer the price of the horse the greater the gap.)</p>"},{"location":"wagering/valueAndOdds/#what-is-market-percentageoverround","title":"What is market percentage/overround?","text":"<p>Market percentage or overround is simply the sum of the winning chance calculated for each horse in the race (using the above method.)  The table below shows a theoretical eight horse fixed odds bookmaker market sorted from highest winning chance to the lowest winning chance. Against each market price you can see the % winning chance and how much you would need to bet on that horse to collect $100</p> Price Win% Bet Return 2.8 35.70% 35.7 100 4.4 22.70% 22.7 100 5.5 18.2% 18.2 100 7 14.3% 14.3 100 10 10% 10 100 17 5.9% 5.9 100 21 4.8% 4.8 100 26 3.8% 3.8 100 115.40% $115.40 <p>There are a few things to note about market percentage and this example:</p> <ul> <li>The total of all winning chances as expressed by this market is 115.4%. Another way to look at it is to see that if you backed every horse to collect $100, you would need to outlay $115.4\u2026 obviously a losing bet.  If market percentages were less than 100% it would mean punters could back every horse in the market and make a profit, regardless of who the winner was.</li> <li>The higher the market percentage the greater the average gap between each horses expressed chance of winning and their actual chance of winning. Sometimes you will hear or read about punters complaining about high market percentages and that\u2019s because they provide a greater advantage to bookmakers.</li> <li>Conversely, the lower the market percentage the closer each horse is on average to its true winning chance and the better it is for punters. This is one reason why the Exchange market can be benefit for punters. The overall market percentage (even after winning commission) is typically much lower than fixed odds markets.</li> </ul>"},{"location":"wagering/valueAndOdds/#what-is-value","title":"What is Value?","text":"<p>VALUE IS THE TERM USED TO DESCRIBE THE RELATIONSHIP BETWEEN A HORSE\u2019S WINNING CHANCE EXPRESSED BY THE BETTING MARKET AND ITS REAL WINNING CHANCE</p> <ul> <li>Good value means the horse\u2019s market price is greater than its real chance of winning (often referred to as \u201cover the odds.\u201d) For example, a horse at $2.80 in the market (implied 35.7% chance of winning) that is actually a 40% chance of winning (implied $2.50 market price), is considered good value. $2.80 is considered \u201cover the odds\u201d as the correct price should be $2.50.</li> <li>Poor value means the horse\u2019s market price is less than its real chance of winning (often referred to as \u201cunder the odds.\u201d) For example, a horse at $3.00 in the market (implied 33.3% chance of winning) that is actually a 28.6% chance of winning (implied $3.50 price), is considered poor value. $3.00 is considered to be \u201cunder the odds\u201d as the correct price should be $3.50. Of course the \u201cactual winning chance\u201d of a horse is very much a subjective concept. What one punter views as good value will be viewed as poor value by another punter. It\u2019s only through a very large sample of bets or assessments that you can start to ascertain whether your overall judgement about value is correct or at least sufficient enough to show a profit.</li> </ul>"},{"location":"wagering/valueAndOdds/#profiting-from-good-and-bad-value","title":"Profiting from good and bad value","text":"<p>The benefit of the Betfair Exchange is that it gives you the opportunity to profit from horses you have a strong opinion on, whether it be that a horse is good value or particularly poor value.</p> <p>Horses that are good value (i.e. available at a better price than their real winning chance) are targets to BACK.</p> <p>For example a horse that has a 40% chance of winning (implied correct price of 100 / 40 = $2.50) that is available in the market at $3.00 is providing a value edge of (40 x 3.0) = 120 \u2013 100 = 20%. In the long-term you would make 20% profit on turnover betting in this scenario, less commission (assuming your judgement is correct.)</p> <p>Horses you believe that are poor value and under their real price are good targets to LAY, providing you can lay them at the right price.</p> <p>For example a horse that in your opinion has a 30% chance of winning (implied correct price of 100 / 30 = $3.33) that is available to lay at $2.50 has a value edge of 25% in your favour. In the long-term you would make 25% profit on total bets held (less commission).</p>"},{"location":"wagering/valueAndOdds/#betting-is-all-about-probability","title":"Betting is all about probability","text":"<p>As a punter it\u2019s important to view betting on racing or any other event as a battle of probability. There are no absolutes such as \u201cthis horse has no chance\u201d or \u201cthis horse is guaranteed to get a perfect run\u201d etc. Everything has a probability or certain percentage chance of occurring.</p> <p>The key to successful betting is to find opportunities where the probability of winning expressed by the market is different to what you believe the real probability is. When it comes to betting on the BF Exchange that can mean finding horses that betting market has underestimated, which are good BACK opportunities, or horses the betting market is overestimating, which present as good LAY prospects.</p>"}]}