{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Betfair is one of the only betting platforms in the world that demands winning clients. Unlike bookies, we don\u2019t ban you when you succeed. We need you, and we want you to be able to keep improving your strategies so you win more. We're here to help you in your automation journey, and this site is dedicated to sharing the tools and resources you need to succeed in this journey. Accessing our API \u00b6 As you may already know, Betfair has its own API to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software. There are lots of resources available to support you in accessing the API effectively: Creating & activating your app key Developer Program knowledge base Dev Docs Developer Forum where you can share your experiences and find out what's worked for other clients Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Our Datascientists' repos for using R and Python to access the API The UK\u2019s Github repo including libraries for other languages API access Customers are able to access our API to embed it into their programs and automate their strategies If you're a programmer there are lots of resources around to help Historic Data \u00b6 We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport \u2013 you can take a look at our explanation of the different data sources if you\u2019re not quite sure where to start. We\u2019ve also shared some tips on learning to create predictive models using this data , which link in with the models shared in the modelling section . Betfair data sources Accessing the official Historic Data site Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API The Stream API dev docs are the best source of information for interpreting the data from the Historic Data site Historic BSP csv files Historic Betfair data There is a lot of historical price data available for all makrets offered on the Exchange, ranging from aggregate, market-level csv files to complete JSON recreations of API Stream data Using third party tools for automation \u00b6 Whilst the following tools are not custom built for your approach, they do allow you to automate your betting strategies. You just set up specific betting conditions and let the third party application do the work for you. Bet Angel and Gruss Betting Assistant are the most popular third party tools. We\u2019re putting together a collection of articles on how to use some of these third party tools to automate basic strategies, to give you a starting point that you can then work from. Bet Angel Excel Automation Ratings automation Multiple market ratings Market favourite automation Tipping automation Automating simultaneous markets Gruss Ratings automation Automating simultaneous markets Cymatic Trader Ratings automation BF Bot Manager Double or Bust MarketFeeder Pro 2-6 Staking BfExplorer intro to automation in Bfexplorer Data modelling \u00b6 An intro to building a predictive model Open source predictive models built by our in-house Data Scientists Modelling the Aus Open EPL modelling series AFL modelling series Brownlow modelling tutorial Predictive modelling Many of our most successful customers use predictive models as the basis for their betting strategies Inspiration & information \u00b6 The Banker: A Quant's AFL Betting Strategy The Mathematician 'Back and Lay' is a subreddit dedicated to discussing trading techniques Our Twitter community is really active Staking Plans and Strategies Staking and Money Management Some extra info There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy. Need extra help? \u00b6 If you\u2019re looking for bespoke advice or have extra questions, please contact us at bdp@betfair.com.au . We have a dedicated in-house resource that is here to automate your betting strategies.","title":"The Automation Hub"},{"location":"#accessing-our-api","text":"As you may already know, Betfair has its own API to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software. There are lots of resources available to support you in accessing the API effectively: Creating & activating your app key Developer Program knowledge base Dev Docs Developer Forum where you can share your experiences and find out what's worked for other clients Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Our Datascientists' repos for using R and Python to access the API The UK\u2019s Github repo including libraries for other languages API access Customers are able to access our API to embed it into their programs and automate their strategies If you're a programmer there are lots of resources around to help","title":"Accessing our API"},{"location":"#historic-data","text":"We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport \u2013 you can take a look at our explanation of the different data sources if you\u2019re not quite sure where to start. We\u2019ve also shared some tips on learning to create predictive models using this data , which link in with the models shared in the modelling section . Betfair data sources Accessing the official Historic Data site Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API The Stream API dev docs are the best source of information for interpreting the data from the Historic Data site Historic BSP csv files Historic Betfair data There is a lot of historical price data available for all makrets offered on the Exchange, ranging from aggregate, market-level csv files to complete JSON recreations of API Stream data","title":"Historic Data"},{"location":"#using-third-party-tools-for-automation","text":"Whilst the following tools are not custom built for your approach, they do allow you to automate your betting strategies. You just set up specific betting conditions and let the third party application do the work for you. Bet Angel and Gruss Betting Assistant are the most popular third party tools. We\u2019re putting together a collection of articles on how to use some of these third party tools to automate basic strategies, to give you a starting point that you can then work from. Bet Angel Excel Automation Ratings automation Multiple market ratings Market favourite automation Tipping automation Automating simultaneous markets Gruss Ratings automation Automating simultaneous markets Cymatic Trader Ratings automation BF Bot Manager Double or Bust MarketFeeder Pro 2-6 Staking BfExplorer intro to automation in Bfexplorer","title":"Using third party tools for automation"},{"location":"#data-modelling","text":"An intro to building a predictive model Open source predictive models built by our in-house Data Scientists Modelling the Aus Open EPL modelling series AFL modelling series Brownlow modelling tutorial Predictive modelling Many of our most successful customers use predictive models as the basis for their betting strategies","title":"Data modelling"},{"location":"#inspiration-information","text":"The Banker: A Quant's AFL Betting Strategy The Mathematician 'Back and Lay' is a subreddit dedicated to discussing trading techniques Our Twitter community is really active Staking Plans and Strategies Staking and Money Management Some extra info There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy.","title":"Inspiration &amp; information"},{"location":"#need-extra-help","text":"If you\u2019re looking for bespoke advice or have extra questions, please contact us at bdp@betfair.com.au . We have a dedicated in-house resource that is here to automate your betting strategies.","title":"Need extra help?"},{"location":"api/apiPythontutorial/","text":"Betfair API tutorial in Python \u00b6 This tutorial will walk you through the process of connecting to Betfair's API, grabbing data and placing a bet in Python. It will utilise the betfairlightweight Python library. Requirements \u00b6 This tutorial will assume that you have an API app key. If you don't, please follow the steps outlined here . This tutorial will also assume that you have a basic understanding of what an API is. For a summary in layman's terms, read this article . Quick Links \u00b6 Here are some other useful links for accessing our API: How to create an API app key Developer Docs - the official dev docs for Betfair's API Sports API Visualiser - Useful for exploring what the API has to offer Account API Visualiser Examples using betfairlightweight There's a more complete list of resources here Getting Started \u00b6 Setting Up Your Certificates \u00b6 To use the API securely, Betfair recommends generating certificates. The betfairlightweight package requires this to login non-interactively. For detailed instructions on how to generate certificates on a windows machine, follow the instructions outlined here . For alternate instructions for Windows, or for Mac/Linux machines, follow the instructions outlined here . You should then create a folder for your certs, perhaps named 'certs' and grab the path loation. Installing betfairlightweight \u00b6 We also need to install betfairlightweight . To do this, simply use pip install betfairlightweight in the cmd prompt/terminal. If this doesn't work, you will have to Google your error. If you're just starting out with Python, you may have to add Python to your environment variables. Sending Requests to the API \u00b6 Log into the API Client \u00b6 Now we're finally ready to log in and use the API. First, we create an APIClient object and then log in. To log in, we'll need to specify where we put our certs. In this example, I'll put them in a folder named 'certs', on my desktop. You'll also need to change the username , password and app_key variables to your own. In [206]: # Import libraries import betfairlightweight from betfairlightweight import filters import pandas as pd import numpy as np import os import datetime import json # Change this certs path to wherever you're storing your certificates certs_path = r'C:\\Users\\wardj\\Desktop\\certs' # Change these login details to your own my_username = \"your_username\" my_password = \"your_password\" my_app_key = \"your_app_key\" trading = betfairlightweight . APIClient(username = my_username, password = my_password, app_key = my_app_key, certs = certs_path) trading . login() Out[206]: < LoginResource > Get Event IDs \u00b6 Betfair's API has a number of operations. For example, if you want to list the market book for a market, you would use the listMarketBook operation. These endpoints are shown in the Sports API Visualiser and in the docs. They are also listed below: Sports API \u00b6 listEventTypes listCompetitions listTimeRanges listEvents listMarketTypes listCountries listVenues listMarketCatalogue listMarketBook listRunnerBook placeOrders cancelOrders updateOrders replaceOrders listCurrentOrders listClearedOrders listMarketProfitAndLoss The Account Operations API operations/endpoints can be found here . First we need to grab the 'Event Type Id'. Each sport has a different ID. Below we will find the ids for all sports by requesting the event_type_ids without a filter. In [43]: # Grab all event type ids. This will return a list which we will iterate over to print out the id and the name of the sport event_types = trading . betting . list_event_types() sport_ids = pd . DataFrame({ 'Sport' : [event_type_object . event_type . name for event_type_object in event_types], 'ID' : [event_type_object . event_type . id for event_type_object in event_types] }) . set_index( 'Sport' ) . sort_index() sport_ids Out[43]: Sport ID American Football 6423 Athletics 3988 Australian Rules 61420 Baseball 7511 Basketball 7522 Boxing 6 Chess 136332 Cricket 4 Cycling 11 Darts 3503 Esports 27454571 Financial Bets 6231 Gaelic Games 2152880 Golf 3 Greyhound Racing 4339 Handball 468328 Horse Racing 7 Ice Hockey 7524 Mixed Martial Arts 26420387 Motor Sport 8 Netball 606611 Politics 2378961 Rugby League 1477 Rugby Union 5 Snooker 6422 Soccer 1 Special Bets 10 Tennis 2 Volleyball 998917 If we just wanted to get the event id for horse racing, we could use the filter function from betfairlightweight as shown in the examples and below. In [50]: # Filter for just horse racing horse_racing_filter = `betfairlightweight` . filters . market_filter(text_query = 'Horse Racing' ) # This returns a list horse_racing_event_type = trading . betting . list_event_types( filter = horse_racing_filter) # Get the first element of the list horse_racing_event_type = horse_racing_event_type[ 0 ] horse_racing_event_type_id = horse_racing_event_type . event_type . id print (f \"The event type id for horse racing is {horse_racing_event_type_id}\" ) The event type id for horse racing is 7 Get Competition IDs \u00b6 Sometimes you may want to get markets based on the competition. An example may be the Brownlow medal, or the EPL. Let's have a look at all the soccer competitions over the next week and filter to only get the EPL Competition ID. In [90]: # Get a datetime object in a week and convert to string datetime_in_a_week = (datetime . datetime . utcnow() + datetime . timedelta(weeks =1 )) . strftime( \"%Y-%m- %d T%TZ\" ) # Create a competition filter competition_filter = `betfairlightweight` . filters . market_filter( event_type_ids = [ 1 ], # Soccer's event type id is 1 market_start_time = { 'to' : datetime_in_a_week }) # Get a list of competitions for soccer competitions = trading . betting . list_competitions( filter = competition_filter ) # Iterate over the competitions and create a dataframe of competitions and competition ids soccer_competitions = pd . DataFrame({ 'Competition' : [competition_object . competition . name for competition_object in competitions], 'ID' : [competition_object . competition . id for competition_object in competitions] }) In [94]: # Get the English Premier League Competition ID soccer_competitions[soccer_competitions . Competition . str . contains( 'English Premier' )] Out[94]: # Competition ID 116 English Premier League 10932509 Get Upcoming Events \u00b6 Say you want to get all the upcoming events for Thoroughbreads for the next 24 hours. We will use the listEvents operation for this. First, as before, we define a market filter, and then using the betting method from our trading object which we defined earlier. In [207]: # Define a market filter thoroughbreds_event_filter = `betfairlightweight` . filters . market_filter( event_type_ids = [horse_racing_event_type_id], market_countries = [ 'AU' ], market_start_time = { 'to' : (datetime . datetime . utcnow() + datetime . timedelta(days =1 )) . strftime( \"%Y-%m- %d T%TZ\" ) } ) # Print the filter thoroughbreds_event_filter Out[207]: { 'eventTypeIds' : [ '7' ], 'marketCountries' : [ 'AU' ], 'marketStartTime' : { 'to' : '2018-10-26T22:25:00Z' }} In [208]: # Get a list of all thoroughbred events as objects aus_thoroughbred_events = trading . betting . list_events( filter = thoroughbreds_event_filter ) # Create a DataFrame with all the events by iterating over each event object aus_thoroughbred_events_today = pd . DataFrame({ 'Event Name' : [event_object . event . name for event_object in aus_thoroughbred_events], 'Event ID' : [event_object . event . id for event_object in aus_thoroughbred_events], 'Event Venue' : [event_object . event . venue for event_object in aus_thoroughbred_events], 'Country Code' : [event_object . event . country_code for event_object in aus_thoroughbred_events], 'Time Zone' : [event_object . event . time_zone for event_object in aus_thoroughbred_events], 'Open Date' : [event_object . event . open_date for event_object in aus_thoroughbred_events], 'Market Count' : [event_object . market_count for event_object in aus_thoroughbred_events] }) aus_thoroughbred_events_today Out[208]: # Event Name Event ID Event Venue Country Code Time Zone Open Date Market Count 0 MVal (AUS) 26 th Oct 28971066 Moonee Valley AU Australia/Sydney 2018-10-26 07:30:00 24 1 Newc (AUS) 26 th Oct 28974559 Newcastle AU Australia/Sydney 2018-10-26 07:07:00 20 2 Bath (AUS) 26 th Oct 28974547 Bathurst AU Australia/Sydney 2018-10-26 02:43:00 16 3 Cant (AUS) 26 th Oct 28974545 Canterbury AU Australia/Sydney 2018-10-26 07:15:00 16 4 Scne (AUS) 26 th Oct 28973942 Scone AU Australia/Sydney 2018-10-26 02:25:00 16 5 Gawl (AUS) 26 th Oct 28974550 Gawler AU Australia/Adelaide 2018-10-26 04:00:00 16 6 Gatt (AUS) 26 th Oct 28974549 Gatton AU Australia/Queensland 2018-10-26 01:55:00 16 7 GlPk (AUS) 26 th Oct 28974562 Gloucester Park AU Australia/Perth 2018-10-26 09:10:00 20 8 Hoba (AUS) 26 th Oct 28974563 Hobart AU Australia/Sydney 2018-10-26 05:23:00 18 9 Echu (AUS) 26 th Oct 28974016 Echuca AU Australia/Sydney 2018-10-26 01:30:00 18 10 Melt (AUS) 26 th Oct 28974560 Melton AU Australia/Sydney 2018-10-26 07:18:00 18 11 MVal (AUS) 26 th Oct 28921730 None AU Australia/Sydney 2018-10-26 11:00:00 1 12 Redc (AUS) 26 th Oct 28974561 Redcliffe AU Australia/Queensland 2018-10-26 02:17:00 16 13 SCst (AUS) 26 th Oct 28974149 Sunshine Coast AU Australia/Queensland 2018-10-26 06:42:00 20 Get Market Types \u00b6 Say we want to know what market types a certain event is offering. To do this, we use the listMarketTypes operation. Let's take the Moonee Valley event from above (ID: 28971066). As this is a horse race we would expect that it would have Win and Place markets. In [209]: # Define a market filter market_types_filter = `betfairlightweight` . filters . market_filter(event_ids = [ '28971066' ]) # Request market types market_types = trading . betting . list_market_types( filter = market_types_filter ) # Create a DataFrame of market types market_types_mooney_valley = pd . DataFrame({ 'Market Type' : [market_type_object . market_type for market_type_object in market_types], }) market_types_mooney_valley Out[209]: Market Type 0 OTHER_PLACE 1 PLACE 2 WIN Get Market Catalogues \u00b6 If we want to know the various market names that there are for a particular event, as well as how much has been matched on each market, we want to request data from the listMarketCatalogue operation. We can provide a number of filters, including the Competition ID, the Event ID, the Venue etc. to the filter. We must also specify the maximum number of results, and if we want additional data like the event data or runner data, we can also request that. For a more comprehensive understanding of the options for filters and what we can request, please have a look at the Sports API Visualiser . The options listed under market filter should be put into a filter, whilst the others should be arguments to the relevant operation function in betfairlightweight . For example, if we want all the markets for Moonee Valley, we should use the following filters and arguments. In [210]: market_catalogue_filter = `betfairlightweight` . filters . market_filter(event_ids = [ '28971066' ]) market_catalogues = trading . betting . list_market_catalogue( filter = market_catalogue_filter, max_results = '100' , sort = 'FIRST_TO_START' ) # Create a DataFrame for each market catalogue market_types_mooney_valley = pd . DataFrame({ 'Market Name' : [market_cat_object . market_name for market_cat_object in market_catalogues], 'Market ID' : [market_cat_object . market_id for market_cat_object in market_catalogues], 'Total Matched' : [market_cat_object . total_matched for market_cat_object in market_catalogues], }) market_types_mooney_valley Out[210]: Market Name Market ID Total Matched 0 4 TBP 1.150090094 1 To Be Placed 1.150090092 2 R1 1000m 3yo 1.150090091 3 4 TBP 1.150090101 4 To Be Placed 1.150090099 5 R2 2040m Hcap 1.150090098 6 To Be Placed 1.150090106 7 R3 1500m Hcap 1.150090105 8 4 TBP 1.150090108 9 To Be Placed 1.150090113 10 R4 2040m Hcap 1.150090112 11 4 TBP 1.150090115 12 4 TBP 1.150090122 13 R5 955m Hcap 1.150090119 14 To Be Placed 1.150090120 15 4 TBP 1.150090129 16 To Be Placed 1.150090127 17 R6 1200m Hcap 1.150090126 18 R7 1200m Grp1 1.150038686 19 4 TBP 1.150038689 20 To Be Placed 1.150038687 21 R8 1500m Hcap 1.150090140 22 4 TBP 1.150090143 23 To Be Placed 1.150090141 Get Market Books \u00b6 If we then want to get the prices available/last traded for a market, we should use the listMarketBook operation. Let's Look at the market book for Moonee Valley R7. We will need to define a function which processes the runner books and collates the data into a DataFrame. In [212]: def process_runner_books (runner_books): ''' This function processes the runner books and returns a DataFrame with the best back/lay prices + vol for each runner :param runner_books: :return: ''' best_back_prices = [runner_book . ex . available_to_back[ 0 ] . price if runner_book . ex . available_to_back[ 0 ] . price else 1.01 for runner_book in runner_books] best_back_sizes = [runner_book . ex . available_to_back[ 0 ] . size if runner_book . ex . available_to_back[ 0 ] . size else 1.01 for runner_book in runner_books] best_lay_prices = [runner_book . ex . available_to_lay[ 0 ] . price if runner_book . ex . available_to_lay[ 0 ] . price else 1000.0 for runner_book in runner_books] best_lay_sizes = [runner_book . ex . available_to_lay[ 0 ] . size if runner_book . ex . available_to_lay[ 0 ] . size else 1.01 for runner_book in runner_books] selection_ids = [runner_book . selection_id for runner_book in runner_books] last_prices_traded = [runner_book . last_price_traded for runner_book in runner_books] total_matched = [runner_book . total_matched for runner_book in runner_books] statuses = [runner_book . status for runner_book in runner_books] scratching_datetimes = [runner_book . removal_date for runner_book in runner_books] adjustment_factors = [runner_book . adjustment_factor for runner_book in runner_books] df = pd . DataFrame({ 'Selection ID' : selection_ids, 'Best Back Price' : best_back_prices, 'Best Back Size' : best_back_sizes, 'Best Lay Price' : best_lay_prices, 'Best Lay Size' : best_lay_sizes, 'Last Price Traded' : last_prices_traded, 'Total Matched' : total_matched, 'Status' : statuses, 'Removal Date' : scratching_datetimes, 'Adjustment Factor' : adjustment_factors }) return df In [213]: # Create a price filter. Get all traded and offer data price_filter = `betfairlightweight` . filters . price_projection( price_data = [ 'EX_BEST_OFFERS' ] ) # Request market books market_books = trading . betting . list_market_book( market_ids = [ '1.150038686' ], price_projection = price_filter ) # Grab the first market book from the returned list as we only requested one market market_book = market_books[ 0 ] runners_df = process_runner_books(market_book . runners) runners_df Out[213]: # Selection ID Best Back Price Best Back Size Best Lay Price Best Lay Size Last Price Traded Total Matched Status Removal Date Adjustment Factor 0 16905731 12.0 65.54 13.0 33.09 12.0 1226.67 ACTIVE None 8.333 1 15815968 6.6 96.64 7.0 9.00 6.6 5858.61 ACTIVE None 14.286 2 9384677 14.0 114.71 15.0 76.71 14.0 964.80 ACTIVE None 6.667 3 8198751 17.5 14.67 19.0 33.02 17.5 940.56 ACTIVE None 5.556 4 9507057 38.0 53.13 100.0 40.22 46.0 224.72 ACTIVE None 3.125 5 21283266 15.0 121.46 19.5 5.56 19.5 1102.37 ACTIVE None 7.692 6 21283267 80.0 37.58 760.0 9.70 760.0 125.30 ACTIVE None 1.087 7 21063807 6.4 1503.62 7.2 50.00 6.6 8011.44 ACTIVE None 13.333 8 21283268 48.0 54.57 60.0 51.93 50.0 150.22 ACTIVE None 2.381 9 21283269 8.8 235.77 9.4 30.40 8.8 1729.96 ACTIVE None 11.111 10 4883975 46.0 33.42 55.0 5.00 46.0 208.45 ACTIVE None 2.381 11 202351 25.0 20.00 30.0 6.00 24.0 658.09 ACTIVE None 2.632 12 21283270 19.5 69.33 22.0 20.00 19.5 825.59 ACTIVE None 4.545 13 21283271 5.3 96.14 5.7 5.03 5.3 12654.32 ACTIVE None 16.871 Orderbook Workflow \u00b6 Now that we have the market book in an easy to read DataFrame, we can go ahead and start placing orders based on the market book. Although it is a simple (and probably not profitable) strategy, in the next few sections we will be backing the favourite and adjusting our orders. Placing Orders \u00b6 To place an order we use the placeOrders operation. A handy component of placeOrders is that you can send your strategy along with the runner that you want to back, so it is extremely easy to analyse how your strategy performed later. Let's place a 5 dollar back bet on the favourite at $7 call this strategy 'back_the_fav' . Note that if you are placing a limit order you must specify a price which is allowed by Betfair. For example, the price 6.3 isn't allowed, whereas 6.4 is, as prices go up by 20c increments at that price range. You can read about tick points here . In [232]: # Get the favourite's price and selection id fav_selection_id = runners_df . loc[runners_df[ 'Best Back Price' ] . idxmin(), 'Selection ID' ] fav_price = runners_df . loc[runners_df[ 'Best Back Price' ] . idxmin(), 'Best Back Price' ] In [276]: # Define a limit order filter limit_order_filter = `betfairlightweight` . filters . limit_order( size =5 , price =7 , persistence_type = 'LAPSE' ) # Define an instructions filter instructions_filter = `betfairlightweight` . filters . place_instruction( selection_id = str (fav_selection_id), order_type = \"LIMIT\" , side = \"BACK\" , limit_order = limit_order_filter ) instructions_filter Out[276]: { 'limitOrder' : { 'persistenceType' : 'LAPSE' , 'price' : 7 , 'size' : 5 }, 'orderType' : 'LIMIT' , 'selectionId' : '21283271' , 'side' : 'BACK' } In [277]: # Place the order order = trading . betting . place_orders( market_id = '1.150038686' , # The market id we obtained from before customer_strategy_ref = 'back_the_fav' , instructions = [instructions_filter] # This must be a list ) Now that we've placed the other, we can check if the order placing was a success and if any has been matched. In [306]: order . __dict__ Out[306]: { '_data' : { 'instructionReports' : [{ 'averagePriceMatched' : 0.0 , 'betId' : '142384852665' , 'instruction' : { 'limitOrder' : { 'persistenceType' : 'LAPSE' , 'price' : 7.0 , 'size' : 5.0 }, 'orderType' : 'LIMIT' , 'selectionId' : 21283271 , 'side' : 'BACK' }, 'orderStatus' : 'EXECUTABLE' , 'placedDate' : '2018-10-26T00:46:46.000Z' , 'sizeMatched' : 0.0 , 'status' : 'SUCCESS' }], 'marketId' : '1.150038686' , 'status' : 'SUCCESS' }, '_datetime_created' : datetime . datetime( 2018 , 10 , 26 , 0 , 46 , 46 , 455349 ), '_datetime_updated' : datetime . datetime( 2018 , 10 , 26 , 0 , 46 , 46 , 455349 ), 'customer_ref' : None , 'elapsed_time' : 1.484069 , 'error_code' : None , 'market_id' : '1.150038686' , 'place_instruction_reports' : [ < betfairlightweight . resources . bettingresources . PlaceOrderInstructionReports at 0x23e0f7952e8> ], 'status' : 'SUCCESS' } As we can see, the status is 'SUCCESS' , whilst the sizeMatched is 0. Let's now look at our current orders. Get Current Orders \u00b6 To get our current orders, we need to use the listCurrentOrders operation. We can then use either the bet id, the market id, or the bet strategy to filter our orders. In [311]: trading . betting . list_current_orders(customer_strategy_refs = [ 'back_the_fav' ]) . __dict__ Out[ 311 ]: { '_data' : { 'currentOrders' : [{ 'averagePriceMatched' : 0.0 , 'betId' : '142384852665' , 'bspLiability' : 0.0 , 'customerStrategyRef' : 'back_the_fav' , 'handicap' : 0.0 , 'marketId' : '1.150038686' , 'orderType' : 'LIMIT' , 'persistenceType' : 'LAPSE' , 'placedDate' : '2018-10-26T00:46:46.000Z' , 'priceSize' : { 'price' : 7.0 , 'size' : 5.0 }, 'regulatorCode' : 'MALTA LOTTERIES AND GAMBLING AUTHORITY' , 'selectionId' : 21283271 , 'side' : 'BACK' , 'sizeCancelled' : 0.0 , 'sizeLapsed' : 0.0 , 'sizeMatched' : 0.0 , 'sizeRemaining' : 5.0 , 'sizeVoided' : 0.0 , 'status' : 'EXECUTABLE' }], 'moreAvailable' : False }, '_datetime_created' : datetime . datetime( 2018 , 10 , 26 , 2 , 14 , 56 , 84036 ), '_datetime_updated' : datetime . datetime( 2018 , 10 , 26 , 2 , 14 , 56 , 84036 ), 'elapsed_time' : 1.327456 , 'more_available' : False , 'orders' : [ < betfairlightweight . resources . bettingresources . CurrentOrder at 0x23e0e7acd30> ], 'publish_time' : None , 'streaming_unique_id' : None , 'streaming_update' : None } As we can see, we have one order which is unmatched for our strategy 'back_the_fav' Cancelling Orders \u00b6 Let's now cancel this bet. To do this, we will use the cancelOrders operation. If you pass in a market ID it will cancel all orders for that specific market ID, like you can do on the website. In [312]: cancelled_order = trading . betting . cancel_orders(market_id = '1.150038686' ) In [328]: # Create a DataFrame to view the instruction report pd . Series(cancelled_order . cancel_instruction_reports[ 0 ] . __dict__ ) . to_frame() . T Out[328]: # status size_cancelled cancelled_date instruction error_code 0 SUCCESS 5 2018-10-26 06:01:26 betfairlightweight.resources.bettingresources... None Get Past Orders and Results \u00b6 If we want to go back and look at past orders we have made, there are two main operations for this: listClearedOrders - this operation takes a range of data down to the individual selection ID level, and returns a summary of those specific orders listMarketProfitAndLoss - this operation is more specific, and only takes Market IDs to return the Profit/Loss for that market Alternatively, we can use the getAccountStatement operation from the Account Operations API. Let's now use both Sports API operations based on our previous orders and then compare it to the getAccountStatement operation. Get Cleared Orders \u00b6 In [346]: # listClearedOrders cleared_orders = trading . betting . list_cleared_orders(bet_status = \"SETTLED\" , market_ids = [ \"1.150038686\" ]) In [371]: # Create a DataFrame from the orders pd . DataFrame(cleared_orders . _data[ 'clearedOrders' ]) Out[371]: # betCount betId betOutcome eventId eventTypeId handicap lastMatchedDate marketId orderType persistenceType placedDate priceMatched priceReduced priceRequested profit selectionId settledDate side sizeSettled 0 1 142383373022 LOST 28971066 7 0.0 2018-10-26T10:31:53.000Z 1.150038686 MARKET_ON_CLOSE LAPSE 2018-10-26T00:12:03.000Z 5.74 False 5.74 -5.0 21283271 2018-10-26T10:34:39.000Z BACK 5.0 1 1 142383570640 WON 28971066 7 0.0 2018-10-26T00:16:32.000Z 1.150038686 LIMIT LAPSE 2018-10-26T00:16:31.000Z 5.40 False 5.50 5.0 21283271 2018-10-26T10:34:39.000Z LAY 5.0 Note that we can also filter for certain dates, bet ids, event ids, selection ids etc. We can also group by the event type, the event, the market, the runner, the side, the bet and the strategy, which is extremely useful if you're looking for a quick summary of how your strategy is performing. Get Market Profit and Loss \u00b6 Now let's find the Profit and Loss for the market. To do this we will use the listMarketProfitAndLoss operation. Note that this function only works with market IDs, and once the website clears the market, the operation will no longer work. However the market is generally up for about a minute after the race, so if your strategy is automated, you can check once if your bet is settled and if it is, hit the getMarketProfitAndLoss endpoint. Because of this, we will check a different market ID to the example above. In [406]: # Get the profit/loss - this returns a list pl = trading . betting . list_market_profit_and_loss(market_ids = [ \"1.150318913\" ], include_bsp_bets = 'true' , include_settled_bets = 'true' ) In [410]: # Create a profit/loss DataFrame pl_df = pd . DataFrame(pl[ 0 ] . _data[ 'profitAndLosses' ]) . assign(marketId = pl[ 0 ] . market_id) pl_df Out[410]: # ifWin selectionId marketId 0 -5.0 10065177 1.150318913 1 14.0 17029506 1.150318913 2 -5.0 5390339 1.150318913 3 -5.0 13771011 1.150318913 4 -5.0 138209 1.150318913 5 -5.0 10503541 1.150318913 6 -5.0 12165809 1.150318913 Get Account Statement \u00b6 Another method is to use the getAccountStatement , which provides an overview of all your bets over a certain time period. You can then filter this for specific dates if you wish. In [428]: # Define a date filter - get all bets for the past 4 days four_days_ago = (datetime . datetime . utcnow() - datetime . timedelta(days =4 )) . strftime( \"%Y-%m- %d T%TZ\" ) acct_statement_date_filter = `betfairlightweight` . filters . time_range(from_ = four_days_ago) # Request account statement account_statement = trading . account . get_account_statement(item_date_range = acct_statement_date_filter) In [450]: # Create df of recent transactions recent_transactions = pd . DataFrame(account_statement . _data[ 'accountStatement' ]) recent_transactions Out[450]: # amount balance itemClass itemClassData itemDate legacyData refId 0 -5.0 256.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":3.8,\"bet... 2018-10-28T23:14:28.000Z {'avgPrice': 3.8, 'betSize': 5.0, 'betType': '... 142845441633 1 5.0 261.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.4,\"bet... 2018-10-26T10:34:39.000Z {'avgPrice': 5.4, 'betSize': 5.0, 'betType': '... 142383570640 2 -5.0 256.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.74,\"be... 2018-10-26T10:34:39.000Z {'avgPrice': 5.74, 'betSize': 5.0, 'betType': ... 142383373022 In [468]: # Create df of itemClassData - iterate over the account statement list and convert to json so that the DataFrame function # can read it correctly class_data = [json . loads(account_statement . account_statement[i] . item_class_data[ 'unknownStatementItem' ]) for i in range ( len (account_statement . account_statement))] In [471]: class_df = pd . DataFrame(class_data) class_df Out [471]: # avgPrice betCategoryType betSize betType commissionRate eventId eventTypeId fullMarketName grossBetAmount marketName marketType placedDate selectionId selectionName startDate transactionId transactionType winLose 0 3.80 M 5.0 B None 150318913 7 USA / TPara (US) 28 th Oct/ 16:06 R8 1m Allw Claim 0.0 R8 1m Allw Claim O 2018-10-28T23:02:28.000Z 17029506 Gato Guapo 2018-10-28T23:06:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST 1 5.40 E 5.0 L None 150038686 7 AUS / MVal (AUS) 26 th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:16:31.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_CREDIT 2 5.74 M 5.0 B None 150038686 7 AUS / MVal (AUS) 26 th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:12:03.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST As we can see, this DataFrame provides a much more comprehensive view of each of our bets. However, it lacks the ability to filter by strategy like the listClearedOrders operation in the Sports API.","title":"API tutorial in Python"},{"location":"api/apiPythontutorial/#betfair-api-tutorial-in-python","text":"This tutorial will walk you through the process of connecting to Betfair's API, grabbing data and placing a bet in Python. It will utilise the betfairlightweight Python library.","title":"Betfair API tutorial in Python"},{"location":"api/apiPythontutorial/#requirements","text":"This tutorial will assume that you have an API app key. If you don't, please follow the steps outlined here . This tutorial will also assume that you have a basic understanding of what an API is. For a summary in layman's terms, read this article .","title":"Requirements"},{"location":"api/apiPythontutorial/#quick-links","text":"Here are some other useful links for accessing our API: How to create an API app key Developer Docs - the official dev docs for Betfair's API Sports API Visualiser - Useful for exploring what the API has to offer Account API Visualiser Examples using betfairlightweight There's a more complete list of resources here","title":"Quick Links"},{"location":"api/apiPythontutorial/#getting-started","text":"","title":"Getting Started"},{"location":"api/apiPythontutorial/#setting-up-your-certificates","text":"To use the API securely, Betfair recommends generating certificates. The betfairlightweight package requires this to login non-interactively. For detailed instructions on how to generate certificates on a windows machine, follow the instructions outlined here . For alternate instructions for Windows, or for Mac/Linux machines, follow the instructions outlined here . You should then create a folder for your certs, perhaps named 'certs' and grab the path loation.","title":"Setting Up Your Certificates"},{"location":"api/apiPythontutorial/#installing-betfairlightweight","text":"We also need to install betfairlightweight . To do this, simply use pip install betfairlightweight in the cmd prompt/terminal. If this doesn't work, you will have to Google your error. If you're just starting out with Python, you may have to add Python to your environment variables.","title":"Installing betfairlightweight"},{"location":"api/apiPythontutorial/#sending-requests-to-the-api","text":"","title":"Sending Requests to the API"},{"location":"api/apiPythontutorial/#log-into-the-api-client","text":"Now we're finally ready to log in and use the API. First, we create an APIClient object and then log in. To log in, we'll need to specify where we put our certs. In this example, I'll put them in a folder named 'certs', on my desktop. You'll also need to change the username , password and app_key variables to your own. In [206]: # Import libraries import betfairlightweight from betfairlightweight import filters import pandas as pd import numpy as np import os import datetime import json # Change this certs path to wherever you're storing your certificates certs_path = r'C:\\Users\\wardj\\Desktop\\certs' # Change these login details to your own my_username = \"your_username\" my_password = \"your_password\" my_app_key = \"your_app_key\" trading = betfairlightweight . APIClient(username = my_username, password = my_password, app_key = my_app_key, certs = certs_path) trading . login() Out[206]: < LoginResource >","title":"Log into the API Client"},{"location":"api/apiPythontutorial/#get-event-ids","text":"Betfair's API has a number of operations. For example, if you want to list the market book for a market, you would use the listMarketBook operation. These endpoints are shown in the Sports API Visualiser and in the docs. They are also listed below:","title":"Get Event IDs"},{"location":"api/apiPythontutorial/#sports-api","text":"listEventTypes listCompetitions listTimeRanges listEvents listMarketTypes listCountries listVenues listMarketCatalogue listMarketBook listRunnerBook placeOrders cancelOrders updateOrders replaceOrders listCurrentOrders listClearedOrders listMarketProfitAndLoss The Account Operations API operations/endpoints can be found here . First we need to grab the 'Event Type Id'. Each sport has a different ID. Below we will find the ids for all sports by requesting the event_type_ids without a filter. In [43]: # Grab all event type ids. This will return a list which we will iterate over to print out the id and the name of the sport event_types = trading . betting . list_event_types() sport_ids = pd . DataFrame({ 'Sport' : [event_type_object . event_type . name for event_type_object in event_types], 'ID' : [event_type_object . event_type . id for event_type_object in event_types] }) . set_index( 'Sport' ) . sort_index() sport_ids Out[43]: Sport ID American Football 6423 Athletics 3988 Australian Rules 61420 Baseball 7511 Basketball 7522 Boxing 6 Chess 136332 Cricket 4 Cycling 11 Darts 3503 Esports 27454571 Financial Bets 6231 Gaelic Games 2152880 Golf 3 Greyhound Racing 4339 Handball 468328 Horse Racing 7 Ice Hockey 7524 Mixed Martial Arts 26420387 Motor Sport 8 Netball 606611 Politics 2378961 Rugby League 1477 Rugby Union 5 Snooker 6422 Soccer 1 Special Bets 10 Tennis 2 Volleyball 998917 If we just wanted to get the event id for horse racing, we could use the filter function from betfairlightweight as shown in the examples and below. In [50]: # Filter for just horse racing horse_racing_filter = `betfairlightweight` . filters . market_filter(text_query = 'Horse Racing' ) # This returns a list horse_racing_event_type = trading . betting . list_event_types( filter = horse_racing_filter) # Get the first element of the list horse_racing_event_type = horse_racing_event_type[ 0 ] horse_racing_event_type_id = horse_racing_event_type . event_type . id print (f \"The event type id for horse racing is {horse_racing_event_type_id}\" ) The event type id for horse racing is 7","title":"Sports API"},{"location":"api/apiPythontutorial/#get-competition-ids","text":"Sometimes you may want to get markets based on the competition. An example may be the Brownlow medal, or the EPL. Let's have a look at all the soccer competitions over the next week and filter to only get the EPL Competition ID. In [90]: # Get a datetime object in a week and convert to string datetime_in_a_week = (datetime . datetime . utcnow() + datetime . timedelta(weeks =1 )) . strftime( \"%Y-%m- %d T%TZ\" ) # Create a competition filter competition_filter = `betfairlightweight` . filters . market_filter( event_type_ids = [ 1 ], # Soccer's event type id is 1 market_start_time = { 'to' : datetime_in_a_week }) # Get a list of competitions for soccer competitions = trading . betting . list_competitions( filter = competition_filter ) # Iterate over the competitions and create a dataframe of competitions and competition ids soccer_competitions = pd . DataFrame({ 'Competition' : [competition_object . competition . name for competition_object in competitions], 'ID' : [competition_object . competition . id for competition_object in competitions] }) In [94]: # Get the English Premier League Competition ID soccer_competitions[soccer_competitions . Competition . str . contains( 'English Premier' )] Out[94]: # Competition ID 116 English Premier League 10932509","title":"Get Competition IDs"},{"location":"api/apiPythontutorial/#get-upcoming-events","text":"Say you want to get all the upcoming events for Thoroughbreads for the next 24 hours. We will use the listEvents operation for this. First, as before, we define a market filter, and then using the betting method from our trading object which we defined earlier. In [207]: # Define a market filter thoroughbreds_event_filter = `betfairlightweight` . filters . market_filter( event_type_ids = [horse_racing_event_type_id], market_countries = [ 'AU' ], market_start_time = { 'to' : (datetime . datetime . utcnow() + datetime . timedelta(days =1 )) . strftime( \"%Y-%m- %d T%TZ\" ) } ) # Print the filter thoroughbreds_event_filter Out[207]: { 'eventTypeIds' : [ '7' ], 'marketCountries' : [ 'AU' ], 'marketStartTime' : { 'to' : '2018-10-26T22:25:00Z' }} In [208]: # Get a list of all thoroughbred events as objects aus_thoroughbred_events = trading . betting . list_events( filter = thoroughbreds_event_filter ) # Create a DataFrame with all the events by iterating over each event object aus_thoroughbred_events_today = pd . DataFrame({ 'Event Name' : [event_object . event . name for event_object in aus_thoroughbred_events], 'Event ID' : [event_object . event . id for event_object in aus_thoroughbred_events], 'Event Venue' : [event_object . event . venue for event_object in aus_thoroughbred_events], 'Country Code' : [event_object . event . country_code for event_object in aus_thoroughbred_events], 'Time Zone' : [event_object . event . time_zone for event_object in aus_thoroughbred_events], 'Open Date' : [event_object . event . open_date for event_object in aus_thoroughbred_events], 'Market Count' : [event_object . market_count for event_object in aus_thoroughbred_events] }) aus_thoroughbred_events_today Out[208]: # Event Name Event ID Event Venue Country Code Time Zone Open Date Market Count 0 MVal (AUS) 26 th Oct 28971066 Moonee Valley AU Australia/Sydney 2018-10-26 07:30:00 24 1 Newc (AUS) 26 th Oct 28974559 Newcastle AU Australia/Sydney 2018-10-26 07:07:00 20 2 Bath (AUS) 26 th Oct 28974547 Bathurst AU Australia/Sydney 2018-10-26 02:43:00 16 3 Cant (AUS) 26 th Oct 28974545 Canterbury AU Australia/Sydney 2018-10-26 07:15:00 16 4 Scne (AUS) 26 th Oct 28973942 Scone AU Australia/Sydney 2018-10-26 02:25:00 16 5 Gawl (AUS) 26 th Oct 28974550 Gawler AU Australia/Adelaide 2018-10-26 04:00:00 16 6 Gatt (AUS) 26 th Oct 28974549 Gatton AU Australia/Queensland 2018-10-26 01:55:00 16 7 GlPk (AUS) 26 th Oct 28974562 Gloucester Park AU Australia/Perth 2018-10-26 09:10:00 20 8 Hoba (AUS) 26 th Oct 28974563 Hobart AU Australia/Sydney 2018-10-26 05:23:00 18 9 Echu (AUS) 26 th Oct 28974016 Echuca AU Australia/Sydney 2018-10-26 01:30:00 18 10 Melt (AUS) 26 th Oct 28974560 Melton AU Australia/Sydney 2018-10-26 07:18:00 18 11 MVal (AUS) 26 th Oct 28921730 None AU Australia/Sydney 2018-10-26 11:00:00 1 12 Redc (AUS) 26 th Oct 28974561 Redcliffe AU Australia/Queensland 2018-10-26 02:17:00 16 13 SCst (AUS) 26 th Oct 28974149 Sunshine Coast AU Australia/Queensland 2018-10-26 06:42:00 20","title":"Get Upcoming Events"},{"location":"api/apiPythontutorial/#get-market-types","text":"Say we want to know what market types a certain event is offering. To do this, we use the listMarketTypes operation. Let's take the Moonee Valley event from above (ID: 28971066). As this is a horse race we would expect that it would have Win and Place markets. In [209]: # Define a market filter market_types_filter = `betfairlightweight` . filters . market_filter(event_ids = [ '28971066' ]) # Request market types market_types = trading . betting . list_market_types( filter = market_types_filter ) # Create a DataFrame of market types market_types_mooney_valley = pd . DataFrame({ 'Market Type' : [market_type_object . market_type for market_type_object in market_types], }) market_types_mooney_valley Out[209]: Market Type 0 OTHER_PLACE 1 PLACE 2 WIN","title":"Get Market Types"},{"location":"api/apiPythontutorial/#get-market-catalogues","text":"If we want to know the various market names that there are for a particular event, as well as how much has been matched on each market, we want to request data from the listMarketCatalogue operation. We can provide a number of filters, including the Competition ID, the Event ID, the Venue etc. to the filter. We must also specify the maximum number of results, and if we want additional data like the event data or runner data, we can also request that. For a more comprehensive understanding of the options for filters and what we can request, please have a look at the Sports API Visualiser . The options listed under market filter should be put into a filter, whilst the others should be arguments to the relevant operation function in betfairlightweight . For example, if we want all the markets for Moonee Valley, we should use the following filters and arguments. In [210]: market_catalogue_filter = `betfairlightweight` . filters . market_filter(event_ids = [ '28971066' ]) market_catalogues = trading . betting . list_market_catalogue( filter = market_catalogue_filter, max_results = '100' , sort = 'FIRST_TO_START' ) # Create a DataFrame for each market catalogue market_types_mooney_valley = pd . DataFrame({ 'Market Name' : [market_cat_object . market_name for market_cat_object in market_catalogues], 'Market ID' : [market_cat_object . market_id for market_cat_object in market_catalogues], 'Total Matched' : [market_cat_object . total_matched for market_cat_object in market_catalogues], }) market_types_mooney_valley Out[210]: Market Name Market ID Total Matched 0 4 TBP 1.150090094 1 To Be Placed 1.150090092 2 R1 1000m 3yo 1.150090091 3 4 TBP 1.150090101 4 To Be Placed 1.150090099 5 R2 2040m Hcap 1.150090098 6 To Be Placed 1.150090106 7 R3 1500m Hcap 1.150090105 8 4 TBP 1.150090108 9 To Be Placed 1.150090113 10 R4 2040m Hcap 1.150090112 11 4 TBP 1.150090115 12 4 TBP 1.150090122 13 R5 955m Hcap 1.150090119 14 To Be Placed 1.150090120 15 4 TBP 1.150090129 16 To Be Placed 1.150090127 17 R6 1200m Hcap 1.150090126 18 R7 1200m Grp1 1.150038686 19 4 TBP 1.150038689 20 To Be Placed 1.150038687 21 R8 1500m Hcap 1.150090140 22 4 TBP 1.150090143 23 To Be Placed 1.150090141","title":"Get Market Catalogues"},{"location":"api/apiPythontutorial/#get-market-books","text":"If we then want to get the prices available/last traded for a market, we should use the listMarketBook operation. Let's Look at the market book for Moonee Valley R7. We will need to define a function which processes the runner books and collates the data into a DataFrame. In [212]: def process_runner_books (runner_books): ''' This function processes the runner books and returns a DataFrame with the best back/lay prices + vol for each runner :param runner_books: :return: ''' best_back_prices = [runner_book . ex . available_to_back[ 0 ] . price if runner_book . ex . available_to_back[ 0 ] . price else 1.01 for runner_book in runner_books] best_back_sizes = [runner_book . ex . available_to_back[ 0 ] . size if runner_book . ex . available_to_back[ 0 ] . size else 1.01 for runner_book in runner_books] best_lay_prices = [runner_book . ex . available_to_lay[ 0 ] . price if runner_book . ex . available_to_lay[ 0 ] . price else 1000.0 for runner_book in runner_books] best_lay_sizes = [runner_book . ex . available_to_lay[ 0 ] . size if runner_book . ex . available_to_lay[ 0 ] . size else 1.01 for runner_book in runner_books] selection_ids = [runner_book . selection_id for runner_book in runner_books] last_prices_traded = [runner_book . last_price_traded for runner_book in runner_books] total_matched = [runner_book . total_matched for runner_book in runner_books] statuses = [runner_book . status for runner_book in runner_books] scratching_datetimes = [runner_book . removal_date for runner_book in runner_books] adjustment_factors = [runner_book . adjustment_factor for runner_book in runner_books] df = pd . DataFrame({ 'Selection ID' : selection_ids, 'Best Back Price' : best_back_prices, 'Best Back Size' : best_back_sizes, 'Best Lay Price' : best_lay_prices, 'Best Lay Size' : best_lay_sizes, 'Last Price Traded' : last_prices_traded, 'Total Matched' : total_matched, 'Status' : statuses, 'Removal Date' : scratching_datetimes, 'Adjustment Factor' : adjustment_factors }) return df In [213]: # Create a price filter. Get all traded and offer data price_filter = `betfairlightweight` . filters . price_projection( price_data = [ 'EX_BEST_OFFERS' ] ) # Request market books market_books = trading . betting . list_market_book( market_ids = [ '1.150038686' ], price_projection = price_filter ) # Grab the first market book from the returned list as we only requested one market market_book = market_books[ 0 ] runners_df = process_runner_books(market_book . runners) runners_df Out[213]: # Selection ID Best Back Price Best Back Size Best Lay Price Best Lay Size Last Price Traded Total Matched Status Removal Date Adjustment Factor 0 16905731 12.0 65.54 13.0 33.09 12.0 1226.67 ACTIVE None 8.333 1 15815968 6.6 96.64 7.0 9.00 6.6 5858.61 ACTIVE None 14.286 2 9384677 14.0 114.71 15.0 76.71 14.0 964.80 ACTIVE None 6.667 3 8198751 17.5 14.67 19.0 33.02 17.5 940.56 ACTIVE None 5.556 4 9507057 38.0 53.13 100.0 40.22 46.0 224.72 ACTIVE None 3.125 5 21283266 15.0 121.46 19.5 5.56 19.5 1102.37 ACTIVE None 7.692 6 21283267 80.0 37.58 760.0 9.70 760.0 125.30 ACTIVE None 1.087 7 21063807 6.4 1503.62 7.2 50.00 6.6 8011.44 ACTIVE None 13.333 8 21283268 48.0 54.57 60.0 51.93 50.0 150.22 ACTIVE None 2.381 9 21283269 8.8 235.77 9.4 30.40 8.8 1729.96 ACTIVE None 11.111 10 4883975 46.0 33.42 55.0 5.00 46.0 208.45 ACTIVE None 2.381 11 202351 25.0 20.00 30.0 6.00 24.0 658.09 ACTIVE None 2.632 12 21283270 19.5 69.33 22.0 20.00 19.5 825.59 ACTIVE None 4.545 13 21283271 5.3 96.14 5.7 5.03 5.3 12654.32 ACTIVE None 16.871","title":"Get Market Books"},{"location":"api/apiPythontutorial/#orderbook-workflow","text":"Now that we have the market book in an easy to read DataFrame, we can go ahead and start placing orders based on the market book. Although it is a simple (and probably not profitable) strategy, in the next few sections we will be backing the favourite and adjusting our orders.","title":"Orderbook Workflow"},{"location":"api/apiPythontutorial/#placing-orders","text":"To place an order we use the placeOrders operation. A handy component of placeOrders is that you can send your strategy along with the runner that you want to back, so it is extremely easy to analyse how your strategy performed later. Let's place a 5 dollar back bet on the favourite at $7 call this strategy 'back_the_fav' . Note that if you are placing a limit order you must specify a price which is allowed by Betfair. For example, the price 6.3 isn't allowed, whereas 6.4 is, as prices go up by 20c increments at that price range. You can read about tick points here . In [232]: # Get the favourite's price and selection id fav_selection_id = runners_df . loc[runners_df[ 'Best Back Price' ] . idxmin(), 'Selection ID' ] fav_price = runners_df . loc[runners_df[ 'Best Back Price' ] . idxmin(), 'Best Back Price' ] In [276]: # Define a limit order filter limit_order_filter = `betfairlightweight` . filters . limit_order( size =5 , price =7 , persistence_type = 'LAPSE' ) # Define an instructions filter instructions_filter = `betfairlightweight` . filters . place_instruction( selection_id = str (fav_selection_id), order_type = \"LIMIT\" , side = \"BACK\" , limit_order = limit_order_filter ) instructions_filter Out[276]: { 'limitOrder' : { 'persistenceType' : 'LAPSE' , 'price' : 7 , 'size' : 5 }, 'orderType' : 'LIMIT' , 'selectionId' : '21283271' , 'side' : 'BACK' } In [277]: # Place the order order = trading . betting . place_orders( market_id = '1.150038686' , # The market id we obtained from before customer_strategy_ref = 'back_the_fav' , instructions = [instructions_filter] # This must be a list ) Now that we've placed the other, we can check if the order placing was a success and if any has been matched. In [306]: order . __dict__ Out[306]: { '_data' : { 'instructionReports' : [{ 'averagePriceMatched' : 0.0 , 'betId' : '142384852665' , 'instruction' : { 'limitOrder' : { 'persistenceType' : 'LAPSE' , 'price' : 7.0 , 'size' : 5.0 }, 'orderType' : 'LIMIT' , 'selectionId' : 21283271 , 'side' : 'BACK' }, 'orderStatus' : 'EXECUTABLE' , 'placedDate' : '2018-10-26T00:46:46.000Z' , 'sizeMatched' : 0.0 , 'status' : 'SUCCESS' }], 'marketId' : '1.150038686' , 'status' : 'SUCCESS' }, '_datetime_created' : datetime . datetime( 2018 , 10 , 26 , 0 , 46 , 46 , 455349 ), '_datetime_updated' : datetime . datetime( 2018 , 10 , 26 , 0 , 46 , 46 , 455349 ), 'customer_ref' : None , 'elapsed_time' : 1.484069 , 'error_code' : None , 'market_id' : '1.150038686' , 'place_instruction_reports' : [ < betfairlightweight . resources . bettingresources . PlaceOrderInstructionReports at 0x23e0f7952e8> ], 'status' : 'SUCCESS' } As we can see, the status is 'SUCCESS' , whilst the sizeMatched is 0. Let's now look at our current orders.","title":"Placing Orders"},{"location":"api/apiPythontutorial/#get-current-orders","text":"To get our current orders, we need to use the listCurrentOrders operation. We can then use either the bet id, the market id, or the bet strategy to filter our orders. In [311]: trading . betting . list_current_orders(customer_strategy_refs = [ 'back_the_fav' ]) . __dict__ Out[ 311 ]: { '_data' : { 'currentOrders' : [{ 'averagePriceMatched' : 0.0 , 'betId' : '142384852665' , 'bspLiability' : 0.0 , 'customerStrategyRef' : 'back_the_fav' , 'handicap' : 0.0 , 'marketId' : '1.150038686' , 'orderType' : 'LIMIT' , 'persistenceType' : 'LAPSE' , 'placedDate' : '2018-10-26T00:46:46.000Z' , 'priceSize' : { 'price' : 7.0 , 'size' : 5.0 }, 'regulatorCode' : 'MALTA LOTTERIES AND GAMBLING AUTHORITY' , 'selectionId' : 21283271 , 'side' : 'BACK' , 'sizeCancelled' : 0.0 , 'sizeLapsed' : 0.0 , 'sizeMatched' : 0.0 , 'sizeRemaining' : 5.0 , 'sizeVoided' : 0.0 , 'status' : 'EXECUTABLE' }], 'moreAvailable' : False }, '_datetime_created' : datetime . datetime( 2018 , 10 , 26 , 2 , 14 , 56 , 84036 ), '_datetime_updated' : datetime . datetime( 2018 , 10 , 26 , 2 , 14 , 56 , 84036 ), 'elapsed_time' : 1.327456 , 'more_available' : False , 'orders' : [ < betfairlightweight . resources . bettingresources . CurrentOrder at 0x23e0e7acd30> ], 'publish_time' : None , 'streaming_unique_id' : None , 'streaming_update' : None } As we can see, we have one order which is unmatched for our strategy 'back_the_fav'","title":"Get Current Orders"},{"location":"api/apiPythontutorial/#cancelling-orders","text":"Let's now cancel this bet. To do this, we will use the cancelOrders operation. If you pass in a market ID it will cancel all orders for that specific market ID, like you can do on the website. In [312]: cancelled_order = trading . betting . cancel_orders(market_id = '1.150038686' ) In [328]: # Create a DataFrame to view the instruction report pd . Series(cancelled_order . cancel_instruction_reports[ 0 ] . __dict__ ) . to_frame() . T Out[328]: # status size_cancelled cancelled_date instruction error_code 0 SUCCESS 5 2018-10-26 06:01:26 betfairlightweight.resources.bettingresources... None","title":"Cancelling Orders"},{"location":"api/apiPythontutorial/#get-past-orders-and-results","text":"If we want to go back and look at past orders we have made, there are two main operations for this: listClearedOrders - this operation takes a range of data down to the individual selection ID level, and returns a summary of those specific orders listMarketProfitAndLoss - this operation is more specific, and only takes Market IDs to return the Profit/Loss for that market Alternatively, we can use the getAccountStatement operation from the Account Operations API. Let's now use both Sports API operations based on our previous orders and then compare it to the getAccountStatement operation.","title":"Get Past Orders and Results"},{"location":"api/apiPythontutorial/#get-cleared-orders","text":"In [346]: # listClearedOrders cleared_orders = trading . betting . list_cleared_orders(bet_status = \"SETTLED\" , market_ids = [ \"1.150038686\" ]) In [371]: # Create a DataFrame from the orders pd . DataFrame(cleared_orders . _data[ 'clearedOrders' ]) Out[371]: # betCount betId betOutcome eventId eventTypeId handicap lastMatchedDate marketId orderType persistenceType placedDate priceMatched priceReduced priceRequested profit selectionId settledDate side sizeSettled 0 1 142383373022 LOST 28971066 7 0.0 2018-10-26T10:31:53.000Z 1.150038686 MARKET_ON_CLOSE LAPSE 2018-10-26T00:12:03.000Z 5.74 False 5.74 -5.0 21283271 2018-10-26T10:34:39.000Z BACK 5.0 1 1 142383570640 WON 28971066 7 0.0 2018-10-26T00:16:32.000Z 1.150038686 LIMIT LAPSE 2018-10-26T00:16:31.000Z 5.40 False 5.50 5.0 21283271 2018-10-26T10:34:39.000Z LAY 5.0 Note that we can also filter for certain dates, bet ids, event ids, selection ids etc. We can also group by the event type, the event, the market, the runner, the side, the bet and the strategy, which is extremely useful if you're looking for a quick summary of how your strategy is performing.","title":"Get Cleared Orders"},{"location":"api/apiPythontutorial/#get-market-profit-and-loss","text":"Now let's find the Profit and Loss for the market. To do this we will use the listMarketProfitAndLoss operation. Note that this function only works with market IDs, and once the website clears the market, the operation will no longer work. However the market is generally up for about a minute after the race, so if your strategy is automated, you can check once if your bet is settled and if it is, hit the getMarketProfitAndLoss endpoint. Because of this, we will check a different market ID to the example above. In [406]: # Get the profit/loss - this returns a list pl = trading . betting . list_market_profit_and_loss(market_ids = [ \"1.150318913\" ], include_bsp_bets = 'true' , include_settled_bets = 'true' ) In [410]: # Create a profit/loss DataFrame pl_df = pd . DataFrame(pl[ 0 ] . _data[ 'profitAndLosses' ]) . assign(marketId = pl[ 0 ] . market_id) pl_df Out[410]: # ifWin selectionId marketId 0 -5.0 10065177 1.150318913 1 14.0 17029506 1.150318913 2 -5.0 5390339 1.150318913 3 -5.0 13771011 1.150318913 4 -5.0 138209 1.150318913 5 -5.0 10503541 1.150318913 6 -5.0 12165809 1.150318913","title":"Get Market Profit and Loss"},{"location":"api/apiPythontutorial/#get-account-statement","text":"Another method is to use the getAccountStatement , which provides an overview of all your bets over a certain time period. You can then filter this for specific dates if you wish. In [428]: # Define a date filter - get all bets for the past 4 days four_days_ago = (datetime . datetime . utcnow() - datetime . timedelta(days =4 )) . strftime( \"%Y-%m- %d T%TZ\" ) acct_statement_date_filter = `betfairlightweight` . filters . time_range(from_ = four_days_ago) # Request account statement account_statement = trading . account . get_account_statement(item_date_range = acct_statement_date_filter) In [450]: # Create df of recent transactions recent_transactions = pd . DataFrame(account_statement . _data[ 'accountStatement' ]) recent_transactions Out[450]: # amount balance itemClass itemClassData itemDate legacyData refId 0 -5.0 256.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":3.8,\"bet... 2018-10-28T23:14:28.000Z {'avgPrice': 3.8, 'betSize': 5.0, 'betType': '... 142845441633 1 5.0 261.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.4,\"bet... 2018-10-26T10:34:39.000Z {'avgPrice': 5.4, 'betSize': 5.0, 'betType': '... 142383570640 2 -5.0 256.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.74,\"be... 2018-10-26T10:34:39.000Z {'avgPrice': 5.74, 'betSize': 5.0, 'betType': ... 142383373022 In [468]: # Create df of itemClassData - iterate over the account statement list and convert to json so that the DataFrame function # can read it correctly class_data = [json . loads(account_statement . account_statement[i] . item_class_data[ 'unknownStatementItem' ]) for i in range ( len (account_statement . account_statement))] In [471]: class_df = pd . DataFrame(class_data) class_df Out [471]: # avgPrice betCategoryType betSize betType commissionRate eventId eventTypeId fullMarketName grossBetAmount marketName marketType placedDate selectionId selectionName startDate transactionId transactionType winLose 0 3.80 M 5.0 B None 150318913 7 USA / TPara (US) 28 th Oct/ 16:06 R8 1m Allw Claim 0.0 R8 1m Allw Claim O 2018-10-28T23:02:28.000Z 17029506 Gato Guapo 2018-10-28T23:06:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST 1 5.40 E 5.0 L None 150038686 7 AUS / MVal (AUS) 26 th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:16:31.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_CREDIT 2 5.74 M 5.0 B None 150038686 7 AUS / MVal (AUS) 26 th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:12:03.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST As we can see, this DataFrame provides a much more comprehensive view of each of our bets. However, it lacks the ability to filter by strategy like the listClearedOrders operation in the Sports API.","title":"Get Account Statement"},{"location":"api/apiRtutorial/","text":"Betfair API tutorials in R \u00b6 Betfair's API can be easily traversed in R. It allows you to retrieve market information, create/cancel bets and manage your account. Here's a collection of easy to follow API tutorials in R: Accessing the API using R Get Worldcup Odds AFL Odds PulleR Tutorial Accessing the API using R \u00b6 Set up R \u00b6 What is R? Download and install R \u2013 get the language set up on your computer Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R Required Packages \u00b6 Two R packages are required: library (tidyverse) library (abettor) The abettor package can be downloaded here . For an in-depth understanding of the package, have a read of the documentation. Instructions are also provided in the sample code. Login to Betfair \u00b6 To login to Betfair, replace the following dummy username, password and app key with your own. abettor :: loginBF(username = \"betfair_username\" , password = \"betfair_password\" , applicationKey = \"betfair_app_key\" ) If you don't have a live app key for the API yet take a look at this page . Finding Event IDs \u00b6 In order to find data for specific markets, you will first need to know the event ID. This is easily achieved with the abettor package . To find the event IDs of events in the next 60 days: abettor :: listEventTypes(toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" ))) This will return a DataFrame of the following structure: eventType.id eventType.name marketCount 1 Soccer 1193 2 Tennis 2184 7522 Basketball 1 4 Cricket 37 7 Horse Racing 509 61420 Australian Rules 31 4339 Greyhound Racing 527 Finding Competition IDs \u00b6 Once you have the event ID, the next logical step is to find the competition IDs for the event you want to get data for. For example, if you want to find the competition IDs for Australian Rules, you would use the following abettor :: listCompetitions( eventTypeIds = 61420 , ## AFL is eventTypeId 61420, toDate = ( format ( Sys.time () + 86400 * 180 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 180 days ) This will return the following structured DataFrame: competition.id competition.name marketCount competitionRegion 11516633 Brownlow Medal 2018 3 AUS 11897406 AFL 78 AUS Finding Specific Markets \u00b6 The next logical step is to find the market that you are interested in. Furthering our example above, if you want the Match Odds for all Australian Rules games over the next 60 days, simply use the Competition ID from above in the following. abettor :: listMarketCatalogue( eventTypeIds = 61420 , marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = 11897406 , toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) This returns a large DataFrame object with each market, participants and associated odds. Get World Cup Odds Tutorial \u00b6 This tutorial walks you through the process of retrieving exchange odds for all the matches from the 2018 FIFA World Cup 2018. This can be modified for other sports and uses. You can run this script in R. ################################################### ### FIFA World Cup Datathon ### Betfair API Tutorial ### ### This script allows you to access the Betfair ### API and retrive exchange odds for all the ### matches from the upcoming FIFA World Cup 2018 ################################################### ################################################### ### Setup ################################################### ## Loading required packages library (tidyverse) ## package for general data manipulation - https://www.tidyverse.org/ library (abettor) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor ## Enter your Betfair API Credentials below betfair_username <- \"\" betfair_password <- \"\" betfair_app_key <- \"\" ## Login to Betfair - should return \"SUCCESS:\" on successful login betfair_login <- abettor :: loginBF(username = betfair_username, password = betfair_password, applicationKey = betfair_app_key) ################################################### ## Retrieving all soccer competitions for ## which markets are currently alive ## on the Betfair Exchange ################################################### all_soccer_markets <- abettor :: listCompetitions( eventTypeIds = 1 , ## Soccer is eventTypeId 1, toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Retrieving the competition id ## for the 2018 World Cup ################################################### world_cup_competition_id <- all_soccer_markets %>% dplyr :: pull(competition) %>% ## Extracting the variable competition which is a nested data frame dplyr :: filter(name == \"2018 FIFA World Cup\" ) %>% ## Filtering for the competition we need dplyr :: pull(id) ## Extracting the id for the competition we need ################################################### ## Obtaining all markets that are currently ## alive on the Betfair Exchange that belong to ## Competition ID that is mapped to the World Cup ################################################### all_world_cup_markets <- abettor :: listMarketCatalogue( eventTypeIds = 1 , ## Soccer is eventTypeId 1 marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = world_cup_competition_id, ## Restrict our search to World Cup matches only toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Obtaining the current odds on the Betfair ## Exchange for all the markets that were ## obtained in the previous step (World Cup Matches) ################################################### ## Creating a vector/array of all market ids all_world_cup_markets_market_ids <- all_world_cup_markets %>% pull(marketId) ## This function takes in a single market id and returns ## the current live odds on the Betfair Exchange for that market fetch_odds <- function (market_id) { ## Retrieving market odds for a single market odds <- abettor :: listMarketBook(marketIds = market_id, ##Runs listMarketBook for given market_id priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices ) %>% pull(runners) %>% ## Extracting the runners field which has details of odds as.data.frame () %>% ## Converting to data frame from list select(lastPriceTraded) %>% ## Extracting team and last matched odds mutate(market_id = market_id) %>% ## Padding market id to the data to make it unique to this match bind_cols( data.frame (outcome = c ( \"o_1\" , \"o_2\" , \"o_3\" ))) %>% ## Creating outcome order to maintain consistency spread(outcome, lastPriceTraded) %>% ## Reshaping data to make it 1 row per match rename(team_1_odds = o_1, team_2_odds = o_2, draw_odds = o_3) %>% ##Renaming columns such that all matches can be combined into one data frame select(market_id, team_1_odds, draw_odds, team_2_odds) ## Ordering columns in the right order return (odds) } ## The code below maps (or loops) each market id in the vector we created ## above through the fetch_odds function and retrives the market odds ## into a single data frame world_cup_market_odds <- map_df( . x = all_world_cup_markets_market_ids, ##Iterate over market ids . f = fetch_odds ## through function fetch_odds ) %>% bind_cols(all_world_cup_markets %>% ## Merge with event names to identify which match odds it is pull(event) %>% select(name)) %>% mutate(team_1 = gsub ( \" v .*\" , \"\" ,name), ## Extracting team 1 from match name team_2 = gsub ( \".* v \" , \"\" , name)) %>% ## Extracing team 2 from match name select(team_1, team_2, team_1_odds, draw_odds, team_2_odds) ## Extracting columns that we need ## Writing output to csv file write_csv(world_cup_market_odds, \"world_cup_market_odds.csv\" ) AFL Odds PulleR Tutorial \u00b6 This tutorial walks you through the process of retrieving exchange odds for the the next round of Australian Rules. You can run this script in R. ################################################### ### AFL Model ### Betfair API Odds GrabbR ### ### This script allows you to access the Betfair ### API and retrive exchange odds for all the ### matches for the upcoming round of AFL games ################################################### ## Loading required packages library (tidyverse) ## package for general data manipulation - https://www.tidyverse.org/ library (abettor) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor ## Login to Betfair - should return \"SUCCESS:\" on successful login betfair_login <- abettor :: loginBF(username = 'your_username' , password = 'your_password' , applicationKey = \"your_betfair_app_key\" ) ################################################### ## Retrieving all AFL competitions for ## which markets are currently alive ## on the Betfair Exchange ################################################### all_afl_markets <- abettor :: listCompetitions( eventTypeIds = 61420 , ## AFL is eventTypeId 61420, toDate = ( format ( Sys.time () + 86400 * 180 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 180 days ) ################################################### ## Retrieving the competition id ## for the regular AFL season ################################################### afl_competition_id <- all_afl_markets %>% dplyr :: pull(competition) %>% ## Extracting the variable competition which is a nested data frame dplyr :: filter(name == \"AFL\" ) %>% ## Filtering for the competition we need dplyr :: pull(id) ## Extracting the id for the competition we need ################################################### ## Obtaining all markets that are currently ## alive on the Betfair Exchange that belong to ## Competition ID that is mapped to the AFL ################################################### all_afl_markets <- abettor :: listMarketCatalogue( eventTypeIds = 61420 , ## AFL is eventTypeId 61420 marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = afl_competition_id, ## Restrict our search to AFL Matches Only toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Obtaining the current odds on the Betfair ## Exchange for all the markets that were ## obtained in the previous step ################################################### ## Creating a vector/array of all market ids all_afl_markets_market_ids <- all_afl_markets %>% pull(marketId) ## This function takes in a single market id and returns ## the current live odds on the Betfair Exchange for that market fetch_odds <- function (market_id) { ## Retrieving market odds for a single market odds <- abettor :: listMarketBook(marketIds = market_id, ##Runs listMarketBook for given market_id priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices ) %>% pull(runners) %>% ## Extracting the runners field which has details of odds as.data.frame () %>% ## Converting to data frame from list select(lastPriceTraded) %>% ## Extracting team and last matched odds mutate(market_id = market_id) %>% ## Padding market id to the data to make it unique to this match bind_cols( data.frame (outcome = c ( \"o_1\" , \"o_2\" ))) %>% ## Creating outcome order to maintain consistency spread(outcome, lastPriceTraded) %>% ## Reshaping data to make it 1 row per match rename(team_1_odds = o_1, team_2_odds = o_2) %>% ##Renaming columns such that all matches can be combined into one data frame select(market_id, team_1_odds, team_2_odds) ## Ordering columns in the right order return (odds) } ## The code below maps (or loops) each market id in the vector we created ## above through the fetch_odds function and retrives the market odds ## into a single data frame afl_market_odds <- map_df( . x = all_afl_markets_market_ids, ##Iterate over market ids . f = fetch_odds ## through function fetch_odds ) %>% bind_cols(all_afl_markets %>% ## Merge with event names to identify which match odds it is pull(event) %>% select(name)) %>% mutate(team_1 = gsub ( \" v .*\" , \"\" ,name), ## Extracting team 1 from match name team_2 = gsub ( \".* v \" , \"\" , name)) %>% ## Extracing team 2 from match name select(team_1, team_2, team_1_odds, team_2_odds) ## Extracting columns that we need ## Writing output to csv file write_csv(afl_market_odds, \"weekly_afl_odds.csv\" )","title":"API tutorials in R"},{"location":"api/apiRtutorial/#betfair-api-tutorials-in-r","text":"Betfair's API can be easily traversed in R. It allows you to retrieve market information, create/cancel bets and manage your account. Here's a collection of easy to follow API tutorials in R: Accessing the API using R Get Worldcup Odds AFL Odds PulleR Tutorial","title":"Betfair API tutorials in R"},{"location":"api/apiRtutorial/#accessing-the-api-using-r","text":"","title":"Accessing the API using R"},{"location":"api/apiRtutorial/#set-up-r","text":"What is R? Download and install R \u2013 get the language set up on your computer Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R","title":"Set up R"},{"location":"api/apiRtutorial/#required-packages","text":"Two R packages are required: library (tidyverse) library (abettor) The abettor package can be downloaded here . For an in-depth understanding of the package, have a read of the documentation. Instructions are also provided in the sample code.","title":"Required Packages"},{"location":"api/apiRtutorial/#login-to-betfair","text":"To login to Betfair, replace the following dummy username, password and app key with your own. abettor :: loginBF(username = \"betfair_username\" , password = \"betfair_password\" , applicationKey = \"betfair_app_key\" ) If you don't have a live app key for the API yet take a look at this page .","title":"Login to Betfair"},{"location":"api/apiRtutorial/#finding-event-ids","text":"In order to find data for specific markets, you will first need to know the event ID. This is easily achieved with the abettor package . To find the event IDs of events in the next 60 days: abettor :: listEventTypes(toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" ))) This will return a DataFrame of the following structure: eventType.id eventType.name marketCount 1 Soccer 1193 2 Tennis 2184 7522 Basketball 1 4 Cricket 37 7 Horse Racing 509 61420 Australian Rules 31 4339 Greyhound Racing 527","title":"Finding Event IDs"},{"location":"api/apiRtutorial/#finding-competition-ids","text":"Once you have the event ID, the next logical step is to find the competition IDs for the event you want to get data for. For example, if you want to find the competition IDs for Australian Rules, you would use the following abettor :: listCompetitions( eventTypeIds = 61420 , ## AFL is eventTypeId 61420, toDate = ( format ( Sys.time () + 86400 * 180 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 180 days ) This will return the following structured DataFrame: competition.id competition.name marketCount competitionRegion 11516633 Brownlow Medal 2018 3 AUS 11897406 AFL 78 AUS","title":"Finding Competition IDs"},{"location":"api/apiRtutorial/#finding-specific-markets","text":"The next logical step is to find the market that you are interested in. Furthering our example above, if you want the Match Odds for all Australian Rules games over the next 60 days, simply use the Competition ID from above in the following. abettor :: listMarketCatalogue( eventTypeIds = 61420 , marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = 11897406 , toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) This returns a large DataFrame object with each market, participants and associated odds.","title":"Finding Specific Markets"},{"location":"api/apiRtutorial/#get-world-cup-odds-tutorial","text":"This tutorial walks you through the process of retrieving exchange odds for all the matches from the 2018 FIFA World Cup 2018. This can be modified for other sports and uses. You can run this script in R. ################################################### ### FIFA World Cup Datathon ### Betfair API Tutorial ### ### This script allows you to access the Betfair ### API and retrive exchange odds for all the ### matches from the upcoming FIFA World Cup 2018 ################################################### ################################################### ### Setup ################################################### ## Loading required packages library (tidyverse) ## package for general data manipulation - https://www.tidyverse.org/ library (abettor) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor ## Enter your Betfair API Credentials below betfair_username <- \"\" betfair_password <- \"\" betfair_app_key <- \"\" ## Login to Betfair - should return \"SUCCESS:\" on successful login betfair_login <- abettor :: loginBF(username = betfair_username, password = betfair_password, applicationKey = betfair_app_key) ################################################### ## Retrieving all soccer competitions for ## which markets are currently alive ## on the Betfair Exchange ################################################### all_soccer_markets <- abettor :: listCompetitions( eventTypeIds = 1 , ## Soccer is eventTypeId 1, toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Retrieving the competition id ## for the 2018 World Cup ################################################### world_cup_competition_id <- all_soccer_markets %>% dplyr :: pull(competition) %>% ## Extracting the variable competition which is a nested data frame dplyr :: filter(name == \"2018 FIFA World Cup\" ) %>% ## Filtering for the competition we need dplyr :: pull(id) ## Extracting the id for the competition we need ################################################### ## Obtaining all markets that are currently ## alive on the Betfair Exchange that belong to ## Competition ID that is mapped to the World Cup ################################################### all_world_cup_markets <- abettor :: listMarketCatalogue( eventTypeIds = 1 , ## Soccer is eventTypeId 1 marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = world_cup_competition_id, ## Restrict our search to World Cup matches only toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Obtaining the current odds on the Betfair ## Exchange for all the markets that were ## obtained in the previous step (World Cup Matches) ################################################### ## Creating a vector/array of all market ids all_world_cup_markets_market_ids <- all_world_cup_markets %>% pull(marketId) ## This function takes in a single market id and returns ## the current live odds on the Betfair Exchange for that market fetch_odds <- function (market_id) { ## Retrieving market odds for a single market odds <- abettor :: listMarketBook(marketIds = market_id, ##Runs listMarketBook for given market_id priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices ) %>% pull(runners) %>% ## Extracting the runners field which has details of odds as.data.frame () %>% ## Converting to data frame from list select(lastPriceTraded) %>% ## Extracting team and last matched odds mutate(market_id = market_id) %>% ## Padding market id to the data to make it unique to this match bind_cols( data.frame (outcome = c ( \"o_1\" , \"o_2\" , \"o_3\" ))) %>% ## Creating outcome order to maintain consistency spread(outcome, lastPriceTraded) %>% ## Reshaping data to make it 1 row per match rename(team_1_odds = o_1, team_2_odds = o_2, draw_odds = o_3) %>% ##Renaming columns such that all matches can be combined into one data frame select(market_id, team_1_odds, draw_odds, team_2_odds) ## Ordering columns in the right order return (odds) } ## The code below maps (or loops) each market id in the vector we created ## above through the fetch_odds function and retrives the market odds ## into a single data frame world_cup_market_odds <- map_df( . x = all_world_cup_markets_market_ids, ##Iterate over market ids . f = fetch_odds ## through function fetch_odds ) %>% bind_cols(all_world_cup_markets %>% ## Merge with event names to identify which match odds it is pull(event) %>% select(name)) %>% mutate(team_1 = gsub ( \" v .*\" , \"\" ,name), ## Extracting team 1 from match name team_2 = gsub ( \".* v \" , \"\" , name)) %>% ## Extracing team 2 from match name select(team_1, team_2, team_1_odds, draw_odds, team_2_odds) ## Extracting columns that we need ## Writing output to csv file write_csv(world_cup_market_odds, \"world_cup_market_odds.csv\" )","title":"Get World Cup Odds Tutorial"},{"location":"api/apiRtutorial/#afl-odds-puller-tutorial","text":"This tutorial walks you through the process of retrieving exchange odds for the the next round of Australian Rules. You can run this script in R. ################################################### ### AFL Model ### Betfair API Odds GrabbR ### ### This script allows you to access the Betfair ### API and retrive exchange odds for all the ### matches for the upcoming round of AFL games ################################################### ## Loading required packages library (tidyverse) ## package for general data manipulation - https://www.tidyverse.org/ library (abettor) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor ## Login to Betfair - should return \"SUCCESS:\" on successful login betfair_login <- abettor :: loginBF(username = 'your_username' , password = 'your_password' , applicationKey = \"your_betfair_app_key\" ) ################################################### ## Retrieving all AFL competitions for ## which markets are currently alive ## on the Betfair Exchange ################################################### all_afl_markets <- abettor :: listCompetitions( eventTypeIds = 61420 , ## AFL is eventTypeId 61420, toDate = ( format ( Sys.time () + 86400 * 180 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 180 days ) ################################################### ## Retrieving the competition id ## for the regular AFL season ################################################### afl_competition_id <- all_afl_markets %>% dplyr :: pull(competition) %>% ## Extracting the variable competition which is a nested data frame dplyr :: filter(name == \"AFL\" ) %>% ## Filtering for the competition we need dplyr :: pull(id) ## Extracting the id for the competition we need ################################################### ## Obtaining all markets that are currently ## alive on the Betfair Exchange that belong to ## Competition ID that is mapped to the AFL ################################################### all_afl_markets <- abettor :: listMarketCatalogue( eventTypeIds = 61420 , ## AFL is eventTypeId 61420 marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = afl_competition_id, ## Restrict our search to AFL Matches Only toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Obtaining the current odds on the Betfair ## Exchange for all the markets that were ## obtained in the previous step ################################################### ## Creating a vector/array of all market ids all_afl_markets_market_ids <- all_afl_markets %>% pull(marketId) ## This function takes in a single market id and returns ## the current live odds on the Betfair Exchange for that market fetch_odds <- function (market_id) { ## Retrieving market odds for a single market odds <- abettor :: listMarketBook(marketIds = market_id, ##Runs listMarketBook for given market_id priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices ) %>% pull(runners) %>% ## Extracting the runners field which has details of odds as.data.frame () %>% ## Converting to data frame from list select(lastPriceTraded) %>% ## Extracting team and last matched odds mutate(market_id = market_id) %>% ## Padding market id to the data to make it unique to this match bind_cols( data.frame (outcome = c ( \"o_1\" , \"o_2\" ))) %>% ## Creating outcome order to maintain consistency spread(outcome, lastPriceTraded) %>% ## Reshaping data to make it 1 row per match rename(team_1_odds = o_1, team_2_odds = o_2) %>% ##Renaming columns such that all matches can be combined into one data frame select(market_id, team_1_odds, team_2_odds) ## Ordering columns in the right order return (odds) } ## The code below maps (or loops) each market id in the vector we created ## above through the fetch_odds function and retrives the market odds ## into a single data frame afl_market_odds <- map_df( . x = all_afl_markets_market_ids, ##Iterate over market ids . f = fetch_odds ## through function fetch_odds ) %>% bind_cols(all_afl_markets %>% ## Merge with event names to identify which match odds it is pull(event) %>% select(name)) %>% mutate(team_1 = gsub ( \" v .*\" , \"\" ,name), ## Extracting team 1 from match name team_2 = gsub ( \".* v \" , \"\" , name)) %>% ## Extracing team 2 from match name select(team_1, team_2, team_1_odds, team_2_odds) ## Extracting columns that we need ## Writing output to csv file write_csv(afl_market_odds, \"weekly_afl_odds.csv\" )","title":"AFL Odds PulleR Tutorial"},{"location":"api/apiappkey/","text":"How to access the Betfair API \u00b6 Betfair has it\u2019s own Exchange API . You can use it to programmatically retrieve live markets, automate successful trading strategies or create your own customised trading interface. Professional punters use it for these functions and many more. This guide helps Australian and New Zealand customers with obtaining their Betfair API Key. If you\u2019re outside of these two regions please go to the UK's Developer Program website . There are four steps involved in getting access to our API Obtain an SSOID token Register your application Obtain your app key Activate your app key Find your SSOID token \u00b6 The simplest way to setup your browser with the SSOID is to follow this link and log in - this will allow for the SSOID to be automatically populated in the next step. After loggin in, you\u2019ll be sent to the main Betfair website. Note: it may not show that you\u2019re logged in on the site. You can ignore that. Proceed to step two. Register your application \u00b6 Navigate to the API-NG accounts visualiser . If you\u2019ve followed step 1 correctly, your SSOID token should be automatically populated in the visualiser. Next click on createDeveloperAppKeys in the left hand navigation. Type in an application name (this is your app key name, so make sure this is unique), then click \u2018Execute\u2019 down the bottom of the page. If you receive an error message saying that your app key couldn\u2019t be created, it\u2019s most likely because you already have one. Use the getDeveloperAppKeys method in the left hand menu to check whether there\u2019s already an app key associated with your account. Find your app key \u00b6 After your key is created, you should see in the right hand panel your application: You\u2019ll notice that two application keys have been created; Version \u2013 1.0-Delay: is a delayed app key for development purposes Version \u2013 1.0: is the live pricing app key; on yours it should have a status \u2018No\u2019 in Active. Grab the application key listed for the live price one - for the example above, that is \u2018MkcBqyZrD53V6A..\u2019 Activate your app key \u00b6 This process will generate two app keys: \u2022 A developer key which is designed for development purposes. This has a variable delay of between 1 and 180 seconds, doesn\u2019t show matched volume and doesn\u2019t need to be activated prior to use. \u2022 A live app key is intended for transacting on the Exchange and should only be used when you\u2019re ready to start placing bets or can no longer test your strategy effectively using the developer key. Please note that if the live key is used to pull data from the Exchange without corresponding bets being placed a delay may be automatically applied to the live key. If you\u2019re ready to start testing your strategy or placing bets, please contact api@betfair.com.au and we will be happy to assist with activating the live key and implementing your strategy.","title":"How to access the Betfair API"},{"location":"api/apiappkey/#how-to-access-the-betfair-api","text":"Betfair has it\u2019s own Exchange API . You can use it to programmatically retrieve live markets, automate successful trading strategies or create your own customised trading interface. Professional punters use it for these functions and many more. This guide helps Australian and New Zealand customers with obtaining their Betfair API Key. If you\u2019re outside of these two regions please go to the UK's Developer Program website . There are four steps involved in getting access to our API Obtain an SSOID token Register your application Obtain your app key Activate your app key","title":"How to access the Betfair API"},{"location":"api/apiappkey/#find-your-ssoid-token","text":"The simplest way to setup your browser with the SSOID is to follow this link and log in - this will allow for the SSOID to be automatically populated in the next step. After loggin in, you\u2019ll be sent to the main Betfair website. Note: it may not show that you\u2019re logged in on the site. You can ignore that. Proceed to step two.","title":"Find your SSOID token"},{"location":"api/apiappkey/#register-your-application","text":"Navigate to the API-NG accounts visualiser . If you\u2019ve followed step 1 correctly, your SSOID token should be automatically populated in the visualiser. Next click on createDeveloperAppKeys in the left hand navigation. Type in an application name (this is your app key name, so make sure this is unique), then click \u2018Execute\u2019 down the bottom of the page. If you receive an error message saying that your app key couldn\u2019t be created, it\u2019s most likely because you already have one. Use the getDeveloperAppKeys method in the left hand menu to check whether there\u2019s already an app key associated with your account.","title":"Register your application"},{"location":"api/apiappkey/#find-your-app-key","text":"After your key is created, you should see in the right hand panel your application: You\u2019ll notice that two application keys have been created; Version \u2013 1.0-Delay: is a delayed app key for development purposes Version \u2013 1.0: is the live pricing app key; on yours it should have a status \u2018No\u2019 in Active. Grab the application key listed for the live price one - for the example above, that is \u2018MkcBqyZrD53V6A..\u2019","title":"Find your app key"},{"location":"api/apiappkey/#activate-your-app-key","text":"This process will generate two app keys: \u2022 A developer key which is designed for development purposes. This has a variable delay of between 1 and 180 seconds, doesn\u2019t show matched volume and doesn\u2019t need to be activated prior to use. \u2022 A live app key is intended for transacting on the Exchange and should only be used when you\u2019re ready to start placing bets or can no longer test your strategy effectively using the developer key. Please note that if the live key is used to pull data from the Exchange without corresponding bets being placed a delay may be automatically applied to the live key. If you\u2019re ready to start testing your strategy or placing bets, please contact api@betfair.com.au and we will be happy to assist with activating the live key and implementing your strategy.","title":"Activate your app key"},{"location":"historicData/dataSources/","text":"Historical Data Sources \u00b6 We know that your automated strategies and models are only as good as your data. We work hard to make sure you have access to the data you need to allow you to achieve what you're setting out to in your automation and modelling projects. There\u2019s a huge variety of historic pricing data available, and hopefully this page shows you how to access what you're looking for. For more information on how to use this data to make your own predictive model, take a look at our modelling section . Historical Stream API data \u00b6 Betfair UK give access to all the historical Stream API data since 2016. It is excellent to use in building models and back testing strategies, however isn't necessarily in an easily accessible format for everyone. What you need to know about this data source: \u00b6 JSON format, downloads as TAR files (zipped) Australian and overseas racing, plus soccer, tennis, cricket, golf and \u2018other sport\u2019 data All Exchange markets included since the Stream API was introduced in 2016 Time-stamped odds and volume data Able to filter by Event ID, market type and other parameters 3 tiers of access: Basic free tier \u2013 1 minute intervals for odds, no volume (free) Advanced tier \u2013 1 second intervals for odds, volume included (cost associated) Pro tier \u2013 50 millisecond intervals for odds, volume included (cost associated) Includes a Historic Data API endpoint for download management Tool available to convert the free data tier TAR files to CSV files Supporting resources to help you access this data: \u00b6 How to download and access the data files Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API Historical racing data \u00b6 This is an excellent resource if you are interested in racing and like to see market level data in a CSV format. What you need to know about this data source: \u00b6 CSV format Free to download All Australian and overseas races, dating back to the beginning of the Exchange Available as a single file per day, per country, win or place market Market snapshot by runner, including Max and min matched prices and volume, pre-play and in-play Weighted average price, pre-play and in-play BSP Winner Select Australian sports data \u00b6 We provide a range of data on Australian sports in an accessible CSV format. CSV format Select Australian sports including AFL, NRL and Super Rugby All matches 2011 \u2013 2018 Market level snapshot, including Max and min matched prices and volume, pre-play and in-play Weighted average price If none of these options suit your needs please contact us at bdp@betfair.com.au to discuss other potential options.","title":"Historic Data Sources"},{"location":"historicData/dataSources/#historical-data-sources","text":"We know that your automated strategies and models are only as good as your data. We work hard to make sure you have access to the data you need to allow you to achieve what you're setting out to in your automation and modelling projects. There\u2019s a huge variety of historic pricing data available, and hopefully this page shows you how to access what you're looking for. For more information on how to use this data to make your own predictive model, take a look at our modelling section .","title":"Historical Data Sources"},{"location":"historicData/dataSources/#historical-stream-api-data","text":"Betfair UK give access to all the historical Stream API data since 2016. It is excellent to use in building models and back testing strategies, however isn't necessarily in an easily accessible format for everyone.","title":"Historical Stream API data"},{"location":"historicData/dataSources/#what-you-need-to-know-about-this-data-source","text":"JSON format, downloads as TAR files (zipped) Australian and overseas racing, plus soccer, tennis, cricket, golf and \u2018other sport\u2019 data All Exchange markets included since the Stream API was introduced in 2016 Time-stamped odds and volume data Able to filter by Event ID, market type and other parameters 3 tiers of access: Basic free tier \u2013 1 minute intervals for odds, no volume (free) Advanced tier \u2013 1 second intervals for odds, volume included (cost associated) Pro tier \u2013 50 millisecond intervals for odds, volume included (cost associated) Includes a Historic Data API endpoint for download management Tool available to convert the free data tier TAR files to CSV files","title":"What you need to know about this data source:"},{"location":"historicData/dataSources/#supporting-resources-to-help-you-access-this-data","text":"How to download and access the data files Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API","title":"Supporting resources to help you access this data:"},{"location":"historicData/dataSources/#historical-racing-data","text":"This is an excellent resource if you are interested in racing and like to see market level data in a CSV format.","title":"Historical racing data"},{"location":"historicData/dataSources/#what-you-need-to-know-about-this-data-source_1","text":"CSV format Free to download All Australian and overseas races, dating back to the beginning of the Exchange Available as a single file per day, per country, win or place market Market snapshot by runner, including Max and min matched prices and volume, pre-play and in-play Weighted average price, pre-play and in-play BSP Winner","title":"What you need to know about this data source:"},{"location":"historicData/dataSources/#select-australian-sports-data","text":"We provide a range of data on Australian sports in an accessible CSV format. CSV format Select Australian sports including AFL, NRL and Super Rugby All matches 2011 \u2013 2018 Market level snapshot, including Max and min matched prices and volume, pre-play and in-play Weighted average price If none of these options suit your needs please contact us at bdp@betfair.com.au to discuss other potential options.","title":"Select Australian sports data"},{"location":"historicData/usingHistoricDataSite/","text":"Using the Historic Data site \u00b6 The Betfair Historic Data site includes complete historic data for nearly all markets offered on the Exchange since 2016, when the new APING was launched. The data available includes prices, volume traded, winning status, average weighted price, BSP, and a variety of other details that are valuable for modelling and strategy development. We know that the process of downloading and extracting these data files can be a bit intimidating the first time round, so here's a walk through of one way to go about it to help make it more accessible. Data tiers There are three tiers of historic data available on this site. You can download samples of each tier of data here . The biggest differece is between the free and paid data. The free data includes a lot of information about the market, but no volume, and only last traded price per minute, not a full price ladder. The two paid tiers include the same data, just at different frequencies. If your strategy isn't particularly price sensitive and doesn't need volume as a variable then you'll probably be fine wtih the free tier, however if you need to see a more granular view of the market then you should probably consider the paid advanced or pro tiers. A full catalogue of the values included in each data tier is available here . Basic Advanced Pro 1 minute intervals last traded price no volume 1 second intervals price ladder volume API tick intervals (50ms) price ladder volume Purchasing the data \u00b6 Start by going to the Betfair Historic Data site and log in using your Betfair account. Note: if you have less than 100 Betfair points you may have problems downloading data. On the Home page select the data set you want to download. Free data You need to 'purchase' the data set you want to download, even if it's from the free tier You can only 'purchase' each time period of data once. For example, if you had previously 'purchased' all Greyhound data for January 2018, then tried to download Greyhound data for January to March 2018 you would receive an error, and would need to purchase the data for February to March instead. Once you 'purchase' your choice of data it's recommended that you go to the My Data page, and choose the subset of data to then download. Downloading the data \u00b6 On the My Data page you can filter the purchased data to the actual markets you're interested in. You can filter by Sport, Date range, EventId, Event Name, Market Type, Country & File Type (M = market, E = Event), which will cut down the size of the data you need to download. For example, if you wanted the win market for Australian and New Zealand greyhound races you'd use these filters. File type The file type filter has two options that you can choose from: E = Event - includes event level data, i.e. Geelong greyhounds on x date M = Market - includes market level data, i.e. the win market for Geelong greyhounds race 3 on x date The site can be pretty slow to download from, and you'll generally have a better experience if you download the data a bit at a time, say month by month. Alternatively if you're going to download a lot of data it might be worth having a look at the historic data API, that can automate the download process and speed it up significantly. There's a guide available here , and some sample code the help get you started. Unzipping the files \u00b6 You'll need to download a program to unzip the TAR files. Here we'll be using 7Zip , which is free, open source and generally well respected. Once you've downloaded it make sure you also instal it onto the computer you'll be using to open the data files. Locate the data.tar file in your computer's file explorer program. Right click on the file, select '7-Zip' from the menu then choose 'Extract files...'. In the model that pops up change the pathmode to 'No pathnames'. You can also change the name and/or path of the folder you want the files extracted to if you want to. You now have a collection of .bz2 files. The final step is to select all the files, right click, select '7-Zip' from the menu then choose 'Extract here'. This will then extract all the individual zipped files which you can then either open in a text editor - you can use something basic like Notepad (installed on basically all computers by default) or a more complete program like Visual Studio Code (my go to), Vim or Notepad++ - or you can parse over the using a program to do the work for you. We'll explore how to parse the data another time. If you're opening the files with a text editor you might need to right click, choose 'open with' and select your preferred program. What's it for? \u00b6 The data available on the Historic Data site is extensive, and can be a really valuable tool or input. For example, you can include some of the columns as variables in a predictive model, compare BSP odds against win rates, or determine the average length of time it takes for 2 year old horses to run 1200m at Geelong. Quality data underpins the vast majority of successful betting strategies, so becoming comfortable working with the data available to you is a really important part of both the modelling and automation processes. Extra resources \u00b6 Here are some other useful resources that can help you work with this Historic Data: Historic Data FAQs Data Specification API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API","title":"Downloading from the Historic Data site"},{"location":"historicData/usingHistoricDataSite/#using-the-historic-data-site","text":"The Betfair Historic Data site includes complete historic data for nearly all markets offered on the Exchange since 2016, when the new APING was launched. The data available includes prices, volume traded, winning status, average weighted price, BSP, and a variety of other details that are valuable for modelling and strategy development. We know that the process of downloading and extracting these data files can be a bit intimidating the first time round, so here's a walk through of one way to go about it to help make it more accessible. Data tiers There are three tiers of historic data available on this site. You can download samples of each tier of data here . The biggest differece is between the free and paid data. The free data includes a lot of information about the market, but no volume, and only last traded price per minute, not a full price ladder. The two paid tiers include the same data, just at different frequencies. If your strategy isn't particularly price sensitive and doesn't need volume as a variable then you'll probably be fine wtih the free tier, however if you need to see a more granular view of the market then you should probably consider the paid advanced or pro tiers. A full catalogue of the values included in each data tier is available here . Basic Advanced Pro 1 minute intervals last traded price no volume 1 second intervals price ladder volume API tick intervals (50ms) price ladder volume","title":"Using the Historic Data site"},{"location":"historicData/usingHistoricDataSite/#purchasing-the-data","text":"Start by going to the Betfair Historic Data site and log in using your Betfair account. Note: if you have less than 100 Betfair points you may have problems downloading data. On the Home page select the data set you want to download. Free data You need to 'purchase' the data set you want to download, even if it's from the free tier You can only 'purchase' each time period of data once. For example, if you had previously 'purchased' all Greyhound data for January 2018, then tried to download Greyhound data for January to March 2018 you would receive an error, and would need to purchase the data for February to March instead. Once you 'purchase' your choice of data it's recommended that you go to the My Data page, and choose the subset of data to then download.","title":"Purchasing the data"},{"location":"historicData/usingHistoricDataSite/#downloading-the-data","text":"On the My Data page you can filter the purchased data to the actual markets you're interested in. You can filter by Sport, Date range, EventId, Event Name, Market Type, Country & File Type (M = market, E = Event), which will cut down the size of the data you need to download. For example, if you wanted the win market for Australian and New Zealand greyhound races you'd use these filters. File type The file type filter has two options that you can choose from: E = Event - includes event level data, i.e. Geelong greyhounds on x date M = Market - includes market level data, i.e. the win market for Geelong greyhounds race 3 on x date The site can be pretty slow to download from, and you'll generally have a better experience if you download the data a bit at a time, say month by month. Alternatively if you're going to download a lot of data it might be worth having a look at the historic data API, that can automate the download process and speed it up significantly. There's a guide available here , and some sample code the help get you started.","title":"Downloading the data"},{"location":"historicData/usingHistoricDataSite/#unzipping-the-files","text":"You'll need to download a program to unzip the TAR files. Here we'll be using 7Zip , which is free, open source and generally well respected. Once you've downloaded it make sure you also instal it onto the computer you'll be using to open the data files. Locate the data.tar file in your computer's file explorer program. Right click on the file, select '7-Zip' from the menu then choose 'Extract files...'. In the model that pops up change the pathmode to 'No pathnames'. You can also change the name and/or path of the folder you want the files extracted to if you want to. You now have a collection of .bz2 files. The final step is to select all the files, right click, select '7-Zip' from the menu then choose 'Extract here'. This will then extract all the individual zipped files which you can then either open in a text editor - you can use something basic like Notepad (installed on basically all computers by default) or a more complete program like Visual Studio Code (my go to), Vim or Notepad++ - or you can parse over the using a program to do the work for you. We'll explore how to parse the data another time. If you're opening the files with a text editor you might need to right click, choose 'open with' and select your preferred program.","title":"Unzipping the files"},{"location":"historicData/usingHistoricDataSite/#whats-it-for","text":"The data available on the Historic Data site is extensive, and can be a really valuable tool or input. For example, you can include some of the columns as variables in a predictive model, compare BSP odds against win rates, or determine the average length of time it takes for 2 year old horses to run 1200m at Geelong. Quality data underpins the vast majority of successful betting strategies, so becoming comfortable working with the data available to you is a really important part of both the modelling and automation processes.","title":"What's it for?"},{"location":"historicData/usingHistoricDataSite/#extra-resources","text":"Here are some other useful resources that can help you work with this Historic Data: Historic Data FAQs Data Specification API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API","title":"Extra resources"},{"location":"modelling/AFLmodelPart1/","text":"AFL Modelling Walkthrough \u00b6 01. Data Cleaning \u00b6 These tutorials will walk you through how to construct your own basic AFL model, using publicly available data. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through the basics of cleaning this dataset and how we have done it. If you want to get straight to feature creation or modelling, feel free to jump ahead! # Import libraries import pandas as pd import numpy as np import re pd . set_option( 'display.max_columns' , None ) We will first explore the DataFrames, and then create functions to wrangle them and clean them into more consistent sets of data. # Read/clean each DataFrame match_results = pd . read_csv( \"data/afl_match_results.csv\" ) odds = pd . read_csv( \"data/afl_odds.csv\" ) player_stats = pd . read_csv( \"data/afl_player_stats.csv\" ) odds . tail( 3 ) trunc event_name path selection_name odds 4179 2018-09-01 Match Odds VFL/Richmond Reserves v Williamstown Williamstown 2.3878 4180 2018-09-01 Match Odds WAFL/South Fremantle v West Perth South Fremantle 1.5024 4181 2018-09-01 Match Odds WAFL/South Fremantle v West Perth West Perth 2.7382 match_results . tail( 3 ) Game Date Round Home.Team Home.Goals Home.Behinds Home.Points Away.Team Away.Goals Away.Behinds Away.Points Venue Margin Season Round.Type Round.Number 15395 15396 2018-08-26 R23 Brisbane Lions 11 6 72 West Coast 14 14 98 Gabba -26 2018 Regular 23 15396 15397 2018-08-26 R23 Melbourne 15 12 102 GWS 8 9 57 M.C.G. 45 2018 Regular 23 15397 15398 2018-08-26 R23 St Kilda 14 10 94 North Melbourne 17 15 117 Docklands -23 2018 Regular 23 player_stats . tail( 3 ) AF B BO CCL CG CL CM CP D DE Date ED FA FF G GA HB HO I50 ITC K M MG MI5 Match_id One.Percenters Opposition Player R50 Round SC SCL SI Season Status T T5 TO TOG Team UP Venue 89317 38 1 0 0.0 0 0 1 2 9 55.6 25/08/2018 5 0 0 0 0 3 0 0 2.0 6 3 132.0 2 9711 0 Fremantle Christopher Mayne 1 Round 23 35 0.0 2.0 2018 Away 1 0.0 1.0 57 Collingwood 7 Optus Stadium 89318 38 0 0 0.0 3 0 0 3 9 55.6 25/08/2018 5 0 1 0 0 3 0 0 4.0 6 3 172.0 0 9711 2 Fremantle Nathan Murphy 5 Round 23 29 0.0 0.0 2018 Away 1 0.0 3.0 70 Collingwood 6 Optus Stadium 89319 56 1 0 0.0 1 0 0 3 8 62.5 25/08/2018 5 0 0 2 0 2 0 0 2.0 6 3 180.0 3 9711 2 Fremantle Jaidyn Stephenson 0 Round 23 56 0.0 4.0 2018 Away 3 1.0 2.0 87 Collingwood 5 Optus Stadium Have a look at the structure of the DataFrames. Notice that for the odds DataFrame, each game is split between two rows, whilst for the match_results each game is on one row. We will have to get around this by splitting the games up onto two rows, as this will allow our feature transformation functions to be applied more easily later on. For the player_stats DataFrame we will aggregate these stats into each game on separate rows. First, we will write functions to make the odds data look a bit nicer, with only a team column, a date column and a 'home_game' column which takes the values 0 or 1 depending on if it was a home game for that team. To do this we will use the regex module to extract the team names from the path column, as well as the to_datetime function from pandas. We will also replace all the inconsistent team names with consistent team names. def get_cleaned_odds (df = None ): # If a df hasn't been specified as a parameter, read the odds df if df is None : df = pd . read_csv( \"data/afl_odds.csv\" ) # Get a dictionary of team names we want to change and their new values team_name_mapping = { 'Adelaide Crows' : 'Adelaide' , 'Brisbane Lions' : 'Brisbane' , 'Carlton Blues' : 'Carlton' , 'Collingwood Magpies' : 'Collingwood' , 'Essendon Bombers' : 'Essendon' , 'Fremantle Dockers' : 'Fremantle' , 'GWS Giants' : 'GWS' , 'Geelong Cats' : 'Geelong' , 'Gold Coast Suns' : 'Gold Coast' , 'Greater Western Sydney' : 'GWS' , 'Greater Western Sydney Giants' : 'GWS' , 'Hawthorn Hawks' : 'Hawthorn' , 'Melbourne Demons' : 'Melbourne' , 'North Melbourne Kangaroos' : 'North Melbourne' , 'Port Adelaide Magpies' : 'Port Adelaide' , 'Port Adelaide Power' : 'Port Adelaide' , 'P Adelaide' : 'Port Adelaide' , 'Richmond Tigers' : 'Richmond' , 'St Kilda Saints' : 'St Kilda' , 'Sydney Swans' : 'Sydney' , 'West Coast Eagles' : 'West Coast' , 'Wetsern Bulldogs' : 'Western Bulldogs' , 'Western Bullbogs' : 'Western Bulldogs' } # Add columns df = (df . assign(date = lambda df: pd . to_datetime(df . trunc), # Create a datetime column home_team = lambda df: df . path . str . extract( '(([\\w\\s]+) v ([\\w\\s]+))' , expand = True )[ 1 ] . str . strip(), away_team = lambda df: df . path . str . extract( '(([\\w\\s]+) v ([\\w\\s]+))' , expand = True )[ 2 ] . str . strip()) . drop(columns = [ 'path' , 'trunc' , 'event_name' ]) # Drop irrelevant columns . rename(columns = { 'selection_name' : 'team' }) # Rename columns . replace(team_name_mapping) . sort_values(by = 'date' ) . reset_index(drop = True ) . assign(home_game = lambda df: df . apply( lambda row: 1 if row . home_team == row . team else 0 , axis = 'columns' )) . drop(columns = [ 'home_team' , 'away_team' ])) return df # Apply the wrangling and cleaning function odds = get_cleaned_odds(odds) odds . tail() team odds date home_game 4177 South Fremantle 1.5024 2018-09-01 1 4178 Port Melbourne 2.8000 2018-09-01 0 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 We now have a DataFrame that looks nice and easy to join with our other DataFrames. Now let's lean up the match_details DataFrame. # Define a function which cleans the match results df, and separates each teams' stats onto individual rows def get_cleaned_match_results (df = None ): # If a df hasn't been specified as a parameter, read the match_results df if df is None : df = pd . read_csv( \"data/afl_match_results.csv\" ) # Create column lists to loop through - these are the columns we want in home and away dfs home_columns = [ 'Game' , 'Date' , 'Round.Number' , 'Home.Team' , 'Home.Goals' , 'Home.Behinds' , 'Home.Points' , 'Margin' , 'Venue' , 'Away.Team' , 'Away.Goals' , 'Away.Behinds' , 'Away.Points' ] away_columns = [ 'Game' , 'Date' , 'Round.Number' , 'Away.Team' , 'Away.Goals' , 'Away.Behinds' , 'Away.Points' , 'Margin' , 'Venue' , 'Home.Team' , 'Home.Goals' , 'Home.Behinds' , 'Home.Points' ] mapping = [ 'game' , 'date' , 'round' , 'team' , 'goals' , 'behinds' , 'points' , 'margin' , 'venue' , 'opponent' , 'opponent_goals' , 'opponent_behinds' , 'opponent_points' ] team_name_mapping = { 'Brisbane Lions' : 'Brisbane' , 'Footscray' : 'Western Bulldogs' } # Create a df with only home games df_home = (df[home_columns] . rename(columns = {old_col: new_col for old_col, new_col in zip (home_columns, mapping)}) . assign(home_game =1 )) # Create a df with only away games df_away = (df[away_columns] . rename(columns = {old_col: new_col for old_col, new_col in zip (away_columns, mapping)}) . assign(home_game =0 , margin = lambda df: df . margin * -1 )) # Append these dfs together new_df = (df_home . append(df_away) . sort_values(by = 'game' ) # Sort by game ID . reset_index(drop = True ) # Reset index . assign(date = lambda df: pd . to_datetime(df . date)) # Create a datetime column . replace(team_name_mapping)) # Rename team names to be consistent with other dfs return new_df match_results = get_cleaned_match_results(match_results) match_results . head() game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 3 2 1897-05-08 1 St Kilda 2 4 16 -25 Victoria Park Collingwood 5 11 41 0 4 3 1897-05-08 1 Geelong 3 6 24 -23 Corio Oval Essendon 7 5 47 1 Now we have both the odds DataFrame and match_results DataFrame ready for feature creation! Finally, we will aggregate the player_stats DataFrame stats for each game rather than individual player stats. For this DataFrame we have regular stats, such as disposals, marks etc. and Advanced Stats, such as Tackles Inside 50 and Metres Gained. However these advanced stats are only available from 2015, so we will not be using them in this tutorial - as there isn't enough data from 2015 to train our models. Let's now aggregate the player_stats DataFrame. def get_cleaned_aggregate_player_stats (df = None ): # If a df hasn't been specified as a parameter, read the player_stats df if df is None : df = pd . read_csv( \"data/afl_player_stats.csv\" ) agg_stats = (df . rename(columns = { # Rename columns to lowercase 'Season' : 'season' , 'Round' : 'round' , 'Team' : 'team' , 'Opposition' : 'opponent' , 'Date' : 'date' }) . groupby(by = [ 'date' , 'season' , 'team' , 'opponent' ], as_index = False ) # Groupby to aggregate the stats for each game . sum() . drop(columns = [ 'DE' , 'TOG' , 'Match_id' ]) # Drop columns . assign(date = lambda df: pd . to_datetime(df . date, format = \" %d /%m/%Y\" )) # Create a datetime object . sort_values(by = 'date' ) . reset_index(drop = True )) return agg_stats agg_stats = get_cleaned_aggregate_player_stats(player_stats) agg_stats . tail() date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3621 2018-08-26 2018 Brisbane West Coast 1652 5 0 14.0 49 37 8 132 394 302 20 18 11 9 167 48 49 59.0 227 104 5571.0 6 48 39 1645 23.0 86.0 62 13.0 69.0 256 3622 2018-08-26 2018 West Coast Brisbane 1548 11 5 13.0 49 42 9 141 360 262 18 20 14 8 137 39 56 70.0 223 95 5809.0 12 39 34 1655 29.0 94.0 55 6.0 59.0 217 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 We now have a three fully prepared DataFrames which are almost ready to be analysed and for a model to be built on! Let's have a look at how they look and then merge them together into our final DataFrame. odds . tail( 3 ) team odds date home_game 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 match_results . tail( 3 ) game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 30793 15397 2018-08-26 23 Melbourne 15 12 102 45 M.C.G. GWS 8 9 57 1 30794 15398 2018-08-26 23 St Kilda 14 10 94 -23 Docklands North Melbourne 17 15 117 1 30795 15398 2018-08-26 23 North Melbourne 17 15 117 23 Docklands St Kilda 14 10 94 0 agg_stats . tail( 3 ) date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 merged_df = (odds[odds . team . isin(agg_stats . team . unique())] . pipe(pd . merge, match_results, on = [ 'date' , 'team' , 'home_game' ]) . pipe(pd . merge, agg_stats, on = [ 'date' , 'team' , 'opponent' ]) . sort_values(by = [ 'game' ])) merged_df . tail( 3 ) team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3199 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3195 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3200 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Great! We now have a clean looking datset with each row representing one team in a game. Let's now eliminate the outliers from a dataset. We know that Essendon had a doping scandal which resulted in their entire team being banned for a year in 2016, so let's remove all of their 2016 games. To do this we will filter based on the team and season, and then invert this with ~. # Define a function which eliminates outliers def outlier_eliminator (df): # Eliminate Essendon 2016 games essendon_filter_criteria = ~ (((df[ 'team' ] == 'Essendon' ) & (df[ 'season' ] == 2016 )) | ((df[ 'opponent' ] == 'Essendon' ) & (df[ 'season' ] == 2016 ))) df = df[essendon_filter_criteria] . reset_index(drop = True ) return df afl_data = outlier_eliminator(merged_df) afl_data . tail( 3 ) team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Finally, let's mark all of the columns that we are going to use in feature creation with the string 'f_' at the start of their column name so that we can easily filter for these columns. non_feature_cols = [ 'team' , 'date' , 'home_game' , 'game' , 'round' , 'venue' , 'opponent' , 'season' ] afl_data = afl_data . rename(columns = {col: 'f_' + col for col in afl_data if col not in non_feature_cols}) afl_data . tail( 3 ) team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Our data is now fully ready to be explored and for features to be created, which we will walk you through in our next tutorial, AFL Feature Creation Tutorial .","title":"AFL 01. Data cleaning"},{"location":"modelling/AFLmodelPart1/#afl-modelling-walkthrough","text":"","title":"AFL Modelling Walkthrough"},{"location":"modelling/AFLmodelPart1/#01-data-cleaning","text":"These tutorials will walk you through how to construct your own basic AFL model, using publicly available data. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through the basics of cleaning this dataset and how we have done it. If you want to get straight to feature creation or modelling, feel free to jump ahead! # Import libraries import pandas as pd import numpy as np import re pd . set_option( 'display.max_columns' , None ) We will first explore the DataFrames, and then create functions to wrangle them and clean them into more consistent sets of data. # Read/clean each DataFrame match_results = pd . read_csv( \"data/afl_match_results.csv\" ) odds = pd . read_csv( \"data/afl_odds.csv\" ) player_stats = pd . read_csv( \"data/afl_player_stats.csv\" ) odds . tail( 3 ) trunc event_name path selection_name odds 4179 2018-09-01 Match Odds VFL/Richmond Reserves v Williamstown Williamstown 2.3878 4180 2018-09-01 Match Odds WAFL/South Fremantle v West Perth South Fremantle 1.5024 4181 2018-09-01 Match Odds WAFL/South Fremantle v West Perth West Perth 2.7382 match_results . tail( 3 ) Game Date Round Home.Team Home.Goals Home.Behinds Home.Points Away.Team Away.Goals Away.Behinds Away.Points Venue Margin Season Round.Type Round.Number 15395 15396 2018-08-26 R23 Brisbane Lions 11 6 72 West Coast 14 14 98 Gabba -26 2018 Regular 23 15396 15397 2018-08-26 R23 Melbourne 15 12 102 GWS 8 9 57 M.C.G. 45 2018 Regular 23 15397 15398 2018-08-26 R23 St Kilda 14 10 94 North Melbourne 17 15 117 Docklands -23 2018 Regular 23 player_stats . tail( 3 ) AF B BO CCL CG CL CM CP D DE Date ED FA FF G GA HB HO I50 ITC K M MG MI5 Match_id One.Percenters Opposition Player R50 Round SC SCL SI Season Status T T5 TO TOG Team UP Venue 89317 38 1 0 0.0 0 0 1 2 9 55.6 25/08/2018 5 0 0 0 0 3 0 0 2.0 6 3 132.0 2 9711 0 Fremantle Christopher Mayne 1 Round 23 35 0.0 2.0 2018 Away 1 0.0 1.0 57 Collingwood 7 Optus Stadium 89318 38 0 0 0.0 3 0 0 3 9 55.6 25/08/2018 5 0 1 0 0 3 0 0 4.0 6 3 172.0 0 9711 2 Fremantle Nathan Murphy 5 Round 23 29 0.0 0.0 2018 Away 1 0.0 3.0 70 Collingwood 6 Optus Stadium 89319 56 1 0 0.0 1 0 0 3 8 62.5 25/08/2018 5 0 0 2 0 2 0 0 2.0 6 3 180.0 3 9711 2 Fremantle Jaidyn Stephenson 0 Round 23 56 0.0 4.0 2018 Away 3 1.0 2.0 87 Collingwood 5 Optus Stadium Have a look at the structure of the DataFrames. Notice that for the odds DataFrame, each game is split between two rows, whilst for the match_results each game is on one row. We will have to get around this by splitting the games up onto two rows, as this will allow our feature transformation functions to be applied more easily later on. For the player_stats DataFrame we will aggregate these stats into each game on separate rows. First, we will write functions to make the odds data look a bit nicer, with only a team column, a date column and a 'home_game' column which takes the values 0 or 1 depending on if it was a home game for that team. To do this we will use the regex module to extract the team names from the path column, as well as the to_datetime function from pandas. We will also replace all the inconsistent team names with consistent team names. def get_cleaned_odds (df = None ): # If a df hasn't been specified as a parameter, read the odds df if df is None : df = pd . read_csv( \"data/afl_odds.csv\" ) # Get a dictionary of team names we want to change and their new values team_name_mapping = { 'Adelaide Crows' : 'Adelaide' , 'Brisbane Lions' : 'Brisbane' , 'Carlton Blues' : 'Carlton' , 'Collingwood Magpies' : 'Collingwood' , 'Essendon Bombers' : 'Essendon' , 'Fremantle Dockers' : 'Fremantle' , 'GWS Giants' : 'GWS' , 'Geelong Cats' : 'Geelong' , 'Gold Coast Suns' : 'Gold Coast' , 'Greater Western Sydney' : 'GWS' , 'Greater Western Sydney Giants' : 'GWS' , 'Hawthorn Hawks' : 'Hawthorn' , 'Melbourne Demons' : 'Melbourne' , 'North Melbourne Kangaroos' : 'North Melbourne' , 'Port Adelaide Magpies' : 'Port Adelaide' , 'Port Adelaide Power' : 'Port Adelaide' , 'P Adelaide' : 'Port Adelaide' , 'Richmond Tigers' : 'Richmond' , 'St Kilda Saints' : 'St Kilda' , 'Sydney Swans' : 'Sydney' , 'West Coast Eagles' : 'West Coast' , 'Wetsern Bulldogs' : 'Western Bulldogs' , 'Western Bullbogs' : 'Western Bulldogs' } # Add columns df = (df . assign(date = lambda df: pd . to_datetime(df . trunc), # Create a datetime column home_team = lambda df: df . path . str . extract( '(([\\w\\s]+) v ([\\w\\s]+))' , expand = True )[ 1 ] . str . strip(), away_team = lambda df: df . path . str . extract( '(([\\w\\s]+) v ([\\w\\s]+))' , expand = True )[ 2 ] . str . strip()) . drop(columns = [ 'path' , 'trunc' , 'event_name' ]) # Drop irrelevant columns . rename(columns = { 'selection_name' : 'team' }) # Rename columns . replace(team_name_mapping) . sort_values(by = 'date' ) . reset_index(drop = True ) . assign(home_game = lambda df: df . apply( lambda row: 1 if row . home_team == row . team else 0 , axis = 'columns' )) . drop(columns = [ 'home_team' , 'away_team' ])) return df # Apply the wrangling and cleaning function odds = get_cleaned_odds(odds) odds . tail() team odds date home_game 4177 South Fremantle 1.5024 2018-09-01 1 4178 Port Melbourne 2.8000 2018-09-01 0 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 We now have a DataFrame that looks nice and easy to join with our other DataFrames. Now let's lean up the match_details DataFrame. # Define a function which cleans the match results df, and separates each teams' stats onto individual rows def get_cleaned_match_results (df = None ): # If a df hasn't been specified as a parameter, read the match_results df if df is None : df = pd . read_csv( \"data/afl_match_results.csv\" ) # Create column lists to loop through - these are the columns we want in home and away dfs home_columns = [ 'Game' , 'Date' , 'Round.Number' , 'Home.Team' , 'Home.Goals' , 'Home.Behinds' , 'Home.Points' , 'Margin' , 'Venue' , 'Away.Team' , 'Away.Goals' , 'Away.Behinds' , 'Away.Points' ] away_columns = [ 'Game' , 'Date' , 'Round.Number' , 'Away.Team' , 'Away.Goals' , 'Away.Behinds' , 'Away.Points' , 'Margin' , 'Venue' , 'Home.Team' , 'Home.Goals' , 'Home.Behinds' , 'Home.Points' ] mapping = [ 'game' , 'date' , 'round' , 'team' , 'goals' , 'behinds' , 'points' , 'margin' , 'venue' , 'opponent' , 'opponent_goals' , 'opponent_behinds' , 'opponent_points' ] team_name_mapping = { 'Brisbane Lions' : 'Brisbane' , 'Footscray' : 'Western Bulldogs' } # Create a df with only home games df_home = (df[home_columns] . rename(columns = {old_col: new_col for old_col, new_col in zip (home_columns, mapping)}) . assign(home_game =1 )) # Create a df with only away games df_away = (df[away_columns] . rename(columns = {old_col: new_col for old_col, new_col in zip (away_columns, mapping)}) . assign(home_game =0 , margin = lambda df: df . margin * -1 )) # Append these dfs together new_df = (df_home . append(df_away) . sort_values(by = 'game' ) # Sort by game ID . reset_index(drop = True ) # Reset index . assign(date = lambda df: pd . to_datetime(df . date)) # Create a datetime column . replace(team_name_mapping)) # Rename team names to be consistent with other dfs return new_df match_results = get_cleaned_match_results(match_results) match_results . head() game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 3 2 1897-05-08 1 St Kilda 2 4 16 -25 Victoria Park Collingwood 5 11 41 0 4 3 1897-05-08 1 Geelong 3 6 24 -23 Corio Oval Essendon 7 5 47 1 Now we have both the odds DataFrame and match_results DataFrame ready for feature creation! Finally, we will aggregate the player_stats DataFrame stats for each game rather than individual player stats. For this DataFrame we have regular stats, such as disposals, marks etc. and Advanced Stats, such as Tackles Inside 50 and Metres Gained. However these advanced stats are only available from 2015, so we will not be using them in this tutorial - as there isn't enough data from 2015 to train our models. Let's now aggregate the player_stats DataFrame. def get_cleaned_aggregate_player_stats (df = None ): # If a df hasn't been specified as a parameter, read the player_stats df if df is None : df = pd . read_csv( \"data/afl_player_stats.csv\" ) agg_stats = (df . rename(columns = { # Rename columns to lowercase 'Season' : 'season' , 'Round' : 'round' , 'Team' : 'team' , 'Opposition' : 'opponent' , 'Date' : 'date' }) . groupby(by = [ 'date' , 'season' , 'team' , 'opponent' ], as_index = False ) # Groupby to aggregate the stats for each game . sum() . drop(columns = [ 'DE' , 'TOG' , 'Match_id' ]) # Drop columns . assign(date = lambda df: pd . to_datetime(df . date, format = \" %d /%m/%Y\" )) # Create a datetime object . sort_values(by = 'date' ) . reset_index(drop = True )) return agg_stats agg_stats = get_cleaned_aggregate_player_stats(player_stats) agg_stats . tail() date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3621 2018-08-26 2018 Brisbane West Coast 1652 5 0 14.0 49 37 8 132 394 302 20 18 11 9 167 48 49 59.0 227 104 5571.0 6 48 39 1645 23.0 86.0 62 13.0 69.0 256 3622 2018-08-26 2018 West Coast Brisbane 1548 11 5 13.0 49 42 9 141 360 262 18 20 14 8 137 39 56 70.0 223 95 5809.0 12 39 34 1655 29.0 94.0 55 6.0 59.0 217 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 We now have a three fully prepared DataFrames which are almost ready to be analysed and for a model to be built on! Let's have a look at how they look and then merge them together into our final DataFrame. odds . tail( 3 ) team odds date home_game 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 match_results . tail( 3 ) game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 30793 15397 2018-08-26 23 Melbourne 15 12 102 45 M.C.G. GWS 8 9 57 1 30794 15398 2018-08-26 23 St Kilda 14 10 94 -23 Docklands North Melbourne 17 15 117 1 30795 15398 2018-08-26 23 North Melbourne 17 15 117 23 Docklands St Kilda 14 10 94 0 agg_stats . tail( 3 ) date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 merged_df = (odds[odds . team . isin(agg_stats . team . unique())] . pipe(pd . merge, match_results, on = [ 'date' , 'team' , 'home_game' ]) . pipe(pd . merge, agg_stats, on = [ 'date' , 'team' , 'opponent' ]) . sort_values(by = [ 'game' ])) merged_df . tail( 3 ) team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3199 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3195 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3200 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Great! We now have a clean looking datset with each row representing one team in a game. Let's now eliminate the outliers from a dataset. We know that Essendon had a doping scandal which resulted in their entire team being banned for a year in 2016, so let's remove all of their 2016 games. To do this we will filter based on the team and season, and then invert this with ~. # Define a function which eliminates outliers def outlier_eliminator (df): # Eliminate Essendon 2016 games essendon_filter_criteria = ~ (((df[ 'team' ] == 'Essendon' ) & (df[ 'season' ] == 2016 )) | ((df[ 'opponent' ] == 'Essendon' ) & (df[ 'season' ] == 2016 ))) df = df[essendon_filter_criteria] . reset_index(drop = True ) return df afl_data = outlier_eliminator(merged_df) afl_data . tail( 3 ) team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Finally, let's mark all of the columns that we are going to use in feature creation with the string 'f_' at the start of their column name so that we can easily filter for these columns. non_feature_cols = [ 'team' , 'date' , 'home_game' , 'game' , 'round' , 'venue' , 'opponent' , 'season' ] afl_data = afl_data . rename(columns = {col: 'f_' + col for col in afl_data if col not in non_feature_cols}) afl_data . tail( 3 ) team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Our data is now fully ready to be explored and for features to be created, which we will walk you through in our next tutorial, AFL Feature Creation Tutorial .","title":"01. Data Cleaning"},{"location":"modelling/AFLmodelPart2/","text":"AFL Modelling Walkthrough \u00b6 02. Feature Creation \u00b6 These tutorials will walk you through how to construct your own basic AFL model. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through creating features from our dataset, which was cleaned in the first tutorial. Feature engineering is an integral part of the Data Science process. Creative and smart features can be the difference between an average performing model and a model profitable which beats the market odds. Grabbing Our Dataset \u00b6 First, we will import our required modules, as well as the prepare_afl_data function which we created in our afl_data_cleaning script. This essentially cleans all the data for us so that we're ready to explore the data and make some features. # Import modules from afl_data_cleaning_v2 import * import afl_data_cleaning_v2 import pandas as pd pd . set_option( 'display.max_columns' , None ) import warnings warnings . filterwarnings( 'ignore' ) import numpy as np # Use the prepare_afl_data function to prepare the data for us; this function condenses what we walked through in the previous tutorial afl_data = prepare_afl_data() afl_data . tail( 3 ) team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Creating A Feature DataFrame \u00b6 Let's create a feature DataFrame and merge all of our features into this DataFrame as we go. features = afl_data[[ 'date' , 'game' , 'team' , 'opponent' , 'venue' , 'home_game' ]] . copy() What Each Column Refers To \u00b6 Below is a DataFrame which outlines what each column refers to. column_abbreviations = pd . read_csv( \"data/afl_data_columns_mapping.csv\" ) column_abbreviations Feature Abbreviated Feature 0 GA Goal Assists 1 CP Contested Possessions 2 UP Uncontested Possessions 3 ED Effective Disposals 4 CM Contested Marks 5 MI5 Marks Inside 50 6 One.Percenters One Percenters 7 BO Bounces 8 K Kicks 9 HB Handballs 10 D Disposals 11 M Marks 12 G Goals 13 B Behinds 14 T Tackles 15 HO Hitouts 16 I50 Inside 50s 17 CL Clearances 18 CG Clangers 19 R50 Rebound 50s 20 FF Frees For 21 FA Frees Against 22 AF AFL Fantasy Points 23 SC Supercoach Points 24 CCL Centre Clearances 25 SCL Stoppage Clearances 26 SI Score Involvements 27 MG Metres Gained 28 TO Turnovers 29 ITC Intercepts 30 T5 Tackles Inside 50 Feature Creation \u00b6 Now let's think about what features we can create. We have a enormous amount of stats to sift through. To start, let's create some simple features based on our domain knowledge of Aussie Rules. Creating Expontentially Weighted Rolling Averages as Features \u00b6 Next, we will create rolling averages of statistics such as Tackles, which we will use as features. It is fair to assume that a team's performance in a certain stat may have predictive power to the overall result. And in general, if a team consistently performs well in this stat, this may have predictive power to the result of their future games. We can't simply train a model on stats from the game which we are trying to predict (i.e. data that we don't have before the game begins), as this will leak the result. We need to train our model on past data. One way of doing this is to train our model on average stats over a certain amount of games. If a team is averaging high in this stat, this may give insight into if they are a strong team. Similarly, if the team is averaging poorly in this stat (relative to the team they are playing), this may have predictive power and give rise to a predicted loss. To do this we will create a function which calculates the rolling averages, known as create_exp_weighted_avgs, which takes our cleaned DataFrame as an input, as well as the alpha which, when higher, weights recent performances more than old performances. To read more about expontentially weighted moving averages, please read the documentation here . First, we will grab all the columns which we want to create EMAs for, and then use our function to create the average for that column. We will create a new DataFrame and add these columns to this new DataFrame. # Define a function which returns a DataFrame with the expontential moving average for each numeric stat def create_exp_weighted_avgs (df, span): # Create a copy of the df with only the game id and the team - we will add cols to this df ema_features = df[[ 'game' , 'team' ]] . copy() feature_names = [col for col in df . columns if col . startswith( 'f_' )] # Get a list of columns we will iterate over for feature_name in feature_names: feature_ema = (df . groupby( 'team' )[feature_name] . transform( lambda row: (row . ewm(span = span) . mean() . shift( 1 )))) ema_features[feature_name] = feature_ema return ema_features features_rolling_averages = create_exp_weighted_avgs(afl_data, span =10 ) features_rolling_averages . tail() game team f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3152 15396 West Coast 2.094236 12.809630 10.047145 86.904928 8.888770 11.435452 9.403444 78.016158 3193.612782 16.472115 11.958482 23.379562 100.095244 68.252001 27.688669 284.463270 719.884644 525.878017 36.762440 44.867118 25.618202 17.522871 270.478779 88.139376 105.698031 148.005305 449.405865 201.198907 11581.929999 20.048124 95.018480 74.180967 3314.157893 44.872398 177.894442 126.985101 20.565549 138.876613 438.848376 3153 15397 GWS 1.805565 13.100372 13.179329 91.781563 18.527618 10.371198 11.026754 73.253945 3165.127358 19.875913 12.947209 25.114002 105.856671 80.609640 23.374884 303.160047 741.439198 534.520295 42.597317 38.160889 26.208715 18.688880 300.188301 81.540693 106.989070 143.032506 441.250897 173.050118 12091.630837 21.106142 103.077097 80.201059 3419.245919 55.495610 219.879895 138.202470 25.313148 135.966798 438.466439 3154 15397 Melbourne 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.787800 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 3155 15398 North Melbourne 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.541130 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3156 15398 St Kilda 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.498760 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 As you can see our function worked perfectly! Now we have a full DataFrame of exponentially weighted moving averages. Note that as these rolling averages have been shifted by 1 to ensure no data leakage, the first round of the data will have all NA values. We can drop these later. Let's add these averages to our features DataFrame features = pd . merge(features, features_rolling_averages, on = [ 'game' , 'team' ]) Creating a 'Form Between the Teams' Feature \u00b6 It is well known in Aussie Rules that often some teams perform better against certain teams than others. If we isolate our features to pure stats based on previous games not between the teams playing, or elo ratings, we won't account for any relationships between certain teams. An example is the Kennett Curse , where Geelong won 11 consecutive games against Hawthorn, despite being similarly matched teams. Let's create a feature which calculates how many games a team has won against their opposition over a given window of games. To do this, we will need to use historical data that dates back well before our current DataFrame starts at. Otherwise we will be using a lot of our games to calculate form, meaning we will have to drop these rows before feeding it into an algorithm. So let's use our prepare_match_results function which we defined in the afl_data_cleaning tutorial to grab a clean DataFrame of all match results since 1897. We can then calculate the form and join this to our current DataFrame. match_results = afl_data_cleaning_v2 . get_cleaned_match_results() match_results . head( 3 ) game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 form_btwn_teams = match_results[[ 'game' , 'team' , 'opponent' , 'margin' ]] . copy() form_btwn_teams[ 'f_form_margin_btwn_teams' ] = (match_results . groupby([ 'team' , 'opponent' ])[ 'margin' ] . transform( lambda row: row . rolling( 5 ) . mean() . shift()) . fillna( 0 )) form_btwn_teams[ 'f_form_past_5_btwn_teams' ] = \\ (match_results . assign(win = lambda df: df . apply( lambda row: 1 if row . margin > 0 else 0 , axis = 'columns' )) . groupby([ 'team' , 'opponent' ])[ 'win' ] . transform( lambda row: row . rolling( 5 ) . mean() . shift() * 5 ) . fillna( 0 )) form_btwn_teams . tail( 3 ) game team opponent margin f_form_margin_btwn_teams f_form_past_5_btwn_teams 30793 15397 Melbourne GWS 45 -23.2 2.0 30794 15398 St Kilda North Melbourne -23 -3.2 2.0 30795 15398 North Melbourne St Kilda 23 3.2 3.0 # Merge to our features df features = pd . merge(features, form_btwn_teams . drop(columns = [ 'margin' ]), on = [ 'game' , 'team' , 'opponent' ]) Creating Efficiency Features \u00b6 Disposal Efficiency \u00b6 Disposal efficiency is pivotal in Aussie Rules football. If you are dispose of the ball effectively you are much more likely to score and much less likely to concede goals than if you dispose of it ineffectively. Let's create a disposal efficiency feature by dividing Effective Disposals by Disposals. Inside 50/Rebound 50 Efficiency \u00b6 Similarly, one could hypothesise that teams who keep the footy in their Inside 50 regularly will be more likely to score, whilst teams who are effective at getting the ball out of their Defensive 50 will be less likely to concede. Let's use this logic to create Inside 50 Efficiency and Rebound 50 Efficiency features. The formula used will be: Inside 50 Efficiency = R50_Opponents / I50 (lower is better). Rebound 50 Efficiency = R50 / I50_Opponents (higher is better). Using these formulas, I50 Efficiency = R50 Efficiency_Opponent. So we will just need to create the formulas for I50 efficiency. To create these features we will need the opposition's Inside 50s/Rebound 50s. So we will split out data into two DataFrames, create a new DataFrame by joining these two DataFrames on the Game, calculate our efficiency features, then join our features with our main features DataFrame. # Get each match on single rows single_row_df = (afl_data[[ 'game' , 'team' , 'f_I50' , 'f_R50' , 'f_D' , 'f_ED' , 'home_game' , ]] . query( 'home_game == 1' ) . rename(columns = { 'team' : 'home_team' , 'f_I50' : 'f_I50_home' , 'f_R50' : 'f_R50_home' , 'f_D' : 'f_D_home' , 'f_ED' : 'f_ED_home' }) . drop(columns = 'home_game' ) . pipe(pd . merge, afl_data[[ 'game' , 'team' , 'f_I50' , 'f_R50' , 'f_D' , 'f_ED' , 'home_game' ]] . query( 'home_game == 0' ) . rename(columns = { 'team' : 'away_team' , 'f_I50' : 'f_I50_away' , 'f_R50' : 'f_R50_away' , 'f_D' : 'f_D_away' , 'f_ED' : 'f_ED_away' }) . drop(columns = 'home_game' ), on = 'game' )) single_row_df . head() game home_team f_I50_home f_R50_home f_D_home f_ED_home away_team f_I50_away f_R50_away f_D_away f_ED_away 0 13764 Carlton 69 21 373 268 Richmond 37 50 316 226 1 13765 Geelong 54 40 428 310 St Kilda 52 45 334 246 2 13766 Collingwood 70 38 398 289 Port Adelaide 50 44 331 232 3 13767 Adelaide 59 38 366 264 Hawthorn 54 38 372 264 4 13768 Brisbane 50 39 343 227 Fremantle 57 30 351 250 single_row_df = single_row_df . assign(f_I50_efficiency_home = lambda df: df . f_R50_away / df . f_I50_home, f_I50_efficiency_away = lambda df: df . f_R50_home / df . f_I50_away) feature_efficiency_cols = [ 'f_I50_efficiency_home' , 'f_I50_efficiency_away' ] # Now let's create an Expontentially Weighted Moving Average for these features - we will need to reshape our DataFrame to do this efficiency_features_multi_row = (single_row_df[[ 'game' , 'home_team' ] + feature_efficiency_cols] . rename(columns = { 'home_team' : 'team' , 'f_I50_efficiency_home' : 'f_I50_efficiency' , 'f_I50_efficiency_away' : 'f_I50_efficiency_opponent' , }) . append((single_row_df[[ 'game' , 'away_team' ] + feature_efficiency_cols] . rename(columns = { 'away_team' : 'team' , 'f_I50_efficiency_home' : 'f_I50_efficiency_opponent' , 'f_I50_efficiency_away' : 'f_I50_efficiency' , })), sort = True ) . sort_values(by = 'game' ) . reset_index(drop = True )) efficiency_features = efficiency_features_multi_row[[ 'game' , 'team' ]] . copy() feature_efficiency_cols = [ 'f_I50_efficiency' , 'f_I50_efficiency_opponent' ] for feature in feature_efficiency_cols: efficiency_features[feature] = (efficiency_features_multi_row . groupby( 'team' )[feature] . transform( lambda row: row . ewm(span =10 ) . mean() . shift( 1 ))) # Get feature efficiency df back onto single rows efficiency_features = pd . merge(efficiency_features, afl_data[[ 'game' , 'team' , 'home_game' ]], on = [ 'game' , 'team' ]) efficiency_features_single_row = (efficiency_features . query( 'home_game == 1' ) . rename(columns = { 'team' : 'home_team' , 'f_I50_efficiency' : 'f_I50_efficiency_home' , 'f_I50_efficiency_opponent' : 'f_R50_efficiency_home' }) . drop(columns = 'home_game' ) . pipe(pd . merge, (efficiency_features . query( 'home_game == 0' ) . rename(columns = { 'team' : 'away_team' , 'f_I50_efficiency' : 'f_I50_efficiency_away' , 'f_I50_efficiency_opponent' : 'f_R50_efficiency_away' }) . drop(columns = 'home_game' )), on = 'game' )) efficiency_features_single_row . tail( 5 ) game home_team f_I50_efficiency_home f_R50_efficiency_home away_team f_I50_efficiency_away f_R50_efficiency_away 1580 15394 Carlton 0.730668 0.675002 Adelaide 0.691614 0.677128 1581 15395 Sydney 0.699994 0.778280 Hawthorn 0.699158 0.673409 1582 15396 Brisbane 0.683604 0.691730 West Coast 0.696822 0.709605 1583 15397 Melbourne 0.667240 0.692632 GWS 0.684525 0.753783 1584 15398 St Kilda 0.730843 0.635819 North Melbourne 0.697018 0.654991 We will merge these features back to our features df later, when the features data frame is on a single row as well. Creating an Elo Feature \u00b6 Another feature which we could create is an Elo feature. If you don't know what Elo is, go ahead and read our article on it here . We have also written a guide on using elo to model the 2018 FIFA World Cup here . Essentially, Elo ratings increase if you win. The amount the rating increases is based on how strong the opponent is relative to the team who won. Weak teams get more points for beating stronger teams than they do for beating weaker teams, and vice versa for losses (teams lose points for losses). Mathematically, Elo ratings can also assign a probability for winning or losing based on the two Elo Ratings of the teams playing. So let's get into it. We will first define a function which calculates the elo for each team and applies these elos to our DataFrame. # Define a function which finds the elo for each team in each game and returns a dictionary with the game ID as a key and the # elos as the key's value, in a list. It also outputs the probabilities and a dictionary of the final elos for each team def elo_applier (df, k_factor): # Initialise a dictionary with default elos for each team elo_dict = {team: 1500 for team in df[ 'team' ] . unique()} elos, elo_probs = {}, {} # Get a home and away dataframe so that we can get the teams on the same row home_df = df . loc[df . home_game == 1 , [ 'team' , 'game' , 'f_margin' , 'home_game' ]] . rename(columns = { 'team' : 'home_team' }) away_df = df . loc[df . home_game == 0 , [ 'team' , 'game' ]] . rename(columns = { 'team' : 'away_team' }) df = (pd . merge(home_df, away_df, on = 'game' ) . sort_values(by = 'game' ) . drop_duplicates(subset = 'game' , keep = 'first' ) . reset_index(drop = True )) # Loop over the rows in the DataFrame for index, row in df . iterrows(): # Get the Game ID game_id = row[ 'game' ] # Get the margin margin = row[ 'f_margin' ] # If the game already has the elos for the home and away team in the elos dictionary, go to the next game if game_id in elos . keys(): continue # Get the team and opposition home_team = row[ 'home_team' ] away_team = row[ 'away_team' ] # Get the team and opposition elo score home_team_elo = elo_dict[home_team] away_team_elo = elo_dict[away_team] # Calculated the probability of winning for the team and opposition prob_win_home = 1 / ( 1 + 10** ((away_team_elo - home_team_elo) / 400 )) prob_win_away = 1 - prob_win_home # Add the elos and probabilities our elos dictionary and elo_probs dictionary based on the Game ID elos[game_id] = [home_team_elo, away_team_elo] elo_probs[game_id] = [prob_win_home, prob_win_away] # Calculate the new elos of each team if margin > 0 : # Home team wins; update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 1 - prob_win_home) new_away_team_elo = away_team_elo + k_factor * ( 0 - prob_win_away) elif margin < 0 : # Away team wins; update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 0 - prob_win_home) new_away_team_elo = away_team_elo + k_factor * ( 1 - prob_win_away) elif margin == 0 : # Drawn game' update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 0.5 - prob_win_home) new_away_team_elo = away_team_elo + k_factor * ( 0.5 - prob_win_away) # Update elos in elo dictionary elo_dict[home_team] = new_home_team_elo elo_dict[away_team] = new_away_team_elo return elos, elo_probs, elo_dict # Use the elo applier function to get the elos and elo probabilities for each game - we will map these later elos, probs, elo_dict = elo_applier(afl_data, 30 ) Great! now we have both rolling averages for stats as a feature, and the elo of the teams! Let's have a quick look at the current elo standings with a k-factor of 30, out of curiosity. for team in sorted (elo_dict, key = elo_dict . get)[:: -1 ]: print (team, elo_dict[team]) Richmond 1695.2241513840117 Sydney 1645.548990879842 Hawthorn 1632.5266709780622 West Coast 1625.871701773721 Geelong 1625.423154644809 GWS 1597.4158602131877 Adelaide 1591.1704934545442 Collingwood 1560.370309216614 Melbourne 1558.5666572771509 Essendon 1529.0198398117086 Port Adelaide 1524.8882517820093 North Melbourne 1465.5637511922569 Western Bulldogs 1452.2110697845148 Fremantle 1393.142087030804 St Kilda 1360.9120149937303 Brisbane 1276.2923772139352 Gold Coast 1239.174528704772 Carlton 1226.6780896643265 This looks extremely similar to the currently AFL ladder, so this is a good sign for elo being an effective predictor of winning. Merging Our Features Into One Features DataFrame \u00b6 Now we need to reshape our features df so that we have all of the statistics for both teams in a game on a single row. We can then merge our elo and efficiency features to this df. # Look at our current features df features . tail( 3 ) date game team opponent venue home_game f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP f_form_margin_btwn_teams f_form_past_5_btwn_teams 3156 2018-08-26 15397 Melbourne GWS M.C.G. 1 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.78780 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 -23.2 2.0 3157 2018-08-26 15398 North Melbourne St Kilda Docklands 0 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.54113 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3.2 3.0 3158 2018-08-26 15398 St Kilda North Melbourne Docklands 1 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.49876 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 -3.2 2.0 one_line_cols = [ 'game' , 'team' , 'home_game' ] + [col for col in features if col . startswith( 'f_' )] # Get all features onto individual rows for each match features_one_line = (features . loc[features . home_game == 1 , one_line_cols] . rename(columns = { 'team' : 'home_team' }) . drop(columns = 'home_game' ) . pipe(pd . merge, (features . loc[features . home_game == 0 , one_line_cols] . drop(columns = 'home_game' ) . rename(columns = { 'team' : 'away_team' }) . rename(columns = {col: col + '_away' for col in features . columns if col . startswith( 'f_' )})), on = 'game' ) . drop(columns = [ 'f_form_margin_btwn_teams_away' , 'f_form_past_5_btwn_teams_away' ])) # Add our created features - elo, efficiency etc. features_one_line = (features_one_line . assign(f_elo_home = lambda df: df . game . map(elos) . apply( lambda x: x[ 0 ]), f_elo_away = lambda df: df . game . map(elos) . apply( lambda x: x[ 1 ])) . pipe(pd . merge, efficiency_features_single_row, on = [ 'game' , 'home_team' , 'away_team' ]) . pipe(pd . merge, afl_data . loc[afl_data . home_game == 1 , [ 'game' , 'date' , 'round' , 'venue' ]], on = [ 'game' ]) . dropna() . reset_index(drop = True ) . assign(season = lambda df: df . date . apply( lambda row: row . year))) ordered_cols = [col for col in features_one_line if col[: 2 ] != 'f_' ] + [col for col in features_one_line if col . startswith( 'f_' )] feature_df = features_one_line[ordered_cols] Finally, let's reduce the dimensionality of the features df by subtracting the home features from the away features. This will reduce the huge amount of columns we have and make our data more manageable. To do this, we will need a list of columns which we are subtracting from each other. We will then loop over each of these columns to create our new differential columns. We will then add in the implied probability from the odds of the home and away team, as our current odds feature is simply an exponential moving average over the past n games. # Create differential df - this df is the home features - the away features diff_cols = [col for col in feature_df . columns if col + '_away' in feature_df . columns and col != 'f_odds' and col . startswith( 'f_' )] non_diff_cols = [col for col in feature_df . columns if col not in diff_cols and col[: -5 ] not in diff_cols] diff_df = feature_df[non_diff_cols] . copy() for col in diff_cols: diff_df[col + '_diff' ] = feature_df[col] - feature_df[col + '_away' ] # Add current odds in to diff_df odds = get_cleaned_odds() home_odds = (odds[odds . home_game == 1 ] . assign(f_current_odds_prob = lambda df: 1 / df . odds) . rename(columns = { 'team' : 'home_team' }) . drop(columns = [ 'home_game' , 'odds' ])) away_odds = (odds[odds . home_game == 0 ] . assign(f_current_odds_prob_away = lambda df: 1 / df . odds) . rename(columns = { 'team' : 'away_team' }) . drop(columns = [ 'home_game' , 'odds' ])) diff_df = (diff_df . pipe(pd . merge, home_odds, on = [ 'date' , 'home_team' ]) . pipe(pd . merge, away_odds, on = [ 'date' , 'away_team' ])) diff_df . tail() game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1626 15394 Carlton Adelaide 2018-08-25 23 Docklands 2018 6.467328 -26.2 1.0 2.066016 1230.072138 1587.776445 0.730668 0.675002 0.691614 0.677128 -3.498547 -5.527193 -26.518474 -34.473769 1.289715 0.217006 7.955295 -341.342677 -9.317269 3.088569 -2.600593 15.192839 -12.518345 -4.136673 -41.855717 -72.258378 -51.998775 9.499447 8.670917 -6.973088 -4.740623 -26.964945 -13.147675 -23.928700 -28.940883 -45.293433 -15.183406 -1900.784014 -0.362402 -1.314627 4.116133 -294.813511 -9.917793 -34.724925 -5.462844 -9.367141 -19.623785 -38.188082 0.187709 0.816860 1627 15395 Sydney Hawthorn 2018-08-25 23 S.C.G. 2018 2.128611 1.0 2.0 1.777290 1662.568452 1615.507209 0.699994 0.778280 0.699158 0.673409 -1.756730 -0.874690 -11.415069 -15.575319 0.014390 4.073909 4.160250 -174.005092 -0.942357 -4.078635 -4.192916 7.814496 -2.225780 6.215760 15.042979 -34.894261 -50.615255 4.214158 0.683548 -3.535594 -3.168608 -12.068691 -30.493980 -9.867332 2.588103 -22.825570 -5.604199 253.086090 -2.697132 -22.612327 25.340623 -90.812188 1.967104 -31.047879 0.007606 -6.880120 11.415593 -49.957313 0.440180 0.561924 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566 Wrapping it Up \u00b6 We now have a fairly decent amount of features. Some other features which could be added include whether the game is in a major Capital city outisde of Mebourne (i.e. Sydney, Adelaide or Peth), how many 'Elite' players are playing (which could be judged by average SuperCoach scores over 110, for example), as well as your own metrics for attacking and defending. Note that all of our features have columns starting with 'f_' so in the next tutorial , we will grab this feature dataframe and use these features to sport predicting the matches.","title":"AFL 02. Feature creation"},{"location":"modelling/AFLmodelPart2/#afl-modelling-walkthrough","text":"","title":"AFL Modelling Walkthrough"},{"location":"modelling/AFLmodelPart2/#02-feature-creation","text":"These tutorials will walk you through how to construct your own basic AFL model. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through creating features from our dataset, which was cleaned in the first tutorial. Feature engineering is an integral part of the Data Science process. Creative and smart features can be the difference between an average performing model and a model profitable which beats the market odds.","title":"02. Feature Creation"},{"location":"modelling/AFLmodelPart2/#grabbing-our-dataset","text":"First, we will import our required modules, as well as the prepare_afl_data function which we created in our afl_data_cleaning script. This essentially cleans all the data for us so that we're ready to explore the data and make some features. # Import modules from afl_data_cleaning_v2 import * import afl_data_cleaning_v2 import pandas as pd pd . set_option( 'display.max_columns' , None ) import warnings warnings . filterwarnings( 'ignore' ) import numpy as np # Use the prepare_afl_data function to prepare the data for us; this function condenses what we walked through in the previous tutorial afl_data = prepare_afl_data() afl_data . tail( 3 ) team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269","title":"Grabbing Our Dataset"},{"location":"modelling/AFLmodelPart2/#creating-a-feature-dataframe","text":"Let's create a feature DataFrame and merge all of our features into this DataFrame as we go. features = afl_data[[ 'date' , 'game' , 'team' , 'opponent' , 'venue' , 'home_game' ]] . copy()","title":"Creating A Feature DataFrame"},{"location":"modelling/AFLmodelPart2/#what-each-column-refers-to","text":"Below is a DataFrame which outlines what each column refers to. column_abbreviations = pd . read_csv( \"data/afl_data_columns_mapping.csv\" ) column_abbreviations Feature Abbreviated Feature 0 GA Goal Assists 1 CP Contested Possessions 2 UP Uncontested Possessions 3 ED Effective Disposals 4 CM Contested Marks 5 MI5 Marks Inside 50 6 One.Percenters One Percenters 7 BO Bounces 8 K Kicks 9 HB Handballs 10 D Disposals 11 M Marks 12 G Goals 13 B Behinds 14 T Tackles 15 HO Hitouts 16 I50 Inside 50s 17 CL Clearances 18 CG Clangers 19 R50 Rebound 50s 20 FF Frees For 21 FA Frees Against 22 AF AFL Fantasy Points 23 SC Supercoach Points 24 CCL Centre Clearances 25 SCL Stoppage Clearances 26 SI Score Involvements 27 MG Metres Gained 28 TO Turnovers 29 ITC Intercepts 30 T5 Tackles Inside 50","title":"What Each Column Refers To"},{"location":"modelling/AFLmodelPart2/#feature-creation","text":"Now let's think about what features we can create. We have a enormous amount of stats to sift through. To start, let's create some simple features based on our domain knowledge of Aussie Rules.","title":"Feature Creation"},{"location":"modelling/AFLmodelPart2/#creating-expontentially-weighted-rolling-averages-as-features","text":"Next, we will create rolling averages of statistics such as Tackles, which we will use as features. It is fair to assume that a team's performance in a certain stat may have predictive power to the overall result. And in general, if a team consistently performs well in this stat, this may have predictive power to the result of their future games. We can't simply train a model on stats from the game which we are trying to predict (i.e. data that we don't have before the game begins), as this will leak the result. We need to train our model on past data. One way of doing this is to train our model on average stats over a certain amount of games. If a team is averaging high in this stat, this may give insight into if they are a strong team. Similarly, if the team is averaging poorly in this stat (relative to the team they are playing), this may have predictive power and give rise to a predicted loss. To do this we will create a function which calculates the rolling averages, known as create_exp_weighted_avgs, which takes our cleaned DataFrame as an input, as well as the alpha which, when higher, weights recent performances more than old performances. To read more about expontentially weighted moving averages, please read the documentation here . First, we will grab all the columns which we want to create EMAs for, and then use our function to create the average for that column. We will create a new DataFrame and add these columns to this new DataFrame. # Define a function which returns a DataFrame with the expontential moving average for each numeric stat def create_exp_weighted_avgs (df, span): # Create a copy of the df with only the game id and the team - we will add cols to this df ema_features = df[[ 'game' , 'team' ]] . copy() feature_names = [col for col in df . columns if col . startswith( 'f_' )] # Get a list of columns we will iterate over for feature_name in feature_names: feature_ema = (df . groupby( 'team' )[feature_name] . transform( lambda row: (row . ewm(span = span) . mean() . shift( 1 )))) ema_features[feature_name] = feature_ema return ema_features features_rolling_averages = create_exp_weighted_avgs(afl_data, span =10 ) features_rolling_averages . tail() game team f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3152 15396 West Coast 2.094236 12.809630 10.047145 86.904928 8.888770 11.435452 9.403444 78.016158 3193.612782 16.472115 11.958482 23.379562 100.095244 68.252001 27.688669 284.463270 719.884644 525.878017 36.762440 44.867118 25.618202 17.522871 270.478779 88.139376 105.698031 148.005305 449.405865 201.198907 11581.929999 20.048124 95.018480 74.180967 3314.157893 44.872398 177.894442 126.985101 20.565549 138.876613 438.848376 3153 15397 GWS 1.805565 13.100372 13.179329 91.781563 18.527618 10.371198 11.026754 73.253945 3165.127358 19.875913 12.947209 25.114002 105.856671 80.609640 23.374884 303.160047 741.439198 534.520295 42.597317 38.160889 26.208715 18.688880 300.188301 81.540693 106.989070 143.032506 441.250897 173.050118 12091.630837 21.106142 103.077097 80.201059 3419.245919 55.495610 219.879895 138.202470 25.313148 135.966798 438.466439 3154 15397 Melbourne 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.787800 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 3155 15398 North Melbourne 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.541130 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3156 15398 St Kilda 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.498760 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 As you can see our function worked perfectly! Now we have a full DataFrame of exponentially weighted moving averages. Note that as these rolling averages have been shifted by 1 to ensure no data leakage, the first round of the data will have all NA values. We can drop these later. Let's add these averages to our features DataFrame features = pd . merge(features, features_rolling_averages, on = [ 'game' , 'team' ])","title":"Creating Expontentially Weighted Rolling Averages as Features"},{"location":"modelling/AFLmodelPart2/#creating-a-form-between-the-teams-feature","text":"It is well known in Aussie Rules that often some teams perform better against certain teams than others. If we isolate our features to pure stats based on previous games not between the teams playing, or elo ratings, we won't account for any relationships between certain teams. An example is the Kennett Curse , where Geelong won 11 consecutive games against Hawthorn, despite being similarly matched teams. Let's create a feature which calculates how many games a team has won against their opposition over a given window of games. To do this, we will need to use historical data that dates back well before our current DataFrame starts at. Otherwise we will be using a lot of our games to calculate form, meaning we will have to drop these rows before feeding it into an algorithm. So let's use our prepare_match_results function which we defined in the afl_data_cleaning tutorial to grab a clean DataFrame of all match results since 1897. We can then calculate the form and join this to our current DataFrame. match_results = afl_data_cleaning_v2 . get_cleaned_match_results() match_results . head( 3 ) game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 form_btwn_teams = match_results[[ 'game' , 'team' , 'opponent' , 'margin' ]] . copy() form_btwn_teams[ 'f_form_margin_btwn_teams' ] = (match_results . groupby([ 'team' , 'opponent' ])[ 'margin' ] . transform( lambda row: row . rolling( 5 ) . mean() . shift()) . fillna( 0 )) form_btwn_teams[ 'f_form_past_5_btwn_teams' ] = \\ (match_results . assign(win = lambda df: df . apply( lambda row: 1 if row . margin > 0 else 0 , axis = 'columns' )) . groupby([ 'team' , 'opponent' ])[ 'win' ] . transform( lambda row: row . rolling( 5 ) . mean() . shift() * 5 ) . fillna( 0 )) form_btwn_teams . tail( 3 ) game team opponent margin f_form_margin_btwn_teams f_form_past_5_btwn_teams 30793 15397 Melbourne GWS 45 -23.2 2.0 30794 15398 St Kilda North Melbourne -23 -3.2 2.0 30795 15398 North Melbourne St Kilda 23 3.2 3.0 # Merge to our features df features = pd . merge(features, form_btwn_teams . drop(columns = [ 'margin' ]), on = [ 'game' , 'team' , 'opponent' ])","title":"Creating a 'Form Between the Teams' Feature"},{"location":"modelling/AFLmodelPart2/#creating-efficiency-features","text":"","title":"Creating Efficiency Features"},{"location":"modelling/AFLmodelPart2/#disposal-efficiency","text":"Disposal efficiency is pivotal in Aussie Rules football. If you are dispose of the ball effectively you are much more likely to score and much less likely to concede goals than if you dispose of it ineffectively. Let's create a disposal efficiency feature by dividing Effective Disposals by Disposals.","title":"Disposal Efficiency"},{"location":"modelling/AFLmodelPart2/#inside-50rebound-50-efficiency","text":"Similarly, one could hypothesise that teams who keep the footy in their Inside 50 regularly will be more likely to score, whilst teams who are effective at getting the ball out of their Defensive 50 will be less likely to concede. Let's use this logic to create Inside 50 Efficiency and Rebound 50 Efficiency features. The formula used will be: Inside 50 Efficiency = R50_Opponents / I50 (lower is better). Rebound 50 Efficiency = R50 / I50_Opponents (higher is better). Using these formulas, I50 Efficiency = R50 Efficiency_Opponent. So we will just need to create the formulas for I50 efficiency. To create these features we will need the opposition's Inside 50s/Rebound 50s. So we will split out data into two DataFrames, create a new DataFrame by joining these two DataFrames on the Game, calculate our efficiency features, then join our features with our main features DataFrame. # Get each match on single rows single_row_df = (afl_data[[ 'game' , 'team' , 'f_I50' , 'f_R50' , 'f_D' , 'f_ED' , 'home_game' , ]] . query( 'home_game == 1' ) . rename(columns = { 'team' : 'home_team' , 'f_I50' : 'f_I50_home' , 'f_R50' : 'f_R50_home' , 'f_D' : 'f_D_home' , 'f_ED' : 'f_ED_home' }) . drop(columns = 'home_game' ) . pipe(pd . merge, afl_data[[ 'game' , 'team' , 'f_I50' , 'f_R50' , 'f_D' , 'f_ED' , 'home_game' ]] . query( 'home_game == 0' ) . rename(columns = { 'team' : 'away_team' , 'f_I50' : 'f_I50_away' , 'f_R50' : 'f_R50_away' , 'f_D' : 'f_D_away' , 'f_ED' : 'f_ED_away' }) . drop(columns = 'home_game' ), on = 'game' )) single_row_df . head() game home_team f_I50_home f_R50_home f_D_home f_ED_home away_team f_I50_away f_R50_away f_D_away f_ED_away 0 13764 Carlton 69 21 373 268 Richmond 37 50 316 226 1 13765 Geelong 54 40 428 310 St Kilda 52 45 334 246 2 13766 Collingwood 70 38 398 289 Port Adelaide 50 44 331 232 3 13767 Adelaide 59 38 366 264 Hawthorn 54 38 372 264 4 13768 Brisbane 50 39 343 227 Fremantle 57 30 351 250 single_row_df = single_row_df . assign(f_I50_efficiency_home = lambda df: df . f_R50_away / df . f_I50_home, f_I50_efficiency_away = lambda df: df . f_R50_home / df . f_I50_away) feature_efficiency_cols = [ 'f_I50_efficiency_home' , 'f_I50_efficiency_away' ] # Now let's create an Expontentially Weighted Moving Average for these features - we will need to reshape our DataFrame to do this efficiency_features_multi_row = (single_row_df[[ 'game' , 'home_team' ] + feature_efficiency_cols] . rename(columns = { 'home_team' : 'team' , 'f_I50_efficiency_home' : 'f_I50_efficiency' , 'f_I50_efficiency_away' : 'f_I50_efficiency_opponent' , }) . append((single_row_df[[ 'game' , 'away_team' ] + feature_efficiency_cols] . rename(columns = { 'away_team' : 'team' , 'f_I50_efficiency_home' : 'f_I50_efficiency_opponent' , 'f_I50_efficiency_away' : 'f_I50_efficiency' , })), sort = True ) . sort_values(by = 'game' ) . reset_index(drop = True )) efficiency_features = efficiency_features_multi_row[[ 'game' , 'team' ]] . copy() feature_efficiency_cols = [ 'f_I50_efficiency' , 'f_I50_efficiency_opponent' ] for feature in feature_efficiency_cols: efficiency_features[feature] = (efficiency_features_multi_row . groupby( 'team' )[feature] . transform( lambda row: row . ewm(span =10 ) . mean() . shift( 1 ))) # Get feature efficiency df back onto single rows efficiency_features = pd . merge(efficiency_features, afl_data[[ 'game' , 'team' , 'home_game' ]], on = [ 'game' , 'team' ]) efficiency_features_single_row = (efficiency_features . query( 'home_game == 1' ) . rename(columns = { 'team' : 'home_team' , 'f_I50_efficiency' : 'f_I50_efficiency_home' , 'f_I50_efficiency_opponent' : 'f_R50_efficiency_home' }) . drop(columns = 'home_game' ) . pipe(pd . merge, (efficiency_features . query( 'home_game == 0' ) . rename(columns = { 'team' : 'away_team' , 'f_I50_efficiency' : 'f_I50_efficiency_away' , 'f_I50_efficiency_opponent' : 'f_R50_efficiency_away' }) . drop(columns = 'home_game' )), on = 'game' )) efficiency_features_single_row . tail( 5 ) game home_team f_I50_efficiency_home f_R50_efficiency_home away_team f_I50_efficiency_away f_R50_efficiency_away 1580 15394 Carlton 0.730668 0.675002 Adelaide 0.691614 0.677128 1581 15395 Sydney 0.699994 0.778280 Hawthorn 0.699158 0.673409 1582 15396 Brisbane 0.683604 0.691730 West Coast 0.696822 0.709605 1583 15397 Melbourne 0.667240 0.692632 GWS 0.684525 0.753783 1584 15398 St Kilda 0.730843 0.635819 North Melbourne 0.697018 0.654991 We will merge these features back to our features df later, when the features data frame is on a single row as well.","title":"Inside 50/Rebound 50 Efficiency"},{"location":"modelling/AFLmodelPart2/#creating-an-elo-feature","text":"Another feature which we could create is an Elo feature. If you don't know what Elo is, go ahead and read our article on it here . We have also written a guide on using elo to model the 2018 FIFA World Cup here . Essentially, Elo ratings increase if you win. The amount the rating increases is based on how strong the opponent is relative to the team who won. Weak teams get more points for beating stronger teams than they do for beating weaker teams, and vice versa for losses (teams lose points for losses). Mathematically, Elo ratings can also assign a probability for winning or losing based on the two Elo Ratings of the teams playing. So let's get into it. We will first define a function which calculates the elo for each team and applies these elos to our DataFrame. # Define a function which finds the elo for each team in each game and returns a dictionary with the game ID as a key and the # elos as the key's value, in a list. It also outputs the probabilities and a dictionary of the final elos for each team def elo_applier (df, k_factor): # Initialise a dictionary with default elos for each team elo_dict = {team: 1500 for team in df[ 'team' ] . unique()} elos, elo_probs = {}, {} # Get a home and away dataframe so that we can get the teams on the same row home_df = df . loc[df . home_game == 1 , [ 'team' , 'game' , 'f_margin' , 'home_game' ]] . rename(columns = { 'team' : 'home_team' }) away_df = df . loc[df . home_game == 0 , [ 'team' , 'game' ]] . rename(columns = { 'team' : 'away_team' }) df = (pd . merge(home_df, away_df, on = 'game' ) . sort_values(by = 'game' ) . drop_duplicates(subset = 'game' , keep = 'first' ) . reset_index(drop = True )) # Loop over the rows in the DataFrame for index, row in df . iterrows(): # Get the Game ID game_id = row[ 'game' ] # Get the margin margin = row[ 'f_margin' ] # If the game already has the elos for the home and away team in the elos dictionary, go to the next game if game_id in elos . keys(): continue # Get the team and opposition home_team = row[ 'home_team' ] away_team = row[ 'away_team' ] # Get the team and opposition elo score home_team_elo = elo_dict[home_team] away_team_elo = elo_dict[away_team] # Calculated the probability of winning for the team and opposition prob_win_home = 1 / ( 1 + 10** ((away_team_elo - home_team_elo) / 400 )) prob_win_away = 1 - prob_win_home # Add the elos and probabilities our elos dictionary and elo_probs dictionary based on the Game ID elos[game_id] = [home_team_elo, away_team_elo] elo_probs[game_id] = [prob_win_home, prob_win_away] # Calculate the new elos of each team if margin > 0 : # Home team wins; update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 1 - prob_win_home) new_away_team_elo = away_team_elo + k_factor * ( 0 - prob_win_away) elif margin < 0 : # Away team wins; update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 0 - prob_win_home) new_away_team_elo = away_team_elo + k_factor * ( 1 - prob_win_away) elif margin == 0 : # Drawn game' update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 0.5 - prob_win_home) new_away_team_elo = away_team_elo + k_factor * ( 0.5 - prob_win_away) # Update elos in elo dictionary elo_dict[home_team] = new_home_team_elo elo_dict[away_team] = new_away_team_elo return elos, elo_probs, elo_dict # Use the elo applier function to get the elos and elo probabilities for each game - we will map these later elos, probs, elo_dict = elo_applier(afl_data, 30 ) Great! now we have both rolling averages for stats as a feature, and the elo of the teams! Let's have a quick look at the current elo standings with a k-factor of 30, out of curiosity. for team in sorted (elo_dict, key = elo_dict . get)[:: -1 ]: print (team, elo_dict[team]) Richmond 1695.2241513840117 Sydney 1645.548990879842 Hawthorn 1632.5266709780622 West Coast 1625.871701773721 Geelong 1625.423154644809 GWS 1597.4158602131877 Adelaide 1591.1704934545442 Collingwood 1560.370309216614 Melbourne 1558.5666572771509 Essendon 1529.0198398117086 Port Adelaide 1524.8882517820093 North Melbourne 1465.5637511922569 Western Bulldogs 1452.2110697845148 Fremantle 1393.142087030804 St Kilda 1360.9120149937303 Brisbane 1276.2923772139352 Gold Coast 1239.174528704772 Carlton 1226.6780896643265 This looks extremely similar to the currently AFL ladder, so this is a good sign for elo being an effective predictor of winning.","title":"Creating an Elo Feature"},{"location":"modelling/AFLmodelPart2/#merging-our-features-into-one-features-dataframe","text":"Now we need to reshape our features df so that we have all of the statistics for both teams in a game on a single row. We can then merge our elo and efficiency features to this df. # Look at our current features df features . tail( 3 ) date game team opponent venue home_game f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP f_form_margin_btwn_teams f_form_past_5_btwn_teams 3156 2018-08-26 15397 Melbourne GWS M.C.G. 1 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.78780 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 -23.2 2.0 3157 2018-08-26 15398 North Melbourne St Kilda Docklands 0 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.54113 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3.2 3.0 3158 2018-08-26 15398 St Kilda North Melbourne Docklands 1 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.49876 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 -3.2 2.0 one_line_cols = [ 'game' , 'team' , 'home_game' ] + [col for col in features if col . startswith( 'f_' )] # Get all features onto individual rows for each match features_one_line = (features . loc[features . home_game == 1 , one_line_cols] . rename(columns = { 'team' : 'home_team' }) . drop(columns = 'home_game' ) . pipe(pd . merge, (features . loc[features . home_game == 0 , one_line_cols] . drop(columns = 'home_game' ) . rename(columns = { 'team' : 'away_team' }) . rename(columns = {col: col + '_away' for col in features . columns if col . startswith( 'f_' )})), on = 'game' ) . drop(columns = [ 'f_form_margin_btwn_teams_away' , 'f_form_past_5_btwn_teams_away' ])) # Add our created features - elo, efficiency etc. features_one_line = (features_one_line . assign(f_elo_home = lambda df: df . game . map(elos) . apply( lambda x: x[ 0 ]), f_elo_away = lambda df: df . game . map(elos) . apply( lambda x: x[ 1 ])) . pipe(pd . merge, efficiency_features_single_row, on = [ 'game' , 'home_team' , 'away_team' ]) . pipe(pd . merge, afl_data . loc[afl_data . home_game == 1 , [ 'game' , 'date' , 'round' , 'venue' ]], on = [ 'game' ]) . dropna() . reset_index(drop = True ) . assign(season = lambda df: df . date . apply( lambda row: row . year))) ordered_cols = [col for col in features_one_line if col[: 2 ] != 'f_' ] + [col for col in features_one_line if col . startswith( 'f_' )] feature_df = features_one_line[ordered_cols] Finally, let's reduce the dimensionality of the features df by subtracting the home features from the away features. This will reduce the huge amount of columns we have and make our data more manageable. To do this, we will need a list of columns which we are subtracting from each other. We will then loop over each of these columns to create our new differential columns. We will then add in the implied probability from the odds of the home and away team, as our current odds feature is simply an exponential moving average over the past n games. # Create differential df - this df is the home features - the away features diff_cols = [col for col in feature_df . columns if col + '_away' in feature_df . columns and col != 'f_odds' and col . startswith( 'f_' )] non_diff_cols = [col for col in feature_df . columns if col not in diff_cols and col[: -5 ] not in diff_cols] diff_df = feature_df[non_diff_cols] . copy() for col in diff_cols: diff_df[col + '_diff' ] = feature_df[col] - feature_df[col + '_away' ] # Add current odds in to diff_df odds = get_cleaned_odds() home_odds = (odds[odds . home_game == 1 ] . assign(f_current_odds_prob = lambda df: 1 / df . odds) . rename(columns = { 'team' : 'home_team' }) . drop(columns = [ 'home_game' , 'odds' ])) away_odds = (odds[odds . home_game == 0 ] . assign(f_current_odds_prob_away = lambda df: 1 / df . odds) . rename(columns = { 'team' : 'away_team' }) . drop(columns = [ 'home_game' , 'odds' ])) diff_df = (diff_df . pipe(pd . merge, home_odds, on = [ 'date' , 'home_team' ]) . pipe(pd . merge, away_odds, on = [ 'date' , 'away_team' ])) diff_df . tail() game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1626 15394 Carlton Adelaide 2018-08-25 23 Docklands 2018 6.467328 -26.2 1.0 2.066016 1230.072138 1587.776445 0.730668 0.675002 0.691614 0.677128 -3.498547 -5.527193 -26.518474 -34.473769 1.289715 0.217006 7.955295 -341.342677 -9.317269 3.088569 -2.600593 15.192839 -12.518345 -4.136673 -41.855717 -72.258378 -51.998775 9.499447 8.670917 -6.973088 -4.740623 -26.964945 -13.147675 -23.928700 -28.940883 -45.293433 -15.183406 -1900.784014 -0.362402 -1.314627 4.116133 -294.813511 -9.917793 -34.724925 -5.462844 -9.367141 -19.623785 -38.188082 0.187709 0.816860 1627 15395 Sydney Hawthorn 2018-08-25 23 S.C.G. 2018 2.128611 1.0 2.0 1.777290 1662.568452 1615.507209 0.699994 0.778280 0.699158 0.673409 -1.756730 -0.874690 -11.415069 -15.575319 0.014390 4.073909 4.160250 -174.005092 -0.942357 -4.078635 -4.192916 7.814496 -2.225780 6.215760 15.042979 -34.894261 -50.615255 4.214158 0.683548 -3.535594 -3.168608 -12.068691 -30.493980 -9.867332 2.588103 -22.825570 -5.604199 253.086090 -2.697132 -22.612327 25.340623 -90.812188 1.967104 -31.047879 0.007606 -6.880120 11.415593 -49.957313 0.440180 0.561924 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566","title":"Merging Our Features Into One Features DataFrame"},{"location":"modelling/AFLmodelPart2/#wrapping-it-up","text":"We now have a fairly decent amount of features. Some other features which could be added include whether the game is in a major Capital city outisde of Mebourne (i.e. Sydney, Adelaide or Peth), how many 'Elite' players are playing (which could be judged by average SuperCoach scores over 110, for example), as well as your own metrics for attacking and defending. Note that all of our features have columns starting with 'f_' so in the next tutorial , we will grab this feature dataframe and use these features to sport predicting the matches.","title":"Wrapping it Up"},{"location":"modelling/AFLmodelPart3/","text":"AFL Modelling Walkthrough \u00b6 03. Modelling \u00b6 These tutorials will walk you through how to construct your own basic AFL model, using publically available data. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through modelling our AFL data to create predictions. We will train a variety of quick and easy models to get a feel of what works and what doesn't. We will then tune our hyperparameters so that we are ready to make week by week predictions. Grabbing Our Dataset \u00b6 First, we will import our required modules, as well as the prepare_afl_features function which we created in our afl_feature_creation script. This essentially creates some basic features for us so that we can get started on the modelling component. # Import libraries from afl_data_cleaning_v2 import * import datetime import pandas as pd import numpy as np from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process # from xgboost import XGBClassifier from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, train_test_split from sklearn.linear_model import LogisticRegressionCV from sklearn.feature_selection import RFECV import seaborn as sns from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler from sklearn import feature_selection from sklearn import metrics from sklearn.linear_model import LogisticRegression, RidgeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB import warnings warnings . filterwarnings( 'ignore' ) import afl_feature_creation_v2 import afl_data_cleaning_v2 # Grab our feature DataFrame which we created in the previous tutorial feature_df = afl_feature_creation_v2 . prepare_afl_features() afl_data = afl_data_cleaning_v2 . prepare_afl_data() feature_df . tail( 3 ) game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566 # Get the result and merge to the feature_df match_results = (pd . read_csv( \"data/afl_match_results.csv\" ) . rename(columns = { 'Game' : 'game' }) . assign(result = lambda df: df . apply( lambda row: 1 if row[ 'Home.Points' ] > row[ 'Away.Points' ] else 0 , axis =1 ))) # Merge result column to feature_df feature_df = pd . merge(feature_df, match_results[[ 'game' , 'result' ]], on = 'game' ) Creating a Training and Testing Set \u00b6 So that we don't train our data on the data that we will later test our model on, we will create separate train and test sets. For this exercise we will use the 2018 season to test how our model performs, whilst the rest of the data can be used to train the model. # Create our test and train sets from our afl DataFrame; drop the columns which leak the result, duplicates, and the advanced # stats which don't have data until 2015 feature_columns = [col for col in feature_df if col . startswith( 'f_' )] # Create our test set test_x = feature_df . loc[feature_df . season == 2018 , [ 'game' ] + feature_columns] test_y = feature_df . loc[feature_df . season == 2018 , 'result' ] # Create our train set X = feature_df . loc[feature_df . season != 2018 , [ 'game' ] + feature_columns] y = feature_df . loc[feature_df . season != 2018 , 'result' ] # Scale features scaler = StandardScaler() X[feature_columns] = scaler . fit_transform(X[feature_columns]) test_x[feature_columns] = scaler . transform(test_x[feature_columns]) Using Cross Validation to Find The Best Algorithms \u00b6 Now that we have our training set, we can run through a list of popular classifiers to determine which classifier is best for modelling our data. To do this we will create a function which uses Kfold cross-validation to find the 'best' algorithms, based on how accurate the algorithms' predictions are. This function will take in a list of classifiers, which we will define below, as well as the training set and it's outcome, and output a DataFrame with the mean and std of the accuracy of each algorithm. Let's jump into it! # Create a list of standard classifiers classifiers = [ #Ensemble Methods ensemble . AdaBoostClassifier(), ensemble . BaggingClassifier(), ensemble . ExtraTreesClassifier(), ensemble . GradientBoostingClassifier(), ensemble . RandomForestClassifier(), #Gaussian Processes gaussian_process . GaussianProcessClassifier(), #GLM linear_model . LogisticRegressionCV(), #Navies Bayes naive_bayes . BernoulliNB(), naive_bayes . GaussianNB(), #SVM svm . SVC(probability = True ), svm . NuSVC(probability = True ), #Discriminant Analysis discriminant_analysis . LinearDiscriminantAnalysis(), discriminant_analysis . QuadraticDiscriminantAnalysis(), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html # XGBClassifier() ] # Define a functiom which finds the best algorithms for our modelling task def find_best_algorithms (classifier_list, X, y): # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling # Cross validate model with Kfold stratified cross validation kfold = StratifiedKFold(n_splits =5 ) # Grab the cross validation scores for each algorithm cv_results = [cross_val_score(classifier, X, y, scoring = \"neg_log_loss\" , cv = kfold) for classifier in classifier_list] cv_means = [cv_result . mean() * -1 for cv_result in cv_results] cv_std = [cv_result . std() for cv_result in cv_results] algorithm_names = [alg . __class__ . __name__ for alg in classifiers] # Create a DataFrame of all the CV results cv_results = pd . DataFrame({ \"Mean Log Loss\" : cv_means, \"Log Loss Std\" : cv_std, \"Algorithm\" : algorithm_names }) return cv_results . sort_values(by = 'Mean Log Loss' ) . reset_index(drop = True ) best_algos = find_best_algorithms(classifiers, X, y) best_algos Mean Log Loss Log Loss Std Algorithm 0 0.539131 3.640578e-02 LogisticRegressionCV 1 0.551241 5.775685e-02 LinearDiscriminantAnalysis 2 0.630994 8.257481e-02 GradientBoostingClassifier 3 0.670041 9.205780e-03 AdaBoostClassifier 4 0.693147 2.360121e-08 GaussianProcessClassifier 5 0.712537 2.770864e-02 SVC 6 0.712896 2.440755e-02 NuSVC 7 0.836191 2.094224e-01 ExtraTreesClassifier 8 0.874307 1.558144e-01 RandomForestClassifier 9 1.288174 3.953037e-01 BaggingClassifier 10 1.884019 4.769589e-01 QuadraticDiscriminantAnalysis 11 2.652161 6.886897e-01 BernoulliNB 12 3.299651 6.427551e-01 GaussianNB # Try a logistic regression model and see how it performs in terms of accuracy kfold = StratifiedKFold(n_splits =5 ) cv_scores = cross_val_score(linear_model . LogisticRegressionCV(), X, y, scoring = 'accuracy' , cv = kfold) cv_scores . mean() 0.7452268937025035 Choosing Our Algorithms \u00b6 As we can see from above, there are some pretty poor algorithms for predicting the winner. On the other hand, whilst attaining an accuracy of 74.5% (at the time of writing) may seem like a decent result; we must first establish a baseline to judge our performance on. In this case, we will have two baselines; the proportion of games won by the home team and what the odds predict. If we can beat the odds we have created a very powerful model. Note that a baseline for the log loss can also be both the odds log loss and randomly guessing. Randomly guessing between two teams attains a log loss of log(2) = 0.69, so we have beaten this result. Once we establish our baseline, we will choose the top algorithms from above and tune their hyperparameters, as well as automatically selecting the best features to be used in our model. Defining Our Baseline \u00b6 As stated above, we must define our baseline so that we have a measure to beat. We will use the proportion of games won by the home team, as well as the proportion of favourites who won, based off the odds. To establish this baseline we will use our feature_df, as this has no dropped rows. # Find the percentage chance of winning at home in each season. afl_data = afl_data_cleaning_v2 . prepare_afl_data() afl_data[ 'home_win' ] = afl_data . apply( lambda x: 1 if x[ 'f_margin' ] > 0 else 0 , axis =1 ) home_games = afl_data[afl_data[ 'home_game' ] == 1 ] home_games[[ \"home_win\" , 'season' ]] . groupby([ 'season' ]) . mean() season home_win 2011 0.561856 2012 0.563725 2013 0.561576 2014 0.574257 2015 0.539604 2016 0.606742 2017 0.604061 2018 0.540404 # Find the proportion of favourites who have won # Define a function which finds if the odds correctly guessed the response def find_odds_prediction (a_row): if a_row[ 'f_odds' ] <= a_row[ 'f_odds_away' ] and a_row[ 'home_win' ] == 1 : return 1 elif a_row[ 'f_odds_away' ] < a_row[ 'f_odds' ] and a_row[ 'home_win' ] == 0 : return 1 else : return 0 # Define a function which splits our DataFrame so each game is on one row instead of two def get_df_on_one_line (df): cols_to_drop = [ 'date' , 'home_game' , 'opponent' , 'f_opponent_behinds' , 'f_opponent_goals' , 'f_opponent_points' , 'f_points' , 'round' , 'venue' , 'season' ] home_df = df[df[ 'home_game' ] == 1 ] . rename(columns = { 'team' : 'home_team' }) away_df = df[df[ 'home_game' ] == 0 ] . rename(columns = { 'team' : 'away_team' }) away_df = away_df . drop(columns = cols_to_drop) # Rename away_df columns away_df_renamed = away_df . rename(columns = {col: col + '_away' for col in away_df . columns if col != 'game' }) merged_df = pd . merge(home_df, away_df_renamed, on = 'game' ) merged_df[ 'home_win' ] = merged_df . f_margin . apply( lambda x: 1 if x > 0 else 0 ) return merged_df afl_data_one_line = get_df_on_one_line(afl_data) afl_data_one_line[ 'odds_prediction' ] = afl_data_one_line . apply(find_odds_prediction, axis =1 ) print ( 'The overall mean accuracy of choosing the favourite based on the odds is {}%' . format( round (afl_data_one_line[ 'odds_prediction' ] . mean() * 100 , 2 ))) afl_data_one_line[[ \"odds_prediction\" , 'season' ]] . groupby([ 'season' ]) . mean() The overall mean accuracy of choosing the favourite based on the odds is 73.15% season odds_prediction 2011 0.784615 2012 0.774510 2013 0.748768 2014 0.727723 2015 0.727723 2016 0.713483 2017 0.659898 2018 0.712121 ## Get a baseline log loss score from the odds afl_data_one_line[ 'odds_home_prob' ] = 1 / afl_data_one_line . f_odds afl_data_one_line[ 'odds_away_prob' ] = 1 / afl_data_one_line . f_odds_away metrics . log_loss(afl_data_one_line . home_win, afl_data_one_line[[ 'odds_away_prob' , 'odds_home_prob' ]]) 0.5375306549682837 We can see that the odds are MUCH more accurate than just choosing the home team to win. We can also see that the mean accuracy of choosing the favourite is around 73%. That means that this is the score we need to beat. Similarly, the log loss of the odds is around 0.5385, whilst our model scores around 0.539 (at the time of writing), without hyperparamter optimisation. Let's choose only the algorithms with log losses below 0.67 chosen_algorithms = best_algos . loc[best_algos[ 'Mean Log Loss' ] < 0.67 , 'Algorithm' ] . tolist() chosen_algorithms [ 'LogisticRegressionCV' , 'LinearDiscriminantAnalysis' , 'GradientBoostingClassifier' ] Using Grid Search To Tune Hyperparameters \u00b6 Now that we have our best models, we can use Grid Search to optimise our hyperparameters. Grid search basically involves searching through a range of different algorithm hyperparameters, and choosing those which result in the best score from some metrics, which in our case is accuracy. Let's do this for the algorithms which have hyperparameters which can be tuned. Note that if you are running this on your own computer it may take up to 10 minutes. # Define a function which optimises the hyperparameters of our chosen algorithms def optimise_hyperparameters (train_x, train_y, algorithms, parameters): kfold = StratifiedKFold(n_splits =5 ) best_estimators = [] for alg, params in zip (algorithms, parameters): gs = GridSearchCV(alg, param_grid = params, cv = kfold, scoring = 'neg_log_loss' , verbose =1 ) gs . fit(train_x, train_y) best_estimators . append(gs . best_estimator_) return best_estimators # Define our parameters to run a grid search over lr_grid = { \"C\" : [ 0.0001 , 0.001 , 0.01 , 0.05 , 0.2 , 0.5 ], \"solver\" : [ \"newton-cg\" , \"lbfgs\" , \"liblinear\" ] } # Add our algorithms and parameters to lists to be used in our function alg_list = [LogisticRegression()] param_list = [lr_grid] # Find the best estimators, then add our other estimators which don't need optimisation best_estimators = optimise_hyperparameters(X, y, alg_list, param_list) Fitting 5 folds for each of 18 candidates, totalling 90 fits [Parallel(n_jobs=1)]: Done 90 out of 90 | elapsed: 5.2s finished lr_best_params = best_estimators[ 0 ] . get_params() lr_best_params { 'C' : 0.01 , 'class_weight' : None , 'dual' : False , 'fit_intercept' : True , 'intercept_scaling' : 1 , 'max_iter' : 100 , 'multi_class' : 'ovr' , 'n_jobs' : 1 , 'penalty' : 'l2' , 'random_state' : None , 'solver' : 'newton-cg' , 'tol' : 0.0001 , 'verbose' : 0 , 'warm_start' : False } kfold = StratifiedKFold(n_splits =10 ) cv_scores = cross_val_score(linear_model . LogisticRegression( ** lr_best_params), X, y, scoring = 'neg_log_loss' , cv = kfold) cv_scores . mean() -0.528741673153639 In the next iteration of this tutorial we will also optimise an XGB model and hopefully outperform our logistic regression model. Creating Predictions for the 2018 Season \u00b6 Now that we have an optimised logistic regression model, let's see how it performs on predicting the 2018 season. lr = LogisticRegression( ** lr_best_params) lr . fit(X, y) final_predictions = lr . predict(test_x) accuracy = (final_predictions == test_y) . mean() * 100 print ( \"Our accuracy in predicting the 2018 season is: {:.2f}%\" . format(accuracy)) Our accuracy in predicting the 2018 season is: 67.68% Now let's have a look at all the games which we incorrectly predicted. game_ids = test_x[(final_predictions != test_y)] . game afl_data_one_line . loc[afl_data_one_line . game . isin(game_ids), [ 'date' , 'home_team' , 'opponent' , 'f_odds' , 'f_odds_away' , 'f_margin' ]] date home_team opponent f_odds f_odds_away f_margin 1386 2018-03-24 Gold Coast North Melbourne 2.0161 1.9784 16 1388 2018-03-25 Melbourne Geelong 1.7737 2.2755 -3 1391 2018-03-30 North Melbourne St Kilda 3.5769 1.3867 52 1392 2018-03-31 Carlton Gold Coast 1.5992 2.6620 -34 1396 2018-04-01 Western Bulldogs West Coast 1.8044 2.2445 -51 1397 2018-04-01 Sydney Port Adelaide 1.4949 3.0060 -23 1398 2018-04-02 Geelong Hawthorn 1.7597 2.3024 -1 1406 2018-04-08 Western Bulldogs Essendon 3.8560 1.3538 21 1408 2018-04-13 Adelaide Collingwood 1.2048 5.9197 -48 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1415 2018-04-15 Hawthorn Melbourne 2.2855 1.7772 67 1417 2018-04-20 Sydney Adelaide 1.2640 4.6929 -10 1420 2018-04-21 Port Adelaide Geelong 1.5053 2.9515 -34 1422 2018-04-22 North Melbourne Hawthorn 2.6170 1.6132 28 1423 2018-04-22 Brisbane Gold Coast 1.7464 2.3277 -5 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1427 2018-04-28 Geelong Sydney 1.5019 2.9833 -17 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 1439 2018-05-05 Sydney North Melbourne 1.2777 4.5690 -2 1444 2018-05-11 Hawthorn Sydney 1.6283 2.5818 -8 1445 2018-05-12 GWS West Coast 1.5425 2.8292 -25 1446 2018-05-12 Carlton Essendon 3.1742 1.4570 13 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1456 2018-05-19 Essendon Geelong 5.6530 1.2104 34 1460 2018-05-20 Brisbane Hawthorn 3.2891 1.4318 56 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1466 2018-05-26 GWS Essendon 1.4364 3.2652 -35 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 ... ... ... ... ... ... ... 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 1485 2018-06-11 Melbourne Collingwood 1.6034 2.6450 -42 1492 2018-06-21 West Coast Essendon 1.3694 3.6843 -28 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1499 2018-06-29 Western Bulldogs Geelong 6.2067 1.1889 2 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1504 2018-07-01 Melbourne St Kilda 1.1405 7.7934 -2 1505 2018-07-01 Essendon North Melbourne 2.0993 1.9022 17 1506 2018-07-01 Fremantle Brisbane 1.2914 4.3743 -55 1507 2018-07-05 Sydney Geelong 1.7807 2.2675 -12 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1516 2018-07-12 Adelaide Geelong 2.0517 1.9444 15 1518 2018-07-14 Hawthorn Brisbane 1.2281 5.4105 -33 1521 2018-07-14 GWS Richmond 2.7257 1.5765 2 1522 2018-07-15 Collingwood West Coast 1.5600 2.7815 -35 1523 2018-07-15 North Melbourne Sydney 1.9263 2.0647 -6 1524 2018-07-15 Fremantle Port Adelaide 5.9110 1.2047 9 1527 2018-07-21 Sydney Gold Coast 1.0342 27.8520 -24 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 1533 2018-07-22 Port Adelaide GWS 1.6480 2.5452 -22 1538 2018-07-28 Gold Coast Carlton 1.3933 3.5296 -35 1546 2018-08-04 Adelaide Port Adelaide 2.0950 1.9135 3 1548 2018-08-04 St Kilda Western Bulldogs 1.6120 2.6368 -35 1555 2018-08-11 Port Adelaide West Coast 1.4187 3.3505 -4 1558 2018-08-12 North Melbourne Western Bulldogs 1.3175 4.1239 -7 1559 2018-08-12 Melbourne Sydney 1.3627 3.7445 -9 1564 2018-08-18 GWS Sydney 1.8478 2.1672 -20 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 Very interesting! Most of the games we got wrong were upsets. Let's have a look at the games we incorrectly predicted that weren't upsets. (afl_data_one_line . loc[afl_data_one_line . game . isin(game_ids), [ 'date' , 'home_team' , 'opponent' , 'f_odds' , 'f_odds_away' , 'f_margin' ]] . assign(home_favourite = lambda df: df . apply( lambda row: 1 if row . f_odds < row . f_odds_away else 0 , axis =1 )) . assign(upset = lambda df: df . apply( lambda row: 1 if row . home_favourite == 1 and row . f_margin < 0 else ( 1 if row . home_favourite == 0 and row . f_margin > 0 else 0 ), axis =1 )) . query( 'upset == 0' )) date home_team opponent f_odds f_odds_away f_margin home_favourite upset 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1 0 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1 0 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 0 0 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 0 0 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 0 0 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1 0 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1 0 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 0 0 1479 2018-06-08 Port Adelaide Richmond 1.7422 2.3420 14 1 0 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 0 0 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1 0 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1 0 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 0 0 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1 0 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 0 0 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 0 0 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 0 0 Let's now look at our model's log loss for the 2018 season compared to the odds. predictions_probs = lr . predict_proba(test_x) metrics . log_loss(test_y, predictions_probs) 0.584824211055384 test_x_unscaled = feature_df . loc[feature_df . season == 2018 , [ 'game' ] + feature_columns] metrics . log_loss(test_y, test_x_unscaled[[ 'f_current_odds_prob_away' , 'f_current_odds_prob' ]]) 0.5545776633924343 So whilst our model performs decently, it doesn't beat the odds in terms of log loss. That's okay, it's still a decent start. In future iterations we can implement other algorithms and create new features which may improve performance. Next Steps \u00b6 Now that we have a model up and running, the next steps are to implement the model on a week to week basis. In the next tutorial we will be predicting the 2018 round of footy.","title":"AFL 03. Modelling"},{"location":"modelling/AFLmodelPart3/#afl-modelling-walkthrough","text":"","title":"AFL Modelling Walkthrough"},{"location":"modelling/AFLmodelPart3/#03-modelling","text":"These tutorials will walk you through how to construct your own basic AFL model, using publically available data. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through modelling our AFL data to create predictions. We will train a variety of quick and easy models to get a feel of what works and what doesn't. We will then tune our hyperparameters so that we are ready to make week by week predictions.","title":"03. Modelling"},{"location":"modelling/AFLmodelPart3/#grabbing-our-dataset","text":"First, we will import our required modules, as well as the prepare_afl_features function which we created in our afl_feature_creation script. This essentially creates some basic features for us so that we can get started on the modelling component. # Import libraries from afl_data_cleaning_v2 import * import datetime import pandas as pd import numpy as np from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process # from xgboost import XGBClassifier from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, train_test_split from sklearn.linear_model import LogisticRegressionCV from sklearn.feature_selection import RFECV import seaborn as sns from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler from sklearn import feature_selection from sklearn import metrics from sklearn.linear_model import LogisticRegression, RidgeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB import warnings warnings . filterwarnings( 'ignore' ) import afl_feature_creation_v2 import afl_data_cleaning_v2 # Grab our feature DataFrame which we created in the previous tutorial feature_df = afl_feature_creation_v2 . prepare_afl_features() afl_data = afl_data_cleaning_v2 . prepare_afl_data() feature_df . tail( 3 ) game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566 # Get the result and merge to the feature_df match_results = (pd . read_csv( \"data/afl_match_results.csv\" ) . rename(columns = { 'Game' : 'game' }) . assign(result = lambda df: df . apply( lambda row: 1 if row[ 'Home.Points' ] > row[ 'Away.Points' ] else 0 , axis =1 ))) # Merge result column to feature_df feature_df = pd . merge(feature_df, match_results[[ 'game' , 'result' ]], on = 'game' )","title":"Grabbing Our Dataset"},{"location":"modelling/AFLmodelPart3/#creating-a-training-and-testing-set","text":"So that we don't train our data on the data that we will later test our model on, we will create separate train and test sets. For this exercise we will use the 2018 season to test how our model performs, whilst the rest of the data can be used to train the model. # Create our test and train sets from our afl DataFrame; drop the columns which leak the result, duplicates, and the advanced # stats which don't have data until 2015 feature_columns = [col for col in feature_df if col . startswith( 'f_' )] # Create our test set test_x = feature_df . loc[feature_df . season == 2018 , [ 'game' ] + feature_columns] test_y = feature_df . loc[feature_df . season == 2018 , 'result' ] # Create our train set X = feature_df . loc[feature_df . season != 2018 , [ 'game' ] + feature_columns] y = feature_df . loc[feature_df . season != 2018 , 'result' ] # Scale features scaler = StandardScaler() X[feature_columns] = scaler . fit_transform(X[feature_columns]) test_x[feature_columns] = scaler . transform(test_x[feature_columns])","title":"Creating a Training and Testing Set"},{"location":"modelling/AFLmodelPart3/#using-cross-validation-to-find-the-best-algorithms","text":"Now that we have our training set, we can run through a list of popular classifiers to determine which classifier is best for modelling our data. To do this we will create a function which uses Kfold cross-validation to find the 'best' algorithms, based on how accurate the algorithms' predictions are. This function will take in a list of classifiers, which we will define below, as well as the training set and it's outcome, and output a DataFrame with the mean and std of the accuracy of each algorithm. Let's jump into it! # Create a list of standard classifiers classifiers = [ #Ensemble Methods ensemble . AdaBoostClassifier(), ensemble . BaggingClassifier(), ensemble . ExtraTreesClassifier(), ensemble . GradientBoostingClassifier(), ensemble . RandomForestClassifier(), #Gaussian Processes gaussian_process . GaussianProcessClassifier(), #GLM linear_model . LogisticRegressionCV(), #Navies Bayes naive_bayes . BernoulliNB(), naive_bayes . GaussianNB(), #SVM svm . SVC(probability = True ), svm . NuSVC(probability = True ), #Discriminant Analysis discriminant_analysis . LinearDiscriminantAnalysis(), discriminant_analysis . QuadraticDiscriminantAnalysis(), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html # XGBClassifier() ] # Define a functiom which finds the best algorithms for our modelling task def find_best_algorithms (classifier_list, X, y): # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling # Cross validate model with Kfold stratified cross validation kfold = StratifiedKFold(n_splits =5 ) # Grab the cross validation scores for each algorithm cv_results = [cross_val_score(classifier, X, y, scoring = \"neg_log_loss\" , cv = kfold) for classifier in classifier_list] cv_means = [cv_result . mean() * -1 for cv_result in cv_results] cv_std = [cv_result . std() for cv_result in cv_results] algorithm_names = [alg . __class__ . __name__ for alg in classifiers] # Create a DataFrame of all the CV results cv_results = pd . DataFrame({ \"Mean Log Loss\" : cv_means, \"Log Loss Std\" : cv_std, \"Algorithm\" : algorithm_names }) return cv_results . sort_values(by = 'Mean Log Loss' ) . reset_index(drop = True ) best_algos = find_best_algorithms(classifiers, X, y) best_algos Mean Log Loss Log Loss Std Algorithm 0 0.539131 3.640578e-02 LogisticRegressionCV 1 0.551241 5.775685e-02 LinearDiscriminantAnalysis 2 0.630994 8.257481e-02 GradientBoostingClassifier 3 0.670041 9.205780e-03 AdaBoostClassifier 4 0.693147 2.360121e-08 GaussianProcessClassifier 5 0.712537 2.770864e-02 SVC 6 0.712896 2.440755e-02 NuSVC 7 0.836191 2.094224e-01 ExtraTreesClassifier 8 0.874307 1.558144e-01 RandomForestClassifier 9 1.288174 3.953037e-01 BaggingClassifier 10 1.884019 4.769589e-01 QuadraticDiscriminantAnalysis 11 2.652161 6.886897e-01 BernoulliNB 12 3.299651 6.427551e-01 GaussianNB # Try a logistic regression model and see how it performs in terms of accuracy kfold = StratifiedKFold(n_splits =5 ) cv_scores = cross_val_score(linear_model . LogisticRegressionCV(), X, y, scoring = 'accuracy' , cv = kfold) cv_scores . mean() 0.7452268937025035","title":"Using Cross Validation to Find The Best Algorithms"},{"location":"modelling/AFLmodelPart3/#choosing-our-algorithms","text":"As we can see from above, there are some pretty poor algorithms for predicting the winner. On the other hand, whilst attaining an accuracy of 74.5% (at the time of writing) may seem like a decent result; we must first establish a baseline to judge our performance on. In this case, we will have two baselines; the proportion of games won by the home team and what the odds predict. If we can beat the odds we have created a very powerful model. Note that a baseline for the log loss can also be both the odds log loss and randomly guessing. Randomly guessing between two teams attains a log loss of log(2) = 0.69, so we have beaten this result. Once we establish our baseline, we will choose the top algorithms from above and tune their hyperparameters, as well as automatically selecting the best features to be used in our model.","title":"Choosing Our Algorithms"},{"location":"modelling/AFLmodelPart3/#defining-our-baseline","text":"As stated above, we must define our baseline so that we have a measure to beat. We will use the proportion of games won by the home team, as well as the proportion of favourites who won, based off the odds. To establish this baseline we will use our feature_df, as this has no dropped rows. # Find the percentage chance of winning at home in each season. afl_data = afl_data_cleaning_v2 . prepare_afl_data() afl_data[ 'home_win' ] = afl_data . apply( lambda x: 1 if x[ 'f_margin' ] > 0 else 0 , axis =1 ) home_games = afl_data[afl_data[ 'home_game' ] == 1 ] home_games[[ \"home_win\" , 'season' ]] . groupby([ 'season' ]) . mean() season home_win 2011 0.561856 2012 0.563725 2013 0.561576 2014 0.574257 2015 0.539604 2016 0.606742 2017 0.604061 2018 0.540404 # Find the proportion of favourites who have won # Define a function which finds if the odds correctly guessed the response def find_odds_prediction (a_row): if a_row[ 'f_odds' ] <= a_row[ 'f_odds_away' ] and a_row[ 'home_win' ] == 1 : return 1 elif a_row[ 'f_odds_away' ] < a_row[ 'f_odds' ] and a_row[ 'home_win' ] == 0 : return 1 else : return 0 # Define a function which splits our DataFrame so each game is on one row instead of two def get_df_on_one_line (df): cols_to_drop = [ 'date' , 'home_game' , 'opponent' , 'f_opponent_behinds' , 'f_opponent_goals' , 'f_opponent_points' , 'f_points' , 'round' , 'venue' , 'season' ] home_df = df[df[ 'home_game' ] == 1 ] . rename(columns = { 'team' : 'home_team' }) away_df = df[df[ 'home_game' ] == 0 ] . rename(columns = { 'team' : 'away_team' }) away_df = away_df . drop(columns = cols_to_drop) # Rename away_df columns away_df_renamed = away_df . rename(columns = {col: col + '_away' for col in away_df . columns if col != 'game' }) merged_df = pd . merge(home_df, away_df_renamed, on = 'game' ) merged_df[ 'home_win' ] = merged_df . f_margin . apply( lambda x: 1 if x > 0 else 0 ) return merged_df afl_data_one_line = get_df_on_one_line(afl_data) afl_data_one_line[ 'odds_prediction' ] = afl_data_one_line . apply(find_odds_prediction, axis =1 ) print ( 'The overall mean accuracy of choosing the favourite based on the odds is {}%' . format( round (afl_data_one_line[ 'odds_prediction' ] . mean() * 100 , 2 ))) afl_data_one_line[[ \"odds_prediction\" , 'season' ]] . groupby([ 'season' ]) . mean() The overall mean accuracy of choosing the favourite based on the odds is 73.15% season odds_prediction 2011 0.784615 2012 0.774510 2013 0.748768 2014 0.727723 2015 0.727723 2016 0.713483 2017 0.659898 2018 0.712121 ## Get a baseline log loss score from the odds afl_data_one_line[ 'odds_home_prob' ] = 1 / afl_data_one_line . f_odds afl_data_one_line[ 'odds_away_prob' ] = 1 / afl_data_one_line . f_odds_away metrics . log_loss(afl_data_one_line . home_win, afl_data_one_line[[ 'odds_away_prob' , 'odds_home_prob' ]]) 0.5375306549682837 We can see that the odds are MUCH more accurate than just choosing the home team to win. We can also see that the mean accuracy of choosing the favourite is around 73%. That means that this is the score we need to beat. Similarly, the log loss of the odds is around 0.5385, whilst our model scores around 0.539 (at the time of writing), without hyperparamter optimisation. Let's choose only the algorithms with log losses below 0.67 chosen_algorithms = best_algos . loc[best_algos[ 'Mean Log Loss' ] < 0.67 , 'Algorithm' ] . tolist() chosen_algorithms [ 'LogisticRegressionCV' , 'LinearDiscriminantAnalysis' , 'GradientBoostingClassifier' ]","title":"Defining Our Baseline"},{"location":"modelling/AFLmodelPart3/#using-grid-search-to-tune-hyperparameters","text":"Now that we have our best models, we can use Grid Search to optimise our hyperparameters. Grid search basically involves searching through a range of different algorithm hyperparameters, and choosing those which result in the best score from some metrics, which in our case is accuracy. Let's do this for the algorithms which have hyperparameters which can be tuned. Note that if you are running this on your own computer it may take up to 10 minutes. # Define a function which optimises the hyperparameters of our chosen algorithms def optimise_hyperparameters (train_x, train_y, algorithms, parameters): kfold = StratifiedKFold(n_splits =5 ) best_estimators = [] for alg, params in zip (algorithms, parameters): gs = GridSearchCV(alg, param_grid = params, cv = kfold, scoring = 'neg_log_loss' , verbose =1 ) gs . fit(train_x, train_y) best_estimators . append(gs . best_estimator_) return best_estimators # Define our parameters to run a grid search over lr_grid = { \"C\" : [ 0.0001 , 0.001 , 0.01 , 0.05 , 0.2 , 0.5 ], \"solver\" : [ \"newton-cg\" , \"lbfgs\" , \"liblinear\" ] } # Add our algorithms and parameters to lists to be used in our function alg_list = [LogisticRegression()] param_list = [lr_grid] # Find the best estimators, then add our other estimators which don't need optimisation best_estimators = optimise_hyperparameters(X, y, alg_list, param_list) Fitting 5 folds for each of 18 candidates, totalling 90 fits [Parallel(n_jobs=1)]: Done 90 out of 90 | elapsed: 5.2s finished lr_best_params = best_estimators[ 0 ] . get_params() lr_best_params { 'C' : 0.01 , 'class_weight' : None , 'dual' : False , 'fit_intercept' : True , 'intercept_scaling' : 1 , 'max_iter' : 100 , 'multi_class' : 'ovr' , 'n_jobs' : 1 , 'penalty' : 'l2' , 'random_state' : None , 'solver' : 'newton-cg' , 'tol' : 0.0001 , 'verbose' : 0 , 'warm_start' : False } kfold = StratifiedKFold(n_splits =10 ) cv_scores = cross_val_score(linear_model . LogisticRegression( ** lr_best_params), X, y, scoring = 'neg_log_loss' , cv = kfold) cv_scores . mean() -0.528741673153639 In the next iteration of this tutorial we will also optimise an XGB model and hopefully outperform our logistic regression model.","title":"Using Grid Search To Tune Hyperparameters"},{"location":"modelling/AFLmodelPart3/#creating-predictions-for-the-2018-season","text":"Now that we have an optimised logistic regression model, let's see how it performs on predicting the 2018 season. lr = LogisticRegression( ** lr_best_params) lr . fit(X, y) final_predictions = lr . predict(test_x) accuracy = (final_predictions == test_y) . mean() * 100 print ( \"Our accuracy in predicting the 2018 season is: {:.2f}%\" . format(accuracy)) Our accuracy in predicting the 2018 season is: 67.68% Now let's have a look at all the games which we incorrectly predicted. game_ids = test_x[(final_predictions != test_y)] . game afl_data_one_line . loc[afl_data_one_line . game . isin(game_ids), [ 'date' , 'home_team' , 'opponent' , 'f_odds' , 'f_odds_away' , 'f_margin' ]] date home_team opponent f_odds f_odds_away f_margin 1386 2018-03-24 Gold Coast North Melbourne 2.0161 1.9784 16 1388 2018-03-25 Melbourne Geelong 1.7737 2.2755 -3 1391 2018-03-30 North Melbourne St Kilda 3.5769 1.3867 52 1392 2018-03-31 Carlton Gold Coast 1.5992 2.6620 -34 1396 2018-04-01 Western Bulldogs West Coast 1.8044 2.2445 -51 1397 2018-04-01 Sydney Port Adelaide 1.4949 3.0060 -23 1398 2018-04-02 Geelong Hawthorn 1.7597 2.3024 -1 1406 2018-04-08 Western Bulldogs Essendon 3.8560 1.3538 21 1408 2018-04-13 Adelaide Collingwood 1.2048 5.9197 -48 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1415 2018-04-15 Hawthorn Melbourne 2.2855 1.7772 67 1417 2018-04-20 Sydney Adelaide 1.2640 4.6929 -10 1420 2018-04-21 Port Adelaide Geelong 1.5053 2.9515 -34 1422 2018-04-22 North Melbourne Hawthorn 2.6170 1.6132 28 1423 2018-04-22 Brisbane Gold Coast 1.7464 2.3277 -5 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1427 2018-04-28 Geelong Sydney 1.5019 2.9833 -17 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 1439 2018-05-05 Sydney North Melbourne 1.2777 4.5690 -2 1444 2018-05-11 Hawthorn Sydney 1.6283 2.5818 -8 1445 2018-05-12 GWS West Coast 1.5425 2.8292 -25 1446 2018-05-12 Carlton Essendon 3.1742 1.4570 13 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1456 2018-05-19 Essendon Geelong 5.6530 1.2104 34 1460 2018-05-20 Brisbane Hawthorn 3.2891 1.4318 56 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1466 2018-05-26 GWS Essendon 1.4364 3.2652 -35 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 ... ... ... ... ... ... ... 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 1485 2018-06-11 Melbourne Collingwood 1.6034 2.6450 -42 1492 2018-06-21 West Coast Essendon 1.3694 3.6843 -28 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1499 2018-06-29 Western Bulldogs Geelong 6.2067 1.1889 2 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1504 2018-07-01 Melbourne St Kilda 1.1405 7.7934 -2 1505 2018-07-01 Essendon North Melbourne 2.0993 1.9022 17 1506 2018-07-01 Fremantle Brisbane 1.2914 4.3743 -55 1507 2018-07-05 Sydney Geelong 1.7807 2.2675 -12 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1516 2018-07-12 Adelaide Geelong 2.0517 1.9444 15 1518 2018-07-14 Hawthorn Brisbane 1.2281 5.4105 -33 1521 2018-07-14 GWS Richmond 2.7257 1.5765 2 1522 2018-07-15 Collingwood West Coast 1.5600 2.7815 -35 1523 2018-07-15 North Melbourne Sydney 1.9263 2.0647 -6 1524 2018-07-15 Fremantle Port Adelaide 5.9110 1.2047 9 1527 2018-07-21 Sydney Gold Coast 1.0342 27.8520 -24 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 1533 2018-07-22 Port Adelaide GWS 1.6480 2.5452 -22 1538 2018-07-28 Gold Coast Carlton 1.3933 3.5296 -35 1546 2018-08-04 Adelaide Port Adelaide 2.0950 1.9135 3 1548 2018-08-04 St Kilda Western Bulldogs 1.6120 2.6368 -35 1555 2018-08-11 Port Adelaide West Coast 1.4187 3.3505 -4 1558 2018-08-12 North Melbourne Western Bulldogs 1.3175 4.1239 -7 1559 2018-08-12 Melbourne Sydney 1.3627 3.7445 -9 1564 2018-08-18 GWS Sydney 1.8478 2.1672 -20 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 Very interesting! Most of the games we got wrong were upsets. Let's have a look at the games we incorrectly predicted that weren't upsets. (afl_data_one_line . loc[afl_data_one_line . game . isin(game_ids), [ 'date' , 'home_team' , 'opponent' , 'f_odds' , 'f_odds_away' , 'f_margin' ]] . assign(home_favourite = lambda df: df . apply( lambda row: 1 if row . f_odds < row . f_odds_away else 0 , axis =1 )) . assign(upset = lambda df: df . apply( lambda row: 1 if row . home_favourite == 1 and row . f_margin < 0 else ( 1 if row . home_favourite == 0 and row . f_margin > 0 else 0 ), axis =1 )) . query( 'upset == 0' )) date home_team opponent f_odds f_odds_away f_margin home_favourite upset 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1 0 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1 0 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 0 0 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 0 0 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 0 0 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1 0 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1 0 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 0 0 1479 2018-06-08 Port Adelaide Richmond 1.7422 2.3420 14 1 0 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 0 0 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1 0 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1 0 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 0 0 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1 0 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 0 0 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 0 0 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 0 0 Let's now look at our model's log loss for the 2018 season compared to the odds. predictions_probs = lr . predict_proba(test_x) metrics . log_loss(test_y, predictions_probs) 0.584824211055384 test_x_unscaled = feature_df . loc[feature_df . season == 2018 , [ 'game' ] + feature_columns] metrics . log_loss(test_y, test_x_unscaled[[ 'f_current_odds_prob_away' , 'f_current_odds_prob' ]]) 0.5545776633924343 So whilst our model performs decently, it doesn't beat the odds in terms of log loss. That's okay, it's still a decent start. In future iterations we can implement other algorithms and create new features which may improve performance.","title":"Creating Predictions for the 2018 Season"},{"location":"modelling/AFLmodelPart3/#next-steps","text":"Now that we have a model up and running, the next steps are to implement the model on a week to week basis. In the next tutorial we will be predicting the 2018 round of footy.","title":"Next Steps"},{"location":"modelling/AFLmodelPart4/","text":"AFL Modelling Walkthrough \u00b6 04. Weekly Predictions \u00b6 Now that we have explored different algorithms for modelling, we can implement our chosen model and predict this week's AFL games! All you need to do is run the afl_modelling script each Thursday or Friday to predict the following week's games. # Import Modules from afl_feature_creation_v2 import prepare_afl_features import afl_data_cleaning_v2 import afl_feature_creation_v2 import afl_modelling_v2 import datetime import pandas as pd import numpy as np pd . set_option( 'display.max_columns' , None ) from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler import warnings warnings . filterwarnings( 'ignore' ) Creating The Features For This Weekend's Games \u00b6 To actually predict this weekend's games, we need to create the same features that we have created in the previous tutorials for the games that will be played this weekend. This includes all the rolling averages, efficiency features, elo features etc. So the majority of this tutorial will be using previously defined functions to create features for the following weekend's games. Create Next Week's DataFrame \u00b6 Let's first get our cleaned afl_data dataset, as well as the odds for next weekend and the 2018 fixture. # Grab the cleaned AFL dataset and the column order afl_data = afl_data_cleaning_v2 . prepare_afl_data() ordered_cols = afl_data . columns # Define a function which grabs the odds for each game for the following weekend def get_next_week_odds (path): # Get next week's odds next_week_odds = pd . read_csv(path) next_week_odds = next_week_odds . rename(columns = { \"team_1\" : \"home_team\" , \"team_2\" : \"away_team\" , \"team_1_odds\" : \"odds\" , \"team_2_odds\" : \"odds_away\" }) return next_week_odds # Import the fixture # Define a function which gets the fixture and cleans it up def get_fixture (path): # Get the afl fixture fixture = pd . read_csv(path) # Replace team names and reformat fixture = fixture . replace({ 'Brisbane Lions' : 'Brisbane' , 'Footscray' : 'Western Bulldogs' }) fixture[ 'Date' ] = pd . to_datetime(fixture[ 'Date' ]) . dt . date . astype( str ) fixture = fixture . rename(columns = { \"Home.Team\" : \"home_team\" , \"Away.Team\" : \"away_team\" }) return fixture next_week_odds = get_next_week_odds( \"data/weekly_odds.csv\" ) fixture = get_fixture( \"data/afl_fixture_2018.csv\" ) fixture . tail() Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG next_week_odds home_team away_team odds odds_away 0 West Coast Collingwood 2.34 1.75 Now that we have these DataFrames, we will define a function which combines the fixture and next week's odds to create a single DataFrame for the games over the next 7 days. To use this function we will need Game IDs for next week. So we will create another function which creates Game IDs by using the Game ID from the last game played and adding 1 to it. # Define a function which creates game IDs for this week's footy games def create_next_weeks_game_ids (afl_data): odds = get_next_week_odds( \"data/weekly_odds.csv\" ) # Get last week's Game ID last_afl_data_game = afl_data[ 'game' ] . iloc[ -1 ] # Create Game IDs for next week game_ids = [(i +1 ) + last_afl_data_game for i in range (odds . shape[ 0 ])] return game_ids # Define a function which creates this week's footy game DataFrame def get_next_week_df (afl_data): # Get the fixture and the odds for next week's footy games fixture = get_fixture( \"data/afl_fixture_2018.csv\" ) next_week_odds = get_next_week_odds( \"data/weekly_odds.csv\" ) next_week_odds[ 'game' ] = create_next_weeks_game_ids(afl_data) # Get today's date and next week's date and create a DataFrame for next week's games # todays_date = datetime.datetime.today().strftime('%Y-%m-%d') # date_in_7_days = (datetime.datetime.today() + datetime.timedelta(days=7)).strftime('%Y-%m-%d') todays_date = '2018-09-27' date_in_7_days = '2018-10-04' fixture = fixture[(fixture[ 'Date' ] >= todays_date) & (fixture[ 'Date' ] < date_in_7_days)] . drop(columns = [ 'Season.Game' ]) next_week_df = pd . merge(fixture, next_week_odds, on = [ 'home_team' , 'away_team' ]) # Split the DataFrame onto two rows for each game h_df = (next_week_df[[ 'Date' , 'game' , 'home_team' , 'away_team' , 'odds' , 'Season' , 'Round' , 'Venue' ]] . rename(columns = { 'home_team' : 'team' , 'away_team' : 'opponent' }) . assign(home_game =1 )) a_df = (next_week_df[[ 'Date' , 'game' , 'home_team' , 'away_team' , 'odds_away' , 'Season' , 'Round' , 'Venue' ]] . rename(columns = { 'odds_away' : 'odds' , 'home_team' : 'opponent' , 'away_team' : 'team' }) . assign(home_game =0 )) next_week = a_df . append(h_df) . sort_values(by = 'game' ) . rename(columns = { 'Date' : 'date' , 'Season' : 'season' , 'Round' : 'round' , 'Venue' : 'venue' }) next_week[ 'date' ] = pd . to_datetime(next_week . date) next_week[ 'round' ] = afl_data[ 'round' ] . iloc[ -1 ] + 1 return next_week next_week_df = get_next_week_df(afl_data) game_ids_next_round = create_next_weeks_game_ids(afl_data) next_week_df date round season venue game home_game odds opponent team 0 2018-09-29 27 2018 MCG 15407 0 1.75 West Coast Collingwood 0 2018-09-29 27 2018 MCG 15407 1 2.34 Collingwood West Coast fixture . tail() Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG Create Each Feature \u00b6 Now let's append next week's DataFrame to our afl_data, match_results and odds DataFrames and then create all the features we used in the AFL Feature Creation Tutorial . We need to append the games and then feed them into our function so that we can create features for upcoming games. # Append next week's games to our afl_data DataFrame afl_data = afl_data . append(next_week_df) . reset_index(drop = True ) # Append next week's games to match results (we need to do this for our feature creation to run) match_results = afl_data_cleaning_v2 . get_cleaned_match_results() . append(next_week_df) # Append next week's games to odds odds = (afl_data_cleaning_v2 . get_cleaned_odds() . pipe( lambda df: df . append(next_week_df[df . columns])) . reset_index(drop = True )) features_df = afl_feature_creation_v2 . prepare_afl_features(afl_data = afl_data, match_results = match_results, odds = odds) features_df . tail() game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_GA1_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_Unnamed: 0_diff f_behinds_diff f_goals_diff f_margin_diff f_opponent_behinds_diff f_opponent_goals_diff f_opponent_points_diff f_points_diff f_current_odds_prob f_current_odds_prob_away 1065 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.966936 -23.2 2.0 1.813998 1523.456734 1609.444874 0.653525 0.680168 0.704767 0.749812 140.535514 0.605144 -9.771981 5.892176 7.172376 6.614609 -1.365211 30.766262 21.998618 0.067228 -1.404730 -3.166732 6.933998 6.675576 0.000000 38.708158 24.587333 12.008987 10.482382 -16.709540 -15.415060 289.188486 6.350287 -2.263536 -20.966818 50.388632 0.723637 15.537783 22.912269 2.065039 10.215523 -6.689429 3259.163465 -0.136383 3.553795 16.563721 -2.353514 1.162696 4.622664 21.186385 0.661551 0.340379 1066 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.089084 -3.2 2.0 2.577161 1397.237139 1499.366007 0.725980 0.655749 0.723949 0.677174 51.799992 3.399035 6.067393 -2.189489 -10.475859 1.154766 -8.883840 -21.810962 33.058382 40.618410 2.286314 -0.345734 -3.778445 -2.182673 0.000000 19.816372 -21.562916 2.678384 -14.777698 13.242010 12.065594 -82.381996 -2.176564 2.335825 -4.952336 45.719406 3.344217 -2.095613 -3.929084 -3.182381 -12.832197 57.226776 -20221.371526 1.968709 -1.897958 -15.177001 1.067099 0.781811 5.757963 -9.419038 0.284269 0.717566 1067 15404 Collingwood GWS 2018-09-15 25 M.C.G. 2018 1.882301 12.6 3.0 2.018344 1546.000498 1590.806454 0.693185 0.706222 0.718446 0.727961 205.916671 -1.642954 -2.980828 -0.266023 8.547225 -3.751909 -0.664977 10.563513 48.175985 43.531908 -5.836979 5.388668 4.395675 2.555152 0.000000 51.588962 11.558254 4.276481 11.284445 -3.412977 -2.206815 -234.577304 2.637758 -10.537765 -11.127876 125.607377 -3.485896 3.532031 15.102292 -2.500685 8.187543 38.053445 12500.525732 -1.006173 2.520135 18.634835 -2.159882 -0.393386 -4.520198 14.114637 0.608495 0.393856 1068 15406 West Coast Melbourne 2018-09-22 26 Perth Stadium 2018 2.013572 21.2 3.0 1.884148 1577.888606 1542.095154 0.688877 0.708941 0.649180 0.698319 -118.135184 -3.005709 2.453190 -5.103869 -14.368949 -12.245458 2.771411 -45.364271 -60.210182 -24.049523 -2.791277 6.115918 -5.041030 -5.335746 0.000000 -78.816902 -18.784547 -13.957754 -5.527613 18.606721 25.366778 -910.988860 -5.515812 -9.483590 8.914093 -131.380758 -7.142529 -49.484957 -13.718798 -4.862994 -9.834616 -23.673638 -3178.282073 -1.785349 -2.569957 -20.008787 0.476202 0.387915 2.803694 -17.205093 0.543774 0.457875 1069 15407 West Coast Collingwood 2018-09-29 27 MCG 2018 1.981832 17.2 3.0 1.838864 1591.348723 1562.924273 0.679011 0.724125 0.711352 0.709346 159.522670 0.893421 -0.475725 3.391070 -5.088751 5.875388 5.352234 7.729063 -7.358202 -4.719968 6.113565 4.822252 2.871241 2.690270 3.636364 -64.238180 -0.631102 2.078832 6.005613 56.879978 34.373271 1016.491933 1.199751 2.454685 12.197047 219.666562 2.484363 0.379162 2.566991 0.639666 2.258377 -23.841529 -368920.360240 -0.646160 0.892051 3.040850 1.589568 0.012622 1.665299 4.706148 0.427350 0.571429 Create Predictions For the Upcoming Round \u00b6 Now that we have our features, we can use our model that we created in part 3 to predict the next round. First we need to filter our features_df into a training df and a df with next round's features/matches. Then we can use the model created in the last tutorial to create predictions. For simplicity, I have hardcoded the parameters we used in the last tutorial. # Get the train df by only taking the games IDs which aren't in the next week df train_df = features_df[ ~ features_df . game . isin(next_week_df . game)] # Get the result and merge to the feature_df match_results = (pd . read_csv( \"data/afl_match_results.csv\" ) . rename(columns = { 'Game' : 'game' }) . assign(result = lambda df: df . apply( lambda row: 1 if row[ 'Home.Points' ] > row[ 'Away.Points' ] else 0 , axis =1 ))) train_df = pd . merge(train_df, match_results[[ 'game' , 'result' ]], on = 'game' ) train_x = train_df . drop(columns = [ 'result' ]) train_y = train_df . result next_round_x = features_df[features_df . game . isin(next_week_df . game)] # Fit out logistic regression model - note that our predictions come out in the order of [away_team_prob, home_team_prob] lr_best_params = { 'C' : 0.01 , 'class_weight' : None , 'dual' : False , 'fit_intercept' : True , 'intercept_scaling' : 1 , 'max_iter' : 100 , 'multi_class' : 'ovr' , 'n_jobs' : 1 , 'penalty' : 'l2' , 'random_state' : None , 'solver' : 'newton-cg' , 'tol' : 0.0001 , 'verbose' : 0 , 'warm_start' : False } feature_cols = [col for col in train_df if col . startswith( 'f_' )] # Scale features scaler = StandardScaler() train_x[feature_cols] = scaler . fit_transform(train_x[feature_cols]) next_round_x[feature_cols] = scaler . transform(next_round_x[feature_cols]) lr = LogisticRegression( ** lr_best_params) lr . fit(train_x[feature_cols], train_y) prediction_probs = lr . predict_proba(next_round_x[feature_cols]) modelled_home_odds = [ 1/ i[ 1 ] for i in prediction_probs] modelled_away_odds = [ 1/ i[ 0 ] for i in prediction_probs] # Create a predictions df preds_df = (next_round_x[[ 'date' , 'home_team' , 'away_team' , 'venue' , 'game' ]] . copy() . assign(modelled_home_odds = modelled_home_odds, modelled_away_odds = modelled_away_odds) . pipe(pd . merge, next_week_odds, on = [ 'home_team' , 'away_team' ]) . pipe(pd . merge, features_df[[ 'game' , 'f_elo_home' , 'f_elo_away' ]], on = 'game' ) . drop(columns = 'game' ) ) preds_df date home_team away_team venue modelled_home_odds modelled_away_odds odds odds_away f_elo_home f_elo_away 0 2018-09-29 West Coast Collingwood MCG 2.326826 1.753679 2.34 1.75 1591.348723 1562.924273 Alternatively, if you want to generate predictions using a script which uses all the above code, just run the following: print (afl_modelling_v2 . create_predictions()) date home_team away_team venue modelled_home_odds \\ 0 2018-09-29 West Coast Collingwood MCG 2.326826 modelled_away_odds odds odds_away f_elo_home f_elo_away 0 1.753679 2.34 1.75 1591.348723 1562.924273 Conclusion \u00b6 Congratulations! You have created AFL predictions for this week. If you are beginner to this, don't be overwhelmed. The process gets easier each time you do it. And it is super rewarding. In future iterations we will update this tutorial to predict actual odds, and then integrate this with Betfair's API so that you can create an automated betting strategy using Machine Learning to create your predictions!","title":"AFL 04. Weekly predictions"},{"location":"modelling/AFLmodelPart4/#afl-modelling-walkthrough","text":"","title":"AFL Modelling Walkthrough"},{"location":"modelling/AFLmodelPart4/#04-weekly-predictions","text":"Now that we have explored different algorithms for modelling, we can implement our chosen model and predict this week's AFL games! All you need to do is run the afl_modelling script each Thursday or Friday to predict the following week's games. # Import Modules from afl_feature_creation_v2 import prepare_afl_features import afl_data_cleaning_v2 import afl_feature_creation_v2 import afl_modelling_v2 import datetime import pandas as pd import numpy as np pd . set_option( 'display.max_columns' , None ) from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler import warnings warnings . filterwarnings( 'ignore' )","title":"04. Weekly Predictions"},{"location":"modelling/AFLmodelPart4/#creating-the-features-for-this-weekends-games","text":"To actually predict this weekend's games, we need to create the same features that we have created in the previous tutorials for the games that will be played this weekend. This includes all the rolling averages, efficiency features, elo features etc. So the majority of this tutorial will be using previously defined functions to create features for the following weekend's games.","title":"Creating The Features For This Weekend's Games"},{"location":"modelling/AFLmodelPart4/#create-next-weeks-dataframe","text":"Let's first get our cleaned afl_data dataset, as well as the odds for next weekend and the 2018 fixture. # Grab the cleaned AFL dataset and the column order afl_data = afl_data_cleaning_v2 . prepare_afl_data() ordered_cols = afl_data . columns # Define a function which grabs the odds for each game for the following weekend def get_next_week_odds (path): # Get next week's odds next_week_odds = pd . read_csv(path) next_week_odds = next_week_odds . rename(columns = { \"team_1\" : \"home_team\" , \"team_2\" : \"away_team\" , \"team_1_odds\" : \"odds\" , \"team_2_odds\" : \"odds_away\" }) return next_week_odds # Import the fixture # Define a function which gets the fixture and cleans it up def get_fixture (path): # Get the afl fixture fixture = pd . read_csv(path) # Replace team names and reformat fixture = fixture . replace({ 'Brisbane Lions' : 'Brisbane' , 'Footscray' : 'Western Bulldogs' }) fixture[ 'Date' ] = pd . to_datetime(fixture[ 'Date' ]) . dt . date . astype( str ) fixture = fixture . rename(columns = { \"Home.Team\" : \"home_team\" , \"Away.Team\" : \"away_team\" }) return fixture next_week_odds = get_next_week_odds( \"data/weekly_odds.csv\" ) fixture = get_fixture( \"data/afl_fixture_2018.csv\" ) fixture . tail() Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG next_week_odds home_team away_team odds odds_away 0 West Coast Collingwood 2.34 1.75 Now that we have these DataFrames, we will define a function which combines the fixture and next week's odds to create a single DataFrame for the games over the next 7 days. To use this function we will need Game IDs for next week. So we will create another function which creates Game IDs by using the Game ID from the last game played and adding 1 to it. # Define a function which creates game IDs for this week's footy games def create_next_weeks_game_ids (afl_data): odds = get_next_week_odds( \"data/weekly_odds.csv\" ) # Get last week's Game ID last_afl_data_game = afl_data[ 'game' ] . iloc[ -1 ] # Create Game IDs for next week game_ids = [(i +1 ) + last_afl_data_game for i in range (odds . shape[ 0 ])] return game_ids # Define a function which creates this week's footy game DataFrame def get_next_week_df (afl_data): # Get the fixture and the odds for next week's footy games fixture = get_fixture( \"data/afl_fixture_2018.csv\" ) next_week_odds = get_next_week_odds( \"data/weekly_odds.csv\" ) next_week_odds[ 'game' ] = create_next_weeks_game_ids(afl_data) # Get today's date and next week's date and create a DataFrame for next week's games # todays_date = datetime.datetime.today().strftime('%Y-%m-%d') # date_in_7_days = (datetime.datetime.today() + datetime.timedelta(days=7)).strftime('%Y-%m-%d') todays_date = '2018-09-27' date_in_7_days = '2018-10-04' fixture = fixture[(fixture[ 'Date' ] >= todays_date) & (fixture[ 'Date' ] < date_in_7_days)] . drop(columns = [ 'Season.Game' ]) next_week_df = pd . merge(fixture, next_week_odds, on = [ 'home_team' , 'away_team' ]) # Split the DataFrame onto two rows for each game h_df = (next_week_df[[ 'Date' , 'game' , 'home_team' , 'away_team' , 'odds' , 'Season' , 'Round' , 'Venue' ]] . rename(columns = { 'home_team' : 'team' , 'away_team' : 'opponent' }) . assign(home_game =1 )) a_df = (next_week_df[[ 'Date' , 'game' , 'home_team' , 'away_team' , 'odds_away' , 'Season' , 'Round' , 'Venue' ]] . rename(columns = { 'odds_away' : 'odds' , 'home_team' : 'opponent' , 'away_team' : 'team' }) . assign(home_game =0 )) next_week = a_df . append(h_df) . sort_values(by = 'game' ) . rename(columns = { 'Date' : 'date' , 'Season' : 'season' , 'Round' : 'round' , 'Venue' : 'venue' }) next_week[ 'date' ] = pd . to_datetime(next_week . date) next_week[ 'round' ] = afl_data[ 'round' ] . iloc[ -1 ] + 1 return next_week next_week_df = get_next_week_df(afl_data) game_ids_next_round = create_next_weeks_game_ids(afl_data) next_week_df date round season venue game home_game odds opponent team 0 2018-09-29 27 2018 MCG 15407 0 1.75 West Coast Collingwood 0 2018-09-29 27 2018 MCG 15407 1 2.34 Collingwood West Coast fixture . tail() Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG","title":"Create Next Week's DataFrame"},{"location":"modelling/AFLmodelPart4/#create-each-feature","text":"Now let's append next week's DataFrame to our afl_data, match_results and odds DataFrames and then create all the features we used in the AFL Feature Creation Tutorial . We need to append the games and then feed them into our function so that we can create features for upcoming games. # Append next week's games to our afl_data DataFrame afl_data = afl_data . append(next_week_df) . reset_index(drop = True ) # Append next week's games to match results (we need to do this for our feature creation to run) match_results = afl_data_cleaning_v2 . get_cleaned_match_results() . append(next_week_df) # Append next week's games to odds odds = (afl_data_cleaning_v2 . get_cleaned_odds() . pipe( lambda df: df . append(next_week_df[df . columns])) . reset_index(drop = True )) features_df = afl_feature_creation_v2 . prepare_afl_features(afl_data = afl_data, match_results = match_results, odds = odds) features_df . tail() game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_GA1_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_Unnamed: 0_diff f_behinds_diff f_goals_diff f_margin_diff f_opponent_behinds_diff f_opponent_goals_diff f_opponent_points_diff f_points_diff f_current_odds_prob f_current_odds_prob_away 1065 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.966936 -23.2 2.0 1.813998 1523.456734 1609.444874 0.653525 0.680168 0.704767 0.749812 140.535514 0.605144 -9.771981 5.892176 7.172376 6.614609 -1.365211 30.766262 21.998618 0.067228 -1.404730 -3.166732 6.933998 6.675576 0.000000 38.708158 24.587333 12.008987 10.482382 -16.709540 -15.415060 289.188486 6.350287 -2.263536 -20.966818 50.388632 0.723637 15.537783 22.912269 2.065039 10.215523 -6.689429 3259.163465 -0.136383 3.553795 16.563721 -2.353514 1.162696 4.622664 21.186385 0.661551 0.340379 1066 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.089084 -3.2 2.0 2.577161 1397.237139 1499.366007 0.725980 0.655749 0.723949 0.677174 51.799992 3.399035 6.067393 -2.189489 -10.475859 1.154766 -8.883840 -21.810962 33.058382 40.618410 2.286314 -0.345734 -3.778445 -2.182673 0.000000 19.816372 -21.562916 2.678384 -14.777698 13.242010 12.065594 -82.381996 -2.176564 2.335825 -4.952336 45.719406 3.344217 -2.095613 -3.929084 -3.182381 -12.832197 57.226776 -20221.371526 1.968709 -1.897958 -15.177001 1.067099 0.781811 5.757963 -9.419038 0.284269 0.717566 1067 15404 Collingwood GWS 2018-09-15 25 M.C.G. 2018 1.882301 12.6 3.0 2.018344 1546.000498 1590.806454 0.693185 0.706222 0.718446 0.727961 205.916671 -1.642954 -2.980828 -0.266023 8.547225 -3.751909 -0.664977 10.563513 48.175985 43.531908 -5.836979 5.388668 4.395675 2.555152 0.000000 51.588962 11.558254 4.276481 11.284445 -3.412977 -2.206815 -234.577304 2.637758 -10.537765 -11.127876 125.607377 -3.485896 3.532031 15.102292 -2.500685 8.187543 38.053445 12500.525732 -1.006173 2.520135 18.634835 -2.159882 -0.393386 -4.520198 14.114637 0.608495 0.393856 1068 15406 West Coast Melbourne 2018-09-22 26 Perth Stadium 2018 2.013572 21.2 3.0 1.884148 1577.888606 1542.095154 0.688877 0.708941 0.649180 0.698319 -118.135184 -3.005709 2.453190 -5.103869 -14.368949 -12.245458 2.771411 -45.364271 -60.210182 -24.049523 -2.791277 6.115918 -5.041030 -5.335746 0.000000 -78.816902 -18.784547 -13.957754 -5.527613 18.606721 25.366778 -910.988860 -5.515812 -9.483590 8.914093 -131.380758 -7.142529 -49.484957 -13.718798 -4.862994 -9.834616 -23.673638 -3178.282073 -1.785349 -2.569957 -20.008787 0.476202 0.387915 2.803694 -17.205093 0.543774 0.457875 1069 15407 West Coast Collingwood 2018-09-29 27 MCG 2018 1.981832 17.2 3.0 1.838864 1591.348723 1562.924273 0.679011 0.724125 0.711352 0.709346 159.522670 0.893421 -0.475725 3.391070 -5.088751 5.875388 5.352234 7.729063 -7.358202 -4.719968 6.113565 4.822252 2.871241 2.690270 3.636364 -64.238180 -0.631102 2.078832 6.005613 56.879978 34.373271 1016.491933 1.199751 2.454685 12.197047 219.666562 2.484363 0.379162 2.566991 0.639666 2.258377 -23.841529 -368920.360240 -0.646160 0.892051 3.040850 1.589568 0.012622 1.665299 4.706148 0.427350 0.571429","title":"Create Each Feature"},{"location":"modelling/AFLmodelPart4/#create-predictions-for-the-upcoming-round","text":"Now that we have our features, we can use our model that we created in part 3 to predict the next round. First we need to filter our features_df into a training df and a df with next round's features/matches. Then we can use the model created in the last tutorial to create predictions. For simplicity, I have hardcoded the parameters we used in the last tutorial. # Get the train df by only taking the games IDs which aren't in the next week df train_df = features_df[ ~ features_df . game . isin(next_week_df . game)] # Get the result and merge to the feature_df match_results = (pd . read_csv( \"data/afl_match_results.csv\" ) . rename(columns = { 'Game' : 'game' }) . assign(result = lambda df: df . apply( lambda row: 1 if row[ 'Home.Points' ] > row[ 'Away.Points' ] else 0 , axis =1 ))) train_df = pd . merge(train_df, match_results[[ 'game' , 'result' ]], on = 'game' ) train_x = train_df . drop(columns = [ 'result' ]) train_y = train_df . result next_round_x = features_df[features_df . game . isin(next_week_df . game)] # Fit out logistic regression model - note that our predictions come out in the order of [away_team_prob, home_team_prob] lr_best_params = { 'C' : 0.01 , 'class_weight' : None , 'dual' : False , 'fit_intercept' : True , 'intercept_scaling' : 1 , 'max_iter' : 100 , 'multi_class' : 'ovr' , 'n_jobs' : 1 , 'penalty' : 'l2' , 'random_state' : None , 'solver' : 'newton-cg' , 'tol' : 0.0001 , 'verbose' : 0 , 'warm_start' : False } feature_cols = [col for col in train_df if col . startswith( 'f_' )] # Scale features scaler = StandardScaler() train_x[feature_cols] = scaler . fit_transform(train_x[feature_cols]) next_round_x[feature_cols] = scaler . transform(next_round_x[feature_cols]) lr = LogisticRegression( ** lr_best_params) lr . fit(train_x[feature_cols], train_y) prediction_probs = lr . predict_proba(next_round_x[feature_cols]) modelled_home_odds = [ 1/ i[ 1 ] for i in prediction_probs] modelled_away_odds = [ 1/ i[ 0 ] for i in prediction_probs] # Create a predictions df preds_df = (next_round_x[[ 'date' , 'home_team' , 'away_team' , 'venue' , 'game' ]] . copy() . assign(modelled_home_odds = modelled_home_odds, modelled_away_odds = modelled_away_odds) . pipe(pd . merge, next_week_odds, on = [ 'home_team' , 'away_team' ]) . pipe(pd . merge, features_df[[ 'game' , 'f_elo_home' , 'f_elo_away' ]], on = 'game' ) . drop(columns = 'game' ) ) preds_df date home_team away_team venue modelled_home_odds modelled_away_odds odds odds_away f_elo_home f_elo_away 0 2018-09-29 West Coast Collingwood MCG 2.326826 1.753679 2.34 1.75 1591.348723 1562.924273 Alternatively, if you want to generate predictions using a script which uses all the above code, just run the following: print (afl_modelling_v2 . create_predictions()) date home_team away_team venue modelled_home_odds \\ 0 2018-09-29 West Coast Collingwood MCG 2.326826 modelled_away_odds odds odds_away f_elo_home f_elo_away 0 1.753679 2.34 1.75 1591.348723 1562.924273","title":"Create Predictions For the Upcoming Round"},{"location":"modelling/AFLmodelPart4/#conclusion","text":"Congratulations! You have created AFL predictions for this week. If you are beginner to this, don't be overwhelmed. The process gets easier each time you do it. And it is super rewarding. In future iterations we will update this tutorial to predict actual odds, and then integrate this with Betfair's API so that you can create an automated betting strategy using Machine Learning to create your predictions!","title":"Conclusion"},{"location":"modelling/AusOpenPythonTutorial/","text":"Australian Open Datathon R Tutorial \u00b6 Overview \u00b6 The Task \u00b6 This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide. The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here . For a detailed outline of the task, the prizes, and to sign up, click here . How an outline of our methodoly and thought process, read this article. Prizes \u00b6 Place Prize Place Prize 1 $5000 9 $500 2 $3000 10 $500 3 $2000 11 $200 4 $1000 12 $200 5 $750 13 $200 6 $500 14 $200 7 $500 15 $200 8 $500 Total $15250 Submission \u00b6 To submit your model, email your final submission to datathon@betfair.com.au . Note that you don't need to email your code, just your predictions in the format that we have specified No submissions will be accepted prior to the Australian Open qualifying matches being completed and the final draw list being shared with registered participants (12 January 2019) Submissions need to include all potential match ups during the Australian Open, i.e. all possible combinations for each men's and women's tournaments (this will be provided after the draw is announced and the Australian Open qualifying matches are completed (Jan 12 th 2019)) Submissions must follow the format outlined above and shown in the 'Dummy Submission File'. Any submissions that are not in the correct format will not be accepted. Submissions need to include the player names for the hypothetical match up and the probability of the first player winning i.e. player_1,player_2,probability_of_player_1_winning, Submissions must be in a csv format Only two models will be accepted per participant (one model for the men's draw, one model for the women's draw) Intention \u00b6 This notebook will demostrate how to: Process the raw data sets Produce simple features Run a predictive model on H2O Outputs the final predictions for the submissions Load the data and required packages import numpy as np import pandas as pd import os import gc import sys import warnings warnings.filterwarnings( 'ignore' ) import h2o from h2o.automl import H2OAutoML pd.options.display.max_columns = 999 # We are loading both the mens and womens match csvs df_atp = pd.read_csv( \"data/ATP_matches.csv\" ) df_wta = pd.read_csv( \"data/WTA_matches.csv\" ) Data preprocessing \u00b6 Filter the matches to hard and indoor hard only due to the fact that Australian Open is on hard surface and we want the models to train specifically for hard surfaces matches Convert the columns in both datasets to the correct types. For example, we want to make sure the date columns are in the datetime format and numerial columns are either interger or floats. This will help reduce the memory in use and make the feature engineering process easier ### Include hard and indoor hard only df_atp = df_atp.loc[df_atp.Court_Surface.isin([ 'Hard' , 'Indoor Hard' ])] df_wta = df_wta.loc[df_wta.Court_Surface.isin([ 'Hard' , 'Indoor Hard' ])] ### Exclude qualifying rounds df_atp = df_atp.loc[df_atp.Round_Description != 'Qualifying' ] df_wta = df_wta.loc[df_wta.Round_Description != 'Qualifying' ] # Store the shape of the data for reference check later atp_shape = df_atp.shape wta_shape = df_wta.shape numeric_columns = [ 'Winner_Rank' , 'Loser_Rank' , 'Retirement_Ind' , 'Winner_Sets_Won' , 'Winner_Games_Won' , 'Winner_Aces' , 'Winner_DoubleFaults' , 'Winner_FirstServes_Won' , 'Winner_FirstServes_In' , 'Winner_SecondServes_Won' , 'Winner_SecondServes_In' , 'Winner_BreakPoints_Won' , 'Winner_BreakPoints' , 'Winner_ReturnPoints_Won' , 'Winner_ReturnPoints_Faced' , 'Winner_TotalPoints_Won' , 'Loser_Sets_Won' , 'Loser_Games_Won' , 'Loser_Aces' , 'Loser_DoubleFaults' , 'Loser_FirstServes_Won' , 'Loser_FirstServes_In' , 'Loser_SecondServes_Won' , 'Loser_SecondServes_In' , 'Loser_BreakPoints_Won' , 'Loser_BreakPoints' , 'Loser_ReturnPoints_Won' , 'Loser_ReturnPoints_Faced' , 'Loser_TotalPoints_Won' ] text_columns = [ 'Winner' , 'Loser' , 'Tournament' , 'Court_Surface' , 'Round_Description' ] date_columns = [ 'Tournament_Date' ] # we set the **erros** to coerce so any non-numerical values (text,special characters) will return an NA df_atp[numeric_columns] = df_atp[numeric_columns] . apply (pd.to_numeric,errors = 'coerce' ) df_wta[numeric_columns] = df_wta[numeric_columns] . apply (pd.to_numeric,errors = 'coerce' ) df_atp[date_columns] = df_atp[date_columns] . apply (pd.to_datetime) df_wta[date_columns] = df_wta[date_columns] . apply (pd.to_datetime) Feature Engineering \u00b6 The raw datasets are constructed in a way that each row will have the seperate stats for both the winner and loser of that match. However, we want to reshape the data so that each row we will only have one player randomly selected from the winner/loser columns and the features are the difference between opponents statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns. In addition, for the features, we will take the rolling average of the player's most recent 15 matches before the particular tournament starts. For example, if the match is the second round of the Australian Open 2018, the features will be the last 15 matches before the first round of Australian Open 2018. The reason of not including the stats in the first round is that we would not have known the player's stats in the first round for the final submissions A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, difference in ELO rating etc. The target variable will be whether or not Player A wins (1=Player A wins and 0=lose). The steps we take are: Convert the raw dataframes into long format: Create some new features Take the rolling average for each player and each match Since we will be only training our models on US Open and Australian Open, we will only be creating features for those matches. However, the rolling average will take into account any hard surface matches before those tournaments Calculate the difference of averages for each match in the data frames Convert the raw dataframes into long format: \u00b6 # Before we split the dataframe into winner and loser, we want to create a feature that captures the total number of games the match takes. # We have to do it before the split or we will lose this information df_atp[ 'Total_Games' ] = df_atp.Winner_Games_Won + df_atp.Loser_Games_Won df_wta[ 'Total_Games' ] = df_wta.Winner_Games_Won + df_wta.Loser_Games_Won # Get the column names for the winner and loser stats winner_cols = [col for col in df_atp.columns if col.startswith( 'Winner' )] loser_cols = [col for col in df_atp.columns if col.startswith( 'Loser' )] # create a winner dataframe to store the winner stats and a loser dataframe for the losers # In addition to the winner and loser columns, we are adding common columns as well (e.g. tournamnt dates) common_cols = [ 'Total_Games' , 'Tournament' , 'Tournament_Date' , 'Court_Surface' , 'Round_Description' ] df_winner_atp = df_atp[winner_cols + common_cols] df_loser_atp = df_atp[loser_cols + common_cols] df_winner_wta = df_wta[winner_cols + common_cols] df_loser_wta = df_wta[loser_cols + common_cols] # Create a new column to show whether the player has won or not. df_winner_atp[ \"won\" ] = 1 df_loser_atp[ \"won\" ] = 0 df_winner_wta[ \"won\" ] = 1 df_loser_wta[ \"won\" ] = 0 # Rename the columns for the winner and loser data frames so we can append them later on. # We will rename the Winner_ / Loser_ columns to Player_ new_column_names = [col.replace( 'Winner' , 'Player' ) for col in winner_cols] df_winner_atp.columns = new_column_names + common_cols + [ 'won' ] # They all should be the same df_loser_atp.columns = df_winner_atp.columns df_winner_wta.columns = df_winner_atp.columns df_loser_wta.columns = df_winner_atp.columns # append the winner and loser dataframes df_long_atp = df_winner_atp.append(df_loser_atp) df_long_wta = df_winner_wta.append(df_loser_wta) So now our dataframes are in long format and should looks like this df_long_atp.head() Player Player_Rank Player_Sets_Won Player_Games_Won Player_Aces Player_DoubleFaults Player_FirstServes_Won Player_FirstServes_In Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Total_Games Tournament Tournament_Date Court_Surface Round_Description won Edouard Roger-Vasselin 106.0 2.0 12 5.0 2.0 22 30 12 19 4.0 7.0 25.0 59.0 59 19 Chennai 2012-01-02 Hard First Round 1 Dudi Sela 83.0 2.0 12 2.0 0.0 14 17 11 16 6.0 14.0 36.0 58.0 61 13 Chennai 2012-01-02 Hard First Round 1 Go Soeda 120.0 2.0 19 6.0 1.0 48 64 19 39 5.0 11.0 42.0 105.0 109 33 Chennai 2012-01-02 Hard First Round 1 Yuki Bhambri 345.0 2.0 12 1.0 2.0 22 29 9 17 5.0 13.0 34.0 62.0 65 17 Chennai 2012-01-02 Hard First Round 1 Yuichi Sugita 235.0 2.0 12 3.0 1.0 37 51 11 27 3.0 7.0 22.0 54.0 70 19 Chennai 2012-01-02 Hard First Round 1 Create some new features \u00b6 Thinking about the dynamics of tennis, we know that players often will matches by \u201cbreaking\u201d the opponent\u2019s serve (i.e. winning a game when the opponent is serving). This is especially important in tennis. Let\u2019s create a feature called Player_BreakPoints_Per_Game, which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let\u2019s also create a feature called Player_Return_Win_Ratio which is the proportion of points won when returning. Similarly, \u201cholding\u201d serve is important (i.e. winning a game when you are serving). Let\u2019s create a feature called Player_Serve_Win_Ratio which is the proportion of points won when serving. Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let\u2019s create a feature called Player_Game_Win_Percentage which is the propotion of games that a player wins. So the four new features we will create are: Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage # Here, we will define a function so we can apply it to both atp and wta dataframes def get_new_features(df) : # Input: # df: dataframe to get the data from # Return: the df with the new features # Point Win ratio when serving df[ 'Player_Serve_Win_Ratio' ] = (df.Player_FirstServes_Won + df.Player_SecondServes_Won - df.Player_DoubleFaults) \\ / (df.Player_FirstServes_In + df.Player_SecondServes_In + df.Player_DoubleFaults) # Point win ratio when returning df[ 'Player_Return_Win_Ratio' ] = df.Player_ReturnPoints_Won / df.Player_ReturnPoints_Faced # Breakpoints per receiving game df[ 'Player_BreakPoints_Per_Return_Game' ] = df.Player_BreakPoints / df.Total_Games df[ 'Player_Game_Win_Percentage' ] = df.Player_Games_Won / df.Total_Games return df # Apply the function we just created to the long dataframes df_long_atp = get_new_features(df_long_atp) df_long_wta = get_new_features(df_long_wta) # The long table should have exactly twice of the rows of the original data assert df_long_atp.shape[ 0 ] == atp_shape[ 0 ] *2 assert df_long_wta.shape[ 0 ] == wta_shape[ 0 ] *2 Take the rolling average for each player and each match \u00b6 To train our models, we cannot simply use the player stats for that current match. In fact, we wont be able to use any stats from the same tournament. The logic behind this is that when we try to predict the results in 2019, we would not know the stats of any of the matches in the Australian Open 2019 tournament. As a result, we will use the players' past performance. Here, we will do a rolling average of the most recent 15 matches before the tournament. To do the above, we will follow the steps below: List all the tournament dates for US and Australian Opens Loop through the dates from point 1, for each date, we filter the data to only include matches before that date and take the most recent 15 games Take the average of those 15 games # the two tournaments we will be using for training and thus the feature generation tournaments = [ 'U.S. Open, New York' , 'Australian Open, Melbourne' ] # Store the dates for the loops tournament_dates_atp = df_atp.loc[df_atp.Tournament.isin(tournaments)] . groupby([ 'Tournament' , 'Tournament_Date' ]) \\ . size() . reset_index()[[ 'Tournament' , 'Tournament_Date' ]] tournament_dates_wta = df_wta.loc[df_wta.Tournament.isin(tournaments)] . groupby([ 'Tournament' , 'Tournament_Date' ]) \\ . size() . reset_index()[[ 'Tournament' , 'Tournament_Date' ]] # We are adding one more date for the final prediction tournament_dates_atp.loc[ -1 ] = [ 'Australian Open, Melbourne' ,pd.to_datetime( '2019-01-15' )] tournament_dates_wta.loc[ -1 ] = [ 'Australian Open, Melbourne' ,pd.to_datetime( '2019-01-15' )] Following are the dates for each tournament tournament_dates_atp Tournament Tournament_Date Australian Open, Melbourne 2012-01-16 Australian Open, Melbourne 2013-01-1 Australian Open, Melbourne 2014-01-1 Australian Open, Melbourne 2015-01-1 Australian Open, Melbourne 2016-01-1 Australian Open, Melbourne 2017-01-1 Australian Open, Melbourne 2018-01-1 U.S. Open, New York 2012-08-2 U.S. Open, New York 2013-08-2 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 tournament_dates_wta Tournament Tournament_Date Australian Open, Melbourne 2014-01-13 Australian Open, Melbourne 2015-01-19 Australian Open, Melbourne 2016-01-18 Australian Open, Melbourne 2017-01-16 Australian Open, Melbourne 2018-01-15 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 They look fine but it is interesting that for men's, we have two more years of data from 2012 to 2013 # Let's define a function to calculate the rolling averages def get_rolling_features (df, date_df = None,rolling_cols = None, last_cols = None) : # Input: # df: dataframe to get the data from # date_df: dataframe that has the start dates for each tournament # rolling_cols: columns to get the rolling averages # last_cols: columns to get the last value (most recent) # Return: the df with the new features # Sort the data by player and dates so the most recent matches are at the bottom df = df.sort_values([ 'Player' , 'Tournament_Date' , 'Tournament' ], ascending = True) # For each tournament, get the rolling averages of that player's past matches before the tournament start date for index, tournament_date in enumerate(date_df.Tournament_Date) : # create a temp df to store the interim results df_temp = df.loc[df.Tournament_Date < tournament_date] # for ranks, we only take the last one. (comment this out if want to take avg of rank) df_temp_last = df_temp.groupby( 'Player' )[last_cols] . last() . reset_index() # take the most recent 15 matches for the rolling average df_temp = df_temp.groupby( 'Player' )[rolling_cols] . rolling( 15 , min_periods =1 ) . mean () . reset_index() df_temp = df_temp.groupby( 'Player' ) . tail ( 1 ) # take the last row of the above df_temp = df_temp.merge(df_temp_last, on = 'Player' , how = 'left' ) if index ==0: df_result = df_temp df_result[ 'tournament_date_index' ] = tournament_date # so we know which tournament this feature is for else : df_temp[ 'tournament_date_index' ] = tournament_date df_result = df_result.append(df_temp) df_result.drop( 'level_1' , axis =1 ,inplace = True) return df_result # columns we are applying the rolling averages on rolling_cols = [ 'Player_Serve_Win_Ratio' , 'Player_Return_Win_Ratio' , 'Player_BreakPoints_Per_Return_Game' , 'Player_Game_Win_Percentage' ] # columns we are taking the most recent values on # For the player rank, we think we can just use the latest rank (before the tournament starts) # as it should refect the most recent performance of the player last_cols = [ 'Player_Rank' ] # Apply the rolling average function to the long dataframes (it will take a few mins to run) df_rolling_atp = get_rolling_features (df_long_atp, tournament_dates_atp, rolling_cols, last_cols = last_cols ) df_rolling_wta = get_rolling_features (df_long_wta, tournament_dates_wta, rolling_cols, last_cols = last_cols) df_rolling_atp.head( 2 ) Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Adrian Mannarino 0.623408 0.353397 0.257859 0.447246 87.0 2012-01-16 Albert Montanes 0.507246 0.195652 0.000000 0.294118 50.0 2012-01-16 df_rolling_wta.head( 2 ) Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Agnieszka Radwanska 0.413333 0.475410 0.350000 0.350000 5.0 2014-01-13 Ajla Tomljanovic 0.468457 0.407319 0.242253 0.462634 75.0 2014-01-13 Calculate the difference of averages for each match in the data frames \u00b6 In the original data frames, the first column is always the winner and followed by the loser. Same for the player stats. Thus, we cannot simply calculate the difference between winner and loser and create a target variable indicating player 1 will win or not because it will always be the winner in this case (target always = 1). As a result, we need to pick a player randomly so the player might or might not be the winner In addition, instead of using both the features for player 1 and 2, we will take the difference of averages between the randomised player 1 and 2. The main benefit is that it will reduce the number of features to half Steps: We will create a random number for each player which only return 0 or 1 If it is zero, we will assign the winner to player 1 and loser to player 2 We will join the features to the player 1 and 2. The join will be on the player names and the tournament date (tournament_index in the feature dataframes) For players who do not have any history, we will fill the stats by zeros and rank by 999 # Randomise the match_wide dataset so the first player is not always the winner # set a seed so the random number is reproducable np.random.seed( 2 ) # randomise a number 0/1 with 50% chance each # if 0 then take the winner, 1 then take loser df_atp[ 'random_number' ] = np.random.randint( 2 , size = len(df_atp)) df_atp[ 'randomised_player_1' ] = np.where(df_atp[ 'random_number' ] ==0 ,df_atp[ 'Winner' ],df_atp[ 'Loser' ]) df_atp[ 'randomised_player_2' ] = np.where(df_atp[ 'random_number' ] ==0 ,df_atp[ 'Loser' ],df_atp[ 'Winner' ]) df_wta[ 'random_number' ] = np.random.randint( 2 , size = len(df_wta)) df_wta[ 'randomised_player_1' ] = np.where(df_wta[ 'random_number' ] ==0 ,df_wta[ 'Winner' ],df_wta[ 'Loser' ]) df_wta[ 'randomised_player_2' ] = np.where(df_wta[ 'random_number' ] ==0 ,df_wta[ 'Loser' ],df_wta[ 'Winner' ]) # set the target (win/loss) based on the new randomise number df_atp[ 'player_1_win' ] = np.where(df_atp[ 'random_number' ] ==0 , 1 , 0 ) df_wta[ 'player_1_win' ] = np.where(df_wta[ 'random_number' ] ==0 , 1 , 0 ) print ( 'After shuffling, the win rate for player 1 for the mens is {}%' . format (df_atp[ 'player_1_win' ] . mean () *100 )) print ( 'After shuffling, the win rate for player 1 for the womens is {}%' . format (df_wta[ 'player_1_win' ] . mean () *100 )) After shuffling, the win rate for player 1 for the mens is 49.64798919857267% After shuffling, the win rate for player 1 for the womens is 49.697671426733564% The win rates are close enough to 50%. So we are good to go # To get our dataframes ready for model training, we will exclude other tournaments from the data now because we have gotten the rolling averages from them and # for training, we only need US and Australian Open matches df_atp = df_atp.loc[df_atp.Tournament.isin(tournaments)] df_wta = df_wta.loc[df_wta.Tournament.isin(tournaments)] # now we can remove other stats columns because we will be using the differences cols_to_keep = [ 'Winner' , 'Loser' , 'Tournament' , 'Tournament_Date' , 'player_1_win' , 'randomised_player_1' , 'randomised_player_2' ] df_atp = df_atp[cols_to_keep] df_wta = df_wta[cols_to_keep] # Here, we are joining the rolling average dataframes to the individual matches. # We need to do it twice. One for player 1 and one for player 2 # Get the rolling features for player 1 df_atp = df_atp.merge(df_rolling_atp, how = 'left' , left_on = [ 'randomised_player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) df_wta = df_wta.merge(df_rolling_wta, how = 'left' , left_on = [ 'randomised_player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) # Get the rolling features for player 2 # we will use '_p1' to denote player 1 and '_p2' for player 2 df_atp = df_atp.merge(df_rolling_atp, how = 'left' , left_on = [ 'randomised_player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_wta = df_wta.merge(df_rolling_wta, how = 'left' , left_on = [ 'randomised_player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament' . format (df_atp.loc[df_atp.Player_p1.isna(), 'randomised_player_1' ] . nunique())) print ( '{} player_2s do Not have previous match history before the tournament' . format (df_atp.loc[df_atp.Player_p2.isna(), 'randomised_player_2' ] . nunique())) 59 player_1s do Not have previous match history before the tournament 56 player_2s do Not have previous match history before the tournament # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament' . format (df_wta.loc[df_wta.Player_p1.isna(), 'randomised_player_1' ] . nunique())) print ( '{} player_2s do Not have previous match history before the tournament' . format (df_wta.loc[df_wta.Player_p2.isna(), 'randomised_player_2' ] . nunique())) 41 player_1s do Not have previous match history before the tournament 37 player_2s do Not have previous match history before the tournament # Most of the missing are for the early years which makes sense as we dont have enough history for them df_wta.loc[df_wta.Player_p1.isna(), 'Tournament_Date' ] . value_counts() 2014-01-13 29 2014-08-25 7 2015-08-31 5 2015-01-19 3 2017-08-28 3 2018-01-15 3 2018-08-27 3 Name: Tournament_Date, dtype: int64 df_atp.loc[df_atp.Player_p1.isna(), 'Tournament_Date' ] . value_counts() 2012-01-16 29 2012-08-27 9 2014-01-13 5 2013-08-26 5 2016-01-18 5 2013-01-14 4 2014-08-25 3 2018-01-15 3 2017-08-28 3 2018-08-27 2 2016-08-29 2 2015-01-19 1 Name: Tournament_Date, dtype: int64 Now we have gotten the rolling averages for both player 1 and 2. What we need to do next is to simply calculat their difference. To calculate the difference, we need to: Split the dataframes into two new dataframes: Player 1 and Player 2 Take the difference between the two dataframes def get_player_difference(df, diff_cols = None) : # Input: # df: dataframe to get the data from # diff_cols: columns we take the difference on. For example is diff_cols = win rate. This function will calculate the # difference of the win rates between player 1 and player 2 # Return: the df with the new features p1_cols = [i + '_p1' for i in diff_cols] # column names for player 1 stats p2_cols = [i + '_p2' for i in diff_cols] # column names for player 2 stats # For any missing values, we will fill them by zeros except the ranking where we will use 999 df[ 'Player_Rank_p1' ] = df[ 'Player_Rank_p1' ] . fillna( 999 ) df[p1_cols] = df[p1_cols] . fillna( 0 ) df[ 'Player_Rank_p2' ] = df[ 'Player_Rank_p2' ] . fillna( 999 ) df[p2_cols] = df[p2_cols] . fillna( 0 ) new_column_name = [i + '_diff' for i in diff_cols] # Take the difference df_p1 = df[p1_cols] df_p2 = df[p2_cols] df_p1.columns = new_column_name df_p2.columns = new_column_name df_diff = df_p1 - df_p2 df_diff.columns = new_column_name # drop the p1 and p2 columns because We have the differences now df.drop(p1_cols + p2_cols, axis =1 , inplace = True) # Concat the df_diff and raw_df df = pd.concat([df, df_diff], axis =1 ) return df,new_column_name diff_cols = [ 'Player_Serve_Win_Ratio' , 'Player_Return_Win_Ratio' , 'Player_BreakPoints_Per_Return_Game' , 'Player_Game_Win_Percentage' , 'Player_Rank' ] # Apply the function and get the difference between player 1 and 2 df_atp,_ = get_player_difference(df_atp,diff_cols = diff_cols) df_wta,_ = get_player_difference(df_wta,diff_cols = diff_cols) # Make a copy of the dataframes in case we need to come back to check the values df_atp_final = df_atp.copy() df_wta_final = df_wta.copy() df_atp_final.head() Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Juan Martin del Potro Adrian Mannarino Australian Open, Melbourne 2012-01-16 1 Juan Martin del Potro Adrian Mannarino Juan Martin del Potro 2012-01-16 Adrian Mannarino 2012-01-16 0.035030 -0.021271 -0.025975 0.103479 -76.0 Pere Riba Albert Montanes Australian Open, Melbourne 2012-01-16 1 Pere Riba Albert Montanes Pere Riba 2012-01-16 Albert Montanes 2012-01-16 -0.156369 0.008893 0.066667 -0.094118 39.0 Tomas Berdych Albert Ramos-Vinolas Australian Open, Melbourne 2012-01-16 0 Albert Ramos-Vinolas Tomas Berdych Albert Ramos-Vinolas 2012-01-16 NaN NaT 0.498027 0.380092 0.414815 0.394444 -934.0 Rafael Nadal Alex Kuznetsov Australian Open, Melbourne 2012-01-16 0 Alex Kuznetsov Rafael Nadal NaN NaT Rafael Nadal 2012-01-16 -0.670139 -0.423057 -0.445623 -0.574767 997.0 Roger Federer Alexander Kudryavtsev Australian Open, Melbourne 2012-01-16 0 Alexander Kudryavtsev Roger Federer NaN NaT Roger Federer 2012-01-16 -0.721415 -0.449516 -0.360255 -0.668090 996.0 Modelling \u00b6 We will trian two models here, one for mens and one for womens. For training, we will use all available data from the second year (too many missing values in the first year) up until 2017. For validation, we will test the model on the 2018 Australian Open data This setup allows us to 'mimic' the final prediction (using historical matches to predict 2019 results) df_train_atp = df_atp_final.loc[(df_atp_final.Tournament_Date != '2018-01-15' ) # excluding Aus Open 2018, and & (df_atp_final.Tournament_Date > '2012-01-16' )] # excluding first year df_valid_atp = df_atp_final.loc[df_atp_final.Tournament_Date == '2018-01-15' ] # Australian Open 2018 only df_train_wta = df_wta_final.loc[(df_wta_final.Tournament_Date != '2018-01-15' ) # excluding Aus Open 2018, and & (df_wta_final.Tournament_Date > '2014-01-13' )] # excluding first year df_train_atp.head() Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Daniel Brands Adrian Ungur U.S. Open, New York 2012-08-27 0 Adrian Ungur Daniel Brands NaN NaT Daniel Brands 2012-08-27 -0.535211 -0.300000 -0.043478 -0.434783 870.0 Richard Gasquet Albert Montanes U.S. Open, New York 2012-08-27 1 Richard Gasquet Albert Montanes Richard Gasquet 2012-08-27 Albert Montanes 2012-08-27 0.080003 0.077451 0.180847 0.131108 -37.0 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 1 Martin Klizan Alejandro Falla Martin Klizan 2012-08-27 Alejandro Falla 2012-08-27 0.077117 -0.044716 -0.087362 0.068180 -2.0 Andy Murray Alex Bogomolov Jr. U.S. Open, New York 2012-08-27 1 Andy Murray Alex Bogomolov Jr. Andy Murray 2012-08-27 Alex Bogomolov Jr. 2012-08-27 0.039641 0.031701 0.094722 0.059010 -69.0 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 1 Tommy Robredo Andreas Seppi Tommy Robredo 2012-08-27 Andreas Seppi 2012-08-27 -0.026814 0.006442 -0.009930 -0.067780 151.0 # target variable target = 'player_1_win' # features being fed into the models feats = [ 'Player_Serve_Win_Ratio_diff' , 'Player_Return_Win_Ratio_diff' , 'Player_BreakPoints_Per_Return_Game_diff' , 'Player_Game_Win_Percentage_diff' , 'Player_Rank_diff' ] print (feats) H2O model for ATP \u00b6 h2o.init() # Convert to an h2o frame df_train_atp_h2o = h2o.H2OFrame(df_train_atp) df_valid_atp_h2o = h2o.H2OFrame(df_valid_atp) # For binary classification, response should be a factor df_train_atp_h2o[target] = df_train_atp_h2o[target] . asfactor() df_valid_atp_h2o[target] = df_valid_atp_h2o[target] . asfactor() # Run AutoML for 20 base models (limited to 1 hour max runtime by default) aml_atp = h2o.automl.H2OAutoML(max_runtime_secs =300 , max_models =100 , stopping_metric = 'logloss' , sort_metric = 'logloss' , balance_classes = True, seed =183 ) aml_atp.train(x = feats, y = target, training_frame = df_train_atp_h2o,validation_frame = df_valid_atp_h2o) # View the AutoML Leaderboard lb = aml_atp.leaderboard lb.head() model_id auc logloss mean_per_class_error rmse mse GBM_5_AutoML_20181221_094949 0.790281 0.554852 0.281363 0.431379 0.186088 GBM_grid_1_AutoML_20181221_094949_model_15 0.789329 0.556804 0.29856 0.431931 0.186564 GBM_grid_1_AutoML_20181221_094949_model_7 0.788013 0.557808 0.295899 0.432968 0.187461 StackedEnsemble_BestOfFamily_AutoML_20181221_094949 0.788131 0.558028 0.285321 0.432849 0.187358 GBM_grid_1_AutoML_20181221_094949_model_20 0.785633 0.561094 0.283932 0.43479 0.189043 StackedEnsemble_AllModels_AutoML_20181221_094949 0.784411 0.561587 0.293244 0.434667 0.188935 GBM_grid_1_AutoML_20181221_094949_model_25 0.785311 0.561783 0.291912 0.434888 0.189127 GBM_grid_1_AutoML_20181221_094949_model_17 0.774832 0.570883 0.295836 0.439375 0.193051 DeepLearning_1_AutoML_20181221_094949 0.779388 0.572823 0.311737 0.438479 0.192264 GBM_grid_1_AutoML_20181221_094949_model_14 0.7718 0.578867 0.285835 0.441373 0.19481 H2O model for WTA # Convert to an h2o frame df_train_wta_h2o = h2o.H2OFrame(df_train_wta) df_valid_wta_h2o = h2o.H2OFrame(df_valid_wta) # For binary classification, response should be a factor df_train_wta_h2o[target] = df_train_wta_h2o[target] . asfactor() df_valid_wta_h2o[target] = df_valid_wta_h2o[target] . asfactor() # Run AutoML for 20 base models (limited to 1 hour max runtime by default) aml_wta = h2o.automl.H2OAutoML(max_runtime_secs =300 , max_models =100 , stopping_metric = 'logloss' , sort_metric = 'logloss' , balance_classes = True, seed =183 ) aml_wta.train(x = feats, y = target, training_frame = df_train_wta_h2o,validation_frame = df_valid_wta_h2o) # View the AutoML Leaderboard lb = aml_wta.leaderboard lb.head() model_id auc logloss mean_per_class_error rmse mse StackedEnsemble_AllModels_AutoML_20181221_095400 0.726046 0.60827 0.321222 0.457117 0.208956 StackedEnsemble_BestOfFamily_AutoML_20181221_095400 0.724911 0.609329 0.337847 0.457659 0.209452 DeepLearning_grid_1_AutoML_20181221_095400_model_3 0.729152 0.612669 0.315971 0.45641 0.20831 GBM_grid_1_AutoML_20181221_095400_model_7 0.721204 0.615763 0.336848 0.460885 0.212415 GBM_5_AutoML_20181221_095400 0.719252 0.616535 0.319179 0.461055 0.212572 GBM_grid_1_AutoML_20181221_095400_model_15 0.715921 0.619263 0.318673 0.462215 0.213643 GLM_grid_1_AutoML_20181221_095400_model_1 0.726048 0.622989 0.366124 0.463099 0.214461 GBM_grid_1_AutoML_20181221_095400_model_17 0.709261 0.624902 0.34876 0.465628 0.216809 GBM_grid_1_AutoML_20181221_095400_model_18 0.70946 0.625704 0.393556 0.466147 0.217293 DeepLearning_grid_1_AutoML_20181221_095400_model_2 0.713419 0.628008 0.311334 0.463638 0.21496 Use the models to predict and make submissions \u00b6 Now let's use the models we just created to make the submissions df_predict_atp = pd.read_csv( \"data/men_dummy_submission_file.csv\" ) df_predict_wta = pd.read_csv( \"data/women_dummy_submission_file.csv\" , encoding = 'latin1' ) # for womens, there are some names need a different encoding df_predict_wta.head( 2 ) player_1 player_2 player_1_win_probability Simona Halep Angelique Kerber 0.5 Simona Halep Caroline Wozniacki 0.5 Get the features for the predict df \u00b6 We need to join the features to the 2019 players # Before we join the features by the names and the dates, we need to convert any non-english characters to english first translationTable = str.maketrans( \"\u00e9\u00e0\u00e8\u00f9\u00e2\u00ea\u00ee\u00f4\u00fb\u00e7\u00f1\u00e1\" , \"eaeuaeioucna\" ) df_predict_atp[ 'player_1' ] = df_predict_atp.player_1.apply(lambda x : x.translate(translationTable)) df_predict_atp[ 'player_2' ] = df_predict_atp.player_2.apply(lambda x : x.translate(translationTable)) df_predict_wta[ 'player_1' ] = df_predict_wta.player_1.apply(lambda x : x.translate(translationTable)) df_predict_wta[ 'player_2' ] = df_predict_wta.player_2.apply(lambda x : x.translate(translationTable)) # Also we need to convert the names into lower cases df_predict_atp[ 'player_1' ] = df_predict_atp[ 'player_1' ] . str.lower() df_predict_atp[ 'player_2' ] = df_predict_atp[ 'player_2' ] . str.lower() df_predict_wta[ 'player_1' ] = df_predict_wta[ 'player_1' ] . str.lower() df_predict_wta[ 'player_2' ] = df_predict_wta[ 'player_2' ] . str.lower() df_rolling_atp[ 'Player' ] = df_rolling_atp[ 'Player' ] . str.lower() df_rolling_wta[ 'Player' ] = df_rolling_wta[ 'Player' ] . str.lower() # Lastly, some players have slightly difference names in the submission data and the match data. So we are editing them here manually df_predict_atp.loc[df_predict_atp.player_1 == 'jaume munar' , 'player_1' ] = 'jaume antoni munar clar' df_predict_atp.loc[df_predict_atp.player_2 == 'jaume munar' , 'player_2' ] = 'jaume antoni munar clar' df_predict_wta.loc[df_predict_wta.player_1 == 'daria kasatkina' , 'player_1' ] = 'darya kasatkina' df_predict_wta.loc[df_predict_wta.player_2 == 'daria kasatkina' , 'player_2' ] = 'darya kasatkina' df_predict_wta.loc[df_predict_wta.player_1 == 'lesia tsurenko' , 'player_1' ] = 'lesya tsurenko' df_predict_wta.loc[df_predict_wta.player_2 == 'lesia tsurenko' , 'player_2' ] = 'lesya tsurenko' df_predict_wta.loc[df_predict_wta.player_1 == 'danielle collins' , 'player_1' ] = 'danielle rose collins' df_predict_wta.loc[df_predict_wta.player_2 == 'danielle collins' , 'player_2' ] = 'danielle rose collins' df_predict_wta.loc[df_predict_wta.player_1 == 'anna karolina schmiedlova' , 'player_1' ] = 'anna schmiedlova' df_predict_wta.loc[df_predict_wta.player_2 == 'anna karolina schmiedlova' , 'player_2' ] = 'anna schmiedlova' df_predict_wta.loc[df_predict_wta.player_1 == 'georgina garcia perez' , 'player_1' ] = 'georgina garcia-perez' df_predict_wta.loc[df_predict_wta.player_2 == 'georgina garcia perez' , 'player_2' ] = 'georgina garcia-perez' # create and tournament date column and set it to 2019 so we can join the lastest features df_predict_atp[ 'Tournament_Date' ] = pd.to_datetime( '2019-01-15' ) df_predict_wta[ 'Tournament_Date' ] = pd.to_datetime( '2019-01-15' ) # Get the rolling features for player 1 df_predict_atp = df_predict_atp.merge(df_rolling_atp, how = 'left' , left_on = [ 'player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ],validate = 'm:1' ) df_predict_wta = df_predict_wta.merge(df_rolling_wta, how = 'left' , left_on = [ 'player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ],validate = 'm:1' ) # Get the rolling features for player 2 # For duplicate columns, we will use '_p1' to denote player 1 and '_p2' for player 2 df_predict_atp = df_predict_atp.merge(df_rolling_atp, how = 'left' , left_on = [ 'player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ],validate = 'm:1' ,suffixes = ( '_p1' , '_p2' )) df_predict_wta = df_predict_wta.merge(df_rolling_wta, how = 'left' , left_on = [ 'player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ],validate = 'm:1' ,suffixes = ( '_p1' , '_p2' )) df_predict_atp.head( 2 ) player_1 player_2 player_1_win_probability Tournament_Date Player_p1 Player_Serve_Win_Ratio_p1 Player_Return_Win_Ratio_p1 Player_BreakPoints_Per_Return_Game_p1 Player_Game_Win_Percentage_p1 Player_Rank_p1 tournament_date_index_p1 Player_p2 Player_Serve_Win_Ratio_p2 Player_Return_Win_Ratio_p2 Player_BreakPoints_Per_Return_Game_p2 Player_Game_Win_Percentage_p2 Player_Rank_p2 tournament_date_index_p2 novak djokovic rafael nadal 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 rafael nadal 0.622425 0.401028 0.334270 0.570833 1.0 2019-01-15 novak djokovic roger federer 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 roger federer 0.620070 0.389781 0.269224 0.564244 3.0 2019-01-15 # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament in the mens' . format (df_predict_atp.loc[df_predict_atp.Player_p1.isna(), 'player_1' ] . nunique())) print ( '{} player_2s do Not have previous match history before the tournament in the mens' . format (df_predict_atp.loc[df_predict_atp.Player_p2.isna(), 'player_2' ] . nunique())) 3 player_1s do Not have previous match history before the tournament in the mens 3 player_2s do Not have previous match history before the tournament in the mens # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament in the womens' . format (df_predict_wta.loc[df_predict_wta.Player_p1.isna(), 'player_1' ] . nunique())) print ( '{} player_2s do Not have previous match history before the tournament in the womens' . format (df_predict_wta.loc[df_predict_wta.Player_p2.isna(), 'player_2' ] . nunique())) 0 player_1s do Not have previous match history before the tournament in the womens 0 player_2s do Not have previous match history before the tournament in the womens print (df_predict_atp.loc[df_predict_atp.Player_p1.isna(), 'player_1' ] . unique () . tolist()) [ 'christian garin' , 'pedro sousa' , 'hugo dellien' ] print (df_predict_wta.loc[df_predict_wta.Player_p1.isna(), 'player_1' ] . unique () . tolist()) [] We will do the differencing again for the prediction dataframes exactly like what we did for training # Apply the function and get the difference between player 1 and 2 df_predict_atp,_ = get_player_difference(df_predict_atp,diff_cols = diff_cols) df_predict_wta,_ = get_player_difference(df_predict_wta,diff_cols = diff_cols) Make the prediction \u00b6 df_predict_atp_h2o = h2o.H2OFrame(df_predict_atp[feats]) df_predict_wta_h2o = h2o.H2OFrame(df_predict_wta[feats]) atp_preds = aml_atp.predict(df_predict_atp_h2o)[ 'p1' ] . as_data_frame() wta_preds = aml_wta.predict(df_predict_wta_h2o)[ 'p1' ] . as_data_frame() df_predict_atp[ 'player_1_win_probability' ] = atp_preds df_predict_wta[ 'player_1_win_probability' ] = wta_preds atp_submission = df_predict_atp[[ 'player_1' , 'player_2' , 'player_1_win_probability' ]] wta_submission = df_predict_wta[[ 'player_1' , 'player_2' , 'player_1_win_probability' ]] atp_submission.head() player_1 player_2 player_1_win_probability novak djokovic rafael nadal 0.571588 novak djokovic roger federer 0.662511 novak djokovic juan martin del potro 0.544306 novak djokovic alexander zverev 0.709483 novak djokovic kevin anderson 0.687195 wta_submission.head() player_1 player_2 player_1_win_probability simona halep angelique kerber 0.455224 simona halep caroline wozniacki 0.546276 simona halep elina svitolina 0.408014 simona halep naomi osaka 0.285125 simona halep sloane stephens 0.576643 Let's look at who has the highest win rate from our models atp_submission.groupby( 'player_1' )[ 'player_1_win_probability' ] . mean () \\ . reset_index() . sort_values( 'player_1_win_probability' ,ascending = False) . head () player_1 player_1_win_probability novak djokovic 0.846377 juan martin del potro 0.787337 karen khachanov 0.782963 rafael nadal 0.778707 roger federer 0.767337 wta_submission.groupby( 'player_1' )[ 'player_1_win_probability' ] . mean () \\ . reset_index() . sort_values( 'player_1_win_probability' ,ascending = False) . head () player_1 player_1_win_probability madison keys 0.750580 naomi osaka 0.749195 caroline wozniacki 0.722409 kiki bertens 0.713904 aryna sabalenka 0.707368 Now we can output the predictions as csvs atp_submission.to_csv( 'submission/atp_submission_python.csv' ,index = False) wta_submission.to_csv( 'submission/wta_submission_pthon.csv' ,index = False) atp_submission.shape (16256, 3)","title":"Aus Open Python Tutorial"},{"location":"modelling/AusOpenPythonTutorial/#australian-open-datathon-r-tutorial","text":"","title":"Australian Open Datathon R Tutorial"},{"location":"modelling/AusOpenPythonTutorial/#overview","text":"","title":"Overview"},{"location":"modelling/AusOpenPythonTutorial/#the-task","text":"This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide. The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here . For a detailed outline of the task, the prizes, and to sign up, click here . How an outline of our methodoly and thought process, read this article.","title":"The Task"},{"location":"modelling/AusOpenPythonTutorial/#prizes","text":"Place Prize Place Prize 1 $5000 9 $500 2 $3000 10 $500 3 $2000 11 $200 4 $1000 12 $200 5 $750 13 $200 6 $500 14 $200 7 $500 15 $200 8 $500 Total $15250","title":"Prizes"},{"location":"modelling/AusOpenPythonTutorial/#submission","text":"To submit your model, email your final submission to datathon@betfair.com.au . Note that you don't need to email your code, just your predictions in the format that we have specified No submissions will be accepted prior to the Australian Open qualifying matches being completed and the final draw list being shared with registered participants (12 January 2019) Submissions need to include all potential match ups during the Australian Open, i.e. all possible combinations for each men's and women's tournaments (this will be provided after the draw is announced and the Australian Open qualifying matches are completed (Jan 12 th 2019)) Submissions must follow the format outlined above and shown in the 'Dummy Submission File'. Any submissions that are not in the correct format will not be accepted. Submissions need to include the player names for the hypothetical match up and the probability of the first player winning i.e. player_1,player_2,probability_of_player_1_winning, Submissions must be in a csv format Only two models will be accepted per participant (one model for the men's draw, one model for the women's draw)","title":"Submission"},{"location":"modelling/AusOpenPythonTutorial/#intention","text":"This notebook will demostrate how to: Process the raw data sets Produce simple features Run a predictive model on H2O Outputs the final predictions for the submissions Load the data and required packages import numpy as np import pandas as pd import os import gc import sys import warnings warnings.filterwarnings( 'ignore' ) import h2o from h2o.automl import H2OAutoML pd.options.display.max_columns = 999 # We are loading both the mens and womens match csvs df_atp = pd.read_csv( \"data/ATP_matches.csv\" ) df_wta = pd.read_csv( \"data/WTA_matches.csv\" )","title":"Intention"},{"location":"modelling/AusOpenPythonTutorial/#data-preprocessing","text":"Filter the matches to hard and indoor hard only due to the fact that Australian Open is on hard surface and we want the models to train specifically for hard surfaces matches Convert the columns in both datasets to the correct types. For example, we want to make sure the date columns are in the datetime format and numerial columns are either interger or floats. This will help reduce the memory in use and make the feature engineering process easier ### Include hard and indoor hard only df_atp = df_atp.loc[df_atp.Court_Surface.isin([ 'Hard' , 'Indoor Hard' ])] df_wta = df_wta.loc[df_wta.Court_Surface.isin([ 'Hard' , 'Indoor Hard' ])] ### Exclude qualifying rounds df_atp = df_atp.loc[df_atp.Round_Description != 'Qualifying' ] df_wta = df_wta.loc[df_wta.Round_Description != 'Qualifying' ] # Store the shape of the data for reference check later atp_shape = df_atp.shape wta_shape = df_wta.shape numeric_columns = [ 'Winner_Rank' , 'Loser_Rank' , 'Retirement_Ind' , 'Winner_Sets_Won' , 'Winner_Games_Won' , 'Winner_Aces' , 'Winner_DoubleFaults' , 'Winner_FirstServes_Won' , 'Winner_FirstServes_In' , 'Winner_SecondServes_Won' , 'Winner_SecondServes_In' , 'Winner_BreakPoints_Won' , 'Winner_BreakPoints' , 'Winner_ReturnPoints_Won' , 'Winner_ReturnPoints_Faced' , 'Winner_TotalPoints_Won' , 'Loser_Sets_Won' , 'Loser_Games_Won' , 'Loser_Aces' , 'Loser_DoubleFaults' , 'Loser_FirstServes_Won' , 'Loser_FirstServes_In' , 'Loser_SecondServes_Won' , 'Loser_SecondServes_In' , 'Loser_BreakPoints_Won' , 'Loser_BreakPoints' , 'Loser_ReturnPoints_Won' , 'Loser_ReturnPoints_Faced' , 'Loser_TotalPoints_Won' ] text_columns = [ 'Winner' , 'Loser' , 'Tournament' , 'Court_Surface' , 'Round_Description' ] date_columns = [ 'Tournament_Date' ] # we set the **erros** to coerce so any non-numerical values (text,special characters) will return an NA df_atp[numeric_columns] = df_atp[numeric_columns] . apply (pd.to_numeric,errors = 'coerce' ) df_wta[numeric_columns] = df_wta[numeric_columns] . apply (pd.to_numeric,errors = 'coerce' ) df_atp[date_columns] = df_atp[date_columns] . apply (pd.to_datetime) df_wta[date_columns] = df_wta[date_columns] . apply (pd.to_datetime)","title":"Data preprocessing"},{"location":"modelling/AusOpenPythonTutorial/#feature-engineering","text":"The raw datasets are constructed in a way that each row will have the seperate stats for both the winner and loser of that match. However, we want to reshape the data so that each row we will only have one player randomly selected from the winner/loser columns and the features are the difference between opponents statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns. In addition, for the features, we will take the rolling average of the player's most recent 15 matches before the particular tournament starts. For example, if the match is the second round of the Australian Open 2018, the features will be the last 15 matches before the first round of Australian Open 2018. The reason of not including the stats in the first round is that we would not have known the player's stats in the first round for the final submissions A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, difference in ELO rating etc. The target variable will be whether or not Player A wins (1=Player A wins and 0=lose). The steps we take are: Convert the raw dataframes into long format: Create some new features Take the rolling average for each player and each match Since we will be only training our models on US Open and Australian Open, we will only be creating features for those matches. However, the rolling average will take into account any hard surface matches before those tournaments Calculate the difference of averages for each match in the data frames","title":"Feature Engineering"},{"location":"modelling/AusOpenPythonTutorial/#convert-the-raw-dataframes-into-long-format","text":"# Before we split the dataframe into winner and loser, we want to create a feature that captures the total number of games the match takes. # We have to do it before the split or we will lose this information df_atp[ 'Total_Games' ] = df_atp.Winner_Games_Won + df_atp.Loser_Games_Won df_wta[ 'Total_Games' ] = df_wta.Winner_Games_Won + df_wta.Loser_Games_Won # Get the column names for the winner and loser stats winner_cols = [col for col in df_atp.columns if col.startswith( 'Winner' )] loser_cols = [col for col in df_atp.columns if col.startswith( 'Loser' )] # create a winner dataframe to store the winner stats and a loser dataframe for the losers # In addition to the winner and loser columns, we are adding common columns as well (e.g. tournamnt dates) common_cols = [ 'Total_Games' , 'Tournament' , 'Tournament_Date' , 'Court_Surface' , 'Round_Description' ] df_winner_atp = df_atp[winner_cols + common_cols] df_loser_atp = df_atp[loser_cols + common_cols] df_winner_wta = df_wta[winner_cols + common_cols] df_loser_wta = df_wta[loser_cols + common_cols] # Create a new column to show whether the player has won or not. df_winner_atp[ \"won\" ] = 1 df_loser_atp[ \"won\" ] = 0 df_winner_wta[ \"won\" ] = 1 df_loser_wta[ \"won\" ] = 0 # Rename the columns for the winner and loser data frames so we can append them later on. # We will rename the Winner_ / Loser_ columns to Player_ new_column_names = [col.replace( 'Winner' , 'Player' ) for col in winner_cols] df_winner_atp.columns = new_column_names + common_cols + [ 'won' ] # They all should be the same df_loser_atp.columns = df_winner_atp.columns df_winner_wta.columns = df_winner_atp.columns df_loser_wta.columns = df_winner_atp.columns # append the winner and loser dataframes df_long_atp = df_winner_atp.append(df_loser_atp) df_long_wta = df_winner_wta.append(df_loser_wta) So now our dataframes are in long format and should looks like this df_long_atp.head() Player Player_Rank Player_Sets_Won Player_Games_Won Player_Aces Player_DoubleFaults Player_FirstServes_Won Player_FirstServes_In Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Total_Games Tournament Tournament_Date Court_Surface Round_Description won Edouard Roger-Vasselin 106.0 2.0 12 5.0 2.0 22 30 12 19 4.0 7.0 25.0 59.0 59 19 Chennai 2012-01-02 Hard First Round 1 Dudi Sela 83.0 2.0 12 2.0 0.0 14 17 11 16 6.0 14.0 36.0 58.0 61 13 Chennai 2012-01-02 Hard First Round 1 Go Soeda 120.0 2.0 19 6.0 1.0 48 64 19 39 5.0 11.0 42.0 105.0 109 33 Chennai 2012-01-02 Hard First Round 1 Yuki Bhambri 345.0 2.0 12 1.0 2.0 22 29 9 17 5.0 13.0 34.0 62.0 65 17 Chennai 2012-01-02 Hard First Round 1 Yuichi Sugita 235.0 2.0 12 3.0 1.0 37 51 11 27 3.0 7.0 22.0 54.0 70 19 Chennai 2012-01-02 Hard First Round 1","title":"Convert the raw dataframes into long format:"},{"location":"modelling/AusOpenPythonTutorial/#create-some-new-features","text":"Thinking about the dynamics of tennis, we know that players often will matches by \u201cbreaking\u201d the opponent\u2019s serve (i.e. winning a game when the opponent is serving). This is especially important in tennis. Let\u2019s create a feature called Player_BreakPoints_Per_Game, which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let\u2019s also create a feature called Player_Return_Win_Ratio which is the proportion of points won when returning. Similarly, \u201cholding\u201d serve is important (i.e. winning a game when you are serving). Let\u2019s create a feature called Player_Serve_Win_Ratio which is the proportion of points won when serving. Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let\u2019s create a feature called Player_Game_Win_Percentage which is the propotion of games that a player wins. So the four new features we will create are: Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage # Here, we will define a function so we can apply it to both atp and wta dataframes def get_new_features(df) : # Input: # df: dataframe to get the data from # Return: the df with the new features # Point Win ratio when serving df[ 'Player_Serve_Win_Ratio' ] = (df.Player_FirstServes_Won + df.Player_SecondServes_Won - df.Player_DoubleFaults) \\ / (df.Player_FirstServes_In + df.Player_SecondServes_In + df.Player_DoubleFaults) # Point win ratio when returning df[ 'Player_Return_Win_Ratio' ] = df.Player_ReturnPoints_Won / df.Player_ReturnPoints_Faced # Breakpoints per receiving game df[ 'Player_BreakPoints_Per_Return_Game' ] = df.Player_BreakPoints / df.Total_Games df[ 'Player_Game_Win_Percentage' ] = df.Player_Games_Won / df.Total_Games return df # Apply the function we just created to the long dataframes df_long_atp = get_new_features(df_long_atp) df_long_wta = get_new_features(df_long_wta) # The long table should have exactly twice of the rows of the original data assert df_long_atp.shape[ 0 ] == atp_shape[ 0 ] *2 assert df_long_wta.shape[ 0 ] == wta_shape[ 0 ] *2","title":"Create some new features"},{"location":"modelling/AusOpenPythonTutorial/#take-the-rolling-average-for-each-player-and-each-match","text":"To train our models, we cannot simply use the player stats for that current match. In fact, we wont be able to use any stats from the same tournament. The logic behind this is that when we try to predict the results in 2019, we would not know the stats of any of the matches in the Australian Open 2019 tournament. As a result, we will use the players' past performance. Here, we will do a rolling average of the most recent 15 matches before the tournament. To do the above, we will follow the steps below: List all the tournament dates for US and Australian Opens Loop through the dates from point 1, for each date, we filter the data to only include matches before that date and take the most recent 15 games Take the average of those 15 games # the two tournaments we will be using for training and thus the feature generation tournaments = [ 'U.S. Open, New York' , 'Australian Open, Melbourne' ] # Store the dates for the loops tournament_dates_atp = df_atp.loc[df_atp.Tournament.isin(tournaments)] . groupby([ 'Tournament' , 'Tournament_Date' ]) \\ . size() . reset_index()[[ 'Tournament' , 'Tournament_Date' ]] tournament_dates_wta = df_wta.loc[df_wta.Tournament.isin(tournaments)] . groupby([ 'Tournament' , 'Tournament_Date' ]) \\ . size() . reset_index()[[ 'Tournament' , 'Tournament_Date' ]] # We are adding one more date for the final prediction tournament_dates_atp.loc[ -1 ] = [ 'Australian Open, Melbourne' ,pd.to_datetime( '2019-01-15' )] tournament_dates_wta.loc[ -1 ] = [ 'Australian Open, Melbourne' ,pd.to_datetime( '2019-01-15' )] Following are the dates for each tournament tournament_dates_atp Tournament Tournament_Date Australian Open, Melbourne 2012-01-16 Australian Open, Melbourne 2013-01-1 Australian Open, Melbourne 2014-01-1 Australian Open, Melbourne 2015-01-1 Australian Open, Melbourne 2016-01-1 Australian Open, Melbourne 2017-01-1 Australian Open, Melbourne 2018-01-1 U.S. Open, New York 2012-08-2 U.S. Open, New York 2013-08-2 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 tournament_dates_wta Tournament Tournament_Date Australian Open, Melbourne 2014-01-13 Australian Open, Melbourne 2015-01-19 Australian Open, Melbourne 2016-01-18 Australian Open, Melbourne 2017-01-16 Australian Open, Melbourne 2018-01-15 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 They look fine but it is interesting that for men's, we have two more years of data from 2012 to 2013 # Let's define a function to calculate the rolling averages def get_rolling_features (df, date_df = None,rolling_cols = None, last_cols = None) : # Input: # df: dataframe to get the data from # date_df: dataframe that has the start dates for each tournament # rolling_cols: columns to get the rolling averages # last_cols: columns to get the last value (most recent) # Return: the df with the new features # Sort the data by player and dates so the most recent matches are at the bottom df = df.sort_values([ 'Player' , 'Tournament_Date' , 'Tournament' ], ascending = True) # For each tournament, get the rolling averages of that player's past matches before the tournament start date for index, tournament_date in enumerate(date_df.Tournament_Date) : # create a temp df to store the interim results df_temp = df.loc[df.Tournament_Date < tournament_date] # for ranks, we only take the last one. (comment this out if want to take avg of rank) df_temp_last = df_temp.groupby( 'Player' )[last_cols] . last() . reset_index() # take the most recent 15 matches for the rolling average df_temp = df_temp.groupby( 'Player' )[rolling_cols] . rolling( 15 , min_periods =1 ) . mean () . reset_index() df_temp = df_temp.groupby( 'Player' ) . tail ( 1 ) # take the last row of the above df_temp = df_temp.merge(df_temp_last, on = 'Player' , how = 'left' ) if index ==0: df_result = df_temp df_result[ 'tournament_date_index' ] = tournament_date # so we know which tournament this feature is for else : df_temp[ 'tournament_date_index' ] = tournament_date df_result = df_result.append(df_temp) df_result.drop( 'level_1' , axis =1 ,inplace = True) return df_result # columns we are applying the rolling averages on rolling_cols = [ 'Player_Serve_Win_Ratio' , 'Player_Return_Win_Ratio' , 'Player_BreakPoints_Per_Return_Game' , 'Player_Game_Win_Percentage' ] # columns we are taking the most recent values on # For the player rank, we think we can just use the latest rank (before the tournament starts) # as it should refect the most recent performance of the player last_cols = [ 'Player_Rank' ] # Apply the rolling average function to the long dataframes (it will take a few mins to run) df_rolling_atp = get_rolling_features (df_long_atp, tournament_dates_atp, rolling_cols, last_cols = last_cols ) df_rolling_wta = get_rolling_features (df_long_wta, tournament_dates_wta, rolling_cols, last_cols = last_cols) df_rolling_atp.head( 2 ) Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Adrian Mannarino 0.623408 0.353397 0.257859 0.447246 87.0 2012-01-16 Albert Montanes 0.507246 0.195652 0.000000 0.294118 50.0 2012-01-16 df_rolling_wta.head( 2 ) Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Agnieszka Radwanska 0.413333 0.475410 0.350000 0.350000 5.0 2014-01-13 Ajla Tomljanovic 0.468457 0.407319 0.242253 0.462634 75.0 2014-01-13","title":"Take the rolling average for each player and each match"},{"location":"modelling/AusOpenPythonTutorial/#calculate-the-difference-of-averages-for-each-match-in-the-data-frames","text":"In the original data frames, the first column is always the winner and followed by the loser. Same for the player stats. Thus, we cannot simply calculate the difference between winner and loser and create a target variable indicating player 1 will win or not because it will always be the winner in this case (target always = 1). As a result, we need to pick a player randomly so the player might or might not be the winner In addition, instead of using both the features for player 1 and 2, we will take the difference of averages between the randomised player 1 and 2. The main benefit is that it will reduce the number of features to half Steps: We will create a random number for each player which only return 0 or 1 If it is zero, we will assign the winner to player 1 and loser to player 2 We will join the features to the player 1 and 2. The join will be on the player names and the tournament date (tournament_index in the feature dataframes) For players who do not have any history, we will fill the stats by zeros and rank by 999 # Randomise the match_wide dataset so the first player is not always the winner # set a seed so the random number is reproducable np.random.seed( 2 ) # randomise a number 0/1 with 50% chance each # if 0 then take the winner, 1 then take loser df_atp[ 'random_number' ] = np.random.randint( 2 , size = len(df_atp)) df_atp[ 'randomised_player_1' ] = np.where(df_atp[ 'random_number' ] ==0 ,df_atp[ 'Winner' ],df_atp[ 'Loser' ]) df_atp[ 'randomised_player_2' ] = np.where(df_atp[ 'random_number' ] ==0 ,df_atp[ 'Loser' ],df_atp[ 'Winner' ]) df_wta[ 'random_number' ] = np.random.randint( 2 , size = len(df_wta)) df_wta[ 'randomised_player_1' ] = np.where(df_wta[ 'random_number' ] ==0 ,df_wta[ 'Winner' ],df_wta[ 'Loser' ]) df_wta[ 'randomised_player_2' ] = np.where(df_wta[ 'random_number' ] ==0 ,df_wta[ 'Loser' ],df_wta[ 'Winner' ]) # set the target (win/loss) based on the new randomise number df_atp[ 'player_1_win' ] = np.where(df_atp[ 'random_number' ] ==0 , 1 , 0 ) df_wta[ 'player_1_win' ] = np.where(df_wta[ 'random_number' ] ==0 , 1 , 0 ) print ( 'After shuffling, the win rate for player 1 for the mens is {}%' . format (df_atp[ 'player_1_win' ] . mean () *100 )) print ( 'After shuffling, the win rate for player 1 for the womens is {}%' . format (df_wta[ 'player_1_win' ] . mean () *100 )) After shuffling, the win rate for player 1 for the mens is 49.64798919857267% After shuffling, the win rate for player 1 for the womens is 49.697671426733564% The win rates are close enough to 50%. So we are good to go # To get our dataframes ready for model training, we will exclude other tournaments from the data now because we have gotten the rolling averages from them and # for training, we only need US and Australian Open matches df_atp = df_atp.loc[df_atp.Tournament.isin(tournaments)] df_wta = df_wta.loc[df_wta.Tournament.isin(tournaments)] # now we can remove other stats columns because we will be using the differences cols_to_keep = [ 'Winner' , 'Loser' , 'Tournament' , 'Tournament_Date' , 'player_1_win' , 'randomised_player_1' , 'randomised_player_2' ] df_atp = df_atp[cols_to_keep] df_wta = df_wta[cols_to_keep] # Here, we are joining the rolling average dataframes to the individual matches. # We need to do it twice. One for player 1 and one for player 2 # Get the rolling features for player 1 df_atp = df_atp.merge(df_rolling_atp, how = 'left' , left_on = [ 'randomised_player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) df_wta = df_wta.merge(df_rolling_wta, how = 'left' , left_on = [ 'randomised_player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) # Get the rolling features for player 2 # we will use '_p1' to denote player 1 and '_p2' for player 2 df_atp = df_atp.merge(df_rolling_atp, how = 'left' , left_on = [ 'randomised_player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_wta = df_wta.merge(df_rolling_wta, how = 'left' , left_on = [ 'randomised_player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament' . format (df_atp.loc[df_atp.Player_p1.isna(), 'randomised_player_1' ] . nunique())) print ( '{} player_2s do Not have previous match history before the tournament' . format (df_atp.loc[df_atp.Player_p2.isna(), 'randomised_player_2' ] . nunique())) 59 player_1s do Not have previous match history before the tournament 56 player_2s do Not have previous match history before the tournament # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament' . format (df_wta.loc[df_wta.Player_p1.isna(), 'randomised_player_1' ] . nunique())) print ( '{} player_2s do Not have previous match history before the tournament' . format (df_wta.loc[df_wta.Player_p2.isna(), 'randomised_player_2' ] . nunique())) 41 player_1s do Not have previous match history before the tournament 37 player_2s do Not have previous match history before the tournament # Most of the missing are for the early years which makes sense as we dont have enough history for them df_wta.loc[df_wta.Player_p1.isna(), 'Tournament_Date' ] . value_counts() 2014-01-13 29 2014-08-25 7 2015-08-31 5 2015-01-19 3 2017-08-28 3 2018-01-15 3 2018-08-27 3 Name: Tournament_Date, dtype: int64 df_atp.loc[df_atp.Player_p1.isna(), 'Tournament_Date' ] . value_counts() 2012-01-16 29 2012-08-27 9 2014-01-13 5 2013-08-26 5 2016-01-18 5 2013-01-14 4 2014-08-25 3 2018-01-15 3 2017-08-28 3 2018-08-27 2 2016-08-29 2 2015-01-19 1 Name: Tournament_Date, dtype: int64 Now we have gotten the rolling averages for both player 1 and 2. What we need to do next is to simply calculat their difference. To calculate the difference, we need to: Split the dataframes into two new dataframes: Player 1 and Player 2 Take the difference between the two dataframes def get_player_difference(df, diff_cols = None) : # Input: # df: dataframe to get the data from # diff_cols: columns we take the difference on. For example is diff_cols = win rate. This function will calculate the # difference of the win rates between player 1 and player 2 # Return: the df with the new features p1_cols = [i + '_p1' for i in diff_cols] # column names for player 1 stats p2_cols = [i + '_p2' for i in diff_cols] # column names for player 2 stats # For any missing values, we will fill them by zeros except the ranking where we will use 999 df[ 'Player_Rank_p1' ] = df[ 'Player_Rank_p1' ] . fillna( 999 ) df[p1_cols] = df[p1_cols] . fillna( 0 ) df[ 'Player_Rank_p2' ] = df[ 'Player_Rank_p2' ] . fillna( 999 ) df[p2_cols] = df[p2_cols] . fillna( 0 ) new_column_name = [i + '_diff' for i in diff_cols] # Take the difference df_p1 = df[p1_cols] df_p2 = df[p2_cols] df_p1.columns = new_column_name df_p2.columns = new_column_name df_diff = df_p1 - df_p2 df_diff.columns = new_column_name # drop the p1 and p2 columns because We have the differences now df.drop(p1_cols + p2_cols, axis =1 , inplace = True) # Concat the df_diff and raw_df df = pd.concat([df, df_diff], axis =1 ) return df,new_column_name diff_cols = [ 'Player_Serve_Win_Ratio' , 'Player_Return_Win_Ratio' , 'Player_BreakPoints_Per_Return_Game' , 'Player_Game_Win_Percentage' , 'Player_Rank' ] # Apply the function and get the difference between player 1 and 2 df_atp,_ = get_player_difference(df_atp,diff_cols = diff_cols) df_wta,_ = get_player_difference(df_wta,diff_cols = diff_cols) # Make a copy of the dataframes in case we need to come back to check the values df_atp_final = df_atp.copy() df_wta_final = df_wta.copy() df_atp_final.head() Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Juan Martin del Potro Adrian Mannarino Australian Open, Melbourne 2012-01-16 1 Juan Martin del Potro Adrian Mannarino Juan Martin del Potro 2012-01-16 Adrian Mannarino 2012-01-16 0.035030 -0.021271 -0.025975 0.103479 -76.0 Pere Riba Albert Montanes Australian Open, Melbourne 2012-01-16 1 Pere Riba Albert Montanes Pere Riba 2012-01-16 Albert Montanes 2012-01-16 -0.156369 0.008893 0.066667 -0.094118 39.0 Tomas Berdych Albert Ramos-Vinolas Australian Open, Melbourne 2012-01-16 0 Albert Ramos-Vinolas Tomas Berdych Albert Ramos-Vinolas 2012-01-16 NaN NaT 0.498027 0.380092 0.414815 0.394444 -934.0 Rafael Nadal Alex Kuznetsov Australian Open, Melbourne 2012-01-16 0 Alex Kuznetsov Rafael Nadal NaN NaT Rafael Nadal 2012-01-16 -0.670139 -0.423057 -0.445623 -0.574767 997.0 Roger Federer Alexander Kudryavtsev Australian Open, Melbourne 2012-01-16 0 Alexander Kudryavtsev Roger Federer NaN NaT Roger Federer 2012-01-16 -0.721415 -0.449516 -0.360255 -0.668090 996.0","title":"Calculate the difference of averages for each match in the data frames"},{"location":"modelling/AusOpenPythonTutorial/#modelling","text":"We will trian two models here, one for mens and one for womens. For training, we will use all available data from the second year (too many missing values in the first year) up until 2017. For validation, we will test the model on the 2018 Australian Open data This setup allows us to 'mimic' the final prediction (using historical matches to predict 2019 results) df_train_atp = df_atp_final.loc[(df_atp_final.Tournament_Date != '2018-01-15' ) # excluding Aus Open 2018, and & (df_atp_final.Tournament_Date > '2012-01-16' )] # excluding first year df_valid_atp = df_atp_final.loc[df_atp_final.Tournament_Date == '2018-01-15' ] # Australian Open 2018 only df_train_wta = df_wta_final.loc[(df_wta_final.Tournament_Date != '2018-01-15' ) # excluding Aus Open 2018, and & (df_wta_final.Tournament_Date > '2014-01-13' )] # excluding first year df_train_atp.head() Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Daniel Brands Adrian Ungur U.S. Open, New York 2012-08-27 0 Adrian Ungur Daniel Brands NaN NaT Daniel Brands 2012-08-27 -0.535211 -0.300000 -0.043478 -0.434783 870.0 Richard Gasquet Albert Montanes U.S. Open, New York 2012-08-27 1 Richard Gasquet Albert Montanes Richard Gasquet 2012-08-27 Albert Montanes 2012-08-27 0.080003 0.077451 0.180847 0.131108 -37.0 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 1 Martin Klizan Alejandro Falla Martin Klizan 2012-08-27 Alejandro Falla 2012-08-27 0.077117 -0.044716 -0.087362 0.068180 -2.0 Andy Murray Alex Bogomolov Jr. U.S. Open, New York 2012-08-27 1 Andy Murray Alex Bogomolov Jr. Andy Murray 2012-08-27 Alex Bogomolov Jr. 2012-08-27 0.039641 0.031701 0.094722 0.059010 -69.0 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 1 Tommy Robredo Andreas Seppi Tommy Robredo 2012-08-27 Andreas Seppi 2012-08-27 -0.026814 0.006442 -0.009930 -0.067780 151.0 # target variable target = 'player_1_win' # features being fed into the models feats = [ 'Player_Serve_Win_Ratio_diff' , 'Player_Return_Win_Ratio_diff' , 'Player_BreakPoints_Per_Return_Game_diff' , 'Player_Game_Win_Percentage_diff' , 'Player_Rank_diff' ] print (feats)","title":"Modelling"},{"location":"modelling/AusOpenPythonTutorial/#h2o-model-for-atp","text":"h2o.init() # Convert to an h2o frame df_train_atp_h2o = h2o.H2OFrame(df_train_atp) df_valid_atp_h2o = h2o.H2OFrame(df_valid_atp) # For binary classification, response should be a factor df_train_atp_h2o[target] = df_train_atp_h2o[target] . asfactor() df_valid_atp_h2o[target] = df_valid_atp_h2o[target] . asfactor() # Run AutoML for 20 base models (limited to 1 hour max runtime by default) aml_atp = h2o.automl.H2OAutoML(max_runtime_secs =300 , max_models =100 , stopping_metric = 'logloss' , sort_metric = 'logloss' , balance_classes = True, seed =183 ) aml_atp.train(x = feats, y = target, training_frame = df_train_atp_h2o,validation_frame = df_valid_atp_h2o) # View the AutoML Leaderboard lb = aml_atp.leaderboard lb.head() model_id auc logloss mean_per_class_error rmse mse GBM_5_AutoML_20181221_094949 0.790281 0.554852 0.281363 0.431379 0.186088 GBM_grid_1_AutoML_20181221_094949_model_15 0.789329 0.556804 0.29856 0.431931 0.186564 GBM_grid_1_AutoML_20181221_094949_model_7 0.788013 0.557808 0.295899 0.432968 0.187461 StackedEnsemble_BestOfFamily_AutoML_20181221_094949 0.788131 0.558028 0.285321 0.432849 0.187358 GBM_grid_1_AutoML_20181221_094949_model_20 0.785633 0.561094 0.283932 0.43479 0.189043 StackedEnsemble_AllModels_AutoML_20181221_094949 0.784411 0.561587 0.293244 0.434667 0.188935 GBM_grid_1_AutoML_20181221_094949_model_25 0.785311 0.561783 0.291912 0.434888 0.189127 GBM_grid_1_AutoML_20181221_094949_model_17 0.774832 0.570883 0.295836 0.439375 0.193051 DeepLearning_1_AutoML_20181221_094949 0.779388 0.572823 0.311737 0.438479 0.192264 GBM_grid_1_AutoML_20181221_094949_model_14 0.7718 0.578867 0.285835 0.441373 0.19481 H2O model for WTA # Convert to an h2o frame df_train_wta_h2o = h2o.H2OFrame(df_train_wta) df_valid_wta_h2o = h2o.H2OFrame(df_valid_wta) # For binary classification, response should be a factor df_train_wta_h2o[target] = df_train_wta_h2o[target] . asfactor() df_valid_wta_h2o[target] = df_valid_wta_h2o[target] . asfactor() # Run AutoML for 20 base models (limited to 1 hour max runtime by default) aml_wta = h2o.automl.H2OAutoML(max_runtime_secs =300 , max_models =100 , stopping_metric = 'logloss' , sort_metric = 'logloss' , balance_classes = True, seed =183 ) aml_wta.train(x = feats, y = target, training_frame = df_train_wta_h2o,validation_frame = df_valid_wta_h2o) # View the AutoML Leaderboard lb = aml_wta.leaderboard lb.head() model_id auc logloss mean_per_class_error rmse mse StackedEnsemble_AllModels_AutoML_20181221_095400 0.726046 0.60827 0.321222 0.457117 0.208956 StackedEnsemble_BestOfFamily_AutoML_20181221_095400 0.724911 0.609329 0.337847 0.457659 0.209452 DeepLearning_grid_1_AutoML_20181221_095400_model_3 0.729152 0.612669 0.315971 0.45641 0.20831 GBM_grid_1_AutoML_20181221_095400_model_7 0.721204 0.615763 0.336848 0.460885 0.212415 GBM_5_AutoML_20181221_095400 0.719252 0.616535 0.319179 0.461055 0.212572 GBM_grid_1_AutoML_20181221_095400_model_15 0.715921 0.619263 0.318673 0.462215 0.213643 GLM_grid_1_AutoML_20181221_095400_model_1 0.726048 0.622989 0.366124 0.463099 0.214461 GBM_grid_1_AutoML_20181221_095400_model_17 0.709261 0.624902 0.34876 0.465628 0.216809 GBM_grid_1_AutoML_20181221_095400_model_18 0.70946 0.625704 0.393556 0.466147 0.217293 DeepLearning_grid_1_AutoML_20181221_095400_model_2 0.713419 0.628008 0.311334 0.463638 0.21496","title":"H2O model for ATP"},{"location":"modelling/AusOpenPythonTutorial/#use-the-models-to-predict-and-make-submissions","text":"Now let's use the models we just created to make the submissions df_predict_atp = pd.read_csv( \"data/men_dummy_submission_file.csv\" ) df_predict_wta = pd.read_csv( \"data/women_dummy_submission_file.csv\" , encoding = 'latin1' ) # for womens, there are some names need a different encoding df_predict_wta.head( 2 ) player_1 player_2 player_1_win_probability Simona Halep Angelique Kerber 0.5 Simona Halep Caroline Wozniacki 0.5","title":"Use the models to predict and make submissions"},{"location":"modelling/AusOpenPythonTutorial/#get-the-features-for-the-predict-df","text":"We need to join the features to the 2019 players # Before we join the features by the names and the dates, we need to convert any non-english characters to english first translationTable = str.maketrans( \"\u00e9\u00e0\u00e8\u00f9\u00e2\u00ea\u00ee\u00f4\u00fb\u00e7\u00f1\u00e1\" , \"eaeuaeioucna\" ) df_predict_atp[ 'player_1' ] = df_predict_atp.player_1.apply(lambda x : x.translate(translationTable)) df_predict_atp[ 'player_2' ] = df_predict_atp.player_2.apply(lambda x : x.translate(translationTable)) df_predict_wta[ 'player_1' ] = df_predict_wta.player_1.apply(lambda x : x.translate(translationTable)) df_predict_wta[ 'player_2' ] = df_predict_wta.player_2.apply(lambda x : x.translate(translationTable)) # Also we need to convert the names into lower cases df_predict_atp[ 'player_1' ] = df_predict_atp[ 'player_1' ] . str.lower() df_predict_atp[ 'player_2' ] = df_predict_atp[ 'player_2' ] . str.lower() df_predict_wta[ 'player_1' ] = df_predict_wta[ 'player_1' ] . str.lower() df_predict_wta[ 'player_2' ] = df_predict_wta[ 'player_2' ] . str.lower() df_rolling_atp[ 'Player' ] = df_rolling_atp[ 'Player' ] . str.lower() df_rolling_wta[ 'Player' ] = df_rolling_wta[ 'Player' ] . str.lower() # Lastly, some players have slightly difference names in the submission data and the match data. So we are editing them here manually df_predict_atp.loc[df_predict_atp.player_1 == 'jaume munar' , 'player_1' ] = 'jaume antoni munar clar' df_predict_atp.loc[df_predict_atp.player_2 == 'jaume munar' , 'player_2' ] = 'jaume antoni munar clar' df_predict_wta.loc[df_predict_wta.player_1 == 'daria kasatkina' , 'player_1' ] = 'darya kasatkina' df_predict_wta.loc[df_predict_wta.player_2 == 'daria kasatkina' , 'player_2' ] = 'darya kasatkina' df_predict_wta.loc[df_predict_wta.player_1 == 'lesia tsurenko' , 'player_1' ] = 'lesya tsurenko' df_predict_wta.loc[df_predict_wta.player_2 == 'lesia tsurenko' , 'player_2' ] = 'lesya tsurenko' df_predict_wta.loc[df_predict_wta.player_1 == 'danielle collins' , 'player_1' ] = 'danielle rose collins' df_predict_wta.loc[df_predict_wta.player_2 == 'danielle collins' , 'player_2' ] = 'danielle rose collins' df_predict_wta.loc[df_predict_wta.player_1 == 'anna karolina schmiedlova' , 'player_1' ] = 'anna schmiedlova' df_predict_wta.loc[df_predict_wta.player_2 == 'anna karolina schmiedlova' , 'player_2' ] = 'anna schmiedlova' df_predict_wta.loc[df_predict_wta.player_1 == 'georgina garcia perez' , 'player_1' ] = 'georgina garcia-perez' df_predict_wta.loc[df_predict_wta.player_2 == 'georgina garcia perez' , 'player_2' ] = 'georgina garcia-perez' # create and tournament date column and set it to 2019 so we can join the lastest features df_predict_atp[ 'Tournament_Date' ] = pd.to_datetime( '2019-01-15' ) df_predict_wta[ 'Tournament_Date' ] = pd.to_datetime( '2019-01-15' ) # Get the rolling features for player 1 df_predict_atp = df_predict_atp.merge(df_rolling_atp, how = 'left' , left_on = [ 'player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ],validate = 'm:1' ) df_predict_wta = df_predict_wta.merge(df_rolling_wta, how = 'left' , left_on = [ 'player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ],validate = 'm:1' ) # Get the rolling features for player 2 # For duplicate columns, we will use '_p1' to denote player 1 and '_p2' for player 2 df_predict_atp = df_predict_atp.merge(df_rolling_atp, how = 'left' , left_on = [ 'player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ],validate = 'm:1' ,suffixes = ( '_p1' , '_p2' )) df_predict_wta = df_predict_wta.merge(df_rolling_wta, how = 'left' , left_on = [ 'player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ],validate = 'm:1' ,suffixes = ( '_p1' , '_p2' )) df_predict_atp.head( 2 ) player_1 player_2 player_1_win_probability Tournament_Date Player_p1 Player_Serve_Win_Ratio_p1 Player_Return_Win_Ratio_p1 Player_BreakPoints_Per_Return_Game_p1 Player_Game_Win_Percentage_p1 Player_Rank_p1 tournament_date_index_p1 Player_p2 Player_Serve_Win_Ratio_p2 Player_Return_Win_Ratio_p2 Player_BreakPoints_Per_Return_Game_p2 Player_Game_Win_Percentage_p2 Player_Rank_p2 tournament_date_index_p2 novak djokovic rafael nadal 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 rafael nadal 0.622425 0.401028 0.334270 0.570833 1.0 2019-01-15 novak djokovic roger federer 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 roger federer 0.620070 0.389781 0.269224 0.564244 3.0 2019-01-15 # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament in the mens' . format (df_predict_atp.loc[df_predict_atp.Player_p1.isna(), 'player_1' ] . nunique())) print ( '{} player_2s do Not have previous match history before the tournament in the mens' . format (df_predict_atp.loc[df_predict_atp.Player_p2.isna(), 'player_2' ] . nunique())) 3 player_1s do Not have previous match history before the tournament in the mens 3 player_2s do Not have previous match history before the tournament in the mens # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament in the womens' . format (df_predict_wta.loc[df_predict_wta.Player_p1.isna(), 'player_1' ] . nunique())) print ( '{} player_2s do Not have previous match history before the tournament in the womens' . format (df_predict_wta.loc[df_predict_wta.Player_p2.isna(), 'player_2' ] . nunique())) 0 player_1s do Not have previous match history before the tournament in the womens 0 player_2s do Not have previous match history before the tournament in the womens print (df_predict_atp.loc[df_predict_atp.Player_p1.isna(), 'player_1' ] . unique () . tolist()) [ 'christian garin' , 'pedro sousa' , 'hugo dellien' ] print (df_predict_wta.loc[df_predict_wta.Player_p1.isna(), 'player_1' ] . unique () . tolist()) [] We will do the differencing again for the prediction dataframes exactly like what we did for training # Apply the function and get the difference between player 1 and 2 df_predict_atp,_ = get_player_difference(df_predict_atp,diff_cols = diff_cols) df_predict_wta,_ = get_player_difference(df_predict_wta,diff_cols = diff_cols)","title":"Get the features for the predict df"},{"location":"modelling/AusOpenPythonTutorial/#make-the-prediction","text":"df_predict_atp_h2o = h2o.H2OFrame(df_predict_atp[feats]) df_predict_wta_h2o = h2o.H2OFrame(df_predict_wta[feats]) atp_preds = aml_atp.predict(df_predict_atp_h2o)[ 'p1' ] . as_data_frame() wta_preds = aml_wta.predict(df_predict_wta_h2o)[ 'p1' ] . as_data_frame() df_predict_atp[ 'player_1_win_probability' ] = atp_preds df_predict_wta[ 'player_1_win_probability' ] = wta_preds atp_submission = df_predict_atp[[ 'player_1' , 'player_2' , 'player_1_win_probability' ]] wta_submission = df_predict_wta[[ 'player_1' , 'player_2' , 'player_1_win_probability' ]] atp_submission.head() player_1 player_2 player_1_win_probability novak djokovic rafael nadal 0.571588 novak djokovic roger federer 0.662511 novak djokovic juan martin del potro 0.544306 novak djokovic alexander zverev 0.709483 novak djokovic kevin anderson 0.687195 wta_submission.head() player_1 player_2 player_1_win_probability simona halep angelique kerber 0.455224 simona halep caroline wozniacki 0.546276 simona halep elina svitolina 0.408014 simona halep naomi osaka 0.285125 simona halep sloane stephens 0.576643 Let's look at who has the highest win rate from our models atp_submission.groupby( 'player_1' )[ 'player_1_win_probability' ] . mean () \\ . reset_index() . sort_values( 'player_1_win_probability' ,ascending = False) . head () player_1 player_1_win_probability novak djokovic 0.846377 juan martin del potro 0.787337 karen khachanov 0.782963 rafael nadal 0.778707 roger federer 0.767337 wta_submission.groupby( 'player_1' )[ 'player_1_win_probability' ] . mean () \\ . reset_index() . sort_values( 'player_1_win_probability' ,ascending = False) . head () player_1 player_1_win_probability madison keys 0.750580 naomi osaka 0.749195 caroline wozniacki 0.722409 kiki bertens 0.713904 aryna sabalenka 0.707368 Now we can output the predictions as csvs atp_submission.to_csv( 'submission/atp_submission_python.csv' ,index = False) wta_submission.to_csv( 'submission/wta_submission_pthon.csv' ,index = False) atp_submission.shape (16256, 3)","title":"Make the prediction"},{"location":"modelling/AusOpenRTutorial/","text":"Australian Open Datathon R Tutorial \u00b6 Overview \u00b6 The Task \u00b6 This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide. The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here . For a detailed outline of the task, the prizes, and to sign up, click here . How an outline of our methodoly and thought process, read this article. Prizes \u00b6 Place Prize Place Prize 1 $5000 9 $500 2 $3000 10 $500 3 $2000 11 $200 4 $1000 12 $200 5 $750 13 $200 6 $500 14 $200 7 $500 15 $200 8 $500 Total $15250 Submission \u00b6 To submit your model, email your final submission to datathon@betfair.com.au . Note that you don't need to email your code, just your predictions in the format that we have specified No submissions will be accepted prior to the Australian Open qualifying matches being completed and the final draw list being shared with registered participants (12 January 2019) Submissions need to include all potential match ups during the Australian Open, i.e. all possible combinations for each men's and women's tournaments (this will be provided after the draw is announced and the Australian Open qualifying matches are completed (Jan 12 th 2019)) Submissions must follow the format outlined above and shown in the 'Dummy Submission File'. Any submissions that are not in the correct format will not be accepted. Submissions need to include the player names for the hypothetical match up and the probability of the first player winning i.e. player_1,player_2,probability_of_player_1_winning, Submissions must be in a csv format Only two models will be accepted per participant (one model for the men's draw, one model for the women's draw) Exploring the Data \u00b6 First we need to get an idea of what the data looks like. Let's read the men's data in and get an idea of what it looks like. Note that you will need to install all the packages listed below unless you already have them. Note that for this tutorial I will be using dplyr , if you are not familiar with the syntax I encourage you to read up on the basics . # Import libraries library (dplyr) library (readr) library (tidyr) library (RcppRoll) library (tidyselect) library (lubridate) library (stringr) library (zoo) library (purrr) library (h2o) library (DT) mens = readr :: read_csv( 'data/ATP_matches.csv' , na = \".\" ) # NAs are indicated by . mens %>% datatable(rownames = FALSE , extensions = 'Scroller' , options = list (dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound(columns = pluck( . , \"x\" , \"data\" ) %>% colnames (), digits =3 ) Winner Loser Tournament Tournament_Date Court_Surface Round_Description Winner_Rank Loser_Rank Retirement_Ind Winner_Sets_Won ... Loser_DoubleFaults Loser_FirstServes_Won Loser_FirstServes_In Loser_SecondServes_Won Loser_SecondServes_In Loser_BreakPoints_Won Loser_BreakPoints Loser_ReturnPoints_Won Loser_ReturnPoints_Faced Loser_TotalPoints_Won Edouard Roger-Vasselin Eric Prodon Chennai 2-Jan-12 Hard First Round 106 97 0 2 ... 3 21 33 13 26 1 3 15 49 49 Dudi Sela Fabio Fognini Chennai 2-Jan-12 Hard First Round 83 48 0 2 ... 4 17 32 5 26 0 1 8 33 30 Go Soeda Frederico Gil Chennai 2-Jan-12 Hard First Round 120 102 0 2 ... 2 45 70 18 35 2 4 36 103 99 Yuki Bhambri Karol Beck Chennai 2-Jan-12 Hard First Round 345 101 0 2 ... 1 15 33 13 29 2 3 15 46 43 Yuichi Sugita Olivier Rochus Chennai 2-Jan-12 Hard First Round 235 67 0 2 ... 0 19 32 13 22 1 7 30 78 62 Benoit Paire Pere Riba Chennai 2-Jan-12 Hard First Round 95 89 0 2 ... 5 13 20 12 32 0 1 9 44 34 Victor Hanescu Sam Querrey Chennai 2-Jan-12 Hard First Round 90 93 0 2 ... 8 29 41 7 24 1 3 17 57 53 Yen-Hsun Lu Thiemo de Bakker Chennai 2-Jan-12 Hard First Round 82 223 0 2 ... 0 20 32 10 24 1 1 19 57 49 Andreas Beck Vasek Pospisil Chennai 2-Jan-12 Hard First Round 98 119 0 2 ... 3 39 57 16 38 1 5 24 74 79 Ivan Dodig Vishnu Vardhan Chennai 2-Jan-12 Hard First Round 36 313 0 2 ... 5 41 59 13 27 2 8 34 101 88 David Goffin Xavier Malisse Chennai 2-Jan-12 Hard First Round 174 49 0 2 ... 1 31 43 19 34 1 4 27 85 77 David Goffin Andreas Beck Chennai 2-Jan-12 Hard Second Round 174 98 0 2 ... 0 43 71 14 27 2 8 27 82 84 Dudi Sela Benoit Paire Chennai 2-Jan-12 Hard Second Round 83 95 0 2 ... 5 40 58 21 46 1 7 26 87 87 Stan Wawrinka Edouard Roger-Vasselin Chennai 2-Jan-12 Hard Second Round 17 106 0 2 ... 0 43 70 16 34 4 6 28 82 87 Go Soeda Ivan Dodig Chennai 2-Jan-12 Hard Second Round 120 36 0 2 ... 2 31 41 11 28 1 4 23 73 65 Milos Raonic Victor Hanescu Chennai 2-Jan-12 Hard Second Round 31 90 0 2 ... 1 25 38 5 14 0 4 15 56 45 Yuichi Sugita Yen-Hsun Lu Chennai 2-Jan-12 Hard Second Round 235 82 0 2 ... 4 34 45 12 34 2 9 38 93 84 Janko Tipsarevic Yuki Bhambri Chennai 2-Jan-12 Hard Second Round 9 345 0 2 ... 2 12 22 9 17 0 1 8 41 29 Janko Tipsarevic David Goffin Chennai 2-Jan-12 Hard Quarter-finals 9 174 0 2 ... 5 34 51 19 40 1 2 18 67 71 Milos Raonic Dudi Sela Chennai 2-Jan-12 Hard Quarter-finals 31 83 0 2 ... 2 23 31 19 28 0 3 16 69 58 Go Soeda Stan Wawrinka Chennai 2-Jan-12 Hard Quarter-finals 120 17 0 2 ... 4 18 34 13 31 3 7 31 74 62 Nicolas Almagro Yuichi Sugita Chennai 2-Jan-12 Hard Quarter-finals 10 235 0 2 ... 1 36 65 30 40 3 12 45 123 111 Janko Tipsarevic Go Soeda Chennai 2-Jan-12 Hard Semi-finals 9 120 0 2 ... 1 21 33 10 28 1 1 10 44 41 Milos Raonic Nicolas Almagro Chennai 2-Jan-12 Hard Semi-finals 31 10 0 2 ... 0 31 45 8 15 0 3 12 54 51 Milos Raonic Janko Tipsarevic Chennai 2-Jan-12 Hard Finals 31 9 0 2 ... 2 59 83 34 55 0 4 25 113 118 Igor Andreev Adrian Mannarino Brisbane 2-Jan-12 Hard First Round 115 87 0 2 ... 3 24 35 13 25 1 3 21 70 58 Alexandr Dolgopolov Alejandro Falla Brisbane 2-Jan-12 Hard First Round 15 74 0 2 ... 3 16 33 12 25 3 7 33 75 61 Tatsuma Ito Benjamin Mitchell Brisbane 2-Jan-12 Hard First Round 122 227 0 2 ... 6 30 44 7 24 0 2 13 52 50 Kei Nishikori Cedrik-Marcel Stebe Brisbane 2-Jan-12 Hard First Round 25 81 0 2 ... 2 27 49 23 41 3 6 28 75 78 Denis Istomin Florian Mayer Brisbane 2-Jan-12 Hard First Round 73 23 1 1 ... 1 28 38 11 17 0 2 15 56 54 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Malek Jaziri Fernando Verdasco Paris 29-Oct-18 Indoor Hard Second Round 55 27 0 2 ... 6 46 60 16 35 3 13 39 104 101 Alexander Zverev Frances Tiafoe Paris 29-Oct-18 Indoor Hard Second Round 5 44 0 2 ... 4 26 40 16 36 2 10 27 72 69 Dominic Thiem Gilles Simon Paris 29-Oct-18 Indoor Hard Second Round 8 31 0 2 ... 1 13 26 12 25 2 2 23 59 48 Novak Djokovic Joao Sousa Paris 29-Oct-18 Indoor Hard Second Round 2 48 0 2 ... 2 25 35 6 22 1 10 27 74 58 Karen Khachanov Matthew Ebden Paris 29-Oct-18 Indoor Hard Second Round 18 39 1 2 ... 6 8 18 5 20 1 2 10 30 23 John Isner Mikhail Kukushkin Paris 29-Oct-18 Indoor Hard Second Round 9 54 0 2 ... 1 54 80 24 39 0 1 13 90 91 Kevin Anderson Nikoloz Basilashvili Paris 29-Oct-18 Indoor Hard Second Round 6 22 0 2 ... 7 43 54 30 49 0 3 26 106 99 Marin Cilic Philipp Kohlschreiber Paris 29-Oct-18 Indoor Hard Second Round 7 43 0 2 ... 1 19 34 12 20 1 1 17 55 48 Jack Sock Richard Gasquet Paris 29-Oct-18 Indoor Hard Second Round 23 28 0 2 ... 4 18 33 16 29 0 4 19 59 53 Grigor Dimitrov Roberto Bautista Agut Paris 29-Oct-18 Indoor Hard Second Round 10 25 0 2 ... 0 34 48 11 20 2 4 27 76 72 Damir Dzumhur Stefanos Tsitsipas Paris 29-Oct-18 Indoor Hard Second Round 52 16 0 2 ... 3 14 26 15 30 2 2 17 52 46 Dominic Thiem Borna Coric Paris 29-Oct-18 Indoor Hard Third Round 8 13 0 2 ... 1 39 57 16 38 2 2 27 88 82 Novak Djokovic Damir Dzumhur Paris 29-Oct-18 Indoor Hard Third Round 2 52 1 2 ... 4 15 28 7 18 0 0 8 28 30 Alexander Zverev Diego Schwartzman Paris 29-Oct-18 Indoor Hard Third Round 5 19 0 2 ... 2 22 37 12 24 0 4 18 58 52 Roger Federer Fabio Fognini Paris 29-Oct-18 Indoor Hard Third Round 3 14 0 2 ... 6 22 32 15 37 1 5 16 54 53 Marin Cilic Grigor Dimitrov Paris 29-Oct-18 Indoor Hard Third Round 7 10 0 2 ... 1 37 55 14 32 1 5 22 71 73 Karen Khachanov John Isner Paris 29-Oct-18 Indoor Hard Third Round 18 9 0 2 ... 4 67 80 19 38 0 0 17 100 103 Kei Nishikori Kevin Anderson Paris 29-Oct-18 Indoor Hard Third Round 11 6 0 2 ... 1 26 33 11 19 0 0 11 51 48 Jack Sock Malek Jaziri Paris 29-Oct-18 Indoor Hard Third Round 23 55 0 2 ... 6 13 21 10 24 0 0 9 41 32 Karen Khachanov Alexander Zverev Paris 29-Oct-18 Indoor Hard Quarter-finals 18 5 0 2 ... 7 26 47 4 21 1 3 10 36 40 Dominic Thiem Jack Sock Paris 29-Oct-18 Indoor Hard Quarter-finals 8 23 0 2 ... 5 44 59 19 37 2 10 34 97 97 Roger Federer Kei Nishikori Paris 29-Oct-18 Indoor Hard Quarter-finals 3 11 0 2 ... 0 21 37 16 26 0 1 12 56 49 Novak Djokovic Marin Cilic Paris 29-Oct-18 Indoor Hard Quarter-finals 2 7 0 2 ... 0 38 55 11 28 2 5 29 85 78 Karen Khachanov Dominic Thiem Paris 29-Oct-18 Indoor Hard Semi-finals 18 8 0 2 ... 0 19 29 8 26 1 3 15 47 42 Novak Djokovic Roger Federer Paris 29-Oct-18 Indoor Hard Semi-finals 2 3 0 2 ... 2 69 93 25 46 1 2 29 113 123 Karen Khachanov Novak Djokovic Paris 29-Oct-18 Indoor Hard Finals 18 2 0 2 ... 1 30 43 14 28 1 5 20 66 64 Jaume Antoni Munar Clar Frances Tiafoe Milan 5-Nov-18 Indoor Hard NA 76 40 0 3 ... 3 21 29 6 17 0 2 5 46 32 Frances Tiafoe Hubert Hurkacz Milan 5-Nov-18 Indoor Hard NA 40 85 0 3 ... 4 35 48 10 19 1 7 22 78 67 Hubert Hurkacz Jaume Antoni Munar Clar Milan 5-Nov-18 Indoor Hard NA 85 76 0 3 ... 1 43 63 15 35 3 9 29 80 87 Andrey Rublev Liam Caruana Milan 5-Nov-18 Indoor Hard NA 68 NA 0 3 ... 1 28 39 4 14 1 3 18 57 50 As we can see, we have a Winner column, a Loser column, as well as other columns detailing the match details, and other columns which have the stats for that match. As we have a Winner column, if we use the current data structure to train a model we will leak the result. The model will simply learn that the actual winner comes from the Winner column, rather than learning from other features that we can create, such as First Serve % . To avoid this problem, let's reshape the data from wide to long, then shuffle the data. For this, we will define a function, split_winner_loser_columns , which splits the raw dataframe into two dataframes, appends them together, and then shuffles the data. Let's also remove all Grass and Clay matches from our data, as we will be modelling the Australian Open which is a hardcourt surface. Additionally, we will add a few columns, such as Match_Id and Total_Games . These will be useful later. split_winner_loser_columns <- function (df) { # This function splits the raw data into two dataframes and appends them together then shuffles them # This output is a dataframe with only one player's stats on each row (i.e. in long format) # Grab a df with only the Winner's stats winner = df %>% select( - contains( \"Loser\" )) %>% # Select only the Winner columns + extra game info columns as a df rename_at( # Rename all columns containing \"Winner\" to \"Player\" vars(contains( \"Winner\" )), ~ str_replace( . , \"Winner\" , \"Player\" ) ) %>% mutate(Winner = 1 ) # Create a target column # Repeat the process with the loser's stats loser = df %>% select( - contains( \"Winner\" )) %>% rename_at( vars(contains( \"Loser\" )), ~ str_replace( . , \"Loser\" , \"Player\" ) ) %>% mutate(Winner = 0 ) set.seed ( 183 ) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut) # Create a df that appends both the Winner and loser df together combined_df = winner %>% rbind (loser) %>% # Append the loser df to the Winner df slice( sample ( 1: n())) %>% # Randomise row order arrange(Match_Id) %>% # Arrange by Match_Id return () } # Read in men and womens data; randomise the data to avoid result leakage mens = readr :: read_csv( 'data/ATP_matches.csv' , na = \".\" ) %>% filter(Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\" ) %>% # Filter to only use hardcourt games mutate(Match_Id = row_number(), # Add a match ID column to be used as a key Tournament_Date = dmy(Tournament_Date), # Change Tournament to datetime Total_Games = Winner_Games_Won + Loser_Games_Won) %>% # Add a total games played column split_winner_loser_columns() # Change the dataframe from wide to long mens %>% datatable(rownames = FALSE , extensions = 'Scroller' , options = list (dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound(columns = pluck( . , \"x\" , \"data\" ) %>% colnames (), digits =3 ) Player Tournament Tournament_Date Court_Surface Round_Description Player_Rank Retirement_Ind Player_Sets_Won Player_Games_Won Player_Aces ... Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Match_Id Total_Games Winner Eric Prodon Chennai 2012-01-02 Hard First Round 97 0 0 7 2 ... 13 26 1 3 15 49 49 1 19 0 Edouard Roger-Vasselin Chennai 2012-01-02 Hard First Round 106 0 2 12 5 ... 12 19 4 7 25 59 59 1 19 1 Dudi Sela Chennai 2012-01-02 Hard First Round 83 0 2 12 2 ... 11 16 6 14 36 58 61 2 13 1 Fabio Fognini Chennai 2012-01-02 Hard First Round 48 0 0 1 1 ... 5 26 0 1 8 33 30 2 13 0 Frederico Gil Chennai 2012-01-02 Hard First Round 102 0 1 14 5 ... 18 35 2 4 36 103 99 3 33 0 Go Soeda Chennai 2012-01-02 Hard First Round 120 0 2 19 6 ... 19 39 5 11 42 105 109 3 33 1 Feature Creation \u00b6 Now that we have a fairly good understanding of what the data looks like, let's add some features. To do this we will define a function. Ideally we want to add features which will provide predictive power to our model. Thinking about the dynamics of tennis, we know that players often will matches by \"breaking\" the opponent's serve (i.e. winning a game when the opponent is serving). This is especially important in mens tennis. Let's create a feature called F_Player_BreakPoints_Per_Game , which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let's also create a feature called F_Player_Return_Win_Ratio which is the proportion of points won when returning. Similarly, \"holding\" serve is important (i.e. winning a game when you are serving). Let's create a feature called F_Player_Serve_Win_Ratio which is the proportion of points won when serving. Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let's create a feature called F_Player_Game_Win_Percentage which is the propotion of games that a player wins. add_ratio_features <- function (df) { # This function adds ratio features to a long df df %>% mutate( # Point Win ratio when serving F_Player_Serve_Win_Ratio = (Player_FirstServes_Won + Player_SecondServes_Won - Player_DoubleFaults) / (Player_FirstServes_In + Player_SecondServes_In + Player_DoubleFaults), # Point win ratio when returning F_Player_Return_Win_Ratio = Player_ReturnPoints_Won / Player_ReturnPoints_Faced, # Breakpoints per receiving game F_Player_BreakPoints_Per_Game = Player_BreakPoints / Total_Games, F_Player_Game_Win_Percentage = Player_Games_Won / Total_Games ) %>% mutate_at( vars( colnames ( . ), - contains( \"Rank\" ), - Tournament_Date), # Replace all NAs with0 apart from Rank, Date ~ ifelse ( is.na ( . ), 0 , . ) ) %>% return () } mens = mens %>% add_ratio_features() # Add features Now that we have added our features, we need to create rolling averages for them. We cannot simply use current match statistics, as they will leak the result to the model. Instead, we need to use past match statistics to predict future matches. Here we will use a rolling mean with a window of 15. If the player hasn't played 15 games, we will instead use a cumulative mean. We will also lag the result so as to not leak the result. This next chunk of code simply takes all the columns starting with F_ and calculates these means. mens = mens %>% group_by(Player) %>% # Group by player mutate_at( # Create a rolling mean with window 15 for each player. vars(starts_with( \"F_\" )), # If the player hasn't played 15 games, use a cumulative mean ~ coalesce(rollmean( . , k = 15 , align = \"right\" , fill = NA_real_ ), cummean( . )) %>% lag() ) %>% ungroup() Creating a Training Feature Matrix \u00b6 In predictive modelling language - features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options We can train the model on every tennis match in the data set, or We can only train the model on Australian Open matches. Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure. Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on. We have decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface - hard court. However, we also need to train our model in the same way that will be used to predict the 2019 Australian Open. When predicting the 2 nd round, we won't have data from the 1 st round. So we will need to build our training feature matrix with this in mind. We should extract features for a player from past games at the start of the tournament and apply them to every matchup that that player plays. To do this, we will create a function, extract_latest_features_for_tournament , which maps over our feature dataframe for the dates in the first round of a tournament and grabs features. First, we need the Australian Open and US Open results - let's grab these and then apply our function. # Get Australian Open and US Open Results aus_us_open_results = mens %>% filter((Tournament == \"Australian Open, Melbourne\" | Tournament == \"U.S. Open, New York\" ) & Round_Description != \"Qualifying\" & Tournament_Date != \"2012-01-16\" ) %>% # Filter out qualifiers select(Match_Id, Player, Tournament, Tournament_Date, Round_Description, Winner) # Create a function which extracts features for each tournament extract_latest_features_for_tournament = function (df, dte) { df %>% # Filter for the 1st round filter(Tournament_Date == dte, Round_Description == \"First Round\" , Tournament_Date != \"2012-01-16\" ) %>% group_by(Player) %>% # Group by player select_at( vars(Match_Id, starts_with( \"F_\" ), Player_Rank) # Grab the players' features ) %>% rename(F_Player_Rank = Player_Rank) %>% ungroup() %>% mutate(Feature_Date = dte) %>% select(Player, Feature_Date, everything()) } # Create a feature matrix in long format feature_matrix_long = aus_us_open_results %>% distinct(Tournament_Date) %>% # Pull all Tournament Dates pull() %>% map_dfr( ~ extract_latest_features_for_tournament(mens, . ) # Get the features ) %>% filter(Feature_Date != \"2012-01-16\" ) %>% # Filter out the first Aus Open mutate_at( # Replace NAs with the mean vars(starts_with( \"F_\" )), ~ ifelse ( is.na ( . ), mean ( . , na.rm = TRUE ), . ) ) Now that we have a feature matrix in long format, we need to convert it to wide format so that the features are on the same row. To do this we will define a function gather_df , which converts the dataframe from long to wide. Let's also join the results to the matrix and convert the Winner column to a factor. Finally, we will take the difference of player1 and player2's features, so as to reduce the dimensionality of the model. gather_df <- function (df) { # This function puts the df back into its original format of each row containing stats for both players df %>% arrange(Match_Id) %>% filter(row_number() %% 2 != 0 ) %>% # Filter for every 2nd row, starting at the 1st index. e.g. 1, 3, 5 rename_at( # Rename columns to player_1 vars(contains( \"Player\" )), ~ str_replace( . , \"Player\" , \"player_1\" ) ) %>% inner_join(df %>% filter(row_number() %% 2 == 0 ) %>% rename_at( vars(contains( \"Player\" )), # Rename columns to player_2 ~ str_replace( . , \"Player\" , \"player_2\" ) ) %>% select(Match_Id, contains( \"Player\" )), by = c ( 'Match_Id' ) ) %>% select(Match_Id, player_1, player_2, Winner, everything()) %>% return () } # Joining results to features feature_matrix_wide = aus_us_open_results %>% inner_join(feature_matrix_long %>% select( - Match_Id), by = c ( \"Player\" , \"Tournament_Date\" = \"Feature_Date\" )) %>% gather_df() %>% mutate( F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio, F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio, F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage, F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game, F_Rank_Diff = (F_player_1_Rank - F_player_2_Rank), Winner = as.factor (Winner) ) %>% select(Match_Id, player_1, player_2, Tournament, Tournament_Date, Round_Description, Winner, contains( \"Diff\" )) train = feature_matrix_wide train %>% datatable(rownames = FALSE , extensions = 'Scroller' , options = list (dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound(columns = pluck( . , \"x\" , \"data\" ) %>% colnames (), digits =3 ) Match_Id player_1 player_2 Tournament Tournament_Date Round_Description Winner F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff 1139 Adrian Ungur Daniel Brands U.S. Open, New York 2012-08-27 First Round 0 0.03279412 -0.014757229 0.002877458 0.073938088 -13 1140 Albert Montanes Richard Gasquet U.S. Open, New York 2012-08-27 First Round 0 -0.08000322 -0.077451342 -0.131108056 -0.180846832 97 1141 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 First Round 1 0.07711693 -0.044715517 0.068179841 -0.087361962 1 1142 Alex Bogomolov Jr. Andy Murray U.S. Open, New York 2012-08-27 First Round 0 -0.03964074 -0.031700826 -0.059010072 -0.094721700 69 1143 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 First Round 1 -0.02681392 0.006442134 -0.067779660 -0.009930089 151 1144 Ryan Harrison Benjamin Becker U.S. Open, New York 2012-08-27 First Round 1 0.04251983 0.018604623 0.026486753 -0.003548973 -24 Creating the Feature Matrix for the 2019 Australian Open \u00b6 Now that we have our training set, train , we need to create a feature matrix to create predictions on. To do this, we need to generate features again. We could simply append a player list to our raw dataframe, create a mock date and then use the extract_latest_features_for_tournament function that we used before. Instead, we're going to create a lookup table for each unique player in the 2019 Australian Open. We will need to get their last 15 games and then find the mean for each feature so that our features are the same. Let's first explore what the dummy submission file looks like, then use it to get the unique players. read_csv( 'data/men_dummy_submission_file.csv' ) %>% glimpse() As we can see, the dummy submission file contains every potential match up for the Open. This will be updated a few days before the Open starts with the actual players playing. Let's now create the lookup feature table. # Get a vector of unique players in this years' open using the dummy submission file unique_players = read_csv( 'data/men_dummy_submission_file.csv' ) %>% pull(player_1) %>% unique () # Get the last 15 games played for each unique player and find their features lookup_feature_table = read_csv( 'data/ATP_matches.csv' , na = \".\" ) %>% filter(Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\" ) %>% mutate(Match_Id = row_number(), # Add a match ID column to be used as a key Tournament_Date = dmy(Tournament_Date), # Change Tournament to datetime Total_Games = Winner_Games_Won + Loser_Games_Won) %>% # Add a total games played column # clean_missing_data() %>% # Clean missing data split_winner_loser_columns() %>% # Change the dataframe from wide to long add_ratio_features() %>% filter(Player %in% unique_players) %>% group_by(Player) %>% top_n( 15 , Match_Id) %>% summarise( F_Player_Serve_Win_Ratio = mean (F_Player_Serve_Win_Ratio), F_Player_Return_Win_Ratio = mean (F_Player_Return_Win_Ratio), F_Player_BreakPoints_Per_Game = mean (F_Player_BreakPoints_Per_Game), F_Player_Game_Win_Percentage = mean (F_Player_Game_Win_Percentage), F_Player_Rank = last(Player_Rank) ) Now let's create features for every single combination. To do this we'll join our lookup_feature_table to the player_1 and player_2 columns in the dummy_submission_file . # Create feature matrix for the Australian Open for all player 1s features_player_1 = read_csv( 'data/men_dummy_submission_file.csv' ) %>% select(player_1) %>% inner_join(lookup_feature_table, by = c ( \"player_1\" = \"Player\" )) %>% rename(F_player_1_Serve_Win_Ratio = F_Player_Serve_Win_Ratio, F_player_1_Return_Win_Ratio = F_Player_Return_Win_Ratio, F_player_1_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game, F_player_1_Game_Win_Percentage = F_Player_Game_Win_Percentage, F_player_1_Rank = F_Player_Rank) # Create feature matrix for the Australian Open for all player 2s features_player_2 = read_csv( 'data/men_dummy_submission_file.csv' ) %>% select(player_2) %>% inner_join(lookup_feature_table, by = c ( \"player_2\" = \"Player\" )) %>% rename(F_player_2_Serve_Win_Ratio = F_Player_Serve_Win_Ratio, F_player_2_Return_Win_Ratio = F_Player_Return_Win_Ratio, F_player_2_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game, F_player_2_Game_Win_Percentage = F_Player_Game_Win_Percentage, F_player_2_Rank = F_Player_Rank) # Join the two dfs together and subtract features to create Difference features aus_open_2019_features = features_player_1 %>% bind_cols(features_player_2) %>% select(player_1, player_2, everything()) %>% mutate( F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio, F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio, F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage, F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game, F_Rank_Diff = (F_player_1_Rank - F_player_2_Rank) ) %>% select(player_1, player_2, contains( \"Diff\" )) aus_open_2019_features %>% datatable(rownames = FALSE , extensions = 'Scroller' , options = list (dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound(columns = pluck( . , \"x\" , \"data\" ) %>% colnames (), digits =3 ) player_1 player_2 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff Novak Djokovic Rafael Nadal 0.06347805 0.02503802 0.07002382 0.08951024 1 Novak Djokovic Roger Federer 0.06583364 0.03628491 0.07661295 0.15455628 -1 Novak Djokovic Juan Martin del Potro 0.01067079 0.03436023 0.06382353 0.11259979 -2 Novak Djokovic Alexander Zverev 0.11117863 0.03125651 0.11055585 0.08661036 -3 Novak Djokovic Kevin Anderson 0.02132375 0.10449337 0.11184503 0.23684083 -4 Novak Djokovic Marin Cilic 0.08410746 0.02434916 0.07653035 0.08355134 -5 Generating 2019 Australian Open Predictions \u00b6 Now that we have our features, we can finally train our model and generate predictions for the 2019 Australian Open. Due to its simplicity, we will use h2o's Auto Machine Learning function h2o.automl . This will train a heap of different models and optimise the hyperparameters, as well as creating stacked ensembles automatically for us. We will use optimising by log loss. First, we must create h2o frames for our training and feature dataframes. Then we will run h2o.automl . Note that we can set the max_runtime_secs parameter. As this is a notebook, I have set it for 30 seconds - but I suggest you give it 10 minutes to create the best model. We can then create our predictions and assign them back to our aus_open_2019_features dataframe. Finally, we will group_by player and find the best player, on average. ## Setup H2O h2o.init(ip = \"localhost\" , port = 54321 , enable_assertions = TRUE , nthreads = 2 , max_mem_size = \"24g\" ) ## Sending file to h2o train_h2o = feature_matrix_wide %>% select(contains( \"Diff\" ), Winner) %>% as.h2o(destination_frame = \"train_h2o\" ) aus_open_2019_features_h2o = aus_open_2019_features %>% select(contains( \"Diff\" )) %>% as.h2o(destination_frame = \"aus_open_2019_features_h2o\" ) ## Running Auto ML mens_model = h2o.automl(y = \"Winner\" , training_frame = train_h2o, max_runtime_secs = 30 , max_models = 100 , stopping_metric = \"logloss\" , sort_metric = \"logloss\" , balance_classes = TRUE , seed = 183 ) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut) ## Predictions on test frame predictions = h2o.predict(mens_model @ leader, aus_open_2019_features_h2o) %>% as.data.frame () aus_open_2019_features $ prob_player_1 = predictions $ p1 aus_open_2019_features $ prob_player_2 = predictions $ p0 h2o.shutdown(prompt = FALSE ) Now let's find the best player by taking the mean of the prediction probability by player. aus_open_2019_features %>% select(player_1, starts_with( \"F_\" ), prob_player_1) %>% group_by(player_1) %>% summarise_all( mean ) %>% arrange(desc(prob_player_1)) %>% datatable(rownames = FALSE , extensions = 'Scroller' , options = list (dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound(columns = pluck( . , \"x\" , \"data\" ) %>% colnames (), digits =3 ) player_1 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff prob_player_1 Novak Djokovic 0.1109364627 0.076150615 0.1483970690 0.17144300 NA 0.8616486 Karen Khachanov 0.0960639298 0.061436164 0.1059967623 0.04544955 NA 0.8339594 Juan Martin del Potro 0.1003931993 0.042025222 0.0847985439 0.05943767 NA 0.8218308 Rafael Nadal 0.0480432305 0.051531252 0.0790179917 0.08181694 NA 0.8032543 Gilles Simon 0.0646937767 0.084843307 0.0901401318 0.08675350 NA 0.7985995 Roger Federer 0.0452014997 0.040992497 0.0725719954 0.01817046 NA 0.7962289 Kei Nishikori 0.0777155934 0.018720226 0.0800648870 0.02740276 NA 0.7843631 Marin Cilic 0.0285413602 0.053017465 0.0736687072 0.08883055 NA 0.7804876 Tomas Berdych 0.0471654691 0.047289449 0.0737401748 0.10584114 NA 0.7739211 Daniil Medvedev 0.0275430665 0.031121856 0.0721948279 0.01803757 NA 0.7543269 Stefanos Tsitsipas 0.0470382377 0.023825850 0.0577628626 0.02105227 NA 0.7511674 Dominic Thiem 0.0258904189 0.032481624 0.0483707080 0.05857158 NA 0.7451547 Alexander Zverev 0.0006199716 0.044811275 0.0380134371 0.08423392 NA 0.7374897 Kyle Edmund 0.0558006240 0.011963627 0.0478850676 0.05142186 NA 0.7304873 Pablo Carreno Busta 0.0321878318 0.029862068 0.0413674481 -0.00229784 NA 0.7302043 Borna Coric 0.0762084129 -0.010097922 0.0413621283 -0.01924267 NA 0.7268124 Kevin Anderson 0.0907358428 -0.027171681 0.0381421997 -0.06362578 NA 0.7260799 David Goffin -0.0034821911 0.037247336 0.0162572061 0.05603565 NA 0.7155908 Fernando Verdasco 0.0229261365 0.032884054 0.0521212576 0.04668854 NA 0.7120831 Roberto Bautista Agut 0.0047641170 0.049939608 0.0218975349 0.07331023 NA 0.7009891 Milos Raonic 0.0849726089 -0.028732182 0.0385944327 -0.08009382 NA 0.6986865 Fabio Fognini -0.0394792678 0.047935185 0.0226546894 0.06213496 NA 0.6982031 Hyeon Chung 0.0042489153 0.047722133 0.0158096386 0.04823304 NA 0.6958943 Jack Sock -0.0099659903 0.026454984 0.0186547428 0.02307214 NA 0.6757770 Diego Schwartzman -0.0317130675 0.032098381 0.0006215006 0.05621187 NA 0.6631067 John Millman 0.0016290285 0.042676556 0.0119857356 0.06228135 NA 0.6603912 Nikoloz Basilashvili -0.0099968609 0.005561102 0.0473876170 0.03661962 NA 0.6602628 John Isner 0.1346946527 -0.070556940 0.0161348609 -0.11425009 NA 0.6598097 Gael Monfils -0.0074254934 0.024286746 0.0295568649 0.04007519 NA 0.6449506 Richard Gasquet 0.0296009556 -0.011382437 0.0013138324 -0.03972967 NA 0.6442043 ... ... ... ... ... ... ... Laslo Djere -0.042300822 -0.0150684095 -0.064667709 -0.0349151578 NA 0.3606923 David Ferrer -0.036179509 0.0532782117 0.012751020 0.0914824480 NA 0.3488057 Bradley Klahn -0.001248083 -0.0444982448 -0.025987040 -0.1181295700 NA 0.3487806 Marcel Granollers -0.031011830 -0.0094056152 -0.049853664 0.0136841358 NA 0.3460035 Ricardas Berankis -0.022557215 -0.0103782963 -0.047937290 -0.0468488990 NA 0.3454980 Radu Albot -0.040829057 0.0076150564 -0.034891704 0.0443672533 NA 0.3420615 Jordan Thompson -0.068554906 0.0261969117 -0.044349181 0.0206636045 NA 0.3358572 Thomas Fabbiano -0.060583307 0.0275756029 -0.025883493 0.0709707306 NA 0.3319778 Roberto Carballes Baena -0.054016396 -0.0091521177 -0.019093050 0.0347187874 NA 0.3312105 Paolo Lorenzi -0.038613500 -0.0212206827 -0.052602703 0.0199474025 NA 0.3299791 Guido Andreozzi -0.038614385 -0.0133763922 0.029549861 0.0636745661 NA 0.3288762 Peter Polansky 0.007461636 -0.0163389196 -0.024034159 -0.0442144260 NA 0.3216756 Ernests Gulbis -0.062827089 -0.0134699552 -0.027633425 -0.0518663252 NA 0.3123511 Thiago Monteiro 0.001235931 -0.0288349103 -0.043831840 -0.0654744344 NA 0.3122069 Casper Ruud 0.016838968 -0.0178511679 0.015234507 0.0219131874 NA 0.3119321 Marco Trungelliti -0.022148774 -0.0005658242 0.048542554 0.1243537739 NA 0.3092636 Jiri Vesely -0.050204009 -0.0351868278 -0.042887646 -0.0160467165 NA 0.3089287 Guillermo Garcia-Lopez -0.090076100 -0.0108663630 -0.048712763 -0.0124446402 NA 0.3080898 Michael Mmoh -0.063802934 -0.0079053251 -0.011112236 -0.0332042032 NA 0.2822330 Jason Kubler -0.124758873 -0.0202756806 -0.013998570 0.1020895301 NA 0.2814246 Ruben Bemelmans -0.029036164 -0.0138846550 -0.032256254 -0.0363563402 NA 0.2772185 Bjorn Fratangelo -0.014149222 0.0033574304 -0.019931504 -0.0360199607 NA 0.2652527 Pablo Andujar -0.042869833 -0.0488261697 -0.070057834 -0.0164918910 NA 0.2647100 Christian Garin -0.046150875 0.0235799476 -0.006209664 0.0736304057 NA 0.2631607 Ivo Karlovic 0.071597162 -0.1093833837 0.001410787 -0.1237762218 NA 0.2500242 Juan Ignacio Londero -0.026454456 -0.0715665271 -0.016749898 -0.0363353678 NA 0.2351747 Ramkumar Ramanathan -0.005371622 -0.0606138479 -0.041631884 -0.0005573405 NA 0.2272977 Reilly Opelka 0.025704824 -0.0607219257 -0.015474944 -0.0720809006 NA 0.2262993 Carlos Berlocq -0.063580460 0.0074576369 -0.054277974 -0.0165235079 NA 0.2112275 Pedro Sousa -0.197333352 -0.0734557562 -0.161962722 -0.1023311674 NA 0.1502313","title":"Aus Open R Tutorial"},{"location":"modelling/AusOpenRTutorial/#australian-open-datathon-r-tutorial","text":"","title":"Australian Open Datathon R Tutorial"},{"location":"modelling/AusOpenRTutorial/#overview","text":"","title":"Overview"},{"location":"modelling/AusOpenRTutorial/#the-task","text":"This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide. The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here . For a detailed outline of the task, the prizes, and to sign up, click here . How an outline of our methodoly and thought process, read this article.","title":"The Task"},{"location":"modelling/AusOpenRTutorial/#prizes","text":"Place Prize Place Prize 1 $5000 9 $500 2 $3000 10 $500 3 $2000 11 $200 4 $1000 12 $200 5 $750 13 $200 6 $500 14 $200 7 $500 15 $200 8 $500 Total $15250","title":"Prizes"},{"location":"modelling/AusOpenRTutorial/#submission","text":"To submit your model, email your final submission to datathon@betfair.com.au . Note that you don't need to email your code, just your predictions in the format that we have specified No submissions will be accepted prior to the Australian Open qualifying matches being completed and the final draw list being shared with registered participants (12 January 2019) Submissions need to include all potential match ups during the Australian Open, i.e. all possible combinations for each men's and women's tournaments (this will be provided after the draw is announced and the Australian Open qualifying matches are completed (Jan 12 th 2019)) Submissions must follow the format outlined above and shown in the 'Dummy Submission File'. Any submissions that are not in the correct format will not be accepted. Submissions need to include the player names for the hypothetical match up and the probability of the first player winning i.e. player_1,player_2,probability_of_player_1_winning, Submissions must be in a csv format Only two models will be accepted per participant (one model for the men's draw, one model for the women's draw)","title":"Submission"},{"location":"modelling/AusOpenRTutorial/#exploring-the-data","text":"First we need to get an idea of what the data looks like. Let's read the men's data in and get an idea of what it looks like. Note that you will need to install all the packages listed below unless you already have them. Note that for this tutorial I will be using dplyr , if you are not familiar with the syntax I encourage you to read up on the basics . # Import libraries library (dplyr) library (readr) library (tidyr) library (RcppRoll) library (tidyselect) library (lubridate) library (stringr) library (zoo) library (purrr) library (h2o) library (DT) mens = readr :: read_csv( 'data/ATP_matches.csv' , na = \".\" ) # NAs are indicated by . mens %>% datatable(rownames = FALSE , extensions = 'Scroller' , options = list (dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound(columns = pluck( . , \"x\" , \"data\" ) %>% colnames (), digits =3 ) Winner Loser Tournament Tournament_Date Court_Surface Round_Description Winner_Rank Loser_Rank Retirement_Ind Winner_Sets_Won ... Loser_DoubleFaults Loser_FirstServes_Won Loser_FirstServes_In Loser_SecondServes_Won Loser_SecondServes_In Loser_BreakPoints_Won Loser_BreakPoints Loser_ReturnPoints_Won Loser_ReturnPoints_Faced Loser_TotalPoints_Won Edouard Roger-Vasselin Eric Prodon Chennai 2-Jan-12 Hard First Round 106 97 0 2 ... 3 21 33 13 26 1 3 15 49 49 Dudi Sela Fabio Fognini Chennai 2-Jan-12 Hard First Round 83 48 0 2 ... 4 17 32 5 26 0 1 8 33 30 Go Soeda Frederico Gil Chennai 2-Jan-12 Hard First Round 120 102 0 2 ... 2 45 70 18 35 2 4 36 103 99 Yuki Bhambri Karol Beck Chennai 2-Jan-12 Hard First Round 345 101 0 2 ... 1 15 33 13 29 2 3 15 46 43 Yuichi Sugita Olivier Rochus Chennai 2-Jan-12 Hard First Round 235 67 0 2 ... 0 19 32 13 22 1 7 30 78 62 Benoit Paire Pere Riba Chennai 2-Jan-12 Hard First Round 95 89 0 2 ... 5 13 20 12 32 0 1 9 44 34 Victor Hanescu Sam Querrey Chennai 2-Jan-12 Hard First Round 90 93 0 2 ... 8 29 41 7 24 1 3 17 57 53 Yen-Hsun Lu Thiemo de Bakker Chennai 2-Jan-12 Hard First Round 82 223 0 2 ... 0 20 32 10 24 1 1 19 57 49 Andreas Beck Vasek Pospisil Chennai 2-Jan-12 Hard First Round 98 119 0 2 ... 3 39 57 16 38 1 5 24 74 79 Ivan Dodig Vishnu Vardhan Chennai 2-Jan-12 Hard First Round 36 313 0 2 ... 5 41 59 13 27 2 8 34 101 88 David Goffin Xavier Malisse Chennai 2-Jan-12 Hard First Round 174 49 0 2 ... 1 31 43 19 34 1 4 27 85 77 David Goffin Andreas Beck Chennai 2-Jan-12 Hard Second Round 174 98 0 2 ... 0 43 71 14 27 2 8 27 82 84 Dudi Sela Benoit Paire Chennai 2-Jan-12 Hard Second Round 83 95 0 2 ... 5 40 58 21 46 1 7 26 87 87 Stan Wawrinka Edouard Roger-Vasselin Chennai 2-Jan-12 Hard Second Round 17 106 0 2 ... 0 43 70 16 34 4 6 28 82 87 Go Soeda Ivan Dodig Chennai 2-Jan-12 Hard Second Round 120 36 0 2 ... 2 31 41 11 28 1 4 23 73 65 Milos Raonic Victor Hanescu Chennai 2-Jan-12 Hard Second Round 31 90 0 2 ... 1 25 38 5 14 0 4 15 56 45 Yuichi Sugita Yen-Hsun Lu Chennai 2-Jan-12 Hard Second Round 235 82 0 2 ... 4 34 45 12 34 2 9 38 93 84 Janko Tipsarevic Yuki Bhambri Chennai 2-Jan-12 Hard Second Round 9 345 0 2 ... 2 12 22 9 17 0 1 8 41 29 Janko Tipsarevic David Goffin Chennai 2-Jan-12 Hard Quarter-finals 9 174 0 2 ... 5 34 51 19 40 1 2 18 67 71 Milos Raonic Dudi Sela Chennai 2-Jan-12 Hard Quarter-finals 31 83 0 2 ... 2 23 31 19 28 0 3 16 69 58 Go Soeda Stan Wawrinka Chennai 2-Jan-12 Hard Quarter-finals 120 17 0 2 ... 4 18 34 13 31 3 7 31 74 62 Nicolas Almagro Yuichi Sugita Chennai 2-Jan-12 Hard Quarter-finals 10 235 0 2 ... 1 36 65 30 40 3 12 45 123 111 Janko Tipsarevic Go Soeda Chennai 2-Jan-12 Hard Semi-finals 9 120 0 2 ... 1 21 33 10 28 1 1 10 44 41 Milos Raonic Nicolas Almagro Chennai 2-Jan-12 Hard Semi-finals 31 10 0 2 ... 0 31 45 8 15 0 3 12 54 51 Milos Raonic Janko Tipsarevic Chennai 2-Jan-12 Hard Finals 31 9 0 2 ... 2 59 83 34 55 0 4 25 113 118 Igor Andreev Adrian Mannarino Brisbane 2-Jan-12 Hard First Round 115 87 0 2 ... 3 24 35 13 25 1 3 21 70 58 Alexandr Dolgopolov Alejandro Falla Brisbane 2-Jan-12 Hard First Round 15 74 0 2 ... 3 16 33 12 25 3 7 33 75 61 Tatsuma Ito Benjamin Mitchell Brisbane 2-Jan-12 Hard First Round 122 227 0 2 ... 6 30 44 7 24 0 2 13 52 50 Kei Nishikori Cedrik-Marcel Stebe Brisbane 2-Jan-12 Hard First Round 25 81 0 2 ... 2 27 49 23 41 3 6 28 75 78 Denis Istomin Florian Mayer Brisbane 2-Jan-12 Hard First Round 73 23 1 1 ... 1 28 38 11 17 0 2 15 56 54 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Malek Jaziri Fernando Verdasco Paris 29-Oct-18 Indoor Hard Second Round 55 27 0 2 ... 6 46 60 16 35 3 13 39 104 101 Alexander Zverev Frances Tiafoe Paris 29-Oct-18 Indoor Hard Second Round 5 44 0 2 ... 4 26 40 16 36 2 10 27 72 69 Dominic Thiem Gilles Simon Paris 29-Oct-18 Indoor Hard Second Round 8 31 0 2 ... 1 13 26 12 25 2 2 23 59 48 Novak Djokovic Joao Sousa Paris 29-Oct-18 Indoor Hard Second Round 2 48 0 2 ... 2 25 35 6 22 1 10 27 74 58 Karen Khachanov Matthew Ebden Paris 29-Oct-18 Indoor Hard Second Round 18 39 1 2 ... 6 8 18 5 20 1 2 10 30 23 John Isner Mikhail Kukushkin Paris 29-Oct-18 Indoor Hard Second Round 9 54 0 2 ... 1 54 80 24 39 0 1 13 90 91 Kevin Anderson Nikoloz Basilashvili Paris 29-Oct-18 Indoor Hard Second Round 6 22 0 2 ... 7 43 54 30 49 0 3 26 106 99 Marin Cilic Philipp Kohlschreiber Paris 29-Oct-18 Indoor Hard Second Round 7 43 0 2 ... 1 19 34 12 20 1 1 17 55 48 Jack Sock Richard Gasquet Paris 29-Oct-18 Indoor Hard Second Round 23 28 0 2 ... 4 18 33 16 29 0 4 19 59 53 Grigor Dimitrov Roberto Bautista Agut Paris 29-Oct-18 Indoor Hard Second Round 10 25 0 2 ... 0 34 48 11 20 2 4 27 76 72 Damir Dzumhur Stefanos Tsitsipas Paris 29-Oct-18 Indoor Hard Second Round 52 16 0 2 ... 3 14 26 15 30 2 2 17 52 46 Dominic Thiem Borna Coric Paris 29-Oct-18 Indoor Hard Third Round 8 13 0 2 ... 1 39 57 16 38 2 2 27 88 82 Novak Djokovic Damir Dzumhur Paris 29-Oct-18 Indoor Hard Third Round 2 52 1 2 ... 4 15 28 7 18 0 0 8 28 30 Alexander Zverev Diego Schwartzman Paris 29-Oct-18 Indoor Hard Third Round 5 19 0 2 ... 2 22 37 12 24 0 4 18 58 52 Roger Federer Fabio Fognini Paris 29-Oct-18 Indoor Hard Third Round 3 14 0 2 ... 6 22 32 15 37 1 5 16 54 53 Marin Cilic Grigor Dimitrov Paris 29-Oct-18 Indoor Hard Third Round 7 10 0 2 ... 1 37 55 14 32 1 5 22 71 73 Karen Khachanov John Isner Paris 29-Oct-18 Indoor Hard Third Round 18 9 0 2 ... 4 67 80 19 38 0 0 17 100 103 Kei Nishikori Kevin Anderson Paris 29-Oct-18 Indoor Hard Third Round 11 6 0 2 ... 1 26 33 11 19 0 0 11 51 48 Jack Sock Malek Jaziri Paris 29-Oct-18 Indoor Hard Third Round 23 55 0 2 ... 6 13 21 10 24 0 0 9 41 32 Karen Khachanov Alexander Zverev Paris 29-Oct-18 Indoor Hard Quarter-finals 18 5 0 2 ... 7 26 47 4 21 1 3 10 36 40 Dominic Thiem Jack Sock Paris 29-Oct-18 Indoor Hard Quarter-finals 8 23 0 2 ... 5 44 59 19 37 2 10 34 97 97 Roger Federer Kei Nishikori Paris 29-Oct-18 Indoor Hard Quarter-finals 3 11 0 2 ... 0 21 37 16 26 0 1 12 56 49 Novak Djokovic Marin Cilic Paris 29-Oct-18 Indoor Hard Quarter-finals 2 7 0 2 ... 0 38 55 11 28 2 5 29 85 78 Karen Khachanov Dominic Thiem Paris 29-Oct-18 Indoor Hard Semi-finals 18 8 0 2 ... 0 19 29 8 26 1 3 15 47 42 Novak Djokovic Roger Federer Paris 29-Oct-18 Indoor Hard Semi-finals 2 3 0 2 ... 2 69 93 25 46 1 2 29 113 123 Karen Khachanov Novak Djokovic Paris 29-Oct-18 Indoor Hard Finals 18 2 0 2 ... 1 30 43 14 28 1 5 20 66 64 Jaume Antoni Munar Clar Frances Tiafoe Milan 5-Nov-18 Indoor Hard NA 76 40 0 3 ... 3 21 29 6 17 0 2 5 46 32 Frances Tiafoe Hubert Hurkacz Milan 5-Nov-18 Indoor Hard NA 40 85 0 3 ... 4 35 48 10 19 1 7 22 78 67 Hubert Hurkacz Jaume Antoni Munar Clar Milan 5-Nov-18 Indoor Hard NA 85 76 0 3 ... 1 43 63 15 35 3 9 29 80 87 Andrey Rublev Liam Caruana Milan 5-Nov-18 Indoor Hard NA 68 NA 0 3 ... 1 28 39 4 14 1 3 18 57 50 As we can see, we have a Winner column, a Loser column, as well as other columns detailing the match details, and other columns which have the stats for that match. As we have a Winner column, if we use the current data structure to train a model we will leak the result. The model will simply learn that the actual winner comes from the Winner column, rather than learning from other features that we can create, such as First Serve % . To avoid this problem, let's reshape the data from wide to long, then shuffle the data. For this, we will define a function, split_winner_loser_columns , which splits the raw dataframe into two dataframes, appends them together, and then shuffles the data. Let's also remove all Grass and Clay matches from our data, as we will be modelling the Australian Open which is a hardcourt surface. Additionally, we will add a few columns, such as Match_Id and Total_Games . These will be useful later. split_winner_loser_columns <- function (df) { # This function splits the raw data into two dataframes and appends them together then shuffles them # This output is a dataframe with only one player's stats on each row (i.e. in long format) # Grab a df with only the Winner's stats winner = df %>% select( - contains( \"Loser\" )) %>% # Select only the Winner columns + extra game info columns as a df rename_at( # Rename all columns containing \"Winner\" to \"Player\" vars(contains( \"Winner\" )), ~ str_replace( . , \"Winner\" , \"Player\" ) ) %>% mutate(Winner = 1 ) # Create a target column # Repeat the process with the loser's stats loser = df %>% select( - contains( \"Winner\" )) %>% rename_at( vars(contains( \"Loser\" )), ~ str_replace( . , \"Loser\" , \"Player\" ) ) %>% mutate(Winner = 0 ) set.seed ( 183 ) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut) # Create a df that appends both the Winner and loser df together combined_df = winner %>% rbind (loser) %>% # Append the loser df to the Winner df slice( sample ( 1: n())) %>% # Randomise row order arrange(Match_Id) %>% # Arrange by Match_Id return () } # Read in men and womens data; randomise the data to avoid result leakage mens = readr :: read_csv( 'data/ATP_matches.csv' , na = \".\" ) %>% filter(Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\" ) %>% # Filter to only use hardcourt games mutate(Match_Id = row_number(), # Add a match ID column to be used as a key Tournament_Date = dmy(Tournament_Date), # Change Tournament to datetime Total_Games = Winner_Games_Won + Loser_Games_Won) %>% # Add a total games played column split_winner_loser_columns() # Change the dataframe from wide to long mens %>% datatable(rownames = FALSE , extensions = 'Scroller' , options = list (dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound(columns = pluck( . , \"x\" , \"data\" ) %>% colnames (), digits =3 ) Player Tournament Tournament_Date Court_Surface Round_Description Player_Rank Retirement_Ind Player_Sets_Won Player_Games_Won Player_Aces ... Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Match_Id Total_Games Winner Eric Prodon Chennai 2012-01-02 Hard First Round 97 0 0 7 2 ... 13 26 1 3 15 49 49 1 19 0 Edouard Roger-Vasselin Chennai 2012-01-02 Hard First Round 106 0 2 12 5 ... 12 19 4 7 25 59 59 1 19 1 Dudi Sela Chennai 2012-01-02 Hard First Round 83 0 2 12 2 ... 11 16 6 14 36 58 61 2 13 1 Fabio Fognini Chennai 2012-01-02 Hard First Round 48 0 0 1 1 ... 5 26 0 1 8 33 30 2 13 0 Frederico Gil Chennai 2012-01-02 Hard First Round 102 0 1 14 5 ... 18 35 2 4 36 103 99 3 33 0 Go Soeda Chennai 2012-01-02 Hard First Round 120 0 2 19 6 ... 19 39 5 11 42 105 109 3 33 1","title":"Exploring the Data"},{"location":"modelling/AusOpenRTutorial/#feature-creation","text":"Now that we have a fairly good understanding of what the data looks like, let's add some features. To do this we will define a function. Ideally we want to add features which will provide predictive power to our model. Thinking about the dynamics of tennis, we know that players often will matches by \"breaking\" the opponent's serve (i.e. winning a game when the opponent is serving). This is especially important in mens tennis. Let's create a feature called F_Player_BreakPoints_Per_Game , which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let's also create a feature called F_Player_Return_Win_Ratio which is the proportion of points won when returning. Similarly, \"holding\" serve is important (i.e. winning a game when you are serving). Let's create a feature called F_Player_Serve_Win_Ratio which is the proportion of points won when serving. Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let's create a feature called F_Player_Game_Win_Percentage which is the propotion of games that a player wins. add_ratio_features <- function (df) { # This function adds ratio features to a long df df %>% mutate( # Point Win ratio when serving F_Player_Serve_Win_Ratio = (Player_FirstServes_Won + Player_SecondServes_Won - Player_DoubleFaults) / (Player_FirstServes_In + Player_SecondServes_In + Player_DoubleFaults), # Point win ratio when returning F_Player_Return_Win_Ratio = Player_ReturnPoints_Won / Player_ReturnPoints_Faced, # Breakpoints per receiving game F_Player_BreakPoints_Per_Game = Player_BreakPoints / Total_Games, F_Player_Game_Win_Percentage = Player_Games_Won / Total_Games ) %>% mutate_at( vars( colnames ( . ), - contains( \"Rank\" ), - Tournament_Date), # Replace all NAs with0 apart from Rank, Date ~ ifelse ( is.na ( . ), 0 , . ) ) %>% return () } mens = mens %>% add_ratio_features() # Add features Now that we have added our features, we need to create rolling averages for them. We cannot simply use current match statistics, as they will leak the result to the model. Instead, we need to use past match statistics to predict future matches. Here we will use a rolling mean with a window of 15. If the player hasn't played 15 games, we will instead use a cumulative mean. We will also lag the result so as to not leak the result. This next chunk of code simply takes all the columns starting with F_ and calculates these means. mens = mens %>% group_by(Player) %>% # Group by player mutate_at( # Create a rolling mean with window 15 for each player. vars(starts_with( \"F_\" )), # If the player hasn't played 15 games, use a cumulative mean ~ coalesce(rollmean( . , k = 15 , align = \"right\" , fill = NA_real_ ), cummean( . )) %>% lag() ) %>% ungroup()","title":"Feature Creation"},{"location":"modelling/AusOpenRTutorial/#creating-a-training-feature-matrix","text":"In predictive modelling language - features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options We can train the model on every tennis match in the data set, or We can only train the model on Australian Open matches. Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure. Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on. We have decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface - hard court. However, we also need to train our model in the same way that will be used to predict the 2019 Australian Open. When predicting the 2 nd round, we won't have data from the 1 st round. So we will need to build our training feature matrix with this in mind. We should extract features for a player from past games at the start of the tournament and apply them to every matchup that that player plays. To do this, we will create a function, extract_latest_features_for_tournament , which maps over our feature dataframe for the dates in the first round of a tournament and grabs features. First, we need the Australian Open and US Open results - let's grab these and then apply our function. # Get Australian Open and US Open Results aus_us_open_results = mens %>% filter((Tournament == \"Australian Open, Melbourne\" | Tournament == \"U.S. Open, New York\" ) & Round_Description != \"Qualifying\" & Tournament_Date != \"2012-01-16\" ) %>% # Filter out qualifiers select(Match_Id, Player, Tournament, Tournament_Date, Round_Description, Winner) # Create a function which extracts features for each tournament extract_latest_features_for_tournament = function (df, dte) { df %>% # Filter for the 1st round filter(Tournament_Date == dte, Round_Description == \"First Round\" , Tournament_Date != \"2012-01-16\" ) %>% group_by(Player) %>% # Group by player select_at( vars(Match_Id, starts_with( \"F_\" ), Player_Rank) # Grab the players' features ) %>% rename(F_Player_Rank = Player_Rank) %>% ungroup() %>% mutate(Feature_Date = dte) %>% select(Player, Feature_Date, everything()) } # Create a feature matrix in long format feature_matrix_long = aus_us_open_results %>% distinct(Tournament_Date) %>% # Pull all Tournament Dates pull() %>% map_dfr( ~ extract_latest_features_for_tournament(mens, . ) # Get the features ) %>% filter(Feature_Date != \"2012-01-16\" ) %>% # Filter out the first Aus Open mutate_at( # Replace NAs with the mean vars(starts_with( \"F_\" )), ~ ifelse ( is.na ( . ), mean ( . , na.rm = TRUE ), . ) ) Now that we have a feature matrix in long format, we need to convert it to wide format so that the features are on the same row. To do this we will define a function gather_df , which converts the dataframe from long to wide. Let's also join the results to the matrix and convert the Winner column to a factor. Finally, we will take the difference of player1 and player2's features, so as to reduce the dimensionality of the model. gather_df <- function (df) { # This function puts the df back into its original format of each row containing stats for both players df %>% arrange(Match_Id) %>% filter(row_number() %% 2 != 0 ) %>% # Filter for every 2nd row, starting at the 1st index. e.g. 1, 3, 5 rename_at( # Rename columns to player_1 vars(contains( \"Player\" )), ~ str_replace( . , \"Player\" , \"player_1\" ) ) %>% inner_join(df %>% filter(row_number() %% 2 == 0 ) %>% rename_at( vars(contains( \"Player\" )), # Rename columns to player_2 ~ str_replace( . , \"Player\" , \"player_2\" ) ) %>% select(Match_Id, contains( \"Player\" )), by = c ( 'Match_Id' ) ) %>% select(Match_Id, player_1, player_2, Winner, everything()) %>% return () } # Joining results to features feature_matrix_wide = aus_us_open_results %>% inner_join(feature_matrix_long %>% select( - Match_Id), by = c ( \"Player\" , \"Tournament_Date\" = \"Feature_Date\" )) %>% gather_df() %>% mutate( F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio, F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio, F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage, F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game, F_Rank_Diff = (F_player_1_Rank - F_player_2_Rank), Winner = as.factor (Winner) ) %>% select(Match_Id, player_1, player_2, Tournament, Tournament_Date, Round_Description, Winner, contains( \"Diff\" )) train = feature_matrix_wide train %>% datatable(rownames = FALSE , extensions = 'Scroller' , options = list (dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound(columns = pluck( . , \"x\" , \"data\" ) %>% colnames (), digits =3 ) Match_Id player_1 player_2 Tournament Tournament_Date Round_Description Winner F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff 1139 Adrian Ungur Daniel Brands U.S. Open, New York 2012-08-27 First Round 0 0.03279412 -0.014757229 0.002877458 0.073938088 -13 1140 Albert Montanes Richard Gasquet U.S. Open, New York 2012-08-27 First Round 0 -0.08000322 -0.077451342 -0.131108056 -0.180846832 97 1141 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 First Round 1 0.07711693 -0.044715517 0.068179841 -0.087361962 1 1142 Alex Bogomolov Jr. Andy Murray U.S. Open, New York 2012-08-27 First Round 0 -0.03964074 -0.031700826 -0.059010072 -0.094721700 69 1143 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 First Round 1 -0.02681392 0.006442134 -0.067779660 -0.009930089 151 1144 Ryan Harrison Benjamin Becker U.S. Open, New York 2012-08-27 First Round 1 0.04251983 0.018604623 0.026486753 -0.003548973 -24","title":"Creating a Training Feature Matrix"},{"location":"modelling/AusOpenRTutorial/#creating-the-feature-matrix-for-the-2019-australian-open","text":"Now that we have our training set, train , we need to create a feature matrix to create predictions on. To do this, we need to generate features again. We could simply append a player list to our raw dataframe, create a mock date and then use the extract_latest_features_for_tournament function that we used before. Instead, we're going to create a lookup table for each unique player in the 2019 Australian Open. We will need to get their last 15 games and then find the mean for each feature so that our features are the same. Let's first explore what the dummy submission file looks like, then use it to get the unique players. read_csv( 'data/men_dummy_submission_file.csv' ) %>% glimpse() As we can see, the dummy submission file contains every potential match up for the Open. This will be updated a few days before the Open starts with the actual players playing. Let's now create the lookup feature table. # Get a vector of unique players in this years' open using the dummy submission file unique_players = read_csv( 'data/men_dummy_submission_file.csv' ) %>% pull(player_1) %>% unique () # Get the last 15 games played for each unique player and find their features lookup_feature_table = read_csv( 'data/ATP_matches.csv' , na = \".\" ) %>% filter(Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\" ) %>% mutate(Match_Id = row_number(), # Add a match ID column to be used as a key Tournament_Date = dmy(Tournament_Date), # Change Tournament to datetime Total_Games = Winner_Games_Won + Loser_Games_Won) %>% # Add a total games played column # clean_missing_data() %>% # Clean missing data split_winner_loser_columns() %>% # Change the dataframe from wide to long add_ratio_features() %>% filter(Player %in% unique_players) %>% group_by(Player) %>% top_n( 15 , Match_Id) %>% summarise( F_Player_Serve_Win_Ratio = mean (F_Player_Serve_Win_Ratio), F_Player_Return_Win_Ratio = mean (F_Player_Return_Win_Ratio), F_Player_BreakPoints_Per_Game = mean (F_Player_BreakPoints_Per_Game), F_Player_Game_Win_Percentage = mean (F_Player_Game_Win_Percentage), F_Player_Rank = last(Player_Rank) ) Now let's create features for every single combination. To do this we'll join our lookup_feature_table to the player_1 and player_2 columns in the dummy_submission_file . # Create feature matrix for the Australian Open for all player 1s features_player_1 = read_csv( 'data/men_dummy_submission_file.csv' ) %>% select(player_1) %>% inner_join(lookup_feature_table, by = c ( \"player_1\" = \"Player\" )) %>% rename(F_player_1_Serve_Win_Ratio = F_Player_Serve_Win_Ratio, F_player_1_Return_Win_Ratio = F_Player_Return_Win_Ratio, F_player_1_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game, F_player_1_Game_Win_Percentage = F_Player_Game_Win_Percentage, F_player_1_Rank = F_Player_Rank) # Create feature matrix for the Australian Open for all player 2s features_player_2 = read_csv( 'data/men_dummy_submission_file.csv' ) %>% select(player_2) %>% inner_join(lookup_feature_table, by = c ( \"player_2\" = \"Player\" )) %>% rename(F_player_2_Serve_Win_Ratio = F_Player_Serve_Win_Ratio, F_player_2_Return_Win_Ratio = F_Player_Return_Win_Ratio, F_player_2_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game, F_player_2_Game_Win_Percentage = F_Player_Game_Win_Percentage, F_player_2_Rank = F_Player_Rank) # Join the two dfs together and subtract features to create Difference features aus_open_2019_features = features_player_1 %>% bind_cols(features_player_2) %>% select(player_1, player_2, everything()) %>% mutate( F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio, F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio, F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage, F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game, F_Rank_Diff = (F_player_1_Rank - F_player_2_Rank) ) %>% select(player_1, player_2, contains( \"Diff\" )) aus_open_2019_features %>% datatable(rownames = FALSE , extensions = 'Scroller' , options = list (dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound(columns = pluck( . , \"x\" , \"data\" ) %>% colnames (), digits =3 ) player_1 player_2 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff Novak Djokovic Rafael Nadal 0.06347805 0.02503802 0.07002382 0.08951024 1 Novak Djokovic Roger Federer 0.06583364 0.03628491 0.07661295 0.15455628 -1 Novak Djokovic Juan Martin del Potro 0.01067079 0.03436023 0.06382353 0.11259979 -2 Novak Djokovic Alexander Zverev 0.11117863 0.03125651 0.11055585 0.08661036 -3 Novak Djokovic Kevin Anderson 0.02132375 0.10449337 0.11184503 0.23684083 -4 Novak Djokovic Marin Cilic 0.08410746 0.02434916 0.07653035 0.08355134 -5","title":"Creating the Feature Matrix for the 2019 Australian Open"},{"location":"modelling/AusOpenRTutorial/#generating-2019-australian-open-predictions","text":"Now that we have our features, we can finally train our model and generate predictions for the 2019 Australian Open. Due to its simplicity, we will use h2o's Auto Machine Learning function h2o.automl . This will train a heap of different models and optimise the hyperparameters, as well as creating stacked ensembles automatically for us. We will use optimising by log loss. First, we must create h2o frames for our training and feature dataframes. Then we will run h2o.automl . Note that we can set the max_runtime_secs parameter. As this is a notebook, I have set it for 30 seconds - but I suggest you give it 10 minutes to create the best model. We can then create our predictions and assign them back to our aus_open_2019_features dataframe. Finally, we will group_by player and find the best player, on average. ## Setup H2O h2o.init(ip = \"localhost\" , port = 54321 , enable_assertions = TRUE , nthreads = 2 , max_mem_size = \"24g\" ) ## Sending file to h2o train_h2o = feature_matrix_wide %>% select(contains( \"Diff\" ), Winner) %>% as.h2o(destination_frame = \"train_h2o\" ) aus_open_2019_features_h2o = aus_open_2019_features %>% select(contains( \"Diff\" )) %>% as.h2o(destination_frame = \"aus_open_2019_features_h2o\" ) ## Running Auto ML mens_model = h2o.automl(y = \"Winner\" , training_frame = train_h2o, max_runtime_secs = 30 , max_models = 100 , stopping_metric = \"logloss\" , sort_metric = \"logloss\" , balance_classes = TRUE , seed = 183 ) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut) ## Predictions on test frame predictions = h2o.predict(mens_model @ leader, aus_open_2019_features_h2o) %>% as.data.frame () aus_open_2019_features $ prob_player_1 = predictions $ p1 aus_open_2019_features $ prob_player_2 = predictions $ p0 h2o.shutdown(prompt = FALSE ) Now let's find the best player by taking the mean of the prediction probability by player. aus_open_2019_features %>% select(player_1, starts_with( \"F_\" ), prob_player_1) %>% group_by(player_1) %>% summarise_all( mean ) %>% arrange(desc(prob_player_1)) %>% datatable(rownames = FALSE , extensions = 'Scroller' , options = list (dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound(columns = pluck( . , \"x\" , \"data\" ) %>% colnames (), digits =3 ) player_1 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff prob_player_1 Novak Djokovic 0.1109364627 0.076150615 0.1483970690 0.17144300 NA 0.8616486 Karen Khachanov 0.0960639298 0.061436164 0.1059967623 0.04544955 NA 0.8339594 Juan Martin del Potro 0.1003931993 0.042025222 0.0847985439 0.05943767 NA 0.8218308 Rafael Nadal 0.0480432305 0.051531252 0.0790179917 0.08181694 NA 0.8032543 Gilles Simon 0.0646937767 0.084843307 0.0901401318 0.08675350 NA 0.7985995 Roger Federer 0.0452014997 0.040992497 0.0725719954 0.01817046 NA 0.7962289 Kei Nishikori 0.0777155934 0.018720226 0.0800648870 0.02740276 NA 0.7843631 Marin Cilic 0.0285413602 0.053017465 0.0736687072 0.08883055 NA 0.7804876 Tomas Berdych 0.0471654691 0.047289449 0.0737401748 0.10584114 NA 0.7739211 Daniil Medvedev 0.0275430665 0.031121856 0.0721948279 0.01803757 NA 0.7543269 Stefanos Tsitsipas 0.0470382377 0.023825850 0.0577628626 0.02105227 NA 0.7511674 Dominic Thiem 0.0258904189 0.032481624 0.0483707080 0.05857158 NA 0.7451547 Alexander Zverev 0.0006199716 0.044811275 0.0380134371 0.08423392 NA 0.7374897 Kyle Edmund 0.0558006240 0.011963627 0.0478850676 0.05142186 NA 0.7304873 Pablo Carreno Busta 0.0321878318 0.029862068 0.0413674481 -0.00229784 NA 0.7302043 Borna Coric 0.0762084129 -0.010097922 0.0413621283 -0.01924267 NA 0.7268124 Kevin Anderson 0.0907358428 -0.027171681 0.0381421997 -0.06362578 NA 0.7260799 David Goffin -0.0034821911 0.037247336 0.0162572061 0.05603565 NA 0.7155908 Fernando Verdasco 0.0229261365 0.032884054 0.0521212576 0.04668854 NA 0.7120831 Roberto Bautista Agut 0.0047641170 0.049939608 0.0218975349 0.07331023 NA 0.7009891 Milos Raonic 0.0849726089 -0.028732182 0.0385944327 -0.08009382 NA 0.6986865 Fabio Fognini -0.0394792678 0.047935185 0.0226546894 0.06213496 NA 0.6982031 Hyeon Chung 0.0042489153 0.047722133 0.0158096386 0.04823304 NA 0.6958943 Jack Sock -0.0099659903 0.026454984 0.0186547428 0.02307214 NA 0.6757770 Diego Schwartzman -0.0317130675 0.032098381 0.0006215006 0.05621187 NA 0.6631067 John Millman 0.0016290285 0.042676556 0.0119857356 0.06228135 NA 0.6603912 Nikoloz Basilashvili -0.0099968609 0.005561102 0.0473876170 0.03661962 NA 0.6602628 John Isner 0.1346946527 -0.070556940 0.0161348609 -0.11425009 NA 0.6598097 Gael Monfils -0.0074254934 0.024286746 0.0295568649 0.04007519 NA 0.6449506 Richard Gasquet 0.0296009556 -0.011382437 0.0013138324 -0.03972967 NA 0.6442043 ... ... ... ... ... ... ... Laslo Djere -0.042300822 -0.0150684095 -0.064667709 -0.0349151578 NA 0.3606923 David Ferrer -0.036179509 0.0532782117 0.012751020 0.0914824480 NA 0.3488057 Bradley Klahn -0.001248083 -0.0444982448 -0.025987040 -0.1181295700 NA 0.3487806 Marcel Granollers -0.031011830 -0.0094056152 -0.049853664 0.0136841358 NA 0.3460035 Ricardas Berankis -0.022557215 -0.0103782963 -0.047937290 -0.0468488990 NA 0.3454980 Radu Albot -0.040829057 0.0076150564 -0.034891704 0.0443672533 NA 0.3420615 Jordan Thompson -0.068554906 0.0261969117 -0.044349181 0.0206636045 NA 0.3358572 Thomas Fabbiano -0.060583307 0.0275756029 -0.025883493 0.0709707306 NA 0.3319778 Roberto Carballes Baena -0.054016396 -0.0091521177 -0.019093050 0.0347187874 NA 0.3312105 Paolo Lorenzi -0.038613500 -0.0212206827 -0.052602703 0.0199474025 NA 0.3299791 Guido Andreozzi -0.038614385 -0.0133763922 0.029549861 0.0636745661 NA 0.3288762 Peter Polansky 0.007461636 -0.0163389196 -0.024034159 -0.0442144260 NA 0.3216756 Ernests Gulbis -0.062827089 -0.0134699552 -0.027633425 -0.0518663252 NA 0.3123511 Thiago Monteiro 0.001235931 -0.0288349103 -0.043831840 -0.0654744344 NA 0.3122069 Casper Ruud 0.016838968 -0.0178511679 0.015234507 0.0219131874 NA 0.3119321 Marco Trungelliti -0.022148774 -0.0005658242 0.048542554 0.1243537739 NA 0.3092636 Jiri Vesely -0.050204009 -0.0351868278 -0.042887646 -0.0160467165 NA 0.3089287 Guillermo Garcia-Lopez -0.090076100 -0.0108663630 -0.048712763 -0.0124446402 NA 0.3080898 Michael Mmoh -0.063802934 -0.0079053251 -0.011112236 -0.0332042032 NA 0.2822330 Jason Kubler -0.124758873 -0.0202756806 -0.013998570 0.1020895301 NA 0.2814246 Ruben Bemelmans -0.029036164 -0.0138846550 -0.032256254 -0.0363563402 NA 0.2772185 Bjorn Fratangelo -0.014149222 0.0033574304 -0.019931504 -0.0360199607 NA 0.2652527 Pablo Andujar -0.042869833 -0.0488261697 -0.070057834 -0.0164918910 NA 0.2647100 Christian Garin -0.046150875 0.0235799476 -0.006209664 0.0736304057 NA 0.2631607 Ivo Karlovic 0.071597162 -0.1093833837 0.001410787 -0.1237762218 NA 0.2500242 Juan Ignacio Londero -0.026454456 -0.0715665271 -0.016749898 -0.0363353678 NA 0.2351747 Ramkumar Ramanathan -0.005371622 -0.0606138479 -0.041631884 -0.0005573405 NA 0.2272977 Reilly Opelka 0.025704824 -0.0607219257 -0.015474944 -0.0720809006 NA 0.2262993 Carlos Berlocq -0.063580460 0.0074576369 -0.054277974 -0.0165235079 NA 0.2112275 Pedro Sousa -0.197333352 -0.0734557562 -0.161962722 -0.1023311674 NA 0.1502313","title":"Generating 2019 Australian Open Predictions"},{"location":"modelling/EPLmodelPart1/","text":"EPL Machine Learning Walkthrough \u00b6 01. Data Acquisition & Exploration \u00b6 Welcome to the first part of this Machine Learning Walkthrough. This tutorial will be made of two parts; how we actually acquired our data (programmatically) and exploring the data to find potential features to use in the next tutorial . Data Acquisition \u00b6 We will be grabbing our data from football-data.co.uk , which has an enormous amount of soccer data dating back to the 90s. They also generously allow us to use it for free! However, the data is in separate CSVs based on the season. That means we would need to manually download 20 different files if we wanted the past 20 seasons. Rather than do this laborious and boring task, let's create a function which downloads the files for us, and appends them all into one big CSV. To do this, we will use BeautifulSoup, a Python library which helps to pull data from HTML and XML files. We will then define a function which collates all the data for us into one DataFrame. # Import Modules import pandas as pd import requests from bs4 import BeautifulSoup import datetime pd . set_option( 'display.max_columns' , 100 ) import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline from data_preparation_functions import * def grab_epl_data (): # Connect to football-data.co.uk res = requests . get( \"http://www.football-data.co.uk/englandm.php\" ) # Create a BeautifulSoup object soup = BeautifulSoup(res . content, 'lxml' ) # Find the tables with the links to the data in them. table = soup . find_all( 'table' , { 'align' : 'center' , 'cellspacing' : '0' , 'width' : '800' })[ 1 ] body = table . find_all( 'td' , { 'valign' : 'top' })[ 1 ] # Grab the urls for the csv files links = [link . get( 'href' ) for link in body . find_all( 'a' )] links_text = [link_text . text for link_text in body . find_all( 'a' )] data_urls = [] # Create a list of links prefix = 'http://www.football-data.co.uk/' for i, text in enumerate (links_text): if text == 'Premier League' : data_urls . append(prefix + links[i]) # Get rid of last 11 uls as these don't include match stats and odds, and we # only want from 2005 onwards data_urls = data_urls[: -12 ] df = pd . DataFrame() # Iterate over the urls for url in data_urls: # Get the season and make it a column season = url . split( '/' )[ 4 ] print (f \"Getting data for season {season}\" ) # Read the data from the url into a DataFrame temp_df = pd . read_csv(url) temp_df[ 'season' ] = season # Create helpful columns like Day, Month, Year, Date etc. so that our data is clean temp_df = (temp_df . dropna(axis = 'columns' , thresh = temp_df . shape[ 0 ] -30 ) . assign(Day = lambda df: df . Date . str . split( '/' ) . str[ 0 ], Month = lambda df: df . Date . str . split( '/' ) . str[ 1 ], Year = lambda df: df . Date . str . split( '/' ) . str[ 2 ]) . assign(Date = lambda df: df . Month + '/' + df . Day + '/' + df . Year) . assign(Date = lambda df: pd . to_datetime(df . Date)) . dropna()) # Append the temp_df to the main df df = df . append(temp_df, sort = True ) # Drop all NAs df = df . dropna(axis =1 ) . dropna() . sort_values(by = 'Date' ) print ( \"Finished grabbing data.\" ) return df df = grab_epl_data() # df.to_csv(\"data/epl_data.csv\", index=False) Getting data for season 1819 Getting data for season 1718 Getting data for season 1617 Getting data for season 1516 Getting data for season 1415 Getting data for season 1314 Getting data for season 1213 Getting data for season 1112 Getting data for season 1011 Getting data for season 0910 Getting data for season 0809 Getting data for season 0708 Getting data for season 0607 Getting data for season 0506 Finished grabbing data . Whenever we want to update our data (for example if we want the most recent Gameweek included), all we have to do is run that function and then save the data to a csv with the commented out line above. Data Exploration \u00b6 Now that we have our data, let's explore it. Let's first look at home team win rates since 2005 to see if there is a consistent trend. To get an idea of what our data looks like, we'll look at the tail of the dataset first. df . tail( 3 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season 28 3.0 11.0 0.0 9.0 3.0 2.0 Crystal Palace 3.00 3.25 2.60 2.95 3.1 2.55 42.0 20.0 -0.25 1.71 2.13 2.92 1.73 2.16 3.22 2.55 1.79 2.21 3.04 1.77 2.23 3.36 2.66 39.0 2018-08-26 26 E0 1.0 2.0 H 6.0 14.0 0.0 13.0 5.0 0.0 0.0 D 4.0 Watford 2.95 3.20 2.5 2.90 3.1 2.50 08 A Taylor 2.90 3.3 2.6 18 1819 27 5.0 8.0 0.0 15.0 3.0 1.0 Chelsea 1.66 4.00 5.75 1.67 3.8 5.25 42.0 22.0 1.00 1.92 1.88 1.67 2.18 1.71 3.90 5.25 2.01 1.95 1.71 2.28 1.76 4.17 5.75 40.0 2018-08-26 26 E0 2.0 1.0 A 4.0 16.0 0.0 6.0 2.0 0.0 0.0 D 3.0 Newcastle 1.70 3.75 5.0 1.67 3.8 5.25 08 P Tierney 1.67 4.0 5.5 18 1819 29 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.90 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.00 1.76 2.25 3.40 2.67 40.0 2018-08-27 27 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.6 2.75 3.2 2.55 08 C Pawson 2.90 3.3 2.6 18 1819 # Create Home Win, Draw Win and Away Win columns df = df . assign(homeWin = lambda df: df . apply( lambda row: 1 if row . FTHG > row . FTAG else 0 , axis = 'columns' ), draw = lambda df: df . apply( lambda row: 1 if row . FTHG == row . FTAG else 0 , axis = 'columns' ), awayWin = lambda df: df . apply( lambda row: 1 if row . FTHG < row . FTAG else 0 , axis = 'columns' )) Home Ground Advantage \u00b6 win_rates = \\ (df . groupby( 'season' ) . mean() . loc[:, [ 'homeWin' , 'draw' , 'awayWin' ]]) win_rates homeWin draw awayWin season 0506 0.505263 0.202632 0.292105 0607 0.477573 0.258575 0.263852 0708 0.463158 0.263158 0.273684 0809 0.453826 0.255937 0.290237 0910 0.507895 0.252632 0.239474 1011 0.471053 0.292105 0.236842 1112 0.450000 0.244737 0.305263 1213 0.433862 0.285714 0.280423 1314 0.472973 0.208108 0.318919 1415 0.453826 0.245383 0.300792 1516 0.414248 0.282322 0.303430 1617 0.492105 0.221053 0.286842 1718 0.455263 0.260526 0.284211 1819 0.466667 0.200000 0.333333 Findings \u00b6 As we can see, winrates across home team wins, draws and away team wins are very consistent. It seems that the home team wins around 46-47% of the time, the draw happens about 25% of the time, and the away team wins about 27% of the time. Let's plot this DataFrame so that we can see the trend more easily. # Set the style plt . style . use( 'ggplot' ) fig = plt . figure() ax = fig . add_subplot( 111 ) home_line = ax . plot(win_rates . homeWin, label = 'Home Win Rate' ) away_line = ax . plot(win_rates . awayWin, label = 'Away Win Rate' ) draw_line = ax . plot(win_rates . draw, label = 'Draw Win Rate' ) ax . set_xlabel( \"season\" ) ax . set_ylabel( \"Win Rate\" ) plt . title( \"Win Rates\" , fontsize =16 ) # Add the legend locations home_legend = plt . legend(handles = home_line, loc = 'upper right' , bbox_to_anchor = ( 1 , 1 )) ax = plt . gca() . add_artist(home_legend) away_legend = plt . legend(handles = away_line, loc = 'center right' , bbox_to_anchor = ( 0.95 , 0.4 )) ax = plt . gca() . add_artist(away_legend) draw_legend = plt . legend(handles = draw_line, loc = 'center right' , bbox_to_anchor = ( 0.95 , 0.06 )) As we can see, the winrates are relatively stable each season, except for in 14/15 when the home win rate drops dramatically. Out of interest, let's also have a look at which team has the best home ground advantage. Let's define HGA as home win rate - away win rate. And then plot some of the big clubs' HGA against each other. home_win_rates = \\ (df . groupby([ 'HomeTeam' ]) . homeWin . mean()) away_win_rates = \\ (df . groupby([ 'AwayTeam' ]) . awayWin . mean()) hga = (home_win_rates - away_win_rates) . reset_index() . rename(columns = { 0 : 'HGA' }) . sort_values(by = 'HGA' , ascending = False ) hga . head( 10 ) HomeTeam HGA 15 Fulham 0.315573 7 Brighton 0.304762 20 Man City 0.244980 14 Everton 0.241935 30 Stoke 0.241131 10 Charlton 0.236842 0 Arsenal 0.236140 27 Reading 0.234962 33 Tottenham 0.220207 21 Man United 0.215620 So the club with the best HGA is Fulham - interesting. This is most likely because Fulham have won 100% of home games in 2018 so far which is skewing the mean. Let's see how the HGA for some of the big clubs based compare over seasons. big_clubs = [ 'Liverpool' , 'Man City' , 'Man United' , 'Chelsea' , 'Arsenal' ] home_win_rates_5 = df[df . HomeTeam . isin(big_clubs)] . groupby([ 'HomeTeam' , 'season' ]) . homeWin . mean() away_win_rates_5 = df[df . AwayTeam . isin(big_clubs)] . groupby([ 'AwayTeam' , 'season' ]) . awayWin . mean() hga_top_5 = home_win_rates_5 - away_win_rates_5 hga_top_5 . unstack(level =0 ) HomeTeam Arsenal Chelsea Liverpool Man City Man United season 0506 0.421053 0.368421 0.263158 0.263158 0.052632 0607 0.263158 0.000000 0.421053 -0.052632 0.105263 0708 0.210526 -0.052632 0.157895 0.368421 0.368421 0809 0.105263 -0.157895 -0.052632 0.578947 0.210526 0910 0.368421 0.368421 0.421053 0.315789 0.263158 1011 0.157895 0.368421 0.368421 0.263158 0.684211 1112 0.157895 0.315789 -0.105263 0.421053 0.105263 1213 0.052632 0.105263 0.105263 0.248538 0.201754 1314 0.143275 0.251462 0.307018 0.362573 -0.026316 1415 0.131579 0.210526 0.105263 0.210526 0.421053 1516 0.210526 -0.105263 0.000000 0.263158 0.263158 1617 0.263158 0.210526 0.105263 -0.052632 -0.105263 1718 0.578947 0.052632 0.157895 0.000000 0.263158 1819 0.500000 0.000000 0.000000 0.500000 0.500000 Now let's plot it. sns . lineplot(x = 'season' , y = 'HGA' , hue = 'team' , data = hga_top_5 . reset_index() . rename(columns = { 0 : 'HGA' , 'HomeTeam' : 'team' })) plt . legend(loc = 'lower center' , ncol =6 , bbox_to_anchor = ( 0.45 , -0.2 )) plt . title( \"HGA Among the top 5 clubs\" , fontsize =14 ) plt . show() The results here seem to be quite erratic, although it seems that Arsenal consistently has a HGA above 0. Let's now look at the distributions of each of our columns. The odds columns are likely to be highly skewed, so we may have to account for this later. for col in df . select_dtypes( 'number' ) . columns: sns . distplot(df[col]) plt . title(f \"Distribution for {col}\" ) plt . show() Exploring Referee Home Ground Bias \u00b6 What may be of interest is whether certain referees are correlated with the home team winning more often. Let's explore referee home ground bias for referees for the top 10 Referees based on games. print ( 'Overall Home Win Rate: {:.4}%' . format(df . homeWin . mean() * 100 )) # Get the top 10 refs based on games top_10_refs = df . Referee . value_counts() . head( 10 ) . index df[df . Referee . isin(top_10_refs)] . groupby( 'Referee' ) . homeWin . mean() . sort_values(ascending = False ) Overall Home Win Rate: 46.55% Referee L Mason 0.510373 C Foy 0.500000 M Clattenburg 0.480000 M Jones 0.475248 P Dowd 0.469880 M Atkinson 0.469565 M Oliver 0.466019 H Webb 0.456604 A Marriner 0.455516 M Dean 0.442049 Name: homeWin, dtype: float64 It seems that L Mason may be the most influenced by the home crowd. Whilst the overall home win rate is 46.5%, the home win rate when he is the Referee is 51%. However it should be noted that this doesn't mean that he causes the win through bias. It could just be that he referees the best clubs, so naturally their home win rate is high. Variable Correlation With Margin \u00b6 Let's now explore different variables' relationships with margin. First, we'll create a margin column, then we will pick a few different variables to look at the correlations amongst each other, using a correlation heatmap. df[ 'margin' ] = df[ 'FTHG' ] - df[ 'FTAG' ] stat_cols = [ 'AC' , 'AF' , 'AR' , 'AS' , 'AST' , 'AY' , 'HC' , 'HF' , 'HR' , 'HS' , 'HST' , 'HTR' , 'HY' , 'margin' ] stat_correlations = df[stat_cols] . corr() stat_correlations[ 'margin' ] . sort_values() AST -0.345703 AS -0.298665 HY -0.153806 HR -0.129393 AC -0.073204 HF -0.067469 AF 0.005474 AY 0.013746 HC 0.067433 AR 0.103528 HS 0.275847 HST 0.367591 margin 1.000000 Name: margin, dtype: float64 Unsurprisingly, Home Shots on Target correlate the most with Margin, and Away Reds is also high. What is surprising is that Home Yellows has quite a strong negative correlation with margin - this may be because players will play more aggresively when they are losing to try and get the lead back, and hence receive more yellow cards. Let's now look at the heatmap between variables. sns . heatmap(stat_correlations, annot = True , annot_kws = { 'size' : 10 }) < matplotlib . axes . _subplots . AxesSubplot at 0x220a4227048> Analysing Features \u00b6 What we are really interested in, is how our features (creating in the next tutorial), correlate with winning. We will skip ahead here and use a function to create our features for us, and then examine how the moving averages/different features correlate with winning. # Create a cleaned df of all of our data pre_features_df = create_df( 'data/epl_data.csv' ) # Create our features features = create_feature_df(pre_features_df) Creating all games feature DataFrame C:\\Users\\wardj\\Documents\\Betfair Public Github\\predictive - models\\epl\\data_preparation_functions . py: 419 : RuntimeWarning : invalid value encountered in double_scalars . pipe( lambda df: (df . eloAgainst * df[goalsForOrAgainstCol]) . sum() / df . eloAgainst . sum())) Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats, odds and market values into one features DataFrame Complete . features = (pre_features_df . assign(margin = lambda df: df . FTHG - df . FTAG) . loc[:, [ 'gameId' , 'margin' ]] . pipe(pd . merge, features, on = [ 'gameId' ])) features . corr() . margin . sort_values(ascending = False )[: 20 ] margin 1.000000 f_awayOdds 0.413893 f_totalMktH % 0.330420 f_defMktH % 0.325392 f_eloAgainstAway 0.317853 f_eloForHome 0.317853 f_midMktH % 0.316080 f_attMktH % 0.312262 f_sizeOfHandicapAway 0.301667 f_goalsForHome 0.298930 f_wtEloGoalsForHome 0.297157 f_shotsForHome 0.286239 f_cornersForHome 0.279917 f_gkMktH % 0.274732 f_homeWinPc38Away 0.271326 f_homeWinPc38Home 0.271326 f_wtEloGoalsAgainstAway 0.269663 f_goalsAgainstAway 0.258418 f_cornersAgainstAway 0.257148 f_drawOdds 0.256807 Name: margin, dtype: float64 As we can see away odds is most highly correlated to margin. This makes sense, as odds generally have most/all information included in the price. What is interesting is that elo seems to also be highly correlated, which is good news for our elo model that we made. Similarly, weighted goals and the the value of the defence relative to other teams ('defMktH%' etc.) is strongly correlated to margin.","title":"EPL 01. Data acquisition & exploration"},{"location":"modelling/EPLmodelPart1/#epl-machine-learning-walkthrough","text":"","title":"EPL Machine Learning Walkthrough"},{"location":"modelling/EPLmodelPart1/#01-data-acquisition-exploration","text":"Welcome to the first part of this Machine Learning Walkthrough. This tutorial will be made of two parts; how we actually acquired our data (programmatically) and exploring the data to find potential features to use in the next tutorial .","title":"01. Data Acquisition &amp; Exploration"},{"location":"modelling/EPLmodelPart1/#data-acquisition","text":"We will be grabbing our data from football-data.co.uk , which has an enormous amount of soccer data dating back to the 90s. They also generously allow us to use it for free! However, the data is in separate CSVs based on the season. That means we would need to manually download 20 different files if we wanted the past 20 seasons. Rather than do this laborious and boring task, let's create a function which downloads the files for us, and appends them all into one big CSV. To do this, we will use BeautifulSoup, a Python library which helps to pull data from HTML and XML files. We will then define a function which collates all the data for us into one DataFrame. # Import Modules import pandas as pd import requests from bs4 import BeautifulSoup import datetime pd . set_option( 'display.max_columns' , 100 ) import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline from data_preparation_functions import * def grab_epl_data (): # Connect to football-data.co.uk res = requests . get( \"http://www.football-data.co.uk/englandm.php\" ) # Create a BeautifulSoup object soup = BeautifulSoup(res . content, 'lxml' ) # Find the tables with the links to the data in them. table = soup . find_all( 'table' , { 'align' : 'center' , 'cellspacing' : '0' , 'width' : '800' })[ 1 ] body = table . find_all( 'td' , { 'valign' : 'top' })[ 1 ] # Grab the urls for the csv files links = [link . get( 'href' ) for link in body . find_all( 'a' )] links_text = [link_text . text for link_text in body . find_all( 'a' )] data_urls = [] # Create a list of links prefix = 'http://www.football-data.co.uk/' for i, text in enumerate (links_text): if text == 'Premier League' : data_urls . append(prefix + links[i]) # Get rid of last 11 uls as these don't include match stats and odds, and we # only want from 2005 onwards data_urls = data_urls[: -12 ] df = pd . DataFrame() # Iterate over the urls for url in data_urls: # Get the season and make it a column season = url . split( '/' )[ 4 ] print (f \"Getting data for season {season}\" ) # Read the data from the url into a DataFrame temp_df = pd . read_csv(url) temp_df[ 'season' ] = season # Create helpful columns like Day, Month, Year, Date etc. so that our data is clean temp_df = (temp_df . dropna(axis = 'columns' , thresh = temp_df . shape[ 0 ] -30 ) . assign(Day = lambda df: df . Date . str . split( '/' ) . str[ 0 ], Month = lambda df: df . Date . str . split( '/' ) . str[ 1 ], Year = lambda df: df . Date . str . split( '/' ) . str[ 2 ]) . assign(Date = lambda df: df . Month + '/' + df . Day + '/' + df . Year) . assign(Date = lambda df: pd . to_datetime(df . Date)) . dropna()) # Append the temp_df to the main df df = df . append(temp_df, sort = True ) # Drop all NAs df = df . dropna(axis =1 ) . dropna() . sort_values(by = 'Date' ) print ( \"Finished grabbing data.\" ) return df df = grab_epl_data() # df.to_csv(\"data/epl_data.csv\", index=False) Getting data for season 1819 Getting data for season 1718 Getting data for season 1617 Getting data for season 1516 Getting data for season 1415 Getting data for season 1314 Getting data for season 1213 Getting data for season 1112 Getting data for season 1011 Getting data for season 0910 Getting data for season 0809 Getting data for season 0708 Getting data for season 0607 Getting data for season 0506 Finished grabbing data . Whenever we want to update our data (for example if we want the most recent Gameweek included), all we have to do is run that function and then save the data to a csv with the commented out line above.","title":"Data Acquisition"},{"location":"modelling/EPLmodelPart1/#data-exploration","text":"Now that we have our data, let's explore it. Let's first look at home team win rates since 2005 to see if there is a consistent trend. To get an idea of what our data looks like, we'll look at the tail of the dataset first. df . tail( 3 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season 28 3.0 11.0 0.0 9.0 3.0 2.0 Crystal Palace 3.00 3.25 2.60 2.95 3.1 2.55 42.0 20.0 -0.25 1.71 2.13 2.92 1.73 2.16 3.22 2.55 1.79 2.21 3.04 1.77 2.23 3.36 2.66 39.0 2018-08-26 26 E0 1.0 2.0 H 6.0 14.0 0.0 13.0 5.0 0.0 0.0 D 4.0 Watford 2.95 3.20 2.5 2.90 3.1 2.50 08 A Taylor 2.90 3.3 2.6 18 1819 27 5.0 8.0 0.0 15.0 3.0 1.0 Chelsea 1.66 4.00 5.75 1.67 3.8 5.25 42.0 22.0 1.00 1.92 1.88 1.67 2.18 1.71 3.90 5.25 2.01 1.95 1.71 2.28 1.76 4.17 5.75 40.0 2018-08-26 26 E0 2.0 1.0 A 4.0 16.0 0.0 6.0 2.0 0.0 0.0 D 3.0 Newcastle 1.70 3.75 5.0 1.67 3.8 5.25 08 P Tierney 1.67 4.0 5.5 18 1819 29 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.90 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.00 1.76 2.25 3.40 2.67 40.0 2018-08-27 27 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.6 2.75 3.2 2.55 08 C Pawson 2.90 3.3 2.6 18 1819 # Create Home Win, Draw Win and Away Win columns df = df . assign(homeWin = lambda df: df . apply( lambda row: 1 if row . FTHG > row . FTAG else 0 , axis = 'columns' ), draw = lambda df: df . apply( lambda row: 1 if row . FTHG == row . FTAG else 0 , axis = 'columns' ), awayWin = lambda df: df . apply( lambda row: 1 if row . FTHG < row . FTAG else 0 , axis = 'columns' ))","title":"Data Exploration"},{"location":"modelling/EPLmodelPart1/#home-ground-advantage","text":"win_rates = \\ (df . groupby( 'season' ) . mean() . loc[:, [ 'homeWin' , 'draw' , 'awayWin' ]]) win_rates homeWin draw awayWin season 0506 0.505263 0.202632 0.292105 0607 0.477573 0.258575 0.263852 0708 0.463158 0.263158 0.273684 0809 0.453826 0.255937 0.290237 0910 0.507895 0.252632 0.239474 1011 0.471053 0.292105 0.236842 1112 0.450000 0.244737 0.305263 1213 0.433862 0.285714 0.280423 1314 0.472973 0.208108 0.318919 1415 0.453826 0.245383 0.300792 1516 0.414248 0.282322 0.303430 1617 0.492105 0.221053 0.286842 1718 0.455263 0.260526 0.284211 1819 0.466667 0.200000 0.333333","title":"Home Ground Advantage"},{"location":"modelling/EPLmodelPart1/#findings","text":"As we can see, winrates across home team wins, draws and away team wins are very consistent. It seems that the home team wins around 46-47% of the time, the draw happens about 25% of the time, and the away team wins about 27% of the time. Let's plot this DataFrame so that we can see the trend more easily. # Set the style plt . style . use( 'ggplot' ) fig = plt . figure() ax = fig . add_subplot( 111 ) home_line = ax . plot(win_rates . homeWin, label = 'Home Win Rate' ) away_line = ax . plot(win_rates . awayWin, label = 'Away Win Rate' ) draw_line = ax . plot(win_rates . draw, label = 'Draw Win Rate' ) ax . set_xlabel( \"season\" ) ax . set_ylabel( \"Win Rate\" ) plt . title( \"Win Rates\" , fontsize =16 ) # Add the legend locations home_legend = plt . legend(handles = home_line, loc = 'upper right' , bbox_to_anchor = ( 1 , 1 )) ax = plt . gca() . add_artist(home_legend) away_legend = plt . legend(handles = away_line, loc = 'center right' , bbox_to_anchor = ( 0.95 , 0.4 )) ax = plt . gca() . add_artist(away_legend) draw_legend = plt . legend(handles = draw_line, loc = 'center right' , bbox_to_anchor = ( 0.95 , 0.06 )) As we can see, the winrates are relatively stable each season, except for in 14/15 when the home win rate drops dramatically. Out of interest, let's also have a look at which team has the best home ground advantage. Let's define HGA as home win rate - away win rate. And then plot some of the big clubs' HGA against each other. home_win_rates = \\ (df . groupby([ 'HomeTeam' ]) . homeWin . mean()) away_win_rates = \\ (df . groupby([ 'AwayTeam' ]) . awayWin . mean()) hga = (home_win_rates - away_win_rates) . reset_index() . rename(columns = { 0 : 'HGA' }) . sort_values(by = 'HGA' , ascending = False ) hga . head( 10 ) HomeTeam HGA 15 Fulham 0.315573 7 Brighton 0.304762 20 Man City 0.244980 14 Everton 0.241935 30 Stoke 0.241131 10 Charlton 0.236842 0 Arsenal 0.236140 27 Reading 0.234962 33 Tottenham 0.220207 21 Man United 0.215620 So the club with the best HGA is Fulham - interesting. This is most likely because Fulham have won 100% of home games in 2018 so far which is skewing the mean. Let's see how the HGA for some of the big clubs based compare over seasons. big_clubs = [ 'Liverpool' , 'Man City' , 'Man United' , 'Chelsea' , 'Arsenal' ] home_win_rates_5 = df[df . HomeTeam . isin(big_clubs)] . groupby([ 'HomeTeam' , 'season' ]) . homeWin . mean() away_win_rates_5 = df[df . AwayTeam . isin(big_clubs)] . groupby([ 'AwayTeam' , 'season' ]) . awayWin . mean() hga_top_5 = home_win_rates_5 - away_win_rates_5 hga_top_5 . unstack(level =0 ) HomeTeam Arsenal Chelsea Liverpool Man City Man United season 0506 0.421053 0.368421 0.263158 0.263158 0.052632 0607 0.263158 0.000000 0.421053 -0.052632 0.105263 0708 0.210526 -0.052632 0.157895 0.368421 0.368421 0809 0.105263 -0.157895 -0.052632 0.578947 0.210526 0910 0.368421 0.368421 0.421053 0.315789 0.263158 1011 0.157895 0.368421 0.368421 0.263158 0.684211 1112 0.157895 0.315789 -0.105263 0.421053 0.105263 1213 0.052632 0.105263 0.105263 0.248538 0.201754 1314 0.143275 0.251462 0.307018 0.362573 -0.026316 1415 0.131579 0.210526 0.105263 0.210526 0.421053 1516 0.210526 -0.105263 0.000000 0.263158 0.263158 1617 0.263158 0.210526 0.105263 -0.052632 -0.105263 1718 0.578947 0.052632 0.157895 0.000000 0.263158 1819 0.500000 0.000000 0.000000 0.500000 0.500000 Now let's plot it. sns . lineplot(x = 'season' , y = 'HGA' , hue = 'team' , data = hga_top_5 . reset_index() . rename(columns = { 0 : 'HGA' , 'HomeTeam' : 'team' })) plt . legend(loc = 'lower center' , ncol =6 , bbox_to_anchor = ( 0.45 , -0.2 )) plt . title( \"HGA Among the top 5 clubs\" , fontsize =14 ) plt . show() The results here seem to be quite erratic, although it seems that Arsenal consistently has a HGA above 0. Let's now look at the distributions of each of our columns. The odds columns are likely to be highly skewed, so we may have to account for this later. for col in df . select_dtypes( 'number' ) . columns: sns . distplot(df[col]) plt . title(f \"Distribution for {col}\" ) plt . show()","title":"Findings"},{"location":"modelling/EPLmodelPart1/#exploring-referee-home-ground-bias","text":"What may be of interest is whether certain referees are correlated with the home team winning more often. Let's explore referee home ground bias for referees for the top 10 Referees based on games. print ( 'Overall Home Win Rate: {:.4}%' . format(df . homeWin . mean() * 100 )) # Get the top 10 refs based on games top_10_refs = df . Referee . value_counts() . head( 10 ) . index df[df . Referee . isin(top_10_refs)] . groupby( 'Referee' ) . homeWin . mean() . sort_values(ascending = False ) Overall Home Win Rate: 46.55% Referee L Mason 0.510373 C Foy 0.500000 M Clattenburg 0.480000 M Jones 0.475248 P Dowd 0.469880 M Atkinson 0.469565 M Oliver 0.466019 H Webb 0.456604 A Marriner 0.455516 M Dean 0.442049 Name: homeWin, dtype: float64 It seems that L Mason may be the most influenced by the home crowd. Whilst the overall home win rate is 46.5%, the home win rate when he is the Referee is 51%. However it should be noted that this doesn't mean that he causes the win through bias. It could just be that he referees the best clubs, so naturally their home win rate is high.","title":"Exploring Referee Home Ground Bias"},{"location":"modelling/EPLmodelPart1/#variable-correlation-with-margin","text":"Let's now explore different variables' relationships with margin. First, we'll create a margin column, then we will pick a few different variables to look at the correlations amongst each other, using a correlation heatmap. df[ 'margin' ] = df[ 'FTHG' ] - df[ 'FTAG' ] stat_cols = [ 'AC' , 'AF' , 'AR' , 'AS' , 'AST' , 'AY' , 'HC' , 'HF' , 'HR' , 'HS' , 'HST' , 'HTR' , 'HY' , 'margin' ] stat_correlations = df[stat_cols] . corr() stat_correlations[ 'margin' ] . sort_values() AST -0.345703 AS -0.298665 HY -0.153806 HR -0.129393 AC -0.073204 HF -0.067469 AF 0.005474 AY 0.013746 HC 0.067433 AR 0.103528 HS 0.275847 HST 0.367591 margin 1.000000 Name: margin, dtype: float64 Unsurprisingly, Home Shots on Target correlate the most with Margin, and Away Reds is also high. What is surprising is that Home Yellows has quite a strong negative correlation with margin - this may be because players will play more aggresively when they are losing to try and get the lead back, and hence receive more yellow cards. Let's now look at the heatmap between variables. sns . heatmap(stat_correlations, annot = True , annot_kws = { 'size' : 10 }) < matplotlib . axes . _subplots . AxesSubplot at 0x220a4227048>","title":"Variable Correlation With Margin"},{"location":"modelling/EPLmodelPart1/#analysing-features","text":"What we are really interested in, is how our features (creating in the next tutorial), correlate with winning. We will skip ahead here and use a function to create our features for us, and then examine how the moving averages/different features correlate with winning. # Create a cleaned df of all of our data pre_features_df = create_df( 'data/epl_data.csv' ) # Create our features features = create_feature_df(pre_features_df) Creating all games feature DataFrame C:\\Users\\wardj\\Documents\\Betfair Public Github\\predictive - models\\epl\\data_preparation_functions . py: 419 : RuntimeWarning : invalid value encountered in double_scalars . pipe( lambda df: (df . eloAgainst * df[goalsForOrAgainstCol]) . sum() / df . eloAgainst . sum())) Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats, odds and market values into one features DataFrame Complete . features = (pre_features_df . assign(margin = lambda df: df . FTHG - df . FTAG) . loc[:, [ 'gameId' , 'margin' ]] . pipe(pd . merge, features, on = [ 'gameId' ])) features . corr() . margin . sort_values(ascending = False )[: 20 ] margin 1.000000 f_awayOdds 0.413893 f_totalMktH % 0.330420 f_defMktH % 0.325392 f_eloAgainstAway 0.317853 f_eloForHome 0.317853 f_midMktH % 0.316080 f_attMktH % 0.312262 f_sizeOfHandicapAway 0.301667 f_goalsForHome 0.298930 f_wtEloGoalsForHome 0.297157 f_shotsForHome 0.286239 f_cornersForHome 0.279917 f_gkMktH % 0.274732 f_homeWinPc38Away 0.271326 f_homeWinPc38Home 0.271326 f_wtEloGoalsAgainstAway 0.269663 f_goalsAgainstAway 0.258418 f_cornersAgainstAway 0.257148 f_drawOdds 0.256807 Name: margin, dtype: float64 As we can see away odds is most highly correlated to margin. This makes sense, as odds generally have most/all information included in the price. What is interesting is that elo seems to also be highly correlated, which is good news for our elo model that we made. Similarly, weighted goals and the the value of the defence relative to other teams ('defMktH%' etc.) is strongly correlated to margin.","title":"Analysing Features"},{"location":"modelling/EPLmodelPart2/","text":"EPL Machine Learning Walkthrough \u00b6 02. Data Preparation & Feature Engineering \u00b6 Welcome to the second part of this Machine Learning Walkthrough. This tutorial will focus on data preparation and feature creation, before we dive into modelling in the next tutorial . Specifically, this tutorial will cover a few things: Data wrangling specifically for sport Feature creation - focussing on commonly used features in sports modelling, such as exponential moving averages Using functions to modularise the data preparation process Data Wrangling \u00b6 We will begin by utilising functions we have defined in our data_preparation_functions script to wrangle our data into a format that can be consumed by Machine Learning algorithms. A typical issue faced by aspect of modelling sport is the issue of Machine Learning algorithms requiring all features for the teams playing to be on the same row of a table, whereas when we actual calculate these features, we usually require the teams to be on separate rows as it makes it a lot easier to calculate typical features, such as expontentially weighted moving averages . We will explore this issue and show how we deal with issues like these. # Import libraries from data_preparation_functions import * from sklearn.metrics import log_loss from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import StratifiedKFold, cross_val_score import matplotlib.pyplot as plt pd . set_option( 'display.max_columns' , 100 ) We have created some functions which prepare the data for you. For thoroughly commented explanation of how the functions work, read through the data_preparation_functions.py script along side this walkthrough. Essentially, each functions wrangles the data through a similar process. It first reads in the data from a csv file, then converts the columns to datatypes that we can work with, such as converting the Date column to a datetime data type. It then adds a Game ID column, so each game is easily identifiable and joined on. We then assign the DataFrame some other columns which may be useful, such as 'Year', 'Result' and 'homeWin'. Finally, we drop redundant column and return the DataFrame. Let us now create six different DataFrames, which we will use to create features. Later, we will join these features back into one main feature DataFrame. Create 6 distinct DataFrames \u00b6 # This table includes all of our data in one big DataFrame df = create_df( 'data/epl_data.csv' ) df . head( 3 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.0 2.38 8 A Wiley 2.75 3.25 2.4 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.0 2.10 8 M Riley 3.10 3.25 2.2 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.2 3.75 8 G Poll 1.80 3.30 4.5 2005 0506 3 0 1 away # This includes only the typical soccer stats, like home corners, home shots on target etc. stats = create_stats_df( 'data/epl_data.csv' ) stats . head( 3 ) gameId HomeTeam AwayTeam FTHG FTAG HTHG HTAG HS AS HST AST HF AF HC AC HY AY HR AR 0 1 West Ham Blackburn 3.0 1.0 0.0 1.0 13.0 11.0 5.0 5.0 11.0 14.0 2.0 6.0 0.0 1.0 0.0 1.0 1 2 Aston Villa Bolton 2.0 2.0 2.0 2.0 3.0 13.0 2.0 6.0 14.0 16.0 7.0 8.0 0.0 2.0 0.0 0.0 2 3 Everton Man United 0.0 2.0 0.0 1.0 10.0 12.0 5.0 5.0 15.0 14.0 8.0 6.0 3.0 1.0 0.0 0.0 # This includes all of our betting related data, such as win/draw/lose odds, asian handicaps etc. betting = create_betting_df( 'data/epl_data.csv' ) betting . head( 3 ) B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Day Div IWA IWD IWH LBA LBD LBH Month VCA VCD VCH Year homeWin awayWin result HomeTeam AwayTeam gameId 0 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 13 E0 2.7 3.0 2.3 2.75 3.0 2.38 8 2.75 3.25 2.4 2005 1 0 home West Ham Blackburn 1 1 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 13 E0 3.1 3.0 2.1 3.20 3.0 2.10 8 3.10 3.25 2.2 2005 0 0 draw Aston Villa Bolton 2 2 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 13 E0 1.8 3.1 3.8 1.83 3.2 3.75 8 1.80 3.30 4.5 2005 0 1 away Everton Man United 3 # This includes all of the team information for each game. team_info = create_team_info_df( 'data/epl_data.csv' ) team_info . head( 3 ) gameId Date season HomeTeam AwayTeam FTR HTR Referee 0 1 2005-08-13 0506 West Ham Blackburn H A A Wiley 1 2 2005-08-13 0506 Aston Villa Bolton D D M Riley 2 3 2005-08-13 0506 Everton Man United A A G Poll # Whilst the other DataFrames date back to 2005, this DataFrame has data from 2001 to 2005. historic_games = create_historic_games_df( 'data/historic_games_pre2005.csv' ) historic_games . head( 3 ) Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin 0 2001-08-18 Charlton Everton 1 2 -1 20012002 0 1 2001-08-18 Derby Blackburn 2 1 -1 20012002 1 2 2001-08-18 Leeds Southampton 2 0 -1 20012002 1 # This is the historic_games DataFrame appended to the df DataFrame. all_games = create_all_games_df( 'data/epl_data.csv' , 'data/historic_games_pre2005.csv' ) all_games . head( 3 ) Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin awayWin homeWinPc5 homeWinPc38 awayWinPc5 awayWinPc38 gameIdHistoric 0 2001-08-18 Charlton Everton 1.0 2.0 -1 20012002 0 1 NaN NaN NaN NaN 1 1 2001-08-18 Derby Blackburn 2.0 1.0 -1 20012002 1 0 NaN NaN NaN NaN 2 2 2001-08-18 Leeds Southampton 2.0 0.0 -1 20012002 1 0 NaN NaN NaN NaN 3 Feature Creation \u00b6 Now that we have all of our pre-prepared DataFrames, and we know that the data is clean, we can move onto feature creation. As is common practice with sports modelling, we are going to start by creating expontentially weighted moving averages (EMA) as features. To get a better understanding of how EMAs work, read here . In short, an EMA is like a simple moving average, except it weights recent instances more than older instances based on an alpha parameter. The documentation for the pandas (emw method)[ https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html ] we will be using states that we can specify alpha in a number of ways. We will specify it in terms of span, where $\\alpha = 2 / (span+1), span \u2265 1 $. Let's first define a function which calculates the exponential moving average for each column in the stats DataFrame. We will then apply this function with other functions we have created, such as create_betting_features_ema, which creates moving averages of betting data. However, we must first change the structure of our data. Notice that currently each row has both the Home Team's data and the Away Team's data on a single row. This makes it difficult to calculate rolling averages, so we will restructure our DataFrames to ensure each row only contains single team's data. To do this, we will define a function, reate_multiline_df_stats. # Define a function which restructures our DataFrame def create_multiline_df_stats (old_stats_df): # Create a list of columns we want and their mappings to more interpretable names home_stats_cols = [ 'HomeTeam' , 'FTHG' , 'FTAG' , 'HTHG' , 'HTAG' , 'HS' , 'AS' , 'HST' , 'AST' , 'HF' , 'AF' , 'HC' , 'AC' , 'HY' , 'AY' , 'HR' , 'AR' ] away_stats_cols = [ 'AwayTeam' , 'FTAG' , 'FTHG' , 'HTAG' , 'HTHG' , 'AS' , 'HS' , 'AST' , 'HST' , 'AF' , 'HF' , 'AC' , 'HC' , 'AY' , 'HY' , 'AR' , 'HR' ] stats_cols_mapping = [ 'team' , 'goalsFor' , 'goalsAgainst' , 'halfTimeGoalsFor' , 'halfTimeGoalsAgainst' , 'shotsFor' , 'shotsAgainst' , 'shotsOnTargetFor' , 'shotsOnTargetAgainst' , 'freesFor' , 'freesAgainst' , 'cornersFor' , 'cornersAgainst' , 'yellowsFor' , 'yellowsAgainst' , 'redsFor' , 'redsAgainst' ] # Create a dictionary of the old column names to new column names home_mapping = {old_col: new_col for old_col, new_col in zip (home_stats_cols, stats_cols_mapping)} away_mapping = {old_col: new_col for old_col, new_col in zip (away_stats_cols, stats_cols_mapping)} # Put each team onto an individual row multi_line_stats = (old_stats_df[[ 'gameId' ] + home_stats_cols] # Filter for only the home team columns . rename(columns = home_mapping) # Rename the columns . assign(homeGame =1 ) # Assign homeGame=1 so that we can use a general function later . append((old_stats_df[[ 'gameId' ] + away_stats_cols]) # Append the away team columns . rename(columns = away_mapping) # Rename the away team columns . assign(homeGame =0 ), sort = True ) . sort_values(by = 'gameId' ) # Sort the values . reset_index(drop = True )) return multi_line_stats # Define a function which creates an EMA DataFrame from the stats DataFrame def create_stats_features_ema (stats, span): # Create a restructured DataFrames so that we can calculate EMA multi_line_stats = create_multiline_df_stats(stats) # Create a copy of the DataFrame ema_features = multi_line_stats[[ 'gameId' , 'team' , 'homeGame' ]] . copy() # Get the columns that we want to create EMA for feature_names = multi_line_stats . drop(columns = [ 'gameId' , 'team' , 'homeGame' ]) . columns # Loop over the features for feature_name in feature_names: feature_ema = (multi_line_stats . groupby( 'team' )[feature_name] # Calculate the EMA . transform( lambda row: row . ewm(span = span, min_periods =2 ) . mean() . shift( 1 ))) # Shift the data down 1 so we don't leak data ema_features[feature_name] = feature_ema # Add the new feature to the DataFrame return ema_features # Apply the function stats_features = create_stats_features_ema(stats, span =5 ) stats_features . tail() gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9903 4952 Newcastle 1 4.301743 4.217300 11.789345 12.245066 0.797647 0.833658 0.644214 0.420832 2.323450e-10 3.333631e-01 11.335147 13.265955 3.211345 4.067990 1.848860 1.627140 9904 4953 Burnley 0 4.880132 5.165915 13.326703 8.800033 1.945502 0.667042 0.609440 0.529409 3.874405e-03 3.356120e-10 13.129631 10.642381 4.825874 3.970285 0.963527 0.847939 9905 4953 Fulham 1 4.550255 4.403060 10.188263 8.555589 2.531046 1.003553 0.860573 0.076949 1.002518e-04 8.670776e-03 17.463779 12.278877 8.334019 4.058213 0.980097 1.102974 9906 4954 Man United 1 3.832573 4.759683 11.640608 10.307946 1.397234 1.495032 1.034251 0.809280 6.683080e-05 1.320468e-05 8.963022 10.198642 3.216957 3.776900 1.040077 1.595650 9907 4954 Tottenham 0 3.042034 5.160211 8.991460 9.955635 1.332704 2.514789 0.573728 1.010491 4.522878e-08 1.354409e-05 12.543406 17.761004 3.757437 7.279845 1.478976 1.026601 As we can see, we now have averages for each team. Let's create a quick table to see the top 10 teams' goalsFor average EMAs since 2005. pd . DataFrame(stats_features . groupby( 'team' ) . goalsFor . mean() . sort_values(ascending = False )[: 10 ]) goalsFor team Man United 1.895026 Chelsea 1.888892 Arsenal 1.876770 Man City 1.835863 Liverpool 1.771125 Tottenham 1.655063 Leicester 1.425309 Blackpool 1.390936 Everton 1.387110 Southampton 1.288349 Optimising Alpha \u00b6 It looks like Man United and Chelsea have been two of the best teams since 2005, based on goalsFor. Now that we have our stats features, we may be tempted to move on. However, we have arbitrarily chosen a span of 5. How do we know that this is the best value? We don't. Let's try and optimise this value. To do this, we will use a simple Logistic Regression model to create probabilistic predictions based on the stats features we created before. We will iterate a range of span values, from say, 3 to 15, and choose the value which produces a model with the lowest log loss, based on cross validation. To do this, we need to restructure our DataFrame back to how it was before. def restructure_stats_features (stats_features): non_features = [ 'homeGame' , 'team' , 'gameId' ] stats_features_restructured = (stats_features . query( 'homeGame == 1' ) . rename(columns = {col: 'f_' + col + 'Home' for col in stats_features . columns if col not in non_features}) . rename(columns = { 'team' : 'HomeTeam' }) . pipe(pd . merge, (stats_features . query( 'homeGame == 0' ) . rename(columns = { 'team' : 'AwayTeam' }) . rename(columns = {col: 'f_' + col + 'Away' for col in stats_features . columns if col not in non_features})), on = [ 'gameId' ]) . pipe(pd . merge, df[[ 'gameId' , 'result' ]], on = 'gameId' ) . dropna()) return stats_features_restructured restructure_stats_features(stats_features) . head() gameId HomeTeam homeGame_x f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome AwayTeam homeGame_y f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway result 20 21 Birmingham 1 4.8 7.8 12.0 9.4 1.2 0.6 0.6 0.6 0.0 0.0 11.4 8.2 6.4 2.8 1.0 2.6 Middlesbrough 0 3.0 5.6 14.0 12.8 1.2 0.0 0.0 0.0 0.0 0.4 17.2 8.8 7.6 2.6 3.0 1.4 away 21 22 Portsmouth 1 2.6 4.6 21.8 16.6 2.0 0.6 1.0 0.0 0.0 0.0 8.0 10.4 3.6 4.0 3.2 1.8 Aston Villa 0 9.8 7.0 14.2 18.2 1.4 0.8 0.8 0.8 0.0 0.0 16.0 3.0 9.6 2.6 2.0 0.6 draw 22 23 Sunderland 1 5.0 5.0 11.6 18.0 1.8 0.4 1.0 0.4 0.4 0.6 14.6 6.0 5.2 3.2 1.2 2.6 Man City 0 7.8 3.6 8.6 12.4 0.6 1.2 0.6 0.6 0.0 0.0 10.6 11.4 2.4 6.8 3.0 1.4 away 23 24 Arsenal 1 3.0 7.4 17.0 18.6 0.6 0.8 0.0 0.0 0.4 0.0 6.2 11.4 4.0 6.6 1.6 1.8 Fulham 0 7.2 3.0 20.8 13.2 1.2 0.6 0.6 0.0 0.0 0.0 12.4 10.8 7.0 5.2 2.0 1.6 home 24 25 Blackburn 1 1.4 7.2 12.8 21.2 1.8 1.6 0.0 1.0 0.0 0.4 10.0 14.0 4.4 7.4 1.2 1.6 Tottenham 0 6.4 3.8 11.2 18.8 0.0 2.0 0.0 0.4 0.0 0.0 11.6 15.2 4.6 7.2 0.6 2.6 draw Now let's write a function that optimises our span based on log loss of the output of a Logistic Regression model. def optimise_alpha (features): le = LabelEncoder() y = le . fit_transform(features . result) # Encode the result from away, draw, home win to 0, 1, 2 X = features[[col for col in features . columns if col . startswith( 'f_' )]] # Only get the features - these all start with f_ lr = LogisticRegression() kfold = StratifiedKFold(n_splits =5 ) ave_cv_score = cross_val_score(lr, X, y, scoring = 'neg_log_loss' , cv = kfold) . mean() return ave_cv_score best_score = np . float( 'inf' ) best_span = 0 cv_scores = [] # Iterate over a range of spans for span in range ( 1 , 120 , 3 ): stats_features = create_stats_features_ema(stats, span = span) restructured_stats_features = restructure_stats_features(stats_features) cv_score = optimise_alpha(restructured_stats_features) cv_scores . append(cv_score) if cv_score * -1 < best_score: best_score = cv_score * -1 best_span = span plt . style . use( 'ggplot' ) plt . plot( list ( range ( 1 , 120 , 3 )), (pd . Series(cv_scores) *-1 )) # Plot our results plt . title( \"Optimising alpha\" ) plt . xlabel( \"Span\" ) plt . ylabel( \"Log Loss\" ) plt . show() print ( \"Our lowest log loss ({:2f}) occurred at a span of {}\" . format(best_score, best_span)) Our lowest log loss (0.980835) occurred at a span of 55 The above method is just an example of how you can optimise hyparameters. Obviously this example has many limitations, such as attempting to optimise each statistic with the same alpha. However, for the rest of these tutorial series we will use this span value. Now let's create the rest of our features. For thorough explanations and the actual code behind some of the functions used, please refer to the data_preparation_functions.py script. Creating our Features DataFrame \u00b6 We will utilise pre-made functions to create all of our features in just a few lines of code. As part of this process we will create features which include margin weighted elo, an exponential average for asian handicap data, and odds as features. Our Elo function is essentially the same as the one we created in the AFL tutorial; if you would like to know more about Elo models please read this article. Note that the cell below may take a few minutes to run. # Create feature DataFrames features_all_games = create_all_games_features(all_games) C:\\Users\\wardj\\Documents\\Betfair Public Github\\predictive-models\\epl\\data_preparation_functions.py:419: RuntimeWarning: invalid value encountered in double_scalars .pipe(lambda df: (df.eloAgainst * df[goalsForOrAgainstCol]).sum() / df.eloAgainst.sum())) The features_all_games df includes elo for each team, as well as their win percentage at home and away over the past 5 and 38 games. For more information on how it was calculated, read through the data_preparation_functions script. features_all_games . head( 3 ) Date awayWin awayWinPc38 awayWinPc5 eloAgainst eloFor gameId gameIdHistoric goalsAgainst goalsFor homeGame homeWin homeWinPc38 homeWinPc5 season team wtEloGoalsFor wtEloGoalsAgainst 0 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 2.0 1.0 1 0 NaN NaN 20012002 Charlton NaN NaN 1 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 1.0 2.0 0 0 NaN NaN 20012002 Everton NaN NaN 2 2001-08-18 0 NaN NaN 1500.0 1500.0 -1 2 1.0 2.0 1 1 NaN NaN 20012002 Derby NaN NaN The features_stats df includes all the expontential weighted averages for each stat in the stats df. # Create feature stats df features_stats = create_stats_features_ema(stats, span = best_span) features_stats . tail( 3 ) gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9905 4953 Fulham 1 6.006967 5.045733 10.228997 9.965651 2.147069 1.093550 0.630485 0.364246 0.032937 0.043696 16.510067 11.718122 7.184386 4.645762 1.310424 1.389716 9906 4954 Man United 1 4.463018 5.461075 11.605712 10.870367 0.843222 1.586308 0.427065 0.730650 0.042588 0.027488 10.865754 13.003121 3.562675 4.626450 1.740735 1.712785 9907 4954 Tottenham 0 3.868619 6.362901 10.784145 10.140388 0.954928 2.100166 0.439129 0.799968 0.024351 0.026211 9.947515 16.460598 3.370010 6.136120 1.925005 1.364268 The features_odds df includes a moving average of some of the odds data. # Create feature_odds df features_odds = create_betting_features_ema(betting, span =10 ) features_odds . tail( 3 ) gameId team avAsianHandicapOddsAgainst avAsianHandicapOddsFor avgreaterthan2.5 avlessthan2.5 sizeOfHandicap 9905 4953 Fulham 1.884552 1.985978 1.756776 2.128261 0.502253 9906 4954 Man United 1.871586 2.031787 1.900655 1.963478 -0.942445 9907 4954 Tottenham 1.947833 1.919607 1.629089 2.383593 -1.235630 The features market values has market values and the % of total market for each position. These values are in millions. # Create feature market values df features_market_values = create_market_values_features(df) # This creates a df with one game per row features_market_values . head( 3 ) gameId Year HomeTeam AwayTeam defMktValH attMktValH gkMktValH totalMktValH midMktValH defMktValA attMktValA gkMktValA totalMktValA midMktValA attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% 0 1 2005 West Ham Blackburn 16.90 18.50 6.40 46.40 4.60 27.25 13.00 3.25 70.70 27.20 2.252911 1.583126 0.588168 3.477861 2.486940 4.010007 4.524247 2.297469 1.913986 2.916354 1 2 2005 Aston Villa Bolton 27.63 31.85 7.60 105.83 38.75 9.60 24.55 8.50 72.40 29.75 3.878659 2.989673 4.954673 3.803910 4.065926 1.412700 5.372543 6.008766 4.365456 2.986478 2 3 2005 Everton Man United 44.35 31.38 8.55 109.78 25.50 82.63 114.60 9.25 288.48 82.00 3.821423 13.955867 3.260494 10.484727 6.526378 12.159517 6.044111 6.538951 4.528392 11.899714 all_games_cols = [ 'Date' , 'gameId' , 'team' , 'season' , 'homeGame' , 'homeWinPc38' , 'homeWinPc5' , 'awayWinPc38' , 'awayWinPc5' , 'eloFor' , 'eloAgainst' , 'wtEloGoalsFor' , 'wtEloGoalsAgainst' ] # Join the features together features_multi_line = (features_all_games[all_games_cols] . pipe(pd . merge, features_stats . drop(columns = 'homeGame' ), on = [ 'gameId' , 'team' ]) . pipe(pd . merge, features_odds, on = [ 'gameId' , 'team' ])) # Put each instance on an individual row features_with_na = put_features_on_one_line(features_multi_line) market_val_feature_names = [ 'attMktH%' , 'attMktA%' , 'midMktH%' , 'midMktA%' , 'defMktH%' , 'defMktA%' , 'gkMktH%' , 'gkMktA%' , 'totalMktH%' , 'totalMktA%' ] # Merge our team values dataframe to features and result from df features_with_na = (features_with_na . pipe(pd . merge, (features_market_values[market_val_feature_names + [ 'gameId' ]]) . rename({col: 'f_' + col for col in market_val_feature_names}), on = 'gameId' ) . pipe(pd . merge, df[[ 'HomeTeam' , 'AwayTeam' , 'gameId' , 'result' , 'B365A' , 'B365D' , 'B365H' ]], on = [ 'HomeTeam' , 'AwayTeam' , 'gameId' ])) # Drop NAs from calculating the rolling averages - don't drop Win Pc 38 and Win Pc 5 columns features = features_with_na . dropna(subset = features_with_na . drop(columns = [col for col in features_with_na . columns if 'WinPc' in col]) . columns) # Fill NAs for the Win Pc columns features = features . fillna(features . mean()) features . head( 3 ) Date gameId HomeTeam season homeGame f_homeWinPc38Home f_homeWinPc5Home f_awayWinPc38Home f_awayWinPc5Home f_eloForHome f_eloAgainstHome f_wtEloGoalsForHome f_wtEloGoalsAgainstHome f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome f_avAsianHandicapOddsAgainstHome f_avAsianHandicapOddsForHome f_avgreaterthan2.5Home f_avlessthan2.5Home f_sizeOfHandicapHome AwayTeam f_homeWinPc38Away f_homeWinPc5Away f_awayWinPc38Away f_awayWinPc5Away f_eloForAway f_eloAgainstAway f_wtEloGoalsForAway f_wtEloGoalsAgainstAway f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway f_avAsianHandicapOddsAgainstAway f_avAsianHandicapOddsForAway f_avgreaterthan2.5Away f_avlessthan2.5Away f_sizeOfHandicapAway attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% result B365A B365D B365H 20 2005-08-23 21 Birmingham 0506 1 0.394737 0.4 0.263158 0.2 1478.687038 1492.866048 1.061763 1.260223 4.981818 7.527273 12.000000 9.945455 1.018182 0.509091 0.509091 0.509091 0.000000 0.000000 11.945455 8.018182 6.490909 2.981818 1.000000 2.509091 1.9090 1.9455 2.0510 1.6735 -0.1375 Middlesbrough 0.394737 0.4 0.263158 0.2 1492.866048 1478.687038 1.12994 1.279873 2.545455 5.509091 13.545455 13.436364 1.018182 0.000000 0.000000 0.000000 0.0 0.490909 17.018182 8.072727 7.509091 2.509091 3.0 1.490909 1.9395 1.9095 2.0035 1.7155 0.3875 5.132983 5.260851 3.341048 4.289788 3.502318 4.168935 2.332815 3.216457 3.934396 4.522205 away 2.75 3.2 2.50 21 2005-08-23 22 Portsmouth 0506 1 0.447368 0.4 0.263158 0.4 1405.968416 1489.229314 1.147101 1.503051 2.509091 4.963636 21.981818 16.054545 2.000000 0.509091 1.000000 0.000000 0.000000 0.000000 8.454545 10.490909 3.963636 4.454545 3.018182 1.527273 1.8965 1.9690 2.0040 1.7005 0.2500 Aston Villa 0.447368 0.4 0.263158 0.4 1489.229314 1405.968416 1.17516 1.263229 9.527273 7.000000 14.472727 17.563636 1.490909 0.981818 0.981818 0.981818 0.0 0.000000 15.545455 3.000000 9.054545 2.509091 2.0 0.509091 1.8565 1.9770 1.8505 1.8485 0.7125 3.738614 3.878659 4.494368 4.954673 2.884262 4.065926 3.746642 5.372543 3.743410 4.365456 draw 2.75 3.2 2.50 22 2005-08-23 23 Sunderland 0506 1 0.236842 0.0 0.236842 0.4 1277.888970 1552.291880 0.650176 1.543716 5.000000 5.000000 12.418182 17.545455 1.981818 0.490909 1.000000 0.490909 0.490909 0.509091 14.509091 6.909091 5.018182 3.927273 1.018182 2.509091 1.8520 1.9915 1.8535 1.8500 0.7125 Man City 0.236842 0.0 0.236842 0.4 1552.291880 1277.888970 1.28875 1.287367 7.527273 3.509091 8.963636 12.490909 0.509091 1.018182 0.509091 0.509091 0.0 0.000000 10.963636 11.945455 2.490909 6.981818 3.0 1.490909 1.8150 2.0395 2.0060 1.7095 -0.2000 0.706318 3.750792 1.476812 1.070209 2.634096 4.455890 0.777605 4.913050 1.499427 3.151477 away 2.50 3.2 2.75 We now have a features DataFrame ready, with all the feature columns beginning with the \"f_\". In the next tutorial, we will walk through the modelling process to try and find the best type of model to use.","title":"EPL 02. Data preparation & feature engineering"},{"location":"modelling/EPLmodelPart2/#epl-machine-learning-walkthrough","text":"","title":"EPL Machine Learning Walkthrough"},{"location":"modelling/EPLmodelPart2/#02-data-preparation-feature-engineering","text":"Welcome to the second part of this Machine Learning Walkthrough. This tutorial will focus on data preparation and feature creation, before we dive into modelling in the next tutorial . Specifically, this tutorial will cover a few things: Data wrangling specifically for sport Feature creation - focussing on commonly used features in sports modelling, such as exponential moving averages Using functions to modularise the data preparation process","title":"02. Data Preparation &amp; Feature Engineering"},{"location":"modelling/EPLmodelPart2/#data-wrangling","text":"We will begin by utilising functions we have defined in our data_preparation_functions script to wrangle our data into a format that can be consumed by Machine Learning algorithms. A typical issue faced by aspect of modelling sport is the issue of Machine Learning algorithms requiring all features for the teams playing to be on the same row of a table, whereas when we actual calculate these features, we usually require the teams to be on separate rows as it makes it a lot easier to calculate typical features, such as expontentially weighted moving averages . We will explore this issue and show how we deal with issues like these. # Import libraries from data_preparation_functions import * from sklearn.metrics import log_loss from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import StratifiedKFold, cross_val_score import matplotlib.pyplot as plt pd . set_option( 'display.max_columns' , 100 ) We have created some functions which prepare the data for you. For thoroughly commented explanation of how the functions work, read through the data_preparation_functions.py script along side this walkthrough. Essentially, each functions wrangles the data through a similar process. It first reads in the data from a csv file, then converts the columns to datatypes that we can work with, such as converting the Date column to a datetime data type. It then adds a Game ID column, so each game is easily identifiable and joined on. We then assign the DataFrame some other columns which may be useful, such as 'Year', 'Result' and 'homeWin'. Finally, we drop redundant column and return the DataFrame. Let us now create six different DataFrames, which we will use to create features. Later, we will join these features back into one main feature DataFrame.","title":"Data Wrangling"},{"location":"modelling/EPLmodelPart2/#create-6-distinct-dataframes","text":"# This table includes all of our data in one big DataFrame df = create_df( 'data/epl_data.csv' ) df . head( 3 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.0 2.38 8 A Wiley 2.75 3.25 2.4 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.0 2.10 8 M Riley 3.10 3.25 2.2 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.2 3.75 8 G Poll 1.80 3.30 4.5 2005 0506 3 0 1 away # This includes only the typical soccer stats, like home corners, home shots on target etc. stats = create_stats_df( 'data/epl_data.csv' ) stats . head( 3 ) gameId HomeTeam AwayTeam FTHG FTAG HTHG HTAG HS AS HST AST HF AF HC AC HY AY HR AR 0 1 West Ham Blackburn 3.0 1.0 0.0 1.0 13.0 11.0 5.0 5.0 11.0 14.0 2.0 6.0 0.0 1.0 0.0 1.0 1 2 Aston Villa Bolton 2.0 2.0 2.0 2.0 3.0 13.0 2.0 6.0 14.0 16.0 7.0 8.0 0.0 2.0 0.0 0.0 2 3 Everton Man United 0.0 2.0 0.0 1.0 10.0 12.0 5.0 5.0 15.0 14.0 8.0 6.0 3.0 1.0 0.0 0.0 # This includes all of our betting related data, such as win/draw/lose odds, asian handicaps etc. betting = create_betting_df( 'data/epl_data.csv' ) betting . head( 3 ) B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Day Div IWA IWD IWH LBA LBD LBH Month VCA VCD VCH Year homeWin awayWin result HomeTeam AwayTeam gameId 0 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 13 E0 2.7 3.0 2.3 2.75 3.0 2.38 8 2.75 3.25 2.4 2005 1 0 home West Ham Blackburn 1 1 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 13 E0 3.1 3.0 2.1 3.20 3.0 2.10 8 3.10 3.25 2.2 2005 0 0 draw Aston Villa Bolton 2 2 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 13 E0 1.8 3.1 3.8 1.83 3.2 3.75 8 1.80 3.30 4.5 2005 0 1 away Everton Man United 3 # This includes all of the team information for each game. team_info = create_team_info_df( 'data/epl_data.csv' ) team_info . head( 3 ) gameId Date season HomeTeam AwayTeam FTR HTR Referee 0 1 2005-08-13 0506 West Ham Blackburn H A A Wiley 1 2 2005-08-13 0506 Aston Villa Bolton D D M Riley 2 3 2005-08-13 0506 Everton Man United A A G Poll # Whilst the other DataFrames date back to 2005, this DataFrame has data from 2001 to 2005. historic_games = create_historic_games_df( 'data/historic_games_pre2005.csv' ) historic_games . head( 3 ) Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin 0 2001-08-18 Charlton Everton 1 2 -1 20012002 0 1 2001-08-18 Derby Blackburn 2 1 -1 20012002 1 2 2001-08-18 Leeds Southampton 2 0 -1 20012002 1 # This is the historic_games DataFrame appended to the df DataFrame. all_games = create_all_games_df( 'data/epl_data.csv' , 'data/historic_games_pre2005.csv' ) all_games . head( 3 ) Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin awayWin homeWinPc5 homeWinPc38 awayWinPc5 awayWinPc38 gameIdHistoric 0 2001-08-18 Charlton Everton 1.0 2.0 -1 20012002 0 1 NaN NaN NaN NaN 1 1 2001-08-18 Derby Blackburn 2.0 1.0 -1 20012002 1 0 NaN NaN NaN NaN 2 2 2001-08-18 Leeds Southampton 2.0 0.0 -1 20012002 1 0 NaN NaN NaN NaN 3","title":"Create 6 distinct DataFrames"},{"location":"modelling/EPLmodelPart2/#feature-creation","text":"Now that we have all of our pre-prepared DataFrames, and we know that the data is clean, we can move onto feature creation. As is common practice with sports modelling, we are going to start by creating expontentially weighted moving averages (EMA) as features. To get a better understanding of how EMAs work, read here . In short, an EMA is like a simple moving average, except it weights recent instances more than older instances based on an alpha parameter. The documentation for the pandas (emw method)[ https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html ] we will be using states that we can specify alpha in a number of ways. We will specify it in terms of span, where $\\alpha = 2 / (span+1), span \u2265 1 $. Let's first define a function which calculates the exponential moving average for each column in the stats DataFrame. We will then apply this function with other functions we have created, such as create_betting_features_ema, which creates moving averages of betting data. However, we must first change the structure of our data. Notice that currently each row has both the Home Team's data and the Away Team's data on a single row. This makes it difficult to calculate rolling averages, so we will restructure our DataFrames to ensure each row only contains single team's data. To do this, we will define a function, reate_multiline_df_stats. # Define a function which restructures our DataFrame def create_multiline_df_stats (old_stats_df): # Create a list of columns we want and their mappings to more interpretable names home_stats_cols = [ 'HomeTeam' , 'FTHG' , 'FTAG' , 'HTHG' , 'HTAG' , 'HS' , 'AS' , 'HST' , 'AST' , 'HF' , 'AF' , 'HC' , 'AC' , 'HY' , 'AY' , 'HR' , 'AR' ] away_stats_cols = [ 'AwayTeam' , 'FTAG' , 'FTHG' , 'HTAG' , 'HTHG' , 'AS' , 'HS' , 'AST' , 'HST' , 'AF' , 'HF' , 'AC' , 'HC' , 'AY' , 'HY' , 'AR' , 'HR' ] stats_cols_mapping = [ 'team' , 'goalsFor' , 'goalsAgainst' , 'halfTimeGoalsFor' , 'halfTimeGoalsAgainst' , 'shotsFor' , 'shotsAgainst' , 'shotsOnTargetFor' , 'shotsOnTargetAgainst' , 'freesFor' , 'freesAgainst' , 'cornersFor' , 'cornersAgainst' , 'yellowsFor' , 'yellowsAgainst' , 'redsFor' , 'redsAgainst' ] # Create a dictionary of the old column names to new column names home_mapping = {old_col: new_col for old_col, new_col in zip (home_stats_cols, stats_cols_mapping)} away_mapping = {old_col: new_col for old_col, new_col in zip (away_stats_cols, stats_cols_mapping)} # Put each team onto an individual row multi_line_stats = (old_stats_df[[ 'gameId' ] + home_stats_cols] # Filter for only the home team columns . rename(columns = home_mapping) # Rename the columns . assign(homeGame =1 ) # Assign homeGame=1 so that we can use a general function later . append((old_stats_df[[ 'gameId' ] + away_stats_cols]) # Append the away team columns . rename(columns = away_mapping) # Rename the away team columns . assign(homeGame =0 ), sort = True ) . sort_values(by = 'gameId' ) # Sort the values . reset_index(drop = True )) return multi_line_stats # Define a function which creates an EMA DataFrame from the stats DataFrame def create_stats_features_ema (stats, span): # Create a restructured DataFrames so that we can calculate EMA multi_line_stats = create_multiline_df_stats(stats) # Create a copy of the DataFrame ema_features = multi_line_stats[[ 'gameId' , 'team' , 'homeGame' ]] . copy() # Get the columns that we want to create EMA for feature_names = multi_line_stats . drop(columns = [ 'gameId' , 'team' , 'homeGame' ]) . columns # Loop over the features for feature_name in feature_names: feature_ema = (multi_line_stats . groupby( 'team' )[feature_name] # Calculate the EMA . transform( lambda row: row . ewm(span = span, min_periods =2 ) . mean() . shift( 1 ))) # Shift the data down 1 so we don't leak data ema_features[feature_name] = feature_ema # Add the new feature to the DataFrame return ema_features # Apply the function stats_features = create_stats_features_ema(stats, span =5 ) stats_features . tail() gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9903 4952 Newcastle 1 4.301743 4.217300 11.789345 12.245066 0.797647 0.833658 0.644214 0.420832 2.323450e-10 3.333631e-01 11.335147 13.265955 3.211345 4.067990 1.848860 1.627140 9904 4953 Burnley 0 4.880132 5.165915 13.326703 8.800033 1.945502 0.667042 0.609440 0.529409 3.874405e-03 3.356120e-10 13.129631 10.642381 4.825874 3.970285 0.963527 0.847939 9905 4953 Fulham 1 4.550255 4.403060 10.188263 8.555589 2.531046 1.003553 0.860573 0.076949 1.002518e-04 8.670776e-03 17.463779 12.278877 8.334019 4.058213 0.980097 1.102974 9906 4954 Man United 1 3.832573 4.759683 11.640608 10.307946 1.397234 1.495032 1.034251 0.809280 6.683080e-05 1.320468e-05 8.963022 10.198642 3.216957 3.776900 1.040077 1.595650 9907 4954 Tottenham 0 3.042034 5.160211 8.991460 9.955635 1.332704 2.514789 0.573728 1.010491 4.522878e-08 1.354409e-05 12.543406 17.761004 3.757437 7.279845 1.478976 1.026601 As we can see, we now have averages for each team. Let's create a quick table to see the top 10 teams' goalsFor average EMAs since 2005. pd . DataFrame(stats_features . groupby( 'team' ) . goalsFor . mean() . sort_values(ascending = False )[: 10 ]) goalsFor team Man United 1.895026 Chelsea 1.888892 Arsenal 1.876770 Man City 1.835863 Liverpool 1.771125 Tottenham 1.655063 Leicester 1.425309 Blackpool 1.390936 Everton 1.387110 Southampton 1.288349","title":"Feature Creation"},{"location":"modelling/EPLmodelPart2/#optimising-alpha","text":"It looks like Man United and Chelsea have been two of the best teams since 2005, based on goalsFor. Now that we have our stats features, we may be tempted to move on. However, we have arbitrarily chosen a span of 5. How do we know that this is the best value? We don't. Let's try and optimise this value. To do this, we will use a simple Logistic Regression model to create probabilistic predictions based on the stats features we created before. We will iterate a range of span values, from say, 3 to 15, and choose the value which produces a model with the lowest log loss, based on cross validation. To do this, we need to restructure our DataFrame back to how it was before. def restructure_stats_features (stats_features): non_features = [ 'homeGame' , 'team' , 'gameId' ] stats_features_restructured = (stats_features . query( 'homeGame == 1' ) . rename(columns = {col: 'f_' + col + 'Home' for col in stats_features . columns if col not in non_features}) . rename(columns = { 'team' : 'HomeTeam' }) . pipe(pd . merge, (stats_features . query( 'homeGame == 0' ) . rename(columns = { 'team' : 'AwayTeam' }) . rename(columns = {col: 'f_' + col + 'Away' for col in stats_features . columns if col not in non_features})), on = [ 'gameId' ]) . pipe(pd . merge, df[[ 'gameId' , 'result' ]], on = 'gameId' ) . dropna()) return stats_features_restructured restructure_stats_features(stats_features) . head() gameId HomeTeam homeGame_x f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome AwayTeam homeGame_y f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway result 20 21 Birmingham 1 4.8 7.8 12.0 9.4 1.2 0.6 0.6 0.6 0.0 0.0 11.4 8.2 6.4 2.8 1.0 2.6 Middlesbrough 0 3.0 5.6 14.0 12.8 1.2 0.0 0.0 0.0 0.0 0.4 17.2 8.8 7.6 2.6 3.0 1.4 away 21 22 Portsmouth 1 2.6 4.6 21.8 16.6 2.0 0.6 1.0 0.0 0.0 0.0 8.0 10.4 3.6 4.0 3.2 1.8 Aston Villa 0 9.8 7.0 14.2 18.2 1.4 0.8 0.8 0.8 0.0 0.0 16.0 3.0 9.6 2.6 2.0 0.6 draw 22 23 Sunderland 1 5.0 5.0 11.6 18.0 1.8 0.4 1.0 0.4 0.4 0.6 14.6 6.0 5.2 3.2 1.2 2.6 Man City 0 7.8 3.6 8.6 12.4 0.6 1.2 0.6 0.6 0.0 0.0 10.6 11.4 2.4 6.8 3.0 1.4 away 23 24 Arsenal 1 3.0 7.4 17.0 18.6 0.6 0.8 0.0 0.0 0.4 0.0 6.2 11.4 4.0 6.6 1.6 1.8 Fulham 0 7.2 3.0 20.8 13.2 1.2 0.6 0.6 0.0 0.0 0.0 12.4 10.8 7.0 5.2 2.0 1.6 home 24 25 Blackburn 1 1.4 7.2 12.8 21.2 1.8 1.6 0.0 1.0 0.0 0.4 10.0 14.0 4.4 7.4 1.2 1.6 Tottenham 0 6.4 3.8 11.2 18.8 0.0 2.0 0.0 0.4 0.0 0.0 11.6 15.2 4.6 7.2 0.6 2.6 draw Now let's write a function that optimises our span based on log loss of the output of a Logistic Regression model. def optimise_alpha (features): le = LabelEncoder() y = le . fit_transform(features . result) # Encode the result from away, draw, home win to 0, 1, 2 X = features[[col for col in features . columns if col . startswith( 'f_' )]] # Only get the features - these all start with f_ lr = LogisticRegression() kfold = StratifiedKFold(n_splits =5 ) ave_cv_score = cross_val_score(lr, X, y, scoring = 'neg_log_loss' , cv = kfold) . mean() return ave_cv_score best_score = np . float( 'inf' ) best_span = 0 cv_scores = [] # Iterate over a range of spans for span in range ( 1 , 120 , 3 ): stats_features = create_stats_features_ema(stats, span = span) restructured_stats_features = restructure_stats_features(stats_features) cv_score = optimise_alpha(restructured_stats_features) cv_scores . append(cv_score) if cv_score * -1 < best_score: best_score = cv_score * -1 best_span = span plt . style . use( 'ggplot' ) plt . plot( list ( range ( 1 , 120 , 3 )), (pd . Series(cv_scores) *-1 )) # Plot our results plt . title( \"Optimising alpha\" ) plt . xlabel( \"Span\" ) plt . ylabel( \"Log Loss\" ) plt . show() print ( \"Our lowest log loss ({:2f}) occurred at a span of {}\" . format(best_score, best_span)) Our lowest log loss (0.980835) occurred at a span of 55 The above method is just an example of how you can optimise hyparameters. Obviously this example has many limitations, such as attempting to optimise each statistic with the same alpha. However, for the rest of these tutorial series we will use this span value. Now let's create the rest of our features. For thorough explanations and the actual code behind some of the functions used, please refer to the data_preparation_functions.py script.","title":"Optimising Alpha"},{"location":"modelling/EPLmodelPart2/#creating-our-features-dataframe","text":"We will utilise pre-made functions to create all of our features in just a few lines of code. As part of this process we will create features which include margin weighted elo, an exponential average for asian handicap data, and odds as features. Our Elo function is essentially the same as the one we created in the AFL tutorial; if you would like to know more about Elo models please read this article. Note that the cell below may take a few minutes to run. # Create feature DataFrames features_all_games = create_all_games_features(all_games) C:\\Users\\wardj\\Documents\\Betfair Public Github\\predictive-models\\epl\\data_preparation_functions.py:419: RuntimeWarning: invalid value encountered in double_scalars .pipe(lambda df: (df.eloAgainst * df[goalsForOrAgainstCol]).sum() / df.eloAgainst.sum())) The features_all_games df includes elo for each team, as well as their win percentage at home and away over the past 5 and 38 games. For more information on how it was calculated, read through the data_preparation_functions script. features_all_games . head( 3 ) Date awayWin awayWinPc38 awayWinPc5 eloAgainst eloFor gameId gameIdHistoric goalsAgainst goalsFor homeGame homeWin homeWinPc38 homeWinPc5 season team wtEloGoalsFor wtEloGoalsAgainst 0 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 2.0 1.0 1 0 NaN NaN 20012002 Charlton NaN NaN 1 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 1.0 2.0 0 0 NaN NaN 20012002 Everton NaN NaN 2 2001-08-18 0 NaN NaN 1500.0 1500.0 -1 2 1.0 2.0 1 1 NaN NaN 20012002 Derby NaN NaN The features_stats df includes all the expontential weighted averages for each stat in the stats df. # Create feature stats df features_stats = create_stats_features_ema(stats, span = best_span) features_stats . tail( 3 ) gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9905 4953 Fulham 1 6.006967 5.045733 10.228997 9.965651 2.147069 1.093550 0.630485 0.364246 0.032937 0.043696 16.510067 11.718122 7.184386 4.645762 1.310424 1.389716 9906 4954 Man United 1 4.463018 5.461075 11.605712 10.870367 0.843222 1.586308 0.427065 0.730650 0.042588 0.027488 10.865754 13.003121 3.562675 4.626450 1.740735 1.712785 9907 4954 Tottenham 0 3.868619 6.362901 10.784145 10.140388 0.954928 2.100166 0.439129 0.799968 0.024351 0.026211 9.947515 16.460598 3.370010 6.136120 1.925005 1.364268 The features_odds df includes a moving average of some of the odds data. # Create feature_odds df features_odds = create_betting_features_ema(betting, span =10 ) features_odds . tail( 3 ) gameId team avAsianHandicapOddsAgainst avAsianHandicapOddsFor avgreaterthan2.5 avlessthan2.5 sizeOfHandicap 9905 4953 Fulham 1.884552 1.985978 1.756776 2.128261 0.502253 9906 4954 Man United 1.871586 2.031787 1.900655 1.963478 -0.942445 9907 4954 Tottenham 1.947833 1.919607 1.629089 2.383593 -1.235630 The features market values has market values and the % of total market for each position. These values are in millions. # Create feature market values df features_market_values = create_market_values_features(df) # This creates a df with one game per row features_market_values . head( 3 ) gameId Year HomeTeam AwayTeam defMktValH attMktValH gkMktValH totalMktValH midMktValH defMktValA attMktValA gkMktValA totalMktValA midMktValA attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% 0 1 2005 West Ham Blackburn 16.90 18.50 6.40 46.40 4.60 27.25 13.00 3.25 70.70 27.20 2.252911 1.583126 0.588168 3.477861 2.486940 4.010007 4.524247 2.297469 1.913986 2.916354 1 2 2005 Aston Villa Bolton 27.63 31.85 7.60 105.83 38.75 9.60 24.55 8.50 72.40 29.75 3.878659 2.989673 4.954673 3.803910 4.065926 1.412700 5.372543 6.008766 4.365456 2.986478 2 3 2005 Everton Man United 44.35 31.38 8.55 109.78 25.50 82.63 114.60 9.25 288.48 82.00 3.821423 13.955867 3.260494 10.484727 6.526378 12.159517 6.044111 6.538951 4.528392 11.899714 all_games_cols = [ 'Date' , 'gameId' , 'team' , 'season' , 'homeGame' , 'homeWinPc38' , 'homeWinPc5' , 'awayWinPc38' , 'awayWinPc5' , 'eloFor' , 'eloAgainst' , 'wtEloGoalsFor' , 'wtEloGoalsAgainst' ] # Join the features together features_multi_line = (features_all_games[all_games_cols] . pipe(pd . merge, features_stats . drop(columns = 'homeGame' ), on = [ 'gameId' , 'team' ]) . pipe(pd . merge, features_odds, on = [ 'gameId' , 'team' ])) # Put each instance on an individual row features_with_na = put_features_on_one_line(features_multi_line) market_val_feature_names = [ 'attMktH%' , 'attMktA%' , 'midMktH%' , 'midMktA%' , 'defMktH%' , 'defMktA%' , 'gkMktH%' , 'gkMktA%' , 'totalMktH%' , 'totalMktA%' ] # Merge our team values dataframe to features and result from df features_with_na = (features_with_na . pipe(pd . merge, (features_market_values[market_val_feature_names + [ 'gameId' ]]) . rename({col: 'f_' + col for col in market_val_feature_names}), on = 'gameId' ) . pipe(pd . merge, df[[ 'HomeTeam' , 'AwayTeam' , 'gameId' , 'result' , 'B365A' , 'B365D' , 'B365H' ]], on = [ 'HomeTeam' , 'AwayTeam' , 'gameId' ])) # Drop NAs from calculating the rolling averages - don't drop Win Pc 38 and Win Pc 5 columns features = features_with_na . dropna(subset = features_with_na . drop(columns = [col for col in features_with_na . columns if 'WinPc' in col]) . columns) # Fill NAs for the Win Pc columns features = features . fillna(features . mean()) features . head( 3 ) Date gameId HomeTeam season homeGame f_homeWinPc38Home f_homeWinPc5Home f_awayWinPc38Home f_awayWinPc5Home f_eloForHome f_eloAgainstHome f_wtEloGoalsForHome f_wtEloGoalsAgainstHome f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome f_avAsianHandicapOddsAgainstHome f_avAsianHandicapOddsForHome f_avgreaterthan2.5Home f_avlessthan2.5Home f_sizeOfHandicapHome AwayTeam f_homeWinPc38Away f_homeWinPc5Away f_awayWinPc38Away f_awayWinPc5Away f_eloForAway f_eloAgainstAway f_wtEloGoalsForAway f_wtEloGoalsAgainstAway f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway f_avAsianHandicapOddsAgainstAway f_avAsianHandicapOddsForAway f_avgreaterthan2.5Away f_avlessthan2.5Away f_sizeOfHandicapAway attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% result B365A B365D B365H 20 2005-08-23 21 Birmingham 0506 1 0.394737 0.4 0.263158 0.2 1478.687038 1492.866048 1.061763 1.260223 4.981818 7.527273 12.000000 9.945455 1.018182 0.509091 0.509091 0.509091 0.000000 0.000000 11.945455 8.018182 6.490909 2.981818 1.000000 2.509091 1.9090 1.9455 2.0510 1.6735 -0.1375 Middlesbrough 0.394737 0.4 0.263158 0.2 1492.866048 1478.687038 1.12994 1.279873 2.545455 5.509091 13.545455 13.436364 1.018182 0.000000 0.000000 0.000000 0.0 0.490909 17.018182 8.072727 7.509091 2.509091 3.0 1.490909 1.9395 1.9095 2.0035 1.7155 0.3875 5.132983 5.260851 3.341048 4.289788 3.502318 4.168935 2.332815 3.216457 3.934396 4.522205 away 2.75 3.2 2.50 21 2005-08-23 22 Portsmouth 0506 1 0.447368 0.4 0.263158 0.4 1405.968416 1489.229314 1.147101 1.503051 2.509091 4.963636 21.981818 16.054545 2.000000 0.509091 1.000000 0.000000 0.000000 0.000000 8.454545 10.490909 3.963636 4.454545 3.018182 1.527273 1.8965 1.9690 2.0040 1.7005 0.2500 Aston Villa 0.447368 0.4 0.263158 0.4 1489.229314 1405.968416 1.17516 1.263229 9.527273 7.000000 14.472727 17.563636 1.490909 0.981818 0.981818 0.981818 0.0 0.000000 15.545455 3.000000 9.054545 2.509091 2.0 0.509091 1.8565 1.9770 1.8505 1.8485 0.7125 3.738614 3.878659 4.494368 4.954673 2.884262 4.065926 3.746642 5.372543 3.743410 4.365456 draw 2.75 3.2 2.50 22 2005-08-23 23 Sunderland 0506 1 0.236842 0.0 0.236842 0.4 1277.888970 1552.291880 0.650176 1.543716 5.000000 5.000000 12.418182 17.545455 1.981818 0.490909 1.000000 0.490909 0.490909 0.509091 14.509091 6.909091 5.018182 3.927273 1.018182 2.509091 1.8520 1.9915 1.8535 1.8500 0.7125 Man City 0.236842 0.0 0.236842 0.4 1552.291880 1277.888970 1.28875 1.287367 7.527273 3.509091 8.963636 12.490909 0.509091 1.018182 0.509091 0.509091 0.0 0.000000 10.963636 11.945455 2.490909 6.981818 3.0 1.490909 1.8150 2.0395 2.0060 1.7095 -0.2000 0.706318 3.750792 1.476812 1.070209 2.634096 4.455890 0.777605 4.913050 1.499427 3.151477 away 2.50 3.2 2.75 We now have a features DataFrame ready, with all the feature columns beginning with the \"f_\". In the next tutorial, we will walk through the modelling process to try and find the best type of model to use.","title":"Creating our Features DataFrame"},{"location":"modelling/EPLmodelPart3/","text":"EPL Machine Learning Walkthrough \u00b6 03. Model Building & Hyperparameter Tuning \u00b6 Welcome to the third part of this Machine Learning Walkthrough. This tutorial will focus on the model building process, including how to tune hyperparameters. In the [next tutorial], we will create weekly predictions based on the model we have created here. Specifically, this tutorial will cover a few things: Choosing which Machine Learning algorithm to use from a variety of choices Hyperparameter Tuning Overfitting/Underfitting Choosing an Algorithm \u00b6 The best way to decide on specific algorithm to use, is to try them all! To do this, we will define a function which we first used in our AFL Predictions tutorial. This will iterate over a number of algorithms and give us a good indication of which algorithms are suited for this dataset and exercise. Let's first use grab the features we created in the last tutorial. This may take a minute or two to run. ## Import libraries from data_preparation_functions import * import pandas as pd import numpy as np import matplotlib as plt import seaborn as sns import warnings from sklearn import linear_model, tree, discriminant_analysis, naive_bayes, ensemble, gaussian_process from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV from sklearn.metrics import log_loss, confusion_matrix warnings . filterwarnings( 'ignore' ) pd . set_option( 'display.max_columns' , 100 ) features = create_feature_df() Creating all games feature DataFrame Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats, odds and market values into one features DataFrame Complete . To start our modelling process, we need to make a training set, a test set and a holdout set. As we are using cross validation, we will make our training set all of the seasons up until 2017/18, and we will use the 2017/18 season as the test set. feature_list = [col for col in features . columns if col . startswith( \"f_\" )] betting_features = [] le = LabelEncoder() # Initiate a label encoder to transform the labels 'away', 'draw', 'home' to 0, 1, 2 # Grab all seasons except for 17/18 to use CV with all_x = features . loc[features . season != '1718' , [ 'gameId' ] + feature_list] all_y = features . loc[features . season != '1718' , 'result' ] all_y = le . fit_transform(all_y) # Create our training vector as the seasons except 16/17 and 17/18 train_x = features . loc[ ~ features . season . isin([ '1617' , '1718' ]), [ 'gameId' ] + feature_list] train_y = le . transform(features . loc[ ~ features . season . isin([ '1617' , '1718' ]), 'result' ]) # Create our holdout vectors as the 16/17 season holdout_x = features . loc[features . season == '1617' , [ 'gameId' ] + feature_list] holdout_y = le . transform(features . loc[features . season == '1617' , 'result' ]) # Create our test vectors as the 17/18 season test_x = features . loc[features . season == '1718' , [ 'gameId' ] + feature_list] test_y = le . transform(features . loc[features . season == '1718' , 'result' ]) # Create a list of standard classifiers classifiers = [ #GLM linear_model . LogisticRegressionCV(), #Navies Bayes naive_bayes . BernoulliNB(), naive_bayes . GaussianNB(), #Discriminant Analysis discriminant_analysis . LinearDiscriminantAnalysis(), discriminant_analysis . QuadraticDiscriminantAnalysis(), #Ensemble Methods ensemble . AdaBoostClassifier(), ensemble . BaggingClassifier(), ensemble . ExtraTreesClassifier(), ensemble . GradientBoostingClassifier(), ensemble . RandomForestClassifier(), #Gaussian Processes gaussian_process . GaussianProcessClassifier(), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html # xgb.XGBClassifier() ] def find_best_algorithms (classifier_list, X, y): # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling # Cross validate model with Kfold stratified cross validation kfold = StratifiedKFold(n_splits =5 ) # Grab the cross validation scores for each algorithm cv_results = [cross_val_score(classifier, X, y, scoring = \"neg_log_loss\" , cv = kfold) for classifier in classifier_list] cv_means = [cv_result . mean() * -1 for cv_result in cv_results] cv_std = [cv_result . std() for cv_result in cv_results] algorithm_names = [alg . __class__ . __name__ for alg in classifiers] # Create a DataFrame of all the CV results cv_results = pd . DataFrame({ \"Mean Log Loss\" : cv_means, \"Log Loss Std\" : cv_std, \"Algorithm\" : algorithm_names }) . sort_values(by = 'Mean Log Loss' ) return cv_results algorithm_results = find_best_algorithms(classifiers, all_x, all_y) algorithm_results Mean Log Loss Log Loss Std Algorithm 0 0.966540 0.020347 LogisticRegressionCV 3 0.986679 0.015601 LinearDiscriminantAnalysis 1 1.015197 0.017466 BernoulliNB 10 1.098612 0.000000 GaussianProcessClassifier 5 1.101281 0.044383 AdaBoostClassifier 8 1.137778 0.153391 GradientBoostingClassifier 7 2.093981 0.284831 ExtraTreesClassifier 9 2.095088 0.130367 RandomForestClassifier 6 2.120571 0.503132 BaggingClassifier 4 4.065796 1.370119 QuadraticDiscriminantAnalysis 2 5.284171 0.826991 GaussianNB We can see that LogisticRegression seems to perform the best out of all the algorithms, and some algorithms have a very high log loss. This is most likely due to overfitting. It would definitely be useful to condense our features down to reduce the dimensionality of the dataset. Hyperparameter Tuning \u00b6 For now, however, we will use logistic regression. Let's first try and tune a logistic regression model with cross validation. To do this, we will use grid search . Grid search essentially tries out each combination of values and finds the model with the lowest error metric, which in our case is log loss. 'C' in logistic regression determines the amount of regularization. Lower values increase regularization. # Define our parameters to run a grid search over lr_grid = { \"C\" : [ 0.0001 , 0.01 , 0.05 , 0.2 , 1 ], \"solver\" : [ \"newton-cg\" , \"lbfgs\" , \"liblinear\" ] } kfold = StratifiedKFold(n_splits =5 ) gs = GridSearchCV(LogisticRegression(), param_grid = lr_grid, cv = kfold, scoring = 'neg_log_loss' ) gs . fit(all_x, all_y) print ( \"Best log loss: {}\" . format(gs . best_score_ *-1 )) best_lr_params = gs . best_params_ Best log loss: 0.9669551970849734 Defining a Baseline \u00b6 We should also define a baseline, as we don't really know if our log loss is good or bad. Randomly assigning a \u2153 chance to each selection yields a log loss of log3 = 1.09. However, what we are really interested in, is how our model performs relative to the odds. So let's find the log loss of the odds. # Finding the log loss of the odds log_loss(all_y, 1 / all_x[[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]]) 0.9590114943474463 This is good news: our algorithm almost beats the bookies in terms of log loss. It would be great if we could beat this result. Analysing the Errors Made \u00b6 Now that we have a logistic regression model tuned, let's see what type of errors it made. To do this we will look at the confusion matrix produced when we predict our holdout set. lr = LogisticRegression( ** best_lr_params) # Instantiate the model lr . fit(train_x, train_y) # Fit our model lr_predict = lr . predict(holdout_x) # Predict the holdout values # Create a confusion matrix c_matrix = (pd . DataFrame(confusion_matrix(holdout_y, lr_predict), columns = le . classes_, index = le . classes_) . rename_axis( 'Actual' ) . rename_axis( 'Predicted' , axis = 'columns' )) c_matrix Predicted away draw home Actual away 77 0 32 draw 26 3 55 home 33 7 147 As we can see, when we predicted 'away' as the result, we correctly predicted 79 / 109 results, a hit rate of 70.6%. However, when we look at our draw hit rate, we only predicted 6 / 84 correctly, meaning we only had a hit rate of around 8.3%. For a more in depth analysis of our predictions, please skip to the Analysing Predictions & Staking Strategies section of the tutorial. Before we move on, however, let's use our model to predict the 17/18 season and compare how we went with the odds. # Get test predictions test_lr = LogisticRegression( ** best_lr_params) test_lr . fit(all_x, all_y) test_predictions_probs = lr . predict_proba(test_x) test_predictions = lr . predict(test_x) test_ll = log_loss(test_y, test_predictions_probs) test_accuracy = (test_predictions == test_y) . mean() print ( \"Our predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.2f}\" . format(test_ll, test_accuracy)) Our predictions for the 2017/18 season have a log loss of: 0.95767 and an accuracy of: 0.56 # Get accuracy and log loss based on the odds odds_ll = log_loss(test_y, 1 / test_x[[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]]) odds_predictions = test_x[[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]] . apply( lambda row: row . idxmin()[ 2 : 6 ], axis =1 ) . values odds_accuracy = (odds_predictions == le . inverse_transform(test_y)) . mean() print ( \"Odds predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.3f}\" . format(odds_ll, odds_accuracy)) Odds predictions for the 2017/18 season have a log loss of: 0.94635 and an accuracy of: 0.545 Results \u00b6 There we have it! The odds predicted 54.5% of EPL games correctly in the 2017/18 season, whilst our model predicted 54% correctly. This is a decent result for the first iteration of our model. In future iterations, we could wait a certain number of matches each season and calculate EMAs for on those first n games. This may help the issue of players switching clubs and teams becoming relatively stronger/weaker compared to previous seasons.","title":"EPL 03. Model building & hyperparameter tuning"},{"location":"modelling/EPLmodelPart3/#epl-machine-learning-walkthrough","text":"","title":"EPL Machine Learning Walkthrough"},{"location":"modelling/EPLmodelPart3/#03-model-building-hyperparameter-tuning","text":"Welcome to the third part of this Machine Learning Walkthrough. This tutorial will focus on the model building process, including how to tune hyperparameters. In the [next tutorial], we will create weekly predictions based on the model we have created here. Specifically, this tutorial will cover a few things: Choosing which Machine Learning algorithm to use from a variety of choices Hyperparameter Tuning Overfitting/Underfitting","title":"03. Model Building &amp; Hyperparameter Tuning"},{"location":"modelling/EPLmodelPart3/#choosing-an-algorithm","text":"The best way to decide on specific algorithm to use, is to try them all! To do this, we will define a function which we first used in our AFL Predictions tutorial. This will iterate over a number of algorithms and give us a good indication of which algorithms are suited for this dataset and exercise. Let's first use grab the features we created in the last tutorial. This may take a minute or two to run. ## Import libraries from data_preparation_functions import * import pandas as pd import numpy as np import matplotlib as plt import seaborn as sns import warnings from sklearn import linear_model, tree, discriminant_analysis, naive_bayes, ensemble, gaussian_process from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV from sklearn.metrics import log_loss, confusion_matrix warnings . filterwarnings( 'ignore' ) pd . set_option( 'display.max_columns' , 100 ) features = create_feature_df() Creating all games feature DataFrame Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats, odds and market values into one features DataFrame Complete . To start our modelling process, we need to make a training set, a test set and a holdout set. As we are using cross validation, we will make our training set all of the seasons up until 2017/18, and we will use the 2017/18 season as the test set. feature_list = [col for col in features . columns if col . startswith( \"f_\" )] betting_features = [] le = LabelEncoder() # Initiate a label encoder to transform the labels 'away', 'draw', 'home' to 0, 1, 2 # Grab all seasons except for 17/18 to use CV with all_x = features . loc[features . season != '1718' , [ 'gameId' ] + feature_list] all_y = features . loc[features . season != '1718' , 'result' ] all_y = le . fit_transform(all_y) # Create our training vector as the seasons except 16/17 and 17/18 train_x = features . loc[ ~ features . season . isin([ '1617' , '1718' ]), [ 'gameId' ] + feature_list] train_y = le . transform(features . loc[ ~ features . season . isin([ '1617' , '1718' ]), 'result' ]) # Create our holdout vectors as the 16/17 season holdout_x = features . loc[features . season == '1617' , [ 'gameId' ] + feature_list] holdout_y = le . transform(features . loc[features . season == '1617' , 'result' ]) # Create our test vectors as the 17/18 season test_x = features . loc[features . season == '1718' , [ 'gameId' ] + feature_list] test_y = le . transform(features . loc[features . season == '1718' , 'result' ]) # Create a list of standard classifiers classifiers = [ #GLM linear_model . LogisticRegressionCV(), #Navies Bayes naive_bayes . BernoulliNB(), naive_bayes . GaussianNB(), #Discriminant Analysis discriminant_analysis . LinearDiscriminantAnalysis(), discriminant_analysis . QuadraticDiscriminantAnalysis(), #Ensemble Methods ensemble . AdaBoostClassifier(), ensemble . BaggingClassifier(), ensemble . ExtraTreesClassifier(), ensemble . GradientBoostingClassifier(), ensemble . RandomForestClassifier(), #Gaussian Processes gaussian_process . GaussianProcessClassifier(), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html # xgb.XGBClassifier() ] def find_best_algorithms (classifier_list, X, y): # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling # Cross validate model with Kfold stratified cross validation kfold = StratifiedKFold(n_splits =5 ) # Grab the cross validation scores for each algorithm cv_results = [cross_val_score(classifier, X, y, scoring = \"neg_log_loss\" , cv = kfold) for classifier in classifier_list] cv_means = [cv_result . mean() * -1 for cv_result in cv_results] cv_std = [cv_result . std() for cv_result in cv_results] algorithm_names = [alg . __class__ . __name__ for alg in classifiers] # Create a DataFrame of all the CV results cv_results = pd . DataFrame({ \"Mean Log Loss\" : cv_means, \"Log Loss Std\" : cv_std, \"Algorithm\" : algorithm_names }) . sort_values(by = 'Mean Log Loss' ) return cv_results algorithm_results = find_best_algorithms(classifiers, all_x, all_y) algorithm_results Mean Log Loss Log Loss Std Algorithm 0 0.966540 0.020347 LogisticRegressionCV 3 0.986679 0.015601 LinearDiscriminantAnalysis 1 1.015197 0.017466 BernoulliNB 10 1.098612 0.000000 GaussianProcessClassifier 5 1.101281 0.044383 AdaBoostClassifier 8 1.137778 0.153391 GradientBoostingClassifier 7 2.093981 0.284831 ExtraTreesClassifier 9 2.095088 0.130367 RandomForestClassifier 6 2.120571 0.503132 BaggingClassifier 4 4.065796 1.370119 QuadraticDiscriminantAnalysis 2 5.284171 0.826991 GaussianNB We can see that LogisticRegression seems to perform the best out of all the algorithms, and some algorithms have a very high log loss. This is most likely due to overfitting. It would definitely be useful to condense our features down to reduce the dimensionality of the dataset.","title":"Choosing an Algorithm"},{"location":"modelling/EPLmodelPart3/#hyperparameter-tuning","text":"For now, however, we will use logistic regression. Let's first try and tune a logistic regression model with cross validation. To do this, we will use grid search . Grid search essentially tries out each combination of values and finds the model with the lowest error metric, which in our case is log loss. 'C' in logistic regression determines the amount of regularization. Lower values increase regularization. # Define our parameters to run a grid search over lr_grid = { \"C\" : [ 0.0001 , 0.01 , 0.05 , 0.2 , 1 ], \"solver\" : [ \"newton-cg\" , \"lbfgs\" , \"liblinear\" ] } kfold = StratifiedKFold(n_splits =5 ) gs = GridSearchCV(LogisticRegression(), param_grid = lr_grid, cv = kfold, scoring = 'neg_log_loss' ) gs . fit(all_x, all_y) print ( \"Best log loss: {}\" . format(gs . best_score_ *-1 )) best_lr_params = gs . best_params_ Best log loss: 0.9669551970849734","title":"Hyperparameter Tuning"},{"location":"modelling/EPLmodelPart3/#defining-a-baseline","text":"We should also define a baseline, as we don't really know if our log loss is good or bad. Randomly assigning a \u2153 chance to each selection yields a log loss of log3 = 1.09. However, what we are really interested in, is how our model performs relative to the odds. So let's find the log loss of the odds. # Finding the log loss of the odds log_loss(all_y, 1 / all_x[[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]]) 0.9590114943474463 This is good news: our algorithm almost beats the bookies in terms of log loss. It would be great if we could beat this result.","title":"Defining a Baseline"},{"location":"modelling/EPLmodelPart3/#analysing-the-errors-made","text":"Now that we have a logistic regression model tuned, let's see what type of errors it made. To do this we will look at the confusion matrix produced when we predict our holdout set. lr = LogisticRegression( ** best_lr_params) # Instantiate the model lr . fit(train_x, train_y) # Fit our model lr_predict = lr . predict(holdout_x) # Predict the holdout values # Create a confusion matrix c_matrix = (pd . DataFrame(confusion_matrix(holdout_y, lr_predict), columns = le . classes_, index = le . classes_) . rename_axis( 'Actual' ) . rename_axis( 'Predicted' , axis = 'columns' )) c_matrix Predicted away draw home Actual away 77 0 32 draw 26 3 55 home 33 7 147 As we can see, when we predicted 'away' as the result, we correctly predicted 79 / 109 results, a hit rate of 70.6%. However, when we look at our draw hit rate, we only predicted 6 / 84 correctly, meaning we only had a hit rate of around 8.3%. For a more in depth analysis of our predictions, please skip to the Analysing Predictions & Staking Strategies section of the tutorial. Before we move on, however, let's use our model to predict the 17/18 season and compare how we went with the odds. # Get test predictions test_lr = LogisticRegression( ** best_lr_params) test_lr . fit(all_x, all_y) test_predictions_probs = lr . predict_proba(test_x) test_predictions = lr . predict(test_x) test_ll = log_loss(test_y, test_predictions_probs) test_accuracy = (test_predictions == test_y) . mean() print ( \"Our predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.2f}\" . format(test_ll, test_accuracy)) Our predictions for the 2017/18 season have a log loss of: 0.95767 and an accuracy of: 0.56 # Get accuracy and log loss based on the odds odds_ll = log_loss(test_y, 1 / test_x[[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]]) odds_predictions = test_x[[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]] . apply( lambda row: row . idxmin()[ 2 : 6 ], axis =1 ) . values odds_accuracy = (odds_predictions == le . inverse_transform(test_y)) . mean() print ( \"Odds predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.3f}\" . format(odds_ll, odds_accuracy)) Odds predictions for the 2017/18 season have a log loss of: 0.94635 and an accuracy of: 0.545","title":"Analysing the Errors Made"},{"location":"modelling/EPLmodelPart3/#results","text":"There we have it! The odds predicted 54.5% of EPL games correctly in the 2017/18 season, whilst our model predicted 54% correctly. This is a decent result for the first iteration of our model. In future iterations, we could wait a certain number of matches each season and calculate EMAs for on those first n games. This may help the issue of players switching clubs and teams becoming relatively stronger/weaker compared to previous seasons.","title":"Results"},{"location":"modelling/EPLmodelPart4/","text":"EPL Machine Learning Walkthrough \u00b6 04. Weekly Predictions \u00b6 Welcome to the third part of this Machine Learning Walkthrough. This tutorial will be a walk through of creating weekly EPL predictions from the basic logistic regression model we built in the previous tutorial. We will then analyse our predictions and create staking strategies in the next tutorial. Specifically, this tutorial will cover a few things: Obtaining Weekly Odds / Game Info Using Betfair's API Data Wrangling This Week's Game Info Into Our Feature Set Obtaining Weekly Odds / Game Info Using Betfair's API \u00b6 The first thing we need to do to create weekly predictions is get both the games being played this week, as well as match odds from Betfair to be used as features. To make this process easier, I have created a csv file with the fixture for the 2018/19 season. Let's load that now. ## Import libraries import pandas as pd from weekly_prediction_functions import * from data_preparation_functions import * from sklearn.metrics import log_loss, confusion_matrix import warnings warnings . filterwarnings( 'ignore' ) pd . set_option( 'display.max_columns' , 100 ) fixture = (pd . read_csv( 'data/fixture.csv' ) . assign(Date = lambda df: pd . to_datetime(df . Date))) fixture . head() Date Time (AEST) HomeTeam AwayTeam Venue TV Year round season 0 2018-08-11 5:00 AM Man United Leicester Old Trafford, Manchester Optus, Fox Sports (delay) 2018 1 1819 1 2018-08-11 9:30 PM Newcastle Tottenham St.James\u2019 Park, Newcastle Optus, SBS 2018 1 1819 2 2018-08-12 12:00 AM Bournemouth Cardiff Vitality Stadium, Bournemouth Optus 2018 1 1819 3 2018-08-12 12:00 AM Fulham Crystal Palace Craven Cottage, London Optus 2018 1 1819 4 2018-08-12 12:00 AM Huddersfield Chelsea John Smith\u2019s Stadium, Huddersfield Optus, Fox Sports (delay) 2018 1 1819 Now we are going to connect to the API and retrieve game level information for the next week. To do this, we will use an R script. If you are not familiar with R, don't worry, it is relatively simple to read through. For this, we will run the script weekly_game_info_puller.R. Go ahead and run that script now. Note that for this step, you will require a Betfair API App Key. If you don't have one, visit this page and follow the instructions . I will upload an updated weekly file, so you can follow along regardless of if you have an App Key or not. Let's load that file in now. game_info = create_game_info_df( \"data/weekly_game_info.csv\" ) game_info . head( 3 ) AwayTeam HomeTeam awaySelectionId drawSelectionId homeSelectionId draw marketId marketStartTime totalMatched eventId eventName homeOdds drawOdds awayOdds competitionId Date localMarketStartTime 0 Arsenal Cardiff 1096 58805 79343 The Draw 1.146897152 2018-09-02 12:30:00 30123.595116 28852020 Cardiff v Arsenal 7.00 4.3 1.62 10932509 2018-09-02 Sun September 2, 10:30PM 1 Bournemouth Chelsea 1141 58805 55190 The Draw 1.146875421 2018-09-01 14:00:00 30821.329656 28851426 Chelsea v Bournemouth 1.32 6.8 12.00 10932509 2018-09-01 Sun September 2, 12:00AM 2 Fulham Brighton 56764 58805 18567 The Draw 1.146875746 2018-09-01 14:00:00 16594.833096 28851429 Brighton v Fulham 2.36 3.5 3.50 10932509 2018-09-01 Sun September 2, 12:00AM Finally, we will use the API to grab the weekly odds. This R script is also provided, but I have also included the weekly odds csv for convenience. odds = (pd . read_csv( 'data/weekly_epl_odds.csv' ) . replace({ 'Man Utd' : 'Man United' , 'C Palace' : 'Crystal Palace' })) odds . head( 3 ) HomeTeam AwayTeam f_homeOdds f_drawOdds f_awayOdds 0 Leicester Liverpool 7.80 5.1 1.48 1 Brighton Fulham 2.36 3.5 3.50 2 Everton Huddersfield 1.54 4.4 8.20 Data Wrangling This Week's Game Info Into Our Feature Set \u00b6 Now we have the arduous task of wrangling all of this info into a feature set that we can use to predict this week's games. Luckily our functions we created earlier should work if we just append the non-features to our main dataframe. df = create_df( 'data/epl_data.csv' ) df . head() AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.50 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.90 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.00 2.38 8 A Wiley 2.75 3.25 2.40 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.30 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.40 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.00 2.10 8 M Riley 3.10 3.25 2.20 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.00 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.80 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.20 3.75 8 G Poll 1.80 3.30 4.50 2005 0506 3 0 1 away 3 6.0 13.0 0.0 7.0 4.0 2.0 Birmingham 2.87 3.25 2.37 2.80 3.20 2.30 56.0 21.0 0.00 1.69 2.04 2.87 2.05 1.81 3.16 2.31 1.77 2.24 3.05 2.11 1.85 3.30 2.60 36.0 2005-08-13 13 E0 0.0 0.0 D 6.0 12.0 0.0 15.0 7.0 0.0 0.0 D 1.0 Fulham 2.9 3.0 2.2 2.88 3.00 2.25 8 R Styles 2.80 3.25 2.35 2005 0506 4 0 0 draw 4 6.0 11.0 0.0 13.0 3.0 3.0 West Brom 5.00 3.40 1.72 4.80 3.45 1.65 55.0 23.0 -0.75 1.77 1.94 4.79 1.76 2.10 3.38 1.69 1.90 2.10 5.60 1.83 2.19 3.63 1.80 36.0 2005-08-13 13 E0 0.0 0.0 D 3.0 13.0 0.0 15.0 8.0 0.0 0.0 D 2.0 Man City 4.2 3.2 1.7 4.50 3.25 1.67 8 C Foy 5.00 3.25 1.75 2005 0506 5 0 0 draw Now we need to specify which game week we would like to predict. We will then filter the fixture for this game week and append this info to the main DataFrame round_to_predict = int ( input ( \"Which game week would you like to predict? Please input next week's Game Week \\n \" )) Which game week would you like to predict? Please input next week's Game Week 4 future_predictions = (fixture . loc[fixture[ 'round' ] == round_to_predict, [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'season' ]] . pipe(pd . merge, odds, on = [ 'HomeTeam' , 'AwayTeam' ]) . rename(columns = { 'f_homeOdds' : 'B365H' , 'f_awayOdds' : 'B365A' , 'f_drawOdds' : 'B365D' }) . assign(season = lambda df: df . season . astype( str ))) df_including_future_games = (pd . read_csv( 'data/epl_data.csv' , dtype = { 'season' : str }) . assign(Date = lambda df: pd . to_datetime(df . Date)) . pipe( lambda df: df . dropna(thresh = len (df) - 2 , axis =1 )) # Drop cols with NAs . dropna(axis =0 ) # Drop rows with NAs . sort_values( 'Date' ) . append(future_predictions, sort = True ) . reset_index(drop = True ) . assign(gameId = lambda df: list (df . index + 1 ), Year = lambda df: df . Date . apply( lambda row: row . year), homeWin = lambda df: df . apply( lambda row: 1 if row . FTHG > row . FTAG else 0 , axis =1 ), awayWin = lambda df: df . apply( lambda row: 1 if row . FTAG > row . FTHG else 0 , axis =1 ), result = lambda df: df . apply( lambda row: 'home' if row . FTHG > row . FTAG else ( 'draw' if row . FTHG == row . FTAG else 'away' ), axis =1 ))) df_including_future_games . tail( 12 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 4952 4.0 8.0 0.0 12.0 2.0 1.0 Burnley 4.33 3.40 2.00 4.0 3.3 2.00 39.0 20.0 -0.25 1.65 2.22 4.14 2.22 1.69 3.36 1.98 1.72 2.31 4.5 2.32 1.74 3.57 2.04 36.0 2018-08-26 26.0 E0 2.0 4.0 H 6.0 11.0 0.0 25.0 12.0 2.0 3.0 H 2.0 Fulham 4.10 3.35 1.97 3.90 3.2 2.00 8.0 D Coote 4.33 3.4 2.0 2018 1819 4953 1 0 home 4953 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.9 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.0 1.76 2.25 3.40 2.67 40.0 2018-08-27 27.0 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.60 2.75 3.2 2.55 8.0 C Pawson 2.90 3.3 2.6 2018 1819 4954 0 1 away 4954 NaN NaN NaN NaN NaN NaN Liverpool 1.48 5.10 7.80 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-01 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Leicester NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4955 0 0 away 4955 NaN NaN NaN NaN NaN NaN Fulham 3.50 3.50 2.36 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Brighton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4956 0 0 away 4956 NaN NaN NaN NaN NaN NaN Man United 1.70 3.90 6.60 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Burnley NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4957 0 0 away 4957 NaN NaN NaN NaN NaN NaN Bournemouth 12.00 6.80 1.32 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Chelsea NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4958 0 0 away 4958 NaN NaN NaN NaN NaN NaN Southampton 4.50 3.55 2.04 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Crystal Palace NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4959 0 0 away 4959 NaN NaN NaN NaN NaN NaN Huddersfield 8.20 4.40 1.54 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Everton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4960 0 0 away 4960 NaN NaN NaN NaN NaN NaN Wolves 2.98 3.50 2.62 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN West Ham NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4961 0 0 away 4961 NaN NaN NaN NaN NaN NaN Newcastle 32.00 12.50 1.12 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Man City NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4962 0 0 away 4962 NaN NaN NaN NaN NaN NaN Arsenal 1.62 4.30 7.00 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Cardiff NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4963 0 0 away 4963 NaN NaN NaN NaN NaN NaN Tottenham 1.68 4.30 5.90 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-03 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Watford NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4964 0 0 away As we can see, what we have done is appended the Game information to our main DataFrame. The rest of the info is left as NAs, but this will be filled when we created our rolling average features. This is a 'hacky' type of way to complete this task, but works well as we can use the same functions that we created in the previous tutorials on this DataFrame. We now need to add the odds from our odds DataFrame, then we can just run our create features functions as usual. Predicting Next Gameweek's Results \u00b6 Now that we have our feature DataFrame, all we need to do is split the feature DataFrame up into a training set and next week's games, then use the model we tuned in the last tutorial to create predictions! features = create_feature_df(df = df_including_future_games) Creating all games feature DataFrame Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats, odds and market values into one features DataFrame Complete . # Create a feature DataFrame for this week's games. production_df = pd . merge(future_predictions, features, on = [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'season' ]) # Create a training DataFrame training_df = features[ ~ features . gameId . isin(production_df . gameId)] feature_names = [col for col in training_df if col . startswith( 'f_' )] le = LabelEncoder() train_y = le . fit_transform(training_df . result) train_x = training_df[feature_names] lr = LogisticRegression(C =0.01 , solver = 'liblinear' ) lr . fit(train_x, train_y) predicted_probs = lr . predict_proba(production_df[feature_names]) predicted_odds = 1 / predicted_probs # Assign the modelled odds to our predictions df predictions_df = (production_df . loc[:, [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'B365H' , 'B365D' , 'B365A' ]] . assign(homeModelledOdds = [i[ 2 ] for i in predicted_odds], drawModelledOdds = [i[ 1 ] for i in predicted_odds], awayModelledOdds = [i[ 0 ] for i in predicted_odds]) . rename(columns = { 'B365H' : 'BetfairHomeOdds' , 'B365D' : 'BetfairDrawOdds' , 'B365A' : 'BetfairAwayOdds' })) predictions_df Date HomeTeam AwayTeam BetfairHomeOdds BetfairDrawOdds BetfairAwayOdds homeModelledOdds drawModelledOdds awayModelledOdds 0 2018-09-01 Leicester Liverpool 7.80 5.10 1.48 5.747661 5.249857 1.573478 1 2018-09-02 Brighton Fulham 2.36 3.50 3.50 2.183193 3.803120 3.584057 2 2018-09-02 Burnley Man United 6.60 3.90 1.70 5.282620 4.497194 1.699700 3 2018-09-02 Chelsea Bournemouth 1.32 6.80 12.00 1.308366 6.079068 14.047070 4 2018-09-02 Crystal Palace Southampton 2.04 3.55 4.50 2.202871 4.213695 3.239122 5 2018-09-02 Everton Huddersfield 1.54 4.40 8.20 1.641222 3.759249 8.020055 6 2018-09-02 West Ham Wolves 2.62 3.50 2.98 1.999816 4.000456 4.000279 7 2018-09-02 Man City Newcastle 1.12 12.50 32.00 1.043103 29.427939 136.231983 8 2018-09-02 Cardiff Arsenal 7.00 4.30 1.62 6.256929 4.893445 1.572767 9 2018-09-03 Watford Tottenham 5.90 4.30 1.68 5.643663 4.338926 1.688224 Above are the predictions for this Gameweek's matches. In the next tutorial we will explore the errors our model has made, and work on creating a profitable betting strategy.","title":"EPL 04. Weekly predictions"},{"location":"modelling/EPLmodelPart4/#epl-machine-learning-walkthrough","text":"","title":"EPL Machine Learning Walkthrough"},{"location":"modelling/EPLmodelPart4/#04-weekly-predictions","text":"Welcome to the third part of this Machine Learning Walkthrough. This tutorial will be a walk through of creating weekly EPL predictions from the basic logistic regression model we built in the previous tutorial. We will then analyse our predictions and create staking strategies in the next tutorial. Specifically, this tutorial will cover a few things: Obtaining Weekly Odds / Game Info Using Betfair's API Data Wrangling This Week's Game Info Into Our Feature Set","title":"04. Weekly Predictions"},{"location":"modelling/EPLmodelPart4/#obtaining-weekly-odds-game-info-using-betfairs-api","text":"The first thing we need to do to create weekly predictions is get both the games being played this week, as well as match odds from Betfair to be used as features. To make this process easier, I have created a csv file with the fixture for the 2018/19 season. Let's load that now. ## Import libraries import pandas as pd from weekly_prediction_functions import * from data_preparation_functions import * from sklearn.metrics import log_loss, confusion_matrix import warnings warnings . filterwarnings( 'ignore' ) pd . set_option( 'display.max_columns' , 100 ) fixture = (pd . read_csv( 'data/fixture.csv' ) . assign(Date = lambda df: pd . to_datetime(df . Date))) fixture . head() Date Time (AEST) HomeTeam AwayTeam Venue TV Year round season 0 2018-08-11 5:00 AM Man United Leicester Old Trafford, Manchester Optus, Fox Sports (delay) 2018 1 1819 1 2018-08-11 9:30 PM Newcastle Tottenham St.James\u2019 Park, Newcastle Optus, SBS 2018 1 1819 2 2018-08-12 12:00 AM Bournemouth Cardiff Vitality Stadium, Bournemouth Optus 2018 1 1819 3 2018-08-12 12:00 AM Fulham Crystal Palace Craven Cottage, London Optus 2018 1 1819 4 2018-08-12 12:00 AM Huddersfield Chelsea John Smith\u2019s Stadium, Huddersfield Optus, Fox Sports (delay) 2018 1 1819 Now we are going to connect to the API and retrieve game level information for the next week. To do this, we will use an R script. If you are not familiar with R, don't worry, it is relatively simple to read through. For this, we will run the script weekly_game_info_puller.R. Go ahead and run that script now. Note that for this step, you will require a Betfair API App Key. If you don't have one, visit this page and follow the instructions . I will upload an updated weekly file, so you can follow along regardless of if you have an App Key or not. Let's load that file in now. game_info = create_game_info_df( \"data/weekly_game_info.csv\" ) game_info . head( 3 ) AwayTeam HomeTeam awaySelectionId drawSelectionId homeSelectionId draw marketId marketStartTime totalMatched eventId eventName homeOdds drawOdds awayOdds competitionId Date localMarketStartTime 0 Arsenal Cardiff 1096 58805 79343 The Draw 1.146897152 2018-09-02 12:30:00 30123.595116 28852020 Cardiff v Arsenal 7.00 4.3 1.62 10932509 2018-09-02 Sun September 2, 10:30PM 1 Bournemouth Chelsea 1141 58805 55190 The Draw 1.146875421 2018-09-01 14:00:00 30821.329656 28851426 Chelsea v Bournemouth 1.32 6.8 12.00 10932509 2018-09-01 Sun September 2, 12:00AM 2 Fulham Brighton 56764 58805 18567 The Draw 1.146875746 2018-09-01 14:00:00 16594.833096 28851429 Brighton v Fulham 2.36 3.5 3.50 10932509 2018-09-01 Sun September 2, 12:00AM Finally, we will use the API to grab the weekly odds. This R script is also provided, but I have also included the weekly odds csv for convenience. odds = (pd . read_csv( 'data/weekly_epl_odds.csv' ) . replace({ 'Man Utd' : 'Man United' , 'C Palace' : 'Crystal Palace' })) odds . head( 3 ) HomeTeam AwayTeam f_homeOdds f_drawOdds f_awayOdds 0 Leicester Liverpool 7.80 5.1 1.48 1 Brighton Fulham 2.36 3.5 3.50 2 Everton Huddersfield 1.54 4.4 8.20","title":"Obtaining Weekly Odds / Game Info Using Betfair's API"},{"location":"modelling/EPLmodelPart4/#data-wrangling-this-weeks-game-info-into-our-feature-set","text":"Now we have the arduous task of wrangling all of this info into a feature set that we can use to predict this week's games. Luckily our functions we created earlier should work if we just append the non-features to our main dataframe. df = create_df( 'data/epl_data.csv' ) df . head() AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.50 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.90 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.00 2.38 8 A Wiley 2.75 3.25 2.40 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.30 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.40 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.00 2.10 8 M Riley 3.10 3.25 2.20 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.00 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.80 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.20 3.75 8 G Poll 1.80 3.30 4.50 2005 0506 3 0 1 away 3 6.0 13.0 0.0 7.0 4.0 2.0 Birmingham 2.87 3.25 2.37 2.80 3.20 2.30 56.0 21.0 0.00 1.69 2.04 2.87 2.05 1.81 3.16 2.31 1.77 2.24 3.05 2.11 1.85 3.30 2.60 36.0 2005-08-13 13 E0 0.0 0.0 D 6.0 12.0 0.0 15.0 7.0 0.0 0.0 D 1.0 Fulham 2.9 3.0 2.2 2.88 3.00 2.25 8 R Styles 2.80 3.25 2.35 2005 0506 4 0 0 draw 4 6.0 11.0 0.0 13.0 3.0 3.0 West Brom 5.00 3.40 1.72 4.80 3.45 1.65 55.0 23.0 -0.75 1.77 1.94 4.79 1.76 2.10 3.38 1.69 1.90 2.10 5.60 1.83 2.19 3.63 1.80 36.0 2005-08-13 13 E0 0.0 0.0 D 3.0 13.0 0.0 15.0 8.0 0.0 0.0 D 2.0 Man City 4.2 3.2 1.7 4.50 3.25 1.67 8 C Foy 5.00 3.25 1.75 2005 0506 5 0 0 draw Now we need to specify which game week we would like to predict. We will then filter the fixture for this game week and append this info to the main DataFrame round_to_predict = int ( input ( \"Which game week would you like to predict? Please input next week's Game Week \\n \" )) Which game week would you like to predict? Please input next week's Game Week 4 future_predictions = (fixture . loc[fixture[ 'round' ] == round_to_predict, [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'season' ]] . pipe(pd . merge, odds, on = [ 'HomeTeam' , 'AwayTeam' ]) . rename(columns = { 'f_homeOdds' : 'B365H' , 'f_awayOdds' : 'B365A' , 'f_drawOdds' : 'B365D' }) . assign(season = lambda df: df . season . astype( str ))) df_including_future_games = (pd . read_csv( 'data/epl_data.csv' , dtype = { 'season' : str }) . assign(Date = lambda df: pd . to_datetime(df . Date)) . pipe( lambda df: df . dropna(thresh = len (df) - 2 , axis =1 )) # Drop cols with NAs . dropna(axis =0 ) # Drop rows with NAs . sort_values( 'Date' ) . append(future_predictions, sort = True ) . reset_index(drop = True ) . assign(gameId = lambda df: list (df . index + 1 ), Year = lambda df: df . Date . apply( lambda row: row . year), homeWin = lambda df: df . apply( lambda row: 1 if row . FTHG > row . FTAG else 0 , axis =1 ), awayWin = lambda df: df . apply( lambda row: 1 if row . FTAG > row . FTHG else 0 , axis =1 ), result = lambda df: df . apply( lambda row: 'home' if row . FTHG > row . FTAG else ( 'draw' if row . FTHG == row . FTAG else 'away' ), axis =1 ))) df_including_future_games . tail( 12 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 4952 4.0 8.0 0.0 12.0 2.0 1.0 Burnley 4.33 3.40 2.00 4.0 3.3 2.00 39.0 20.0 -0.25 1.65 2.22 4.14 2.22 1.69 3.36 1.98 1.72 2.31 4.5 2.32 1.74 3.57 2.04 36.0 2018-08-26 26.0 E0 2.0 4.0 H 6.0 11.0 0.0 25.0 12.0 2.0 3.0 H 2.0 Fulham 4.10 3.35 1.97 3.90 3.2 2.00 8.0 D Coote 4.33 3.4 2.0 2018 1819 4953 1 0 home 4953 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.9 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.0 1.76 2.25 3.40 2.67 40.0 2018-08-27 27.0 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.60 2.75 3.2 2.55 8.0 C Pawson 2.90 3.3 2.6 2018 1819 4954 0 1 away 4954 NaN NaN NaN NaN NaN NaN Liverpool 1.48 5.10 7.80 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-01 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Leicester NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4955 0 0 away 4955 NaN NaN NaN NaN NaN NaN Fulham 3.50 3.50 2.36 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Brighton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4956 0 0 away 4956 NaN NaN NaN NaN NaN NaN Man United 1.70 3.90 6.60 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Burnley NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4957 0 0 away 4957 NaN NaN NaN NaN NaN NaN Bournemouth 12.00 6.80 1.32 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Chelsea NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4958 0 0 away 4958 NaN NaN NaN NaN NaN NaN Southampton 4.50 3.55 2.04 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Crystal Palace NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4959 0 0 away 4959 NaN NaN NaN NaN NaN NaN Huddersfield 8.20 4.40 1.54 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Everton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4960 0 0 away 4960 NaN NaN NaN NaN NaN NaN Wolves 2.98 3.50 2.62 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN West Ham NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4961 0 0 away 4961 NaN NaN NaN NaN NaN NaN Newcastle 32.00 12.50 1.12 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Man City NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4962 0 0 away 4962 NaN NaN NaN NaN NaN NaN Arsenal 1.62 4.30 7.00 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Cardiff NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4963 0 0 away 4963 NaN NaN NaN NaN NaN NaN Tottenham 1.68 4.30 5.90 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-03 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Watford NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4964 0 0 away As we can see, what we have done is appended the Game information to our main DataFrame. The rest of the info is left as NAs, but this will be filled when we created our rolling average features. This is a 'hacky' type of way to complete this task, but works well as we can use the same functions that we created in the previous tutorials on this DataFrame. We now need to add the odds from our odds DataFrame, then we can just run our create features functions as usual.","title":"Data Wrangling This Week's Game Info Into Our Feature Set"},{"location":"modelling/EPLmodelPart4/#predicting-next-gameweeks-results","text":"Now that we have our feature DataFrame, all we need to do is split the feature DataFrame up into a training set and next week's games, then use the model we tuned in the last tutorial to create predictions! features = create_feature_df(df = df_including_future_games) Creating all games feature DataFrame Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats, odds and market values into one features DataFrame Complete . # Create a feature DataFrame for this week's games. production_df = pd . merge(future_predictions, features, on = [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'season' ]) # Create a training DataFrame training_df = features[ ~ features . gameId . isin(production_df . gameId)] feature_names = [col for col in training_df if col . startswith( 'f_' )] le = LabelEncoder() train_y = le . fit_transform(training_df . result) train_x = training_df[feature_names] lr = LogisticRegression(C =0.01 , solver = 'liblinear' ) lr . fit(train_x, train_y) predicted_probs = lr . predict_proba(production_df[feature_names]) predicted_odds = 1 / predicted_probs # Assign the modelled odds to our predictions df predictions_df = (production_df . loc[:, [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'B365H' , 'B365D' , 'B365A' ]] . assign(homeModelledOdds = [i[ 2 ] for i in predicted_odds], drawModelledOdds = [i[ 1 ] for i in predicted_odds], awayModelledOdds = [i[ 0 ] for i in predicted_odds]) . rename(columns = { 'B365H' : 'BetfairHomeOdds' , 'B365D' : 'BetfairDrawOdds' , 'B365A' : 'BetfairAwayOdds' })) predictions_df Date HomeTeam AwayTeam BetfairHomeOdds BetfairDrawOdds BetfairAwayOdds homeModelledOdds drawModelledOdds awayModelledOdds 0 2018-09-01 Leicester Liverpool 7.80 5.10 1.48 5.747661 5.249857 1.573478 1 2018-09-02 Brighton Fulham 2.36 3.50 3.50 2.183193 3.803120 3.584057 2 2018-09-02 Burnley Man United 6.60 3.90 1.70 5.282620 4.497194 1.699700 3 2018-09-02 Chelsea Bournemouth 1.32 6.80 12.00 1.308366 6.079068 14.047070 4 2018-09-02 Crystal Palace Southampton 2.04 3.55 4.50 2.202871 4.213695 3.239122 5 2018-09-02 Everton Huddersfield 1.54 4.40 8.20 1.641222 3.759249 8.020055 6 2018-09-02 West Ham Wolves 2.62 3.50 2.98 1.999816 4.000456 4.000279 7 2018-09-02 Man City Newcastle 1.12 12.50 32.00 1.043103 29.427939 136.231983 8 2018-09-02 Cardiff Arsenal 7.00 4.30 1.62 6.256929 4.893445 1.572767 9 2018-09-03 Watford Tottenham 5.90 4.30 1.68 5.643663 4.338926 1.688224 Above are the predictions for this Gameweek's matches. In the next tutorial we will explore the errors our model has made, and work on creating a profitable betting strategy.","title":"Predicting Next Gameweek's Results"},{"location":"modelling/brownlowModelTutorial/","text":"Modelling the Brownlow \u00b6 This walkthrough will provide a brief, yet effective tutorial on how to model the Brownlow medal. We will use data from 2010 to 2018, which includes Supercoach points and other useful stats. The output will be the number of votes predicted for each player in each match, and we will aggregate these to create aggregates for each team and for the whole competition. No doubt we'll have Mitchell right up the top, and if we don't, then we know we've done something wrong! # Import modules libraries import pandas as pd import h2o from h2o.automl import H2OAutoML import numpy as np from sklearn.preprocessing import StandardScaler import os import pickle # Change notebook settings pd . options . display . max_columns = None pd . options . display . max_rows = 300 from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" EDA - Read in the data \u00b6 I have collated this data using the fitzRoy R package and merging the afltables dataset with the footywire dataset, so that we can Supercoach and other advanced stats with Brownlow votes. Let's read in the data and have a sneak peak at what it looks like. brownlow_data = pd . read_csv( 'data/afl_brownlow_data.csv' ) brownlow_data . tail( 3 ) date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 76585 2018-08-26 2018 23 Etihad Stadium 12312 9708.0 M Wood 32 North Melbourne St Kilda Away 117 94 23 NaN 0 3 2 66.7 0 0 2 0 0 34 3 0 3 3 1 0 0 0 0 0 0 0 0 0 24 25 76586 2018-08-26 2018 23 Etihad Stadium 11755 9708.0 S Wright 19 North Melbourne St Kilda Away 117 94 23 NaN 10 17 22 75.9 0 0 0 0 1 83 16 13 29 8 0 0 2 0 3 0 4 4 1 0 107 96 76587 2018-08-26 2018 23 Etihad Stadium 11724 9708.0 J Ziebell 7 North Melbourne St Kilda Away 117 94 23 NaN 9 8 11 73.3 0 0 2 4 0 94 12 3 15 6 3 1 2 0 1 0 0 0 2 0 89 109 It looks like we've got about 76,000 rows of data and have stats like hitouts, clangers, effective disposals etc. Let's explore some certain scenarios. Using my domain knowledge of footy, I can hypothesise that if a player kicks 5 goals, he is pretty likely to poll votes. Similarly, if a player gets 30 possessions and 2+ goals, he is also probably likely to poll votes. Let's have a look at the mean votes for players for both of these situations. Exploring votes if a bag is kicked (5+ goals) \u00b6 brownlow_data . query( 'G >= 5' ) . groupby( 'season' ) . brownlow_votes . mean() print ( \"Mean votes is a player has kicked a bag:\" , brownlow_data . query( 'G >= 5' ) . brownlow_votes . mean()) season 2010 1.420455 2011 1.313433 2012 1.413333 2013 1.253731 2014 1.915254 2015 1.765625 2016 1.788732 2017 2.098361 2018 0.000000 Name: brownlow_votes, dtype: float64 Mean votes is a player has kicked a bag: 1.4708818635607321 Exploring votes if the player has 30+ possies & 2+ goals \u00b6 brownlow_data . query( 'G >= 2 and D >= 30' ) . groupby( 'season' ) . brownlow_votes . mean() print ( \"Mean votes if a player has 30 possies and kicks 2+ goals:\" , brownlow_data . query( 'G >= 2 and D >= 30' ) . brownlow_votes . mean()) season 2010 1.826923 2011 1.756410 2012 2.118421 2013 2.000000 2014 2.253731 2015 2.047619 2016 2.103448 2017 2.050000 2018 0.000000 Name: brownlow_votes, dtype: float64 Mean votes if a player has 30 possies and kicks 2+ goals: 1.8741379310344828 As suspected, the average votes for these two situations is 1.87! That's huge. Let's get an idea of the average votes for each player. It should be around 6/44, as there are always 6 votes per match and around 44 players per match. brownlow_data . brownlow_votes . mean() 0.12347341475121326 So the average vote is 0.12. Let's see how this changes is the player is a captain. I have collected data on if players are captains from wikipedia and collated it into a csv. Let's load this in and create a \"Is the player captain\" feature, then check the average votes for captains. Create Is Player Captain Feature \u00b6 captains = pd . read_csv( 'data/captains.csv' ) . set_index( 'player' ) def is_captain_for_that_season (captains_df, player, year): if player in captains_df . index: # Get years they were captain seasons = captains_df . loc[player] . season . split( '-' ) if len (seasons) == 1 : seasons_captain = list ( map ( int , seasons)) elif len (seasons) == 2 : if seasons[ 1 ] == '' : seasons_captain = list ( range ( int (seasons[ 0 ]), 2019 )) else : seasons_captain = list ( range ( int (seasons[ 0 ]), int (seasons[ 1 ]) + 1 )) if year in seasons_captain: return 1 return 0 brownlow_data[ 'is_captain' ] = brownlow_data . apply( lambda x: is_captain_for_that_season(captains, x . player, x . season), axis = 'columns' ) brownlow_data . query( 'is_captain == 1' ) . groupby( 'season' ) . brownlow_votes . mean() print ( \"Mean votes if a player is captain:\" , brownlow_data . query( 'is_captain == 1' ) . brownlow_votes . mean()) season 2010 0.408497 2011 0.429936 2012 0.274194 2013 0.438725 2014 0.519663 2015 0.447222 2016 0.347826 2017 0.425806 2018 0.000000 Name: brownlow_votes, dtype: float64 Mean votes if a player is captain: 0.36661698956780925 This is significantly higher than if they aren't captain. What would be interesting is to look at the average difference in votes between when they were captain and when they weren't, to try and find if there is a 'captain bias' in brownlow votes. Go ahead and try. For now, we're going to move onto feature creation Feature Creation \u00b6 Let's make a range of features, including: Ratios of each statistic per game If the player is a captain If they kicked a bag (\u2158+) If they kicked 2 and had 30+ possies First we will make features of ratios. What is important is not how many of a certain stat a player has, but how much of that stat a player has relative to everyone else in the same match. It doesn't matter if Dusty Martin has 31 possessions if Tom Mitchell has had 50 - Mitchell is probably more likely to poll (assuming all else is equal). So rather than using the actual number of possessions for example, we can divide these possessions by the total amount of possessions in the game. To do this we'll use pandas groupby and transform methods. Create Ratios As Features \u00b6 %% time # Get a list of stats of which to create ratios for ratio_cols = [ 'CP' , 'UP' , 'ED' , 'DE' , 'CM' , 'GA' , 'MI5' , 'one_perc' , 'BO' , 'TOG' , 'K' , 'HB' , 'D' , 'M' , 'G' , 'B' , 'T' , 'HO' , 'I50' , 'CL' , 'CG' , 'R50' , 'FF' , 'FA' , 'AF' , 'SC' ] # Create a ratios df ratios = (brownlow_data . copy() . loc[:, [ 'match_id' ] + ratio_cols] . groupby( 'match_id' ) . transform( lambda x: x / x . sum())) feature_cols = [ 'date' , 'season' , 'round' , 'venue' , 'ID' , 'match_id' , 'player' , 'jumper_no' , 'team' , 'opposition' , 'status' , 'team_score' , 'opposition_score' , 'margin' , 'brownlow_votes' ] # Create a features df - join the ratios to this df features = (brownlow_data[feature_cols] . copy() . join(ratios)) Wall time: 11.3 s features . head() date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 0 2010-03-28 2010 1 Domain Stadium 11807 5096.0 T Armstrong 38 Adelaide Fremantle Away 62 118 -56 0.0 0.013393 0.016304 0.015517 0.025529 0.0 0.0 0.00 0.013333 0.000000 0.020516 0.013441 0.014599 0.014049 0.022599 0.000000 0.000000 0.000000 0.000000 0.011494 0.000000 0.036145 0.018868 0.000000 0.037037 0.011568 0.005456 1 2010-03-28 2010 1 Domain Stadium 3968 5096.0 N Bock 44 Adelaide Fremantle Away 62 118 -56 0.0 0.008929 0.025362 0.022414 0.023875 0.0 0.0 0.00 0.080000 0.000000 0.027169 0.032258 0.012165 0.021711 0.016949 0.038462 0.000000 0.026549 0.029851 0.022989 0.000000 0.024096 0.018868 0.037037 0.000000 0.024422 0.021825 2 2010-03-28 2010 1 Domain Stadium 11712 5096.0 M Cook 8 Adelaide Fremantle Away 62 118 -56 0.0 0.035714 0.025362 0.031034 0.026746 0.0 0.0 0.00 0.013333 0.034483 0.021625 0.021505 0.031630 0.026820 0.022599 0.000000 0.045455 0.008850 0.014925 0.011494 0.028986 0.048193 0.056604 0.000000 0.037037 0.020887 0.019703 3 2010-03-28 2010 1 Domain Stadium 11700 5096.0 P Dangerfield 32 Adelaide Fremantle Away 62 118 -56 0.0 0.049107 0.016304 0.017241 0.015605 0.0 0.0 0.04 0.026667 0.172414 0.022734 0.021505 0.029197 0.025543 0.016949 0.038462 0.090909 0.026549 0.000000 0.022989 0.072464 0.024096 0.018868 0.074074 0.037037 0.024422 0.028190 4 2010-03-28 2010 1 Domain Stadium 85 5096.0 M Doughty 11 Adelaide Fremantle Away 62 118 -56 0.0 0.017857 0.021739 0.025862 0.027526 0.0 0.0 0.00 0.053333 0.000000 0.025229 0.010753 0.031630 0.021711 0.016949 0.000000 0.000000 0.026549 0.000000 0.011494 0.000000 0.036145 0.037736 0.000000 0.000000 0.018959 0.018490 Kicked A Bag Feature \u00b6 features[ 'kicked_a_bag' ] = brownlow_data . G . apply( lambda x: 1 if x >= 5 else 0 ) Is Captain Feature \u00b6 features[ 'is_captain' ] = features . apply( lambda x: is_captain_for_that_season(captains, x . player, x . season), axis = 'columns' ) Won the Game Feature \u00b6 features[ 'team_won' ] = np . where(features . margin > 0 , 1 , 0 ) 30+ & 2+ Goals Feature \u00b6 features[ 'got_30_possies_2_goals' ] = np . where((brownlow_data . G >= 2 ) & (brownlow_data . D >= 30 ), 1 , 0 ) Previous Top 10 Finish Feature \u00b6 I have a strong feeling that past performance may be a predictor of future performance in the brownlow. For example, last year Dusty Martin won the Brownlow. The umpires may have a bias towards Dusty this year because he is known to be on their radar as being a good player. Let's create a feature which is categorical and is 1 if the player has previously finished in the top 10. Let's create a function for this and then apply it to the afltables dataset, which has data back to 1897. We will then create a lookup table for the top 10 for each season and merge this table with our current features df. afltables = pd . read_csv( 'data/afltables_stats.csv' ) . query( 'Season >= 2000' ) def replace_special_characters (name): name = name . replace( \"'\" , \"\" ) . replace( \"-\" , \" \" ) . lower() name_split = name . split() if len (name_split) > 2 : first_name = name_split[ 0 ] last_name = name_split[ -1 ] name = first_name + ' ' + last_name name_split_2 = name . split() name = name_split_2[ 0 ][ 0 ] + ' ' + name_split_2[ 1 ] return name . title() afltables = (afltables . assign(player = lambda df: df[ 'First.name' ] + ' ' + df . Surname) . assign(player = lambda df: df . player . apply(replace_special_characters)) . rename(columns = { 'Brownlow.Votes' : 'brownlow_votes' , 'Season' : 'season' , 'Playing.for' : 'team' })) ### Create Top 10 rank look up table brownlow_votes_yearly = (afltables . groupby([ 'season' , 'player' , 'team' ], as_index = False ) . brownlow_votes . sum()) brownlow_votes_yearly[ 'yearly_rank' ] = (brownlow_votes_yearly . groupby( 'season' ) . brownlow_votes . rank(method = 'max' , ascending = False )) # Filter to only get a dataframe since 2000 and only the top 10 players from each season brownlow_votes_top_10 = brownlow_votes_yearly . query( 'yearly_rank < 11 & season >= 2000' ) brownlow_votes_top_10 . head( 3 ) def how_many_times_top_10 (top_10_df, player, year): times = len (top_10_df[(top_10_df . player == player) & (top_10_df . season < year)]) return times features[ 'times_in_top_10' ] = features . apply( lambda x: how_many_times_top_10(brownlow_votes_top_10, x . player, x . season), axis =1 ) season player team brownlow_votes yearly_rank 27 2000.0 A Koutoufides Carlton 19.0 4.0 36 2000.0 A Mcleod Adelaide 20.0 3.0 105 2000.0 B Ratten Carlton 18.0 6.0 Average Brownlow Votes Per Game Last Season Feature \u00b6 # Create a brownlow votes lookup table brownlow_votes_lookup_table = (brownlow_data . groupby([ 'player' , 'team' , 'season' ], as_index = False ) . brownlow_votes . mean() . assign(next_season = lambda df: df . season + 1 ) . rename(columns = { 'brownlow_votes' : 'ave_votes_last_season' })) # Have a look at Cripps to check if it's working brownlow_votes_lookup_table[brownlow_votes_lookup_table . player == 'P Cripps' ] # Merge it to our features df features_with_votes_last_season = (pd . merge(features, brownlow_votes_lookup_table . drop(columns = 'season' ), left_on = [ 'player' , 'team' , 'season' ], right_on = [ 'player' , 'team' , 'next_season' ], how = 'left' ) . drop(columns = [ 'next_season' ]) . fillna( 0 )) player team season ave_votes_last_season next_season 4377 P Cripps Carlton 2014 0.000000 2015 4378 P Cripps Carlton 2015 0.300000 2016 4379 P Cripps Carlton 2016 0.857143 2017 4380 P Cripps Carlton 2017 0.333333 2018 4381 P Cripps Carlton 2018 0.000000 2019 Historic Performance Relative To Model Feature \u00b6 It is well known that some players are good Brownlow performers. For whatever reason, they always poll much better than their stats may suggest. Lance Franklin and Bontempelli are probably in this category. Perhaps these players have an X-factor that Machine Learning models struggle to pick up on. To get around this, let's create a feature which looks at the player's performance relative to the model's prediction. To do this, we'll need to train and predict 7 different models - from 2011 to 2017. To create a model for each season, we will use h2o's AutoML. If you're new to h2o, please read about it here. It can be used in both R and Python. The metric we will use for loss in Mean Absolute Error (MAE). As we are using regression, some values are negative. We will convert these negative values to 0 as it doesn't make sense to poll negative brownlow votes. Similarly, some matches won't predict exactly 6 votes, so we will scale these predictions so that we predict exactly 6 votes for each match. So that you don't have to train these models yourself, I have saved the models and we will load them in. If you are keen to train the models yourself, simply uncomment out the code below and run the cell. To bulk uncomment, highlight the rows and press ctrl + '/' h2o . init() # Uncomment the code below if you want to train the models yourself - otherwise, we will load them in the load cell from disk ## Join to our features df # aml_yearly_model_objects = {} # yearly_predictions_dfs = {} # feature_cols = ['margin', 'CP', 'UP', 'ED', 'DE', # 'CM', 'GA', 'MI5', 'one_perc', 'BO', 'TOG', 'K', 'HB', 'D', 'M', 'G', # 'B', 'T', 'HO', 'I50', 'CL', 'CG', 'R50', 'FF', 'FA', 'AF', 'SC'] # for year in range(2011, 2018): # # Filter the data to only include past data # train_historic = brownlow_data[brownlow_data.season < year].copy() # # Convert to an h2o frame # train_h2o_historic = h2o.H2OFrame(train_historic) # # Create an AutoML object # aml = H2OAutoML(max_runtime_secs=30, # balance_classes=True, # seed=42) # # Train the model # aml.train(y='brownlow_votes', x=feature_cols, training_frame=train_h2o_historic) # # save the model # model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True) # # Get model id # model_name = aml.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/yearly_model_{year}') # # Append the best model to a list # aml_yearly_model_objects[year] = aml.leader # # Make predictions on test set for that year # test_historic = brownlow_data[brownlow_data.season == year].copy() # test_h2o_historic = h2o.H2OFrame(test_historic) # preds = aml.predict(test_h2o_historic).as_data_frame() # test_historic['predicted_votes'] = preds.values # # Convert negative predictions to 0 # test_historic['predicted_votes_neg_to_0'] = test_historic.predicted_votes.apply(lambda x: 0 if x < 0 else x) # # Create a total match votes column - which calculates the number of votes predicted in each game when the predictions # # are unscaled # test_historic['unscaled_match_votes'] = test_historic.groupby('match_id').predicted_votes_neg_to_0.transform('sum') # # Scale predictions # test_historic['predicted_votes_scaled'] = test_historic.predicted_votes_neg_to_0 / test_historic.unscaled_match_votes * 6 # # Aggregate the predictions # test_grouped = (test_historic.groupby(['player', 'team'], as_index=False) # .sum() # .sort_values(by='brownlow_votes', ascending=False) # .assign(mae=lambda df: abs(df.predicted_votes_scaled - df.brownlow_votes))) # test_grouped['error'] = test_grouped.predicted_votes_scaled - test_grouped.brownlow_votes # test_grouped['next_year'] = year + 1 # # Add this years predictions df to a dictionary to use later # yearly_predictions_dfs[year] = test_grouped # preds_errors = None # for key, value in yearly_predictions_dfs.items(): # if preds_errors is None: # preds_errors = value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']] # else: # preds_errors = preds_errors.append(value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']], sort=True) # with open('data/prediction_errors_df.pickle', 'wb') as handle: # pickle.dump(yearly_predicted_errors, handle) Checking whether there is an H2O instance running at http://localhost:54321 . connected. H2O cluster uptime: 1 hour 28 mins H2O cluster timezone: Australia/Hobart H2O data parsing timezone: UTC H2O cluster version: 3.20.0.4 H2O cluster version age: 1 month and 18 days H2O cluster name: H2O_from_python_WardJ_tt2ak5 H2O cluster total nodes: 1 H2O cluster free memory: 7.018 Gb H2O cluster total cores: 4 H2O cluster allowed cores: 4 H2O cluster status: locked, healthy H2O connection url: http://localhost:54321 H2O connection proxy: None H2O internal security: False H2O API Extensions: Algos, AutoML, Core V3, Core V4 Python version: 3.6.4 final # Load predictions error df with open ( 'data/prediction_errors_df.pickle' , 'rb' ) as handle: preds_errors = pickle . load(handle) # Look at last years predictions preds_errors . query( 'next_year == 2018' ) . sort_values(by = 'brownlow_votes' , ascending = False ) . head( 20 ) brownlow_votes error next_year player predicted_votes_scaled season 139 36.0 -4.915157 2018 D Martin 31.084843 44374 486 33.0 -4.413780 2018 P Dangerfield 28.586220 42357 619 25.0 0.072223 2018 T Mitchell 25.072223 44374 279 23.0 -9.296209 2018 J Kennedy 13.703791 38323 376 22.0 -7.621336 2018 L Franklin 14.378664 44374 278 21.0 -3.016176 2018 J Kelly 17.983824 42357 519 20.0 -4.182915 2018 R Sloane 15.817085 44374 410 19.0 -7.740142 2018 M Bontempelli 11.259858 44374 483 18.0 -4.522835 2018 O Wines 13.477165 44374 121 17.0 -4.446749 2018 D Beams 12.553251 38323 390 16.0 -3.620094 2018 L Parker 12.379906 44374 561 15.0 -5.846506 2018 S Pendlebury 9.153494 32272 463 15.0 -4.402600 2018 N Fyfe 10.597400 42357 42 15.0 -4.787914 2018 B Ebert 10.212086 44374 651 15.0 0.824510 2018 Z Merrett 15.824510 42357 578 14.0 2.300607 2018 T Adams 16.300607 44374 34 14.0 -5.796604 2018 B Brown 8.203396 44374 172 14.0 -1.650752 2018 D Zorko 12.349248 42357 184 14.0 -1.233663 2018 G Ablett 12.766337 28238 389 14.0 -2.099613 2018 L Neale 11.900387 42357 Look at that! A simply Machine Learning ensemble model, using AutoML predicted last year's winner! That's impressive. As we can see it also predicted Bontempelli would only score 11.26, when he actually scored 19 - a huge discrepency. Let's use this as a feature. features_with_historic_perf_relative_to_model = \\ (features_with_votes_last_season . pipe(pd . merge, preds_errors[[ 'player' , 'next_year' , 'error' ]], left_on = [ 'player' , 'season' ], right_on = [ 'player' , 'next_year' ], how = 'left' ) . fillna( 0 ) . rename(columns = { 'error' : 'error_last_season' }) . drop_duplicates(subset = [ 'player' , 'round' , 'SC' ])) Filtering the data to only include the top 20 SC for each match \u00b6 Logically, it is extremely unlikely that a player will poll votes if their Supercoach score is not in the top 20 players. By eliminating the other 20+ players, we can reduce the noise in the data, as we are almost certain the players won't poll from the bottom half. Let's explore how many players poll if they're not in the top 20, and then filter our df if this number is not significant. # Find number of players who vote when in top 15 SC brownlow_data[ 'SC_rank_match' ] = brownlow_data . groupby( 'match_id' ) . SC . rank(method = 'max' , ascending = False ) brownlow_data . query( 'SC_rank_match > 20 and season > 2014' ) . brownlow_votes . value_counts() 0.0 18330 1.0 14 2.0 8 3.0 2 Name: brownlow_votes, dtype: int64 Since 2014, there have only been 24 players who have voted and not been in the top 20 SC. features_with_sc_rank = features_with_historic_perf_relative_to_model . copy() features_with_sc_rank[ 'SC_rank_match' ] = features_with_sc_rank . groupby( 'match_id' ) . SC . rank(method = 'max' , ascending = False ) # Filter out rows with a SC rank of below 20 features_with_sc_rank_filtered = features_with_sc_rank . query( 'SC_rank_match <= 20' ) # Filter out 2010 and 2011 as we used these seasons to create historic model performance features features_last_before_train = features_with_sc_rank_filtered . query( 'season != 2010 and season != 2011' ) . reset_index(drop = True ) features_last_before_train . head( 3 ) date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC kicked_a_bag is_captain team_won got_30_possies_2_goals times_in_top_10 ave_votes_last_season next_year error_last_season SC_rank_match 0 2012-03-31 2012 1 Metricon Stadium 11985 5347.0 I Callinan 37 Adelaide Gold Coast Away 137 68 69 0.0 0.031359 0.018947 0.016071 0.015745 0.0 0.000000 0.12 0.000000 0.000000 0.023333 0.031674 0.012270 0.023438 0.026042 0.068966 0.217391 0.041322 0.0 0.017241 0.013699 0.042553 0.000000 0.000000 0.032258 0.030593 0.026962 0 0 1 0 0 0.000000 2012.0 0.042048 14.0 1 2012-03-31 2012 1 Metricon Stadium 11700 5347.0 P Dangerfield 32 Adelaide Gold Coast Away 137 68 69 0.0 0.048780 0.023158 0.037500 0.026451 0.0 0.000000 0.04 0.009804 0.090909 0.025556 0.029412 0.036810 0.032552 0.015625 0.068966 0.000000 0.016529 0.0 0.051724 0.123288 0.042553 0.000000 0.000000 0.032258 0.027503 0.028173 0 0 1 0 0 0.333333 2012.0 -2.983636 12.0 2 2012-03-31 2012 1 Metricon Stadium 2381 5347.0 R Douglas 26 Adelaide Gold Coast Away 137 68 69 0.0 0.020906 0.033684 0.021429 0.019901 0.0 0.083333 0.04 0.049020 0.000000 0.023056 0.031674 0.015337 0.024740 0.031250 0.034483 0.130435 0.057851 0.0 0.043103 0.000000 0.031915 0.014493 0.064516 0.000000 0.033684 0.030597 0 0 1 0 0 0.000000 2012.0 1.724915 10.0 Modeling The 2017 Brownlow \u00b6 Now that we have all of our features, we can simply create a training set (2012-2016), and a test set (2017), and make our predictions for last year! We will use AutoML for this process again. Again, rather than waiting for the model to train, I will save the model so you can simply load it in. We will also scale our features. We can then see how our model went in predicting last year's brownlow, creating a baseline for this years' predictions. We will then predict this year's vote count. train_baseline = features_last_before_train . query( \"season < 2017\" ) holdout = features_last_before_train . query( \"season == 2017\" ) scale_cols = [ 'team_score' , 'opposition_score' , 'margin' , 'CP' , 'UP' , 'ED' , 'DE' , 'CM' , 'GA' , 'MI5' , 'one_perc' , 'BO' , 'K' , 'HB' , 'D' , 'M' , 'G' , 'B' , 'T' , 'HO' , 'I50' , 'CL' , 'CG' , 'R50' , 'FF' , 'FA' , 'AF' , 'SC' ] other_feature_cols = [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' , 'times_in_top_10' , 'ave_votes_last_season' , 'error_last_season' , 'SC_rank_match' ] all_feature_cols = scale_cols + other_feature_cols # Scale features scaler = StandardScaler() train_baseline_scaled = train_baseline . copy() train_baseline_scaled[scale_cols] = scaler . fit_transform(train_baseline[scale_cols]) holdout_scaled = holdout . copy() holdout_scaled[scale_cols] = scaler . transform(holdout[scale_cols]) # Convert categorical columns to categoricals train_baseline_h2o = h2o . H2OFrame(train_baseline_scaled) holdout_h2o = h2o . H2OFrame(holdout_scaled) for col in [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' ]: train_baseline_h2o[col] = train_baseline_h2o[col] . asfactor() holdout_h2o[col] = holdout_h2o[col] . asfactor() C:\\Users\\wardj\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h2o\\utils\\shared_utils.py:177: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. data = _handle_python_lists(python_obj.as_matrix().tolist(), -1)[1] Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% Below I have commented out training and saving the 2017 model. Rather than training it again, we will just load it in. Uncomment this part out if you want to train it yourself. # aml_2017_model = H2OAutoML(max_runtime_secs = 60*3, # balance_classes=True, # seed=42) # aml_2017_model.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_baseline_h2o) # save the model # model_path = h2o.save_model(model=aml_2017_model.leader, path=\"models\", force=True) # Get model id # model_name = aml_2017_model.leaderboard[0, 'model_id'] # Rename the model on disk # os.rename(f'models/{model_name}', f'models/brownlow_2017_model_v1') # Load model in aml_2017_model = h2o . load_model( 'models/brownlow_2017_model_v1' ) # Predict the 2017 brownlow count preds_final_2017_model = aml_2017_model . predict(holdout_h2o) # Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total holdout = (holdout . assign(predicted_votes = preds_final_2017_model . as_data_frame() . values) . assign(predicted_votes_neg_to_0 = lambda df: df . predicted_votes . apply( lambda x: 0 if x <0 else x)) . assign(unscaled_match_votes = lambda df: df . groupby( 'match_id' ) . predicted_votes_neg_to_0 . transform( 'sum' )) . assign(predicted_votes_scaled = lambda df: df . predicted_votes_neg_to_0 / df . unscaled_match_votes * 6 )) # Create an aggregate votes df and show the average SC points and goals scored agg_predictions_2017 = (holdout . groupby([ 'player' , 'team' ], as_index = False ) . agg({ 'brownlow_votes' : sum , 'predicted_votes_scaled' : sum , 'SC' : 'mean' , 'G' : 'mean' }) . sort_values(by = 'brownlow_votes' , ascending = False ) . assign(mae = lambda df: abs (df . brownlow_votes - df . predicted_votes_scaled)) . reset_index(drop = True )) stackedensemble prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% agg_predictions_2017 . head( 15 ) player team brownlow_votes predicted_votes_scaled SC G mae 0 D Martin Richmond 36.0 37.271060 0.037862 0.064869 1.271060 1 P Dangerfield Geelong 33.0 39.288122 0.042441 0.070819 6.288122 2 T Mitchell Hawthorn 25.0 28.629859 0.036040 0.016928 3.629859 3 L Franklin Sydney 22.0 16.353733 0.034640 0.149203 5.646267 4 J Kelly GWS 21.0 19.565321 0.034652 0.033772 1.434679 5 R Sloane Adelaide 20.0 21.417347 0.037068 0.034821 1.417347 6 J Kennedy Sydney 20.0 13.671891 0.032014 0.030508 6.328109 7 M Bontempelli Western Bulldogs 19.0 17.461889 0.033233 0.040498 1.538111 8 D Beams Brisbane 17.0 15.414730 0.034848 0.044998 1.585270 9 O Wines Port Adelaide 16.0 12.973973 0.031601 0.021967 3.026027 10 N Fyfe Fremantle 15.0 11.926030 0.033761 0.031680 3.073970 11 S Pendlebury Collingwood 15.0 10.845214 0.033855 0.013660 4.154786 12 B Ebert Port Adelaide 15.0 7.633526 0.032795 0.008431 7.366474 13 L Parker Sydney 15.0 14.680079 0.031366 0.030311 0.319921 14 Z Merrett Essendon 15.0 21.063889 0.033737 0.015362 6.063889 So whilst our model predicted Dangerfield to win, it was pretty damn accurate! Let's find the MAE for the top 100, 50, 25, and 10, and then compare it to 2018's MAE in week, when the Brownlow has been counted. for top_x in [ 10 , 25 , 50 , 100 ]: temp_mae = round (agg_predictions_2017 . iloc[:top_x] . mae . mean(), 3 ) print (f \"The Average Mean Absolute Error for the top {top_x} is {temp_mae}\" ) The Average Mean Absolute Error for the top 10 is 3.216 The Average Mean Absolute Error for the top 25 is 2.931 The Average Mean Absolute Error for the top 50 is 3.15 The Average Mean Absolute Error for the top 100 is 2.577 Modelling This Year's Brownlow \u00b6 Let's now predict this year's vote count. These predictions will be on the front page of the GitHub. train = features_last_before_train . query( \"season < 2018\" ) test = features_last_before_train . query( \"season == 2018\" ) # Scale features scaler = StandardScaler() train_scaled = train . copy() train_scaled[scale_cols] = scaler . fit_transform(train[scale_cols]) test_scaled = test . copy() test_scaled[scale_cols] = scaler . transform(test[scale_cols]) # Convert categorical columns to categoricals train_h2o = h2o . H2OFrame(train_scaled) test_h2o = h2o . H2OFrame(test_scaled) for col in [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' ]: train_h2o[col] = train_h2o[col] . asfactor() test_h2o[col] = test_h2o[col] . asfactor() C:\\Users\\wardj\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h2o\\utils\\shared_utils.py:177: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. data = _handle_python_lists(python_obj.as_matrix().tolist(), -1)[1] Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% # Train the model - this part is commented out as we will just load our model from disk # aml = H2OAutoML(max_runtime_secs = 60*3, # balance_classes=True, # seed=42) # aml.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_h2o) # # save the model # model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True) # # Get model id # model_name = aml.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/brownlow_2018_model_v1') # Load model in aml = h2o . load_model( 'models/brownlow_2018_model_v1' ) # Predict the 2018 brownlow count preds_final_2018_model = aml . predict(test_h2o) # Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total test = (test . assign(predicted_votes = preds_final_2018_model . as_data_frame() . values) . assign(predicted_votes_neg_to_0 = lambda df: df . predicted_votes . apply( lambda x: 0 if x <0 else x)) . assign(unscaled_match_votes = lambda df: df . groupby( 'match_id' ) . predicted_votes_neg_to_0 . transform( 'sum' )) . assign(predicted_votes_scaled = lambda df: df . predicted_votes_neg_to_0 / df . unscaled_match_votes * 6 )) # Create an aggregate votes df and show the average SC points and goals scored agg_predictions_2018 = (test . groupby([ 'player' , 'team' ], as_index = False ) . agg({ 'predicted_votes_scaled' : sum , 'match_id' : 'count' }) # shows how many games they played . sort_values(by = 'predicted_votes_scaled' , ascending = False ) . reset_index(drop = True )) stackedensemble prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% # Show the top 25 predictions agg_predictions_2018 . head( 25 ) player team predicted_votes_scaled match_id 0 T Mitchell Hawthorn 35.484614 20 1 M Gawn Melbourne 21.544278 22 2 D Martin Richmond 20.444488 19 3 B Grundy Collingwood 19.543511 22 4 C Oliver Melbourne 19.009628 20 5 J Macrae Western Bulldogs 18.931594 17 6 P Dangerfield Geelong 18.621242 21 7 D Beams Brisbane 17.621222 15 8 E Yeo West Coast 16.015638 20 9 L Neale Fremantle 15.495083 21 10 A Gaff West Coast 15.165629 18 11 D Heppell Essendon 15.083797 19 12 J Selwood Geelong 14.989096 18 13 S Sidebottom Collingwood 14.863136 18 14 N Fyfe Fremantle 14.692243 11 15 J Kennedy Sydney 14.404489 16 16 Z Merrett Essendon 13.632131 18 17 M Crouch Adelaide 13.503858 16 18 R Laird Adelaide 13.274869 19 19 P Cripps Carlton 13.240568 21 20 G Ablett Geelong 13.018950 15 21 L Franklin Sydney 12.792476 13 22 J Lloyd Sydney 12.174224 20 23 J Kelly GWS 11.982157 14 24 C Ward GWS 11.892443 19 print (agg_predictions_2018 . head( 15 )) player team predicted_votes_scaled match_id 0 T Mitchell Hawthorn 35.484614 1 M Gawn Melbourne 21.544278 2 D Martin Richmond 20.444488 3 B Grundy Collingwood 19.543511 4 C Oliver Melbourne 19.009628 5 J Macrae Western Bulldogs 18.931594 6 P Dangerfield Geelong 18.621242 7 D Beams Brisbane 17.621222 8 E Yeo West Coast 16.015638 9 L Neale Fremantle 15.495083 10 A Gaff West Coast 15.165629 11 D Heppell Essendon 15.083797 12 J Selwood Geelong 14.989096 13 S Sidebottom Collingwood 14.863136 14 N Fyfe Fremantle 14.692243 Now that we have the top 25, let's also look at the top 3 from each team. agg_predictions_2018 . sort_values(by = [ 'team' , 'predicted_votes_scaled' ], ascending = [ True , False ]) . groupby( 'team' ) . head( 3 ) player team predicted_votes_scaled match_id 17 M Crouch Adelaide 13.503858 16 18 R Laird Adelaide 13.274869 19 51 B Gibbs Adelaide 7.783425 18 7 D Beams Brisbane 17.621222 15 46 D Zorko Brisbane 8.123915 14 55 S Martin Brisbane 7.243428 19 19 P Cripps Carlton 13.240568 21 50 K Simpson Carlton 7.864995 18 88 E Curnow Carlton 3.725292 19 3 B Grundy Collingwood 19.543511 22 13 S Sidebottom Collingwood 14.863136 18 31 A Treloar Collingwood 10.487535 11 11 D Heppell Essendon 15.083797 19 16 Z Merrett Essendon 13.632131 18 57 D Smith Essendon 6.838857 20 9 L Neale Fremantle 15.495083 21 14 N Fyfe Fremantle 14.692243 11 74 M Walters Fremantle 5.120949 12 23 J Kelly GWS 11.982157 14 24 C Ward GWS 11.892443 19 25 S Coniglio GWS 11.785273 20 6 P Dangerfield Geelong 18.621242 21 12 J Selwood Geelong 14.989096 18 20 G Ablett Geelong 13.018950 15 80 J Witts Gold Coast 4.617273 13 85 J Lyons Gold Coast 4.043711 14 114 B Fiorini Gold Coast 2.798683 6 0 T Mitchell Hawthorn 35.484614 20 39 L Breust Hawthorn 9.286521 16 45 J Gunston Hawthorn 8.341824 19 1 M Gawn Melbourne 21.544278 22 4 C Oliver Melbourne 19.009628 20 32 J Hogan Melbourne 10.198873 13 26 S Higgins North Melbourne 11.327596 19 37 B Brown North Melbourne 9.550205 13 42 B Cunnington North Melbourne 8.857294 17 27 O Wines Port Adelaide 11.118795 16 36 R Gray Port Adelaide 9.606960 17 54 J Westhoff Port Adelaide 7.355449 21 2 D Martin Richmond 20.444488 19 30 J Riewoldt Richmond 10.557749 15 59 T Cotchin Richmond 6.741732 12 41 S Ross St Kilda 9.093395 17 48 J Steven St Kilda 8.099164 17 90 J Steele St Kilda 3.617769 16 15 J Kennedy Sydney 14.404489 16 21 L Franklin Sydney 12.792476 13 22 J Lloyd Sydney 12.174224 20 8 E Yeo West Coast 16.015638 20 10 A Gaff West Coast 15.165629 18 38 J Redden West Coast 9.368163 16 5 J Macrae Western Bulldogs 18.931594 17 40 M Bontempelli Western Bulldogs 9.272771 16 44 L Hunter Western Bulldogs 8.349606 17 If you're looking for a round by round breakdown, just have a look at the test dataframe. test[[ 'date' , 'round' , 'player' , 'team' , 'opposition' , 'margin' , 'SC' , 'predicted_votes_scaled' ]] . tail( 25 ) date round player team opposition margin SC predicted_votes_scaled 27231 2018-08-26 23 R Lobb GWS Melbourne -45 0.027576 0.007501 27232 2018-08-26 23 H Perryman GWS Melbourne -45 0.025758 0.000000 27233 2018-08-26 23 D Shiel GWS Melbourne -45 0.026364 0.000000 27234 2018-08-26 23 A Tomlinson GWS Melbourne -45 0.026667 0.000000 27235 2018-08-26 23 C Ward GWS Melbourne -45 0.035455 0.332057 27236 2018-08-26 23 L Austin St Kilda North Melbourne -23 0.027853 0.003928 27237 2018-08-26 23 J Geary St Kilda North Melbourne -23 0.025129 0.000000 27238 2018-08-26 23 S Gilbert St Kilda North Melbourne -23 0.025734 0.000000 27239 2018-08-26 23 R Marshall St Kilda North Melbourne -23 0.028156 0.103389 27240 2018-08-26 23 B Paton St Kilda North Melbourne -23 0.023918 0.000000 27241 2018-08-26 23 S Ross St Kilda North Melbourne -23 0.039055 0.300392 27242 2018-08-26 23 J Steele St Kilda North Melbourne -23 0.042386 1.094723 27243 2018-08-26 23 J Steven St Kilda North Melbourne -23 0.046624 1.119684 27244 2018-08-26 23 R Clarke North Melbourne St Kilda 23 0.021193 0.050859 27245 2018-08-26 23 B Cunnington North Melbourne St Kilda 23 0.032698 0.212364 27246 2018-08-26 23 M Daw North Melbourne St Kilda 23 0.022404 0.021889 27247 2018-08-26 23 T Dumont North Melbourne St Kilda 23 0.049046 2.042550 27248 2018-08-26 23 T Goldstein North Melbourne St Kilda 23 0.037844 0.347199 27249 2018-08-26 23 S Higgins North Melbourne St Kilda 23 0.039358 0.508419 27250 2018-08-26 23 N Hrovat North Melbourne St Kilda 23 0.025129 0.012548 27251 2018-08-26 23 J Macmillan North Melbourne St Kilda 23 0.024220 0.000000 27252 2018-08-26 23 J Waite North Melbourne St Kilda 23 0.025734 0.057826 27253 2018-08-26 23 M Williams North Melbourne St Kilda 23 0.023312 0.000000 27254 2018-08-26 23 S Wright North Melbourne St Kilda 23 0.029064 0.093219 27255 2018-08-26 23 J Ziebell North Melbourne St Kilda 23 0.033000 0.031010 And there we have it! In a single notebook we have made a fairly good Brownlow predictive model. Enjoy.","title":"Modelling the Brownlow Medal"},{"location":"modelling/brownlowModelTutorial/#modelling-the-brownlow","text":"This walkthrough will provide a brief, yet effective tutorial on how to model the Brownlow medal. We will use data from 2010 to 2018, which includes Supercoach points and other useful stats. The output will be the number of votes predicted for each player in each match, and we will aggregate these to create aggregates for each team and for the whole competition. No doubt we'll have Mitchell right up the top, and if we don't, then we know we've done something wrong! # Import modules libraries import pandas as pd import h2o from h2o.automl import H2OAutoML import numpy as np from sklearn.preprocessing import StandardScaler import os import pickle # Change notebook settings pd . options . display . max_columns = None pd . options . display . max_rows = 300 from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\"","title":"Modelling the Brownlow"},{"location":"modelling/brownlowModelTutorial/#eda-read-in-the-data","text":"I have collated this data using the fitzRoy R package and merging the afltables dataset with the footywire dataset, so that we can Supercoach and other advanced stats with Brownlow votes. Let's read in the data and have a sneak peak at what it looks like. brownlow_data = pd . read_csv( 'data/afl_brownlow_data.csv' ) brownlow_data . tail( 3 ) date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 76585 2018-08-26 2018 23 Etihad Stadium 12312 9708.0 M Wood 32 North Melbourne St Kilda Away 117 94 23 NaN 0 3 2 66.7 0 0 2 0 0 34 3 0 3 3 1 0 0 0 0 0 0 0 0 0 24 25 76586 2018-08-26 2018 23 Etihad Stadium 11755 9708.0 S Wright 19 North Melbourne St Kilda Away 117 94 23 NaN 10 17 22 75.9 0 0 0 0 1 83 16 13 29 8 0 0 2 0 3 0 4 4 1 0 107 96 76587 2018-08-26 2018 23 Etihad Stadium 11724 9708.0 J Ziebell 7 North Melbourne St Kilda Away 117 94 23 NaN 9 8 11 73.3 0 0 2 4 0 94 12 3 15 6 3 1 2 0 1 0 0 0 2 0 89 109 It looks like we've got about 76,000 rows of data and have stats like hitouts, clangers, effective disposals etc. Let's explore some certain scenarios. Using my domain knowledge of footy, I can hypothesise that if a player kicks 5 goals, he is pretty likely to poll votes. Similarly, if a player gets 30 possessions and 2+ goals, he is also probably likely to poll votes. Let's have a look at the mean votes for players for both of these situations.","title":"EDA - Read in the data"},{"location":"modelling/brownlowModelTutorial/#exploring-votes-if-a-bag-is-kicked-5-goals","text":"brownlow_data . query( 'G >= 5' ) . groupby( 'season' ) . brownlow_votes . mean() print ( \"Mean votes is a player has kicked a bag:\" , brownlow_data . query( 'G >= 5' ) . brownlow_votes . mean()) season 2010 1.420455 2011 1.313433 2012 1.413333 2013 1.253731 2014 1.915254 2015 1.765625 2016 1.788732 2017 2.098361 2018 0.000000 Name: brownlow_votes, dtype: float64 Mean votes is a player has kicked a bag: 1.4708818635607321","title":"Exploring votes if a bag is kicked (5+ goals)"},{"location":"modelling/brownlowModelTutorial/#exploring-votes-if-the-player-has-30-possies-2-goals","text":"brownlow_data . query( 'G >= 2 and D >= 30' ) . groupby( 'season' ) . brownlow_votes . mean() print ( \"Mean votes if a player has 30 possies and kicks 2+ goals:\" , brownlow_data . query( 'G >= 2 and D >= 30' ) . brownlow_votes . mean()) season 2010 1.826923 2011 1.756410 2012 2.118421 2013 2.000000 2014 2.253731 2015 2.047619 2016 2.103448 2017 2.050000 2018 0.000000 Name: brownlow_votes, dtype: float64 Mean votes if a player has 30 possies and kicks 2+ goals: 1.8741379310344828 As suspected, the average votes for these two situations is 1.87! That's huge. Let's get an idea of the average votes for each player. It should be around 6/44, as there are always 6 votes per match and around 44 players per match. brownlow_data . brownlow_votes . mean() 0.12347341475121326 So the average vote is 0.12. Let's see how this changes is the player is a captain. I have collected data on if players are captains from wikipedia and collated it into a csv. Let's load this in and create a \"Is the player captain\" feature, then check the average votes for captains.","title":"Exploring votes if the player has 30+ possies &amp; 2+ goals"},{"location":"modelling/brownlowModelTutorial/#create-is-player-captain-feature","text":"captains = pd . read_csv( 'data/captains.csv' ) . set_index( 'player' ) def is_captain_for_that_season (captains_df, player, year): if player in captains_df . index: # Get years they were captain seasons = captains_df . loc[player] . season . split( '-' ) if len (seasons) == 1 : seasons_captain = list ( map ( int , seasons)) elif len (seasons) == 2 : if seasons[ 1 ] == '' : seasons_captain = list ( range ( int (seasons[ 0 ]), 2019 )) else : seasons_captain = list ( range ( int (seasons[ 0 ]), int (seasons[ 1 ]) + 1 )) if year in seasons_captain: return 1 return 0 brownlow_data[ 'is_captain' ] = brownlow_data . apply( lambda x: is_captain_for_that_season(captains, x . player, x . season), axis = 'columns' ) brownlow_data . query( 'is_captain == 1' ) . groupby( 'season' ) . brownlow_votes . mean() print ( \"Mean votes if a player is captain:\" , brownlow_data . query( 'is_captain == 1' ) . brownlow_votes . mean()) season 2010 0.408497 2011 0.429936 2012 0.274194 2013 0.438725 2014 0.519663 2015 0.447222 2016 0.347826 2017 0.425806 2018 0.000000 Name: brownlow_votes, dtype: float64 Mean votes if a player is captain: 0.36661698956780925 This is significantly higher than if they aren't captain. What would be interesting is to look at the average difference in votes between when they were captain and when they weren't, to try and find if there is a 'captain bias' in brownlow votes. Go ahead and try. For now, we're going to move onto feature creation","title":"Create Is Player Captain Feature"},{"location":"modelling/brownlowModelTutorial/#feature-creation","text":"Let's make a range of features, including: Ratios of each statistic per game If the player is a captain If they kicked a bag (\u2158+) If they kicked 2 and had 30+ possies First we will make features of ratios. What is important is not how many of a certain stat a player has, but how much of that stat a player has relative to everyone else in the same match. It doesn't matter if Dusty Martin has 31 possessions if Tom Mitchell has had 50 - Mitchell is probably more likely to poll (assuming all else is equal). So rather than using the actual number of possessions for example, we can divide these possessions by the total amount of possessions in the game. To do this we'll use pandas groupby and transform methods.","title":"Feature Creation"},{"location":"modelling/brownlowModelTutorial/#create-ratios-as-features","text":"%% time # Get a list of stats of which to create ratios for ratio_cols = [ 'CP' , 'UP' , 'ED' , 'DE' , 'CM' , 'GA' , 'MI5' , 'one_perc' , 'BO' , 'TOG' , 'K' , 'HB' , 'D' , 'M' , 'G' , 'B' , 'T' , 'HO' , 'I50' , 'CL' , 'CG' , 'R50' , 'FF' , 'FA' , 'AF' , 'SC' ] # Create a ratios df ratios = (brownlow_data . copy() . loc[:, [ 'match_id' ] + ratio_cols] . groupby( 'match_id' ) . transform( lambda x: x / x . sum())) feature_cols = [ 'date' , 'season' , 'round' , 'venue' , 'ID' , 'match_id' , 'player' , 'jumper_no' , 'team' , 'opposition' , 'status' , 'team_score' , 'opposition_score' , 'margin' , 'brownlow_votes' ] # Create a features df - join the ratios to this df features = (brownlow_data[feature_cols] . copy() . join(ratios)) Wall time: 11.3 s features . head() date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 0 2010-03-28 2010 1 Domain Stadium 11807 5096.0 T Armstrong 38 Adelaide Fremantle Away 62 118 -56 0.0 0.013393 0.016304 0.015517 0.025529 0.0 0.0 0.00 0.013333 0.000000 0.020516 0.013441 0.014599 0.014049 0.022599 0.000000 0.000000 0.000000 0.000000 0.011494 0.000000 0.036145 0.018868 0.000000 0.037037 0.011568 0.005456 1 2010-03-28 2010 1 Domain Stadium 3968 5096.0 N Bock 44 Adelaide Fremantle Away 62 118 -56 0.0 0.008929 0.025362 0.022414 0.023875 0.0 0.0 0.00 0.080000 0.000000 0.027169 0.032258 0.012165 0.021711 0.016949 0.038462 0.000000 0.026549 0.029851 0.022989 0.000000 0.024096 0.018868 0.037037 0.000000 0.024422 0.021825 2 2010-03-28 2010 1 Domain Stadium 11712 5096.0 M Cook 8 Adelaide Fremantle Away 62 118 -56 0.0 0.035714 0.025362 0.031034 0.026746 0.0 0.0 0.00 0.013333 0.034483 0.021625 0.021505 0.031630 0.026820 0.022599 0.000000 0.045455 0.008850 0.014925 0.011494 0.028986 0.048193 0.056604 0.000000 0.037037 0.020887 0.019703 3 2010-03-28 2010 1 Domain Stadium 11700 5096.0 P Dangerfield 32 Adelaide Fremantle Away 62 118 -56 0.0 0.049107 0.016304 0.017241 0.015605 0.0 0.0 0.04 0.026667 0.172414 0.022734 0.021505 0.029197 0.025543 0.016949 0.038462 0.090909 0.026549 0.000000 0.022989 0.072464 0.024096 0.018868 0.074074 0.037037 0.024422 0.028190 4 2010-03-28 2010 1 Domain Stadium 85 5096.0 M Doughty 11 Adelaide Fremantle Away 62 118 -56 0.0 0.017857 0.021739 0.025862 0.027526 0.0 0.0 0.00 0.053333 0.000000 0.025229 0.010753 0.031630 0.021711 0.016949 0.000000 0.000000 0.026549 0.000000 0.011494 0.000000 0.036145 0.037736 0.000000 0.000000 0.018959 0.018490","title":"Create Ratios As Features"},{"location":"modelling/brownlowModelTutorial/#kicked-a-bag-feature","text":"features[ 'kicked_a_bag' ] = brownlow_data . G . apply( lambda x: 1 if x >= 5 else 0 )","title":"Kicked A Bag Feature"},{"location":"modelling/brownlowModelTutorial/#is-captain-feature","text":"features[ 'is_captain' ] = features . apply( lambda x: is_captain_for_that_season(captains, x . player, x . season), axis = 'columns' )","title":"Is Captain Feature"},{"location":"modelling/brownlowModelTutorial/#won-the-game-feature","text":"features[ 'team_won' ] = np . where(features . margin > 0 , 1 , 0 )","title":"Won the Game Feature"},{"location":"modelling/brownlowModelTutorial/#30-2-goals-feature","text":"features[ 'got_30_possies_2_goals' ] = np . where((brownlow_data . G >= 2 ) & (brownlow_data . D >= 30 ), 1 , 0 )","title":"30+ &amp; 2+ Goals Feature"},{"location":"modelling/brownlowModelTutorial/#previous-top-10-finish-feature","text":"I have a strong feeling that past performance may be a predictor of future performance in the brownlow. For example, last year Dusty Martin won the Brownlow. The umpires may have a bias towards Dusty this year because he is known to be on their radar as being a good player. Let's create a feature which is categorical and is 1 if the player has previously finished in the top 10. Let's create a function for this and then apply it to the afltables dataset, which has data back to 1897. We will then create a lookup table for the top 10 for each season and merge this table with our current features df. afltables = pd . read_csv( 'data/afltables_stats.csv' ) . query( 'Season >= 2000' ) def replace_special_characters (name): name = name . replace( \"'\" , \"\" ) . replace( \"-\" , \" \" ) . lower() name_split = name . split() if len (name_split) > 2 : first_name = name_split[ 0 ] last_name = name_split[ -1 ] name = first_name + ' ' + last_name name_split_2 = name . split() name = name_split_2[ 0 ][ 0 ] + ' ' + name_split_2[ 1 ] return name . title() afltables = (afltables . assign(player = lambda df: df[ 'First.name' ] + ' ' + df . Surname) . assign(player = lambda df: df . player . apply(replace_special_characters)) . rename(columns = { 'Brownlow.Votes' : 'brownlow_votes' , 'Season' : 'season' , 'Playing.for' : 'team' })) ### Create Top 10 rank look up table brownlow_votes_yearly = (afltables . groupby([ 'season' , 'player' , 'team' ], as_index = False ) . brownlow_votes . sum()) brownlow_votes_yearly[ 'yearly_rank' ] = (brownlow_votes_yearly . groupby( 'season' ) . brownlow_votes . rank(method = 'max' , ascending = False )) # Filter to only get a dataframe since 2000 and only the top 10 players from each season brownlow_votes_top_10 = brownlow_votes_yearly . query( 'yearly_rank < 11 & season >= 2000' ) brownlow_votes_top_10 . head( 3 ) def how_many_times_top_10 (top_10_df, player, year): times = len (top_10_df[(top_10_df . player == player) & (top_10_df . season < year)]) return times features[ 'times_in_top_10' ] = features . apply( lambda x: how_many_times_top_10(brownlow_votes_top_10, x . player, x . season), axis =1 ) season player team brownlow_votes yearly_rank 27 2000.0 A Koutoufides Carlton 19.0 4.0 36 2000.0 A Mcleod Adelaide 20.0 3.0 105 2000.0 B Ratten Carlton 18.0 6.0","title":"Previous Top 10 Finish Feature"},{"location":"modelling/brownlowModelTutorial/#average-brownlow-votes-per-game-last-season-feature","text":"# Create a brownlow votes lookup table brownlow_votes_lookup_table = (brownlow_data . groupby([ 'player' , 'team' , 'season' ], as_index = False ) . brownlow_votes . mean() . assign(next_season = lambda df: df . season + 1 ) . rename(columns = { 'brownlow_votes' : 'ave_votes_last_season' })) # Have a look at Cripps to check if it's working brownlow_votes_lookup_table[brownlow_votes_lookup_table . player == 'P Cripps' ] # Merge it to our features df features_with_votes_last_season = (pd . merge(features, brownlow_votes_lookup_table . drop(columns = 'season' ), left_on = [ 'player' , 'team' , 'season' ], right_on = [ 'player' , 'team' , 'next_season' ], how = 'left' ) . drop(columns = [ 'next_season' ]) . fillna( 0 )) player team season ave_votes_last_season next_season 4377 P Cripps Carlton 2014 0.000000 2015 4378 P Cripps Carlton 2015 0.300000 2016 4379 P Cripps Carlton 2016 0.857143 2017 4380 P Cripps Carlton 2017 0.333333 2018 4381 P Cripps Carlton 2018 0.000000 2019","title":"Average Brownlow Votes Per Game Last Season Feature"},{"location":"modelling/brownlowModelTutorial/#historic-performance-relative-to-model-feature","text":"It is well known that some players are good Brownlow performers. For whatever reason, they always poll much better than their stats may suggest. Lance Franklin and Bontempelli are probably in this category. Perhaps these players have an X-factor that Machine Learning models struggle to pick up on. To get around this, let's create a feature which looks at the player's performance relative to the model's prediction. To do this, we'll need to train and predict 7 different models - from 2011 to 2017. To create a model for each season, we will use h2o's AutoML. If you're new to h2o, please read about it here. It can be used in both R and Python. The metric we will use for loss in Mean Absolute Error (MAE). As we are using regression, some values are negative. We will convert these negative values to 0 as it doesn't make sense to poll negative brownlow votes. Similarly, some matches won't predict exactly 6 votes, so we will scale these predictions so that we predict exactly 6 votes for each match. So that you don't have to train these models yourself, I have saved the models and we will load them in. If you are keen to train the models yourself, simply uncomment out the code below and run the cell. To bulk uncomment, highlight the rows and press ctrl + '/' h2o . init() # Uncomment the code below if you want to train the models yourself - otherwise, we will load them in the load cell from disk ## Join to our features df # aml_yearly_model_objects = {} # yearly_predictions_dfs = {} # feature_cols = ['margin', 'CP', 'UP', 'ED', 'DE', # 'CM', 'GA', 'MI5', 'one_perc', 'BO', 'TOG', 'K', 'HB', 'D', 'M', 'G', # 'B', 'T', 'HO', 'I50', 'CL', 'CG', 'R50', 'FF', 'FA', 'AF', 'SC'] # for year in range(2011, 2018): # # Filter the data to only include past data # train_historic = brownlow_data[brownlow_data.season < year].copy() # # Convert to an h2o frame # train_h2o_historic = h2o.H2OFrame(train_historic) # # Create an AutoML object # aml = H2OAutoML(max_runtime_secs=30, # balance_classes=True, # seed=42) # # Train the model # aml.train(y='brownlow_votes', x=feature_cols, training_frame=train_h2o_historic) # # save the model # model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True) # # Get model id # model_name = aml.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/yearly_model_{year}') # # Append the best model to a list # aml_yearly_model_objects[year] = aml.leader # # Make predictions on test set for that year # test_historic = brownlow_data[brownlow_data.season == year].copy() # test_h2o_historic = h2o.H2OFrame(test_historic) # preds = aml.predict(test_h2o_historic).as_data_frame() # test_historic['predicted_votes'] = preds.values # # Convert negative predictions to 0 # test_historic['predicted_votes_neg_to_0'] = test_historic.predicted_votes.apply(lambda x: 0 if x < 0 else x) # # Create a total match votes column - which calculates the number of votes predicted in each game when the predictions # # are unscaled # test_historic['unscaled_match_votes'] = test_historic.groupby('match_id').predicted_votes_neg_to_0.transform('sum') # # Scale predictions # test_historic['predicted_votes_scaled'] = test_historic.predicted_votes_neg_to_0 / test_historic.unscaled_match_votes * 6 # # Aggregate the predictions # test_grouped = (test_historic.groupby(['player', 'team'], as_index=False) # .sum() # .sort_values(by='brownlow_votes', ascending=False) # .assign(mae=lambda df: abs(df.predicted_votes_scaled - df.brownlow_votes))) # test_grouped['error'] = test_grouped.predicted_votes_scaled - test_grouped.brownlow_votes # test_grouped['next_year'] = year + 1 # # Add this years predictions df to a dictionary to use later # yearly_predictions_dfs[year] = test_grouped # preds_errors = None # for key, value in yearly_predictions_dfs.items(): # if preds_errors is None: # preds_errors = value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']] # else: # preds_errors = preds_errors.append(value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']], sort=True) # with open('data/prediction_errors_df.pickle', 'wb') as handle: # pickle.dump(yearly_predicted_errors, handle) Checking whether there is an H2O instance running at http://localhost:54321 . connected. H2O cluster uptime: 1 hour 28 mins H2O cluster timezone: Australia/Hobart H2O data parsing timezone: UTC H2O cluster version: 3.20.0.4 H2O cluster version age: 1 month and 18 days H2O cluster name: H2O_from_python_WardJ_tt2ak5 H2O cluster total nodes: 1 H2O cluster free memory: 7.018 Gb H2O cluster total cores: 4 H2O cluster allowed cores: 4 H2O cluster status: locked, healthy H2O connection url: http://localhost:54321 H2O connection proxy: None H2O internal security: False H2O API Extensions: Algos, AutoML, Core V3, Core V4 Python version: 3.6.4 final # Load predictions error df with open ( 'data/prediction_errors_df.pickle' , 'rb' ) as handle: preds_errors = pickle . load(handle) # Look at last years predictions preds_errors . query( 'next_year == 2018' ) . sort_values(by = 'brownlow_votes' , ascending = False ) . head( 20 ) brownlow_votes error next_year player predicted_votes_scaled season 139 36.0 -4.915157 2018 D Martin 31.084843 44374 486 33.0 -4.413780 2018 P Dangerfield 28.586220 42357 619 25.0 0.072223 2018 T Mitchell 25.072223 44374 279 23.0 -9.296209 2018 J Kennedy 13.703791 38323 376 22.0 -7.621336 2018 L Franklin 14.378664 44374 278 21.0 -3.016176 2018 J Kelly 17.983824 42357 519 20.0 -4.182915 2018 R Sloane 15.817085 44374 410 19.0 -7.740142 2018 M Bontempelli 11.259858 44374 483 18.0 -4.522835 2018 O Wines 13.477165 44374 121 17.0 -4.446749 2018 D Beams 12.553251 38323 390 16.0 -3.620094 2018 L Parker 12.379906 44374 561 15.0 -5.846506 2018 S Pendlebury 9.153494 32272 463 15.0 -4.402600 2018 N Fyfe 10.597400 42357 42 15.0 -4.787914 2018 B Ebert 10.212086 44374 651 15.0 0.824510 2018 Z Merrett 15.824510 42357 578 14.0 2.300607 2018 T Adams 16.300607 44374 34 14.0 -5.796604 2018 B Brown 8.203396 44374 172 14.0 -1.650752 2018 D Zorko 12.349248 42357 184 14.0 -1.233663 2018 G Ablett 12.766337 28238 389 14.0 -2.099613 2018 L Neale 11.900387 42357 Look at that! A simply Machine Learning ensemble model, using AutoML predicted last year's winner! That's impressive. As we can see it also predicted Bontempelli would only score 11.26, when he actually scored 19 - a huge discrepency. Let's use this as a feature. features_with_historic_perf_relative_to_model = \\ (features_with_votes_last_season . pipe(pd . merge, preds_errors[[ 'player' , 'next_year' , 'error' ]], left_on = [ 'player' , 'season' ], right_on = [ 'player' , 'next_year' ], how = 'left' ) . fillna( 0 ) . rename(columns = { 'error' : 'error_last_season' }) . drop_duplicates(subset = [ 'player' , 'round' , 'SC' ]))","title":"Historic Performance Relative To Model Feature"},{"location":"modelling/brownlowModelTutorial/#filtering-the-data-to-only-include-the-top-20-sc-for-each-match","text":"Logically, it is extremely unlikely that a player will poll votes if their Supercoach score is not in the top 20 players. By eliminating the other 20+ players, we can reduce the noise in the data, as we are almost certain the players won't poll from the bottom half. Let's explore how many players poll if they're not in the top 20, and then filter our df if this number is not significant. # Find number of players who vote when in top 15 SC brownlow_data[ 'SC_rank_match' ] = brownlow_data . groupby( 'match_id' ) . SC . rank(method = 'max' , ascending = False ) brownlow_data . query( 'SC_rank_match > 20 and season > 2014' ) . brownlow_votes . value_counts() 0.0 18330 1.0 14 2.0 8 3.0 2 Name: brownlow_votes, dtype: int64 Since 2014, there have only been 24 players who have voted and not been in the top 20 SC. features_with_sc_rank = features_with_historic_perf_relative_to_model . copy() features_with_sc_rank[ 'SC_rank_match' ] = features_with_sc_rank . groupby( 'match_id' ) . SC . rank(method = 'max' , ascending = False ) # Filter out rows with a SC rank of below 20 features_with_sc_rank_filtered = features_with_sc_rank . query( 'SC_rank_match <= 20' ) # Filter out 2010 and 2011 as we used these seasons to create historic model performance features features_last_before_train = features_with_sc_rank_filtered . query( 'season != 2010 and season != 2011' ) . reset_index(drop = True ) features_last_before_train . head( 3 ) date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC kicked_a_bag is_captain team_won got_30_possies_2_goals times_in_top_10 ave_votes_last_season next_year error_last_season SC_rank_match 0 2012-03-31 2012 1 Metricon Stadium 11985 5347.0 I Callinan 37 Adelaide Gold Coast Away 137 68 69 0.0 0.031359 0.018947 0.016071 0.015745 0.0 0.000000 0.12 0.000000 0.000000 0.023333 0.031674 0.012270 0.023438 0.026042 0.068966 0.217391 0.041322 0.0 0.017241 0.013699 0.042553 0.000000 0.000000 0.032258 0.030593 0.026962 0 0 1 0 0 0.000000 2012.0 0.042048 14.0 1 2012-03-31 2012 1 Metricon Stadium 11700 5347.0 P Dangerfield 32 Adelaide Gold Coast Away 137 68 69 0.0 0.048780 0.023158 0.037500 0.026451 0.0 0.000000 0.04 0.009804 0.090909 0.025556 0.029412 0.036810 0.032552 0.015625 0.068966 0.000000 0.016529 0.0 0.051724 0.123288 0.042553 0.000000 0.000000 0.032258 0.027503 0.028173 0 0 1 0 0 0.333333 2012.0 -2.983636 12.0 2 2012-03-31 2012 1 Metricon Stadium 2381 5347.0 R Douglas 26 Adelaide Gold Coast Away 137 68 69 0.0 0.020906 0.033684 0.021429 0.019901 0.0 0.083333 0.04 0.049020 0.000000 0.023056 0.031674 0.015337 0.024740 0.031250 0.034483 0.130435 0.057851 0.0 0.043103 0.000000 0.031915 0.014493 0.064516 0.000000 0.033684 0.030597 0 0 1 0 0 0.000000 2012.0 1.724915 10.0","title":"Filtering the data to only include the top 20 SC for each match"},{"location":"modelling/brownlowModelTutorial/#modeling-the-2017-brownlow","text":"Now that we have all of our features, we can simply create a training set (2012-2016), and a test set (2017), and make our predictions for last year! We will use AutoML for this process again. Again, rather than waiting for the model to train, I will save the model so you can simply load it in. We will also scale our features. We can then see how our model went in predicting last year's brownlow, creating a baseline for this years' predictions. We will then predict this year's vote count. train_baseline = features_last_before_train . query( \"season < 2017\" ) holdout = features_last_before_train . query( \"season == 2017\" ) scale_cols = [ 'team_score' , 'opposition_score' , 'margin' , 'CP' , 'UP' , 'ED' , 'DE' , 'CM' , 'GA' , 'MI5' , 'one_perc' , 'BO' , 'K' , 'HB' , 'D' , 'M' , 'G' , 'B' , 'T' , 'HO' , 'I50' , 'CL' , 'CG' , 'R50' , 'FF' , 'FA' , 'AF' , 'SC' ] other_feature_cols = [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' , 'times_in_top_10' , 'ave_votes_last_season' , 'error_last_season' , 'SC_rank_match' ] all_feature_cols = scale_cols + other_feature_cols # Scale features scaler = StandardScaler() train_baseline_scaled = train_baseline . copy() train_baseline_scaled[scale_cols] = scaler . fit_transform(train_baseline[scale_cols]) holdout_scaled = holdout . copy() holdout_scaled[scale_cols] = scaler . transform(holdout[scale_cols]) # Convert categorical columns to categoricals train_baseline_h2o = h2o . H2OFrame(train_baseline_scaled) holdout_h2o = h2o . H2OFrame(holdout_scaled) for col in [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' ]: train_baseline_h2o[col] = train_baseline_h2o[col] . asfactor() holdout_h2o[col] = holdout_h2o[col] . asfactor() C:\\Users\\wardj\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h2o\\utils\\shared_utils.py:177: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. data = _handle_python_lists(python_obj.as_matrix().tolist(), -1)[1] Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% Below I have commented out training and saving the 2017 model. Rather than training it again, we will just load it in. Uncomment this part out if you want to train it yourself. # aml_2017_model = H2OAutoML(max_runtime_secs = 60*3, # balance_classes=True, # seed=42) # aml_2017_model.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_baseline_h2o) # save the model # model_path = h2o.save_model(model=aml_2017_model.leader, path=\"models\", force=True) # Get model id # model_name = aml_2017_model.leaderboard[0, 'model_id'] # Rename the model on disk # os.rename(f'models/{model_name}', f'models/brownlow_2017_model_v1') # Load model in aml_2017_model = h2o . load_model( 'models/brownlow_2017_model_v1' ) # Predict the 2017 brownlow count preds_final_2017_model = aml_2017_model . predict(holdout_h2o) # Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total holdout = (holdout . assign(predicted_votes = preds_final_2017_model . as_data_frame() . values) . assign(predicted_votes_neg_to_0 = lambda df: df . predicted_votes . apply( lambda x: 0 if x <0 else x)) . assign(unscaled_match_votes = lambda df: df . groupby( 'match_id' ) . predicted_votes_neg_to_0 . transform( 'sum' )) . assign(predicted_votes_scaled = lambda df: df . predicted_votes_neg_to_0 / df . unscaled_match_votes * 6 )) # Create an aggregate votes df and show the average SC points and goals scored agg_predictions_2017 = (holdout . groupby([ 'player' , 'team' ], as_index = False ) . agg({ 'brownlow_votes' : sum , 'predicted_votes_scaled' : sum , 'SC' : 'mean' , 'G' : 'mean' }) . sort_values(by = 'brownlow_votes' , ascending = False ) . assign(mae = lambda df: abs (df . brownlow_votes - df . predicted_votes_scaled)) . reset_index(drop = True )) stackedensemble prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% agg_predictions_2017 . head( 15 ) player team brownlow_votes predicted_votes_scaled SC G mae 0 D Martin Richmond 36.0 37.271060 0.037862 0.064869 1.271060 1 P Dangerfield Geelong 33.0 39.288122 0.042441 0.070819 6.288122 2 T Mitchell Hawthorn 25.0 28.629859 0.036040 0.016928 3.629859 3 L Franklin Sydney 22.0 16.353733 0.034640 0.149203 5.646267 4 J Kelly GWS 21.0 19.565321 0.034652 0.033772 1.434679 5 R Sloane Adelaide 20.0 21.417347 0.037068 0.034821 1.417347 6 J Kennedy Sydney 20.0 13.671891 0.032014 0.030508 6.328109 7 M Bontempelli Western Bulldogs 19.0 17.461889 0.033233 0.040498 1.538111 8 D Beams Brisbane 17.0 15.414730 0.034848 0.044998 1.585270 9 O Wines Port Adelaide 16.0 12.973973 0.031601 0.021967 3.026027 10 N Fyfe Fremantle 15.0 11.926030 0.033761 0.031680 3.073970 11 S Pendlebury Collingwood 15.0 10.845214 0.033855 0.013660 4.154786 12 B Ebert Port Adelaide 15.0 7.633526 0.032795 0.008431 7.366474 13 L Parker Sydney 15.0 14.680079 0.031366 0.030311 0.319921 14 Z Merrett Essendon 15.0 21.063889 0.033737 0.015362 6.063889 So whilst our model predicted Dangerfield to win, it was pretty damn accurate! Let's find the MAE for the top 100, 50, 25, and 10, and then compare it to 2018's MAE in week, when the Brownlow has been counted. for top_x in [ 10 , 25 , 50 , 100 ]: temp_mae = round (agg_predictions_2017 . iloc[:top_x] . mae . mean(), 3 ) print (f \"The Average Mean Absolute Error for the top {top_x} is {temp_mae}\" ) The Average Mean Absolute Error for the top 10 is 3.216 The Average Mean Absolute Error for the top 25 is 2.931 The Average Mean Absolute Error for the top 50 is 3.15 The Average Mean Absolute Error for the top 100 is 2.577","title":"Modeling The 2017 Brownlow"},{"location":"modelling/brownlowModelTutorial/#modelling-this-years-brownlow","text":"Let's now predict this year's vote count. These predictions will be on the front page of the GitHub. train = features_last_before_train . query( \"season < 2018\" ) test = features_last_before_train . query( \"season == 2018\" ) # Scale features scaler = StandardScaler() train_scaled = train . copy() train_scaled[scale_cols] = scaler . fit_transform(train[scale_cols]) test_scaled = test . copy() test_scaled[scale_cols] = scaler . transform(test[scale_cols]) # Convert categorical columns to categoricals train_h2o = h2o . H2OFrame(train_scaled) test_h2o = h2o . H2OFrame(test_scaled) for col in [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' ]: train_h2o[col] = train_h2o[col] . asfactor() test_h2o[col] = test_h2o[col] . asfactor() C:\\Users\\wardj\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h2o\\utils\\shared_utils.py:177: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. data = _handle_python_lists(python_obj.as_matrix().tolist(), -1)[1] Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% # Train the model - this part is commented out as we will just load our model from disk # aml = H2OAutoML(max_runtime_secs = 60*3, # balance_classes=True, # seed=42) # aml.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_h2o) # # save the model # model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True) # # Get model id # model_name = aml.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/brownlow_2018_model_v1') # Load model in aml = h2o . load_model( 'models/brownlow_2018_model_v1' ) # Predict the 2018 brownlow count preds_final_2018_model = aml . predict(test_h2o) # Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total test = (test . assign(predicted_votes = preds_final_2018_model . as_data_frame() . values) . assign(predicted_votes_neg_to_0 = lambda df: df . predicted_votes . apply( lambda x: 0 if x <0 else x)) . assign(unscaled_match_votes = lambda df: df . groupby( 'match_id' ) . predicted_votes_neg_to_0 . transform( 'sum' )) . assign(predicted_votes_scaled = lambda df: df . predicted_votes_neg_to_0 / df . unscaled_match_votes * 6 )) # Create an aggregate votes df and show the average SC points and goals scored agg_predictions_2018 = (test . groupby([ 'player' , 'team' ], as_index = False ) . agg({ 'predicted_votes_scaled' : sum , 'match_id' : 'count' }) # shows how many games they played . sort_values(by = 'predicted_votes_scaled' , ascending = False ) . reset_index(drop = True )) stackedensemble prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% # Show the top 25 predictions agg_predictions_2018 . head( 25 ) player team predicted_votes_scaled match_id 0 T Mitchell Hawthorn 35.484614 20 1 M Gawn Melbourne 21.544278 22 2 D Martin Richmond 20.444488 19 3 B Grundy Collingwood 19.543511 22 4 C Oliver Melbourne 19.009628 20 5 J Macrae Western Bulldogs 18.931594 17 6 P Dangerfield Geelong 18.621242 21 7 D Beams Brisbane 17.621222 15 8 E Yeo West Coast 16.015638 20 9 L Neale Fremantle 15.495083 21 10 A Gaff West Coast 15.165629 18 11 D Heppell Essendon 15.083797 19 12 J Selwood Geelong 14.989096 18 13 S Sidebottom Collingwood 14.863136 18 14 N Fyfe Fremantle 14.692243 11 15 J Kennedy Sydney 14.404489 16 16 Z Merrett Essendon 13.632131 18 17 M Crouch Adelaide 13.503858 16 18 R Laird Adelaide 13.274869 19 19 P Cripps Carlton 13.240568 21 20 G Ablett Geelong 13.018950 15 21 L Franklin Sydney 12.792476 13 22 J Lloyd Sydney 12.174224 20 23 J Kelly GWS 11.982157 14 24 C Ward GWS 11.892443 19 print (agg_predictions_2018 . head( 15 )) player team predicted_votes_scaled match_id 0 T Mitchell Hawthorn 35.484614 1 M Gawn Melbourne 21.544278 2 D Martin Richmond 20.444488 3 B Grundy Collingwood 19.543511 4 C Oliver Melbourne 19.009628 5 J Macrae Western Bulldogs 18.931594 6 P Dangerfield Geelong 18.621242 7 D Beams Brisbane 17.621222 8 E Yeo West Coast 16.015638 9 L Neale Fremantle 15.495083 10 A Gaff West Coast 15.165629 11 D Heppell Essendon 15.083797 12 J Selwood Geelong 14.989096 13 S Sidebottom Collingwood 14.863136 14 N Fyfe Fremantle 14.692243 Now that we have the top 25, let's also look at the top 3 from each team. agg_predictions_2018 . sort_values(by = [ 'team' , 'predicted_votes_scaled' ], ascending = [ True , False ]) . groupby( 'team' ) . head( 3 ) player team predicted_votes_scaled match_id 17 M Crouch Adelaide 13.503858 16 18 R Laird Adelaide 13.274869 19 51 B Gibbs Adelaide 7.783425 18 7 D Beams Brisbane 17.621222 15 46 D Zorko Brisbane 8.123915 14 55 S Martin Brisbane 7.243428 19 19 P Cripps Carlton 13.240568 21 50 K Simpson Carlton 7.864995 18 88 E Curnow Carlton 3.725292 19 3 B Grundy Collingwood 19.543511 22 13 S Sidebottom Collingwood 14.863136 18 31 A Treloar Collingwood 10.487535 11 11 D Heppell Essendon 15.083797 19 16 Z Merrett Essendon 13.632131 18 57 D Smith Essendon 6.838857 20 9 L Neale Fremantle 15.495083 21 14 N Fyfe Fremantle 14.692243 11 74 M Walters Fremantle 5.120949 12 23 J Kelly GWS 11.982157 14 24 C Ward GWS 11.892443 19 25 S Coniglio GWS 11.785273 20 6 P Dangerfield Geelong 18.621242 21 12 J Selwood Geelong 14.989096 18 20 G Ablett Geelong 13.018950 15 80 J Witts Gold Coast 4.617273 13 85 J Lyons Gold Coast 4.043711 14 114 B Fiorini Gold Coast 2.798683 6 0 T Mitchell Hawthorn 35.484614 20 39 L Breust Hawthorn 9.286521 16 45 J Gunston Hawthorn 8.341824 19 1 M Gawn Melbourne 21.544278 22 4 C Oliver Melbourne 19.009628 20 32 J Hogan Melbourne 10.198873 13 26 S Higgins North Melbourne 11.327596 19 37 B Brown North Melbourne 9.550205 13 42 B Cunnington North Melbourne 8.857294 17 27 O Wines Port Adelaide 11.118795 16 36 R Gray Port Adelaide 9.606960 17 54 J Westhoff Port Adelaide 7.355449 21 2 D Martin Richmond 20.444488 19 30 J Riewoldt Richmond 10.557749 15 59 T Cotchin Richmond 6.741732 12 41 S Ross St Kilda 9.093395 17 48 J Steven St Kilda 8.099164 17 90 J Steele St Kilda 3.617769 16 15 J Kennedy Sydney 14.404489 16 21 L Franklin Sydney 12.792476 13 22 J Lloyd Sydney 12.174224 20 8 E Yeo West Coast 16.015638 20 10 A Gaff West Coast 15.165629 18 38 J Redden West Coast 9.368163 16 5 J Macrae Western Bulldogs 18.931594 17 40 M Bontempelli Western Bulldogs 9.272771 16 44 L Hunter Western Bulldogs 8.349606 17 If you're looking for a round by round breakdown, just have a look at the test dataframe. test[[ 'date' , 'round' , 'player' , 'team' , 'opposition' , 'margin' , 'SC' , 'predicted_votes_scaled' ]] . tail( 25 ) date round player team opposition margin SC predicted_votes_scaled 27231 2018-08-26 23 R Lobb GWS Melbourne -45 0.027576 0.007501 27232 2018-08-26 23 H Perryman GWS Melbourne -45 0.025758 0.000000 27233 2018-08-26 23 D Shiel GWS Melbourne -45 0.026364 0.000000 27234 2018-08-26 23 A Tomlinson GWS Melbourne -45 0.026667 0.000000 27235 2018-08-26 23 C Ward GWS Melbourne -45 0.035455 0.332057 27236 2018-08-26 23 L Austin St Kilda North Melbourne -23 0.027853 0.003928 27237 2018-08-26 23 J Geary St Kilda North Melbourne -23 0.025129 0.000000 27238 2018-08-26 23 S Gilbert St Kilda North Melbourne -23 0.025734 0.000000 27239 2018-08-26 23 R Marshall St Kilda North Melbourne -23 0.028156 0.103389 27240 2018-08-26 23 B Paton St Kilda North Melbourne -23 0.023918 0.000000 27241 2018-08-26 23 S Ross St Kilda North Melbourne -23 0.039055 0.300392 27242 2018-08-26 23 J Steele St Kilda North Melbourne -23 0.042386 1.094723 27243 2018-08-26 23 J Steven St Kilda North Melbourne -23 0.046624 1.119684 27244 2018-08-26 23 R Clarke North Melbourne St Kilda 23 0.021193 0.050859 27245 2018-08-26 23 B Cunnington North Melbourne St Kilda 23 0.032698 0.212364 27246 2018-08-26 23 M Daw North Melbourne St Kilda 23 0.022404 0.021889 27247 2018-08-26 23 T Dumont North Melbourne St Kilda 23 0.049046 2.042550 27248 2018-08-26 23 T Goldstein North Melbourne St Kilda 23 0.037844 0.347199 27249 2018-08-26 23 S Higgins North Melbourne St Kilda 23 0.039358 0.508419 27250 2018-08-26 23 N Hrovat North Melbourne St Kilda 23 0.025129 0.012548 27251 2018-08-26 23 J Macmillan North Melbourne St Kilda 23 0.024220 0.000000 27252 2018-08-26 23 J Waite North Melbourne St Kilda 23 0.025734 0.057826 27253 2018-08-26 23 M Williams North Melbourne St Kilda 23 0.023312 0.000000 27254 2018-08-26 23 S Wright North Melbourne St Kilda 23 0.029064 0.093219 27255 2018-08-26 23 J Ziebell North Melbourne St Kilda 23 0.033000 0.031010 And there we have it! In a single notebook we have made a fairly good Brownlow predictive model. Enjoy.","title":"Modelling This Year's Brownlow"},{"location":"modelling/howToModel/","text":"Intro to modelling \u00b6 Want to learn how to create your own predictive model using sports or racing data, but you don\u2019t know where to start? We\u2019re here to help. The Data Scientists at Betfair have put together the first few steps we suggest you take to get you started on your data modelling journey. We also run occasional data modelling workshops to help you get the basics down \u2013 reach out and let us know if you\u2019re interested in being notified about upcoming data events. Choose your language \u00b6 There are lots of programming languages to choose from. For our data modelling workshops we work in R and Python, as they\u2019re both relatively easy to learn and designed for working with data. If you\u2019re new to these languages, here are some resources that will help get you set up. Language 1: R \u00b6 What is R? Download and install R \u2013 get the language set up on your computer Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R Take a look at the some of the existing R libraries you can use if you want to connect to our API, including abettor and our Data Scientists\u2019 R repo . Language 2: Python \u00b6 What is Python? Download and install Anaconda Distribution \u2013 this will install Python and a heap of data science packages along with it Find a data source \u00b6 Finding quality data is crucial to being able to create a successful model. We have lots of historical Exchange data that we\u2019re happy to share, and there are lots of other sources of sports or racing specific data available online, depending on what you\u2019re looking for. For our workshops we use historical NBA odds data from the Exchange ( which you can download directly from here , along with NBA game data from a variety of sources including: ESPN.com NBA.com basketball-reference.com Stattleship\u2019s API Learn to Program \u00b6 Okay, so easier said than done, but you don't actually need a high level of programming knowledge to be able to build a decent model, and there are so many excellent resources available online that the barrier to entry is much lower than it's been in the past. These are some of our favourites if you want to learn to use R or Python for data modelling: Dataquest \u2013 free coding resource for learning both Python and R for data science Datacamp \u2013 another popular free resource to learn both R and Python for data science Codeacademy \u2013 free online programming courses with community engagement We've also shared a R repo for connecting with our API , which might make that part of the learning process easier for you, if you go down that path. Learn how to model data \u00b6 We\u2019ve put together some articles to give you an introduction to some of the different approaches you can take to modelling data, but again there are also lots of resources available online. Here are some good places to start: Work through the modelling tutorials we've put together using AFL and soccer data This Introduction to Tennis Modelling gives a good overview of ranking-based models, regression-based models and point-based models How we used ELO and machine learning as different approaches to modelling the recent World Cup Get your hands dirty \u00b6 The best way to learn is by doing. Make sure you have a solid foundation knowledge to work from, then get excited, get your hands dirty and see what you can create! Here are a final few thoughts to help you decide where to from here: Make sure you\u2019ve got your betting basics and wagering fundamentals knowledge solid Learn about the importance of ratings and prices and get inspired by the models created by our Data Scientists Consider how you could use our API in building and automating your model Read about how successful some of our customers have been in their modelling journeys","title":"Intro to modelling"},{"location":"modelling/howToModel/#intro-to-modelling","text":"Want to learn how to create your own predictive model using sports or racing data, but you don\u2019t know where to start? We\u2019re here to help. The Data Scientists at Betfair have put together the first few steps we suggest you take to get you started on your data modelling journey. We also run occasional data modelling workshops to help you get the basics down \u2013 reach out and let us know if you\u2019re interested in being notified about upcoming data events.","title":"Intro to modelling"},{"location":"modelling/howToModel/#choose-your-language","text":"There are lots of programming languages to choose from. For our data modelling workshops we work in R and Python, as they\u2019re both relatively easy to learn and designed for working with data. If you\u2019re new to these languages, here are some resources that will help get you set up.","title":"Choose your language"},{"location":"modelling/howToModel/#language-1-r","text":"What is R? Download and install R \u2013 get the language set up on your computer Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R Take a look at the some of the existing R libraries you can use if you want to connect to our API, including abettor and our Data Scientists\u2019 R repo .","title":"Language 1: R"},{"location":"modelling/howToModel/#language-2-python","text":"What is Python? Download and install Anaconda Distribution \u2013 this will install Python and a heap of data science packages along with it","title":"Language 2: Python"},{"location":"modelling/howToModel/#find-a-data-source","text":"Finding quality data is crucial to being able to create a successful model. We have lots of historical Exchange data that we\u2019re happy to share, and there are lots of other sources of sports or racing specific data available online, depending on what you\u2019re looking for. For our workshops we use historical NBA odds data from the Exchange ( which you can download directly from here , along with NBA game data from a variety of sources including: ESPN.com NBA.com basketball-reference.com Stattleship\u2019s API","title":"Find a data source"},{"location":"modelling/howToModel/#learn-to-program","text":"Okay, so easier said than done, but you don't actually need a high level of programming knowledge to be able to build a decent model, and there are so many excellent resources available online that the barrier to entry is much lower than it's been in the past. These are some of our favourites if you want to learn to use R or Python for data modelling: Dataquest \u2013 free coding resource for learning both Python and R for data science Datacamp \u2013 another popular free resource to learn both R and Python for data science Codeacademy \u2013 free online programming courses with community engagement We've also shared a R repo for connecting with our API , which might make that part of the learning process easier for you, if you go down that path.","title":"Learn to Program"},{"location":"modelling/howToModel/#learn-how-to-model-data","text":"We\u2019ve put together some articles to give you an introduction to some of the different approaches you can take to modelling data, but again there are also lots of resources available online. Here are some good places to start: Work through the modelling tutorials we've put together using AFL and soccer data This Introduction to Tennis Modelling gives a good overview of ranking-based models, regression-based models and point-based models How we used ELO and machine learning as different approaches to modelling the recent World Cup","title":"Learn how to model data"},{"location":"modelling/howToModel/#get-your-hands-dirty","text":"The best way to learn is by doing. Make sure you have a solid foundation knowledge to work from, then get excited, get your hands dirty and see what you can create! Here are a final few thoughts to help you decide where to from here: Make sure you\u2019ve got your betting basics and wagering fundamentals knowledge solid Learn about the importance of ratings and prices and get inspired by the models created by our Data Scientists Consider how you could use our API in building and automating your model Read about how successful some of our customers have been in their modelling journeys","title":"Get your hands dirty"},{"location":"modelling/howToModelTheAusOpen/","text":"How to model the Australian Open \u00b6 Betfair\u2019s Data Scientists Team are putting together a collection of articles on How to Build a Model and submit an entry to the Betfair Aus Open Datathon . This article will outline their thought process and share their approach. Subsequent articles will be posted with code examples that outline how this approach can be put into practice. Tools for creating our model \u00b6 We will be providing a step by step tutorial in two languages \u2013 Python and R. These are the two most popular languages used in data science nowadays. Both code examples will follow identical approaches. Tournament structure \u00b6 The Datathon structure requires contestants to predict every possible tournament match-up only using data that is available at the start of the tournament. This means we can\u2019t use information from previous rounds (data from Round 1 matches for potential Round 2 matches and so on). For example, if we were to just train our model on all the tennis matches in the data set, our model would have been trained assuming that it had the result from previous rounds in the Australian Open. But this isn\u2019t the case, so we need to account for this nuance of the competition. We need to ensure that we don\u2019t include previous round data from the same tournament in the way we structure our features for predicting results. How to set up data and features \u00b6 In predictive modelling language \u2013 features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options Training the model We can train the model on every tennis match in the data set or We can only train the model on Australian Open matches Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure. Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on. In the end, we decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface \u2013 hard court. Next decision is to decide the features (or the metrics we feed into the model, which makes the decision on who the winner is going to be). We don\u2019t have a definitive list of features that we will use, but we will most likely keep the number of features quite low (between 4-5). Features set Likely features may include: ELO First serve % Winners-unforced error ratio We will also use the difference between opponents' statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns. This will reduce the dimensionality of the model. A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, the difference in ELO rating etc. Target variable \u00b6 Our target variable (what we are trying to predict) is whether player A wins or not against player B. In machine learning terms this is a classification problem. The output will be a probability number between 0 and 1. A number closer to 0 means Player B is likely to win, and a number closer to 1 will mean Player A is likely to win. Another positive of a probabilistic outcome is that they can easily be converted to odds, and can also be compared with the historical Betfair odds that have been provided, and test if our model would have been profitable for previous seasons. Sports modelling nuances \u00b6 Sports data is inherently complex to model. Generally when predicting something, like \u201cwill it rain today\u201d, you have information for that day, such as the temperature, which you can use in formulating your prediction. However with sports data, you cannot use the majority of information that is provided in the raw dataset, such as aces, winners, etc, as this will create what is called feature leakage \u2013 using data from after the event, which you won\u2019t have access to before the event, to predict the result. You will also need to use historic results in such a way that will have predictive power for the sports event that you are trying to predict. This means that you run into little nuances like needing to use rolling averages, as well as whether to model each match on a single row or multiple rows. In the next article in this series, we will show you how we tackle this problem using code examples that anyone can replicate. Betfair's Aus Open Datathon Make sure you register for our Australian Open Datathon to receive 7 years of match data for tournments all around the world, along with historic Aus Open price data. Submit a model and you'll also be in the running to take home some of the $15k prize pool!","title":"How to model the Australian Open"},{"location":"modelling/howToModelTheAusOpen/#how-to-model-the-australian-open","text":"Betfair\u2019s Data Scientists Team are putting together a collection of articles on How to Build a Model and submit an entry to the Betfair Aus Open Datathon . This article will outline their thought process and share their approach. Subsequent articles will be posted with code examples that outline how this approach can be put into practice.","title":"How to model the Australian Open"},{"location":"modelling/howToModelTheAusOpen/#tools-for-creating-our-model","text":"We will be providing a step by step tutorial in two languages \u2013 Python and R. These are the two most popular languages used in data science nowadays. Both code examples will follow identical approaches.","title":"Tools for creating our model"},{"location":"modelling/howToModelTheAusOpen/#tournament-structure","text":"The Datathon structure requires contestants to predict every possible tournament match-up only using data that is available at the start of the tournament. This means we can\u2019t use information from previous rounds (data from Round 1 matches for potential Round 2 matches and so on). For example, if we were to just train our model on all the tennis matches in the data set, our model would have been trained assuming that it had the result from previous rounds in the Australian Open. But this isn\u2019t the case, so we need to account for this nuance of the competition. We need to ensure that we don\u2019t include previous round data from the same tournament in the way we structure our features for predicting results.","title":"Tournament structure"},{"location":"modelling/howToModelTheAusOpen/#how-to-set-up-data-and-features","text":"In predictive modelling language \u2013 features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options Training the model We can train the model on every tennis match in the data set or We can only train the model on Australian Open matches Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure. Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on. In the end, we decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface \u2013 hard court. Next decision is to decide the features (or the metrics we feed into the model, which makes the decision on who the winner is going to be). We don\u2019t have a definitive list of features that we will use, but we will most likely keep the number of features quite low (between 4-5). Features set Likely features may include: ELO First serve % Winners-unforced error ratio We will also use the difference between opponents' statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns. This will reduce the dimensionality of the model. A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, the difference in ELO rating etc.","title":"How to set up data and features"},{"location":"modelling/howToModelTheAusOpen/#target-variable","text":"Our target variable (what we are trying to predict) is whether player A wins or not against player B. In machine learning terms this is a classification problem. The output will be a probability number between 0 and 1. A number closer to 0 means Player B is likely to win, and a number closer to 1 will mean Player A is likely to win. Another positive of a probabilistic outcome is that they can easily be converted to odds, and can also be compared with the historical Betfair odds that have been provided, and test if our model would have been profitable for previous seasons.","title":"Target variable"},{"location":"modelling/howToModelTheAusOpen/#sports-modelling-nuances","text":"Sports data is inherently complex to model. Generally when predicting something, like \u201cwill it rain today\u201d, you have information for that day, such as the temperature, which you can use in formulating your prediction. However with sports data, you cannot use the majority of information that is provided in the raw dataset, such as aces, winners, etc, as this will create what is called feature leakage \u2013 using data from after the event, which you won\u2019t have access to before the event, to predict the result. You will also need to use historic results in such a way that will have predictive power for the sports event that you are trying to predict. This means that you run into little nuances like needing to use rolling averages, as well as whether to model each match on a single row or multiple rows. In the next article in this series, we will show you how we tackle this problem using code examples that anyone can replicate. Betfair's Aus Open Datathon Make sure you register for our Australian Open Datathon to receive 7 years of match data for tournments all around the world, along with historic Aus Open price data. Submit a model and you'll also be in the running to take home some of the $15k prize pool!","title":"Sports modelling nuances"},{"location":"thirdPartyTools/","text":"Betfair is one of the only betting platforms in the world that demands winning clients. Unlike bookies, we don\u2019t ban you when you succeed. We need you, and we want you to be able to keep improving your strategies so you win more. We're here to help you in your automation journey, and this site is dedicated to sharing the tools and resources you need to succeed in this journey. Accessing our API \u00b6 As you may already know, Betfair has its own API to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software. There are lots of resources available to support you in accessing the API effectively: Creating & activating your app key Developer Program knowledge base Dev Docs Developer Forum where you can share your experiences and find out what's worked for other clients Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Our Datascientists' repos for using R and Python to access the API The UK\u2019s Github repo including libraries for other languages API access Customers are able to access our API to embed it into their programs and automate their strategies If you're a programmer there are lots of resources around to help Historic Data \u00b6 We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport \u2013 you can take a look at our explanation of the different data sources if you\u2019re not quite sure where to start. We\u2019ve also shared some tips on learning to create predictive models using this data , which link in with the models shared in the modelling section . Betfair data sources Accessing the official Historic Data site Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API The Stream API dev docs are the best source of information for interpreting the data from the Historic Data site Historic BSP csv files Historic Betfair data There is a lot of historical price data available for all makrets offered on the Exchange, ranging from aggregate, market-level csv files to complete JSON recreations of API Stream data Using third party tools for automation \u00b6 Whilst the following tools are not custom built for your approach, they do allow you to automate your betting strategies. You just set up specific betting conditions and let the third party application do the work for you. Bet Angel and Gruss Betting Assistant are the most popular third party tools. We\u2019re putting together a collection of articles on how to use some of these third party tools to automate basic strategies, to give you a starting point that you can then work from. Bet Angel Overview Ratings automation Market favourite automation Tipping automation Automating multiple simultaneous markets Gruss Ratings automation Automating multiple simultaneous markets Cymatic Trader Ratings automation BF Bot Manager Double or Bust Data modelling \u00b6 An intro to building a predictive model Open source predictive models built by our in-house Data Scientists Modelling the Aus Open EPL modelling series AFL modelling series Brownlow modelling tutorial Predictive modelling Many of our most successful customers use predictive models as the basis for their betting strategies Inspiration & information \u00b6 The Banker: A Quant's AFL Betting Strategy The Mathematician 'Back and Lay' is a subreddit dedicated to discussing trading techniques Our Twitter community is really active Staking Plans and Strategies Staking and Money Management Some extra info There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy. Need extra help? \u00b6 If you\u2019re looking for bespoke advice or have extra questions, please contact us at bdp@betfair.com.au . We have a dedicated in-house resource that is here to automate your betting strategies.","title":"Home"},{"location":"thirdPartyTools/#accessing-our-api","text":"As you may already know, Betfair has its own API to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software. There are lots of resources available to support you in accessing the API effectively: Creating & activating your app key Developer Program knowledge base Dev Docs Developer Forum where you can share your experiences and find out what's worked for other clients Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Our Datascientists' repos for using R and Python to access the API The UK\u2019s Github repo including libraries for other languages API access Customers are able to access our API to embed it into their programs and automate their strategies If you're a programmer there are lots of resources around to help","title":"Accessing our API"},{"location":"thirdPartyTools/#historic-data","text":"We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport \u2013 you can take a look at our explanation of the different data sources if you\u2019re not quite sure where to start. We\u2019ve also shared some tips on learning to create predictive models using this data , which link in with the models shared in the modelling section . Betfair data sources Accessing the official Historic Data site Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API The Stream API dev docs are the best source of information for interpreting the data from the Historic Data site Historic BSP csv files Historic Betfair data There is a lot of historical price data available for all makrets offered on the Exchange, ranging from aggregate, market-level csv files to complete JSON recreations of API Stream data","title":"Historic Data"},{"location":"thirdPartyTools/#using-third-party-tools-for-automation","text":"Whilst the following tools are not custom built for your approach, they do allow you to automate your betting strategies. You just set up specific betting conditions and let the third party application do the work for you. Bet Angel and Gruss Betting Assistant are the most popular third party tools. We\u2019re putting together a collection of articles on how to use some of these third party tools to automate basic strategies, to give you a starting point that you can then work from. Bet Angel Overview Ratings automation Market favourite automation Tipping automation Automating multiple simultaneous markets Gruss Ratings automation Automating multiple simultaneous markets Cymatic Trader Ratings automation BF Bot Manager Double or Bust","title":"Using third party tools for automation"},{"location":"thirdPartyTools/#data-modelling","text":"An intro to building a predictive model Open source predictive models built by our in-house Data Scientists Modelling the Aus Open EPL modelling series AFL modelling series Brownlow modelling tutorial Predictive modelling Many of our most successful customers use predictive models as the basis for their betting strategies","title":"Data modelling"},{"location":"thirdPartyTools/#inspiration-information","text":"The Banker: A Quant's AFL Betting Strategy The Mathematician 'Back and Lay' is a subreddit dedicated to discussing trading techniques Our Twitter community is really active Staking Plans and Strategies Staking and Money Management Some extra info There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy.","title":"Inspiration &amp; information"},{"location":"thirdPartyTools/#need-extra-help","text":"If you\u2019re looking for bespoke advice or have extra questions, please contact us at bdp@betfair.com.au . We have a dedicated in-house resource that is here to automate your betting strategies.","title":"Need extra help?"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/","text":"Bet Angel: Multiple market ratings \u00b6 Automating a ratings based strategy across multiple markets using Bet Angel Pro with some help from BetSmart \u00b6 Using ratings from reputable sources such as BetSmart can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to some of our other Betfair automation tutorials, but here we'll be using the ratings provided by BetSmart and incorporate them into our automation in Bet Angel. We've also made changes to the spreadsheet to allow you to automate three simultaneous markets. This will allow you to be confident that you won't miss a market due to delays or other unforseen events. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. Increasing Wagering IQ through BetSmart Ratings \u00b6 - The plan \u00b6 We're using the BetSmart Ratings Model which is led by pro punter, Daniel O\u2019Sullivan. BetSmart specializes in ratings for VIC and NSW markets and are shared on our Hub . Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here, incorporating BetSmart ratings into the auotmation. We'll step through how we went about getting Bet Angel Pro to place bets using the ratings from BetSmart . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program running and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: BetSmart Ratings Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Bet Angel Pro Additional info: BetSmart web page Assigning multiple markets to your Excel worksheets in Bet Angel so you dont miss a race: Betfair automating simultaneous markets tutorial - Set up \u00b6 Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. - Finding & formatting ratings \u00b6 Here we're using the ratings shared by BetSmart on the Hub . This makes for a bit of prep work, copying the list of runners and their rating into an Excel spreadsheet. As a minimum you'll need a list of runner names (including the runner number followed by a full stop, i.e. 1. Runner Name) in one column and their rating in another in an Excel sheet. If you have a list of ratings already in a spreadsheet that's even better - you'll be able to tweak the Excel formulas to work with whatever format your data is in. Wherever your ratings come from, you'll need to include them in the spreadsheet you're using to interact with Bet Angel. Here we're using an Excel spreadsheet we edited for this strategy , and we've included a tab called RATINGS where you can copy in the runner names and ratings. - Writing your rules \u00b6 As with any automated strategy, one of the most important steps is deciding what logical approach you want to take, and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our BetSmart ratings and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet \u00b6 In short, we want to back runners when: the available to back price is better than the rating for that runner the scheduled event start time is less than a certain number of seconds that we choose the event isn't in play back market percentage is less than a certain value that we choose - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Rating refers to the entire Column B in the RATINGS work sheet RunnerName refers to the entire column A in the RATINGS work sheet BMP refers to cell AF8 in the Bet Angel work sheet where the overrounds are calculated UserBMP refers to cell H4 in the SETTINGS work sheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell E4 in the SETTINGS work sheet UserTimeTillJump refers to cell H3 in the SETTINGS work sheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell G1 in the Bet Angel work sheet. Bet Angel will populate a status in this cell such as \"In Play\" or \"Suspended\" BACKLAY refers to cell H5 in the SETTINGS work sheet which allows you to easily switch between Back and Lay bet typers via a drop down box and will automatically update the formulas for all runners This is our trigger on Excel formula: Multi line =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Single line =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))),BMP<UserBMP,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay)),BACKLAY,\"\") Stepping through each step: Chcking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the SETTINGS worksheet. If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the RATINGS worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Back market percentage (BMP) is less than what we define: Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the SETTINGS worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown are smaller than 120 (2 minutes), as the majority of markets don't fully form until the last few minutes before the off. This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the SETTINGS sheet (TimeTillJump1), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the SETTINGS worksheet on columns C, D and E which have been hidden as to not over complicate this tutorial. In the 'Bet Angel' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the Bet Angel 2 and Bet Angel 3 worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'Bet Angel' worksheet will display and track the correct time till jump for their own applicable market. Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the SETTINGS worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Excel functions IF statement: IF(if this is true, do this, else do this) AND statement: AND(this is true, and so is this, and so is this) - returns true or false And Or statement: checks that the statement meets more than one condition. If this OR that, then do the following. VLOOKUP: looking up a value from a table based on the value you pass in Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). Multi line =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Single line =IF(AND(OR(AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),BMP<UserBMP,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay)),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. =G9 Stake: It's completely up to you what staking approach you want to take. We've kept it simple, and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the SETTINGS worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the SETTINGS worksheet. The formula is referencing cell H2 in the SETTINGS worksheet where we can easily update all the formulas by changing this single value. It divides $10 by the current available best back odds (cell G9 for the first runner) minus one to get the stake required to win $10. This goes in column N (N9 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =stake/(G9-1) - Connecting to Bet Angel \u00b6 Video walk through \u00b6 We've put together a litte video walk through to help make this process easier. - Selecting markets \u00b6 We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that BetSmart have provided ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature. - Linking the spreadsheet \u00b6 Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Bet Angel features \u00b6 Here are some Bet Angel features that you'll need to consider. - Multiple bets/clearing status cells \u00b6 The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use each worksheet over and over again for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs. - Turning off bet confirmation \u00b6 Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. Bet Angel allows you to save different settings configurations as profiles. Depending what you are wanting to use Bet Angel for each time you open it up, you can select the appropriate setting profile to suit your needs without having to go through and change them every time. - Editing the spreadsheet \u00b6 The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits. Areas for improvement \u00b6 There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Bet Angel - Multiple market ratings"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#bet-angel-multiple-market-ratings","text":"","title":"Bet Angel: Multiple market ratings"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#automating-a-ratings-based-strategy-across-multiple-markets-using-bet-angel-pro-with-some-help-from-betsmart","text":"Using ratings from reputable sources such as BetSmart can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to some of our other Betfair automation tutorials, but here we'll be using the ratings provided by BetSmart and incorporate them into our automation in Bet Angel. We've also made changes to the spreadsheet to allow you to automate three simultaneous markets. This will allow you to be confident that you won't miss a market due to delays or other unforseen events. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating a ratings based strategy across multiple markets using Bet Angel Pro with some help from BetSmart"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#increasing-wagering-iq-through-betsmart-ratings","text":"","title":"Increasing Wagering IQ through BetSmart Ratings"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#-the-plan","text":"We're using the BetSmart Ratings Model which is led by pro punter, Daniel O\u2019Sullivan. BetSmart specializes in ratings for VIC and NSW markets and are shared on our Hub . Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here, incorporating BetSmart ratings into the auotmation. We'll step through how we went about getting Bet Angel Pro to place bets using the ratings from BetSmart . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program running and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: BetSmart Ratings Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Bet Angel Pro Additional info: BetSmart web page Assigning multiple markets to your Excel worksheets in Bet Angel so you dont miss a race: Betfair automating simultaneous markets tutorial","title":"- The plan"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#-set-up","text":"Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.","title":"- Set up"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#-finding-formatting-ratings","text":"Here we're using the ratings shared by BetSmart on the Hub . This makes for a bit of prep work, copying the list of runners and their rating into an Excel spreadsheet. As a minimum you'll need a list of runner names (including the runner number followed by a full stop, i.e. 1. Runner Name) in one column and their rating in another in an Excel sheet. If you have a list of ratings already in a spreadsheet that's even better - you'll be able to tweak the Excel formulas to work with whatever format your data is in. Wherever your ratings come from, you'll need to include them in the spreadsheet you're using to interact with Bet Angel. Here we're using an Excel spreadsheet we edited for this strategy , and we've included a tab called RATINGS where you can copy in the runner names and ratings.","title":"- Finding &amp; formatting ratings"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take, and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our BetSmart ratings and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#-trigger-to-place-bet","text":"In short, we want to back runners when: the available to back price is better than the rating for that runner the scheduled event start time is less than a certain number of seconds that we choose the event isn't in play back market percentage is less than a certain value that we choose","title":"- Trigger to place bet"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Rating refers to the entire Column B in the RATINGS work sheet RunnerName refers to the entire column A in the RATINGS work sheet BMP refers to cell AF8 in the Bet Angel work sheet where the overrounds are calculated UserBMP refers to cell H4 in the SETTINGS work sheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell E4 in the SETTINGS work sheet UserTimeTillJump refers to cell H3 in the SETTINGS work sheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell G1 in the Bet Angel work sheet. Bet Angel will populate a status in this cell such as \"In Play\" or \"Suspended\" BACKLAY refers to cell H5 in the SETTINGS work sheet which allows you to easily switch between Back and Lay bet typers via a drop down box and will automatically update the formulas for all runners This is our trigger on Excel formula: Multi line =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Single line =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))),BMP<UserBMP,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay)),BACKLAY,\"\") Stepping through each step: Chcking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the SETTINGS worksheet. If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the RATINGS worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Back market percentage (BMP) is less than what we define: Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the SETTINGS worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown are smaller than 120 (2 minutes), as the majority of markets don't fully form until the last few minutes before the off. This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the SETTINGS sheet (TimeTillJump1), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the SETTINGS worksheet on columns C, D and E which have been hidden as to not over complicate this tutorial. In the 'Bet Angel' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the Bet Angel 2 and Bet Angel 3 worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'Bet Angel' worksheet will display and track the correct time till jump for their own applicable market. Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the SETTINGS worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Excel functions IF statement: IF(if this is true, do this, else do this) AND statement: AND(this is true, and so is this, and so is this) - returns true or false And Or statement: checks that the statement meets more than one condition. If this OR that, then do the following. VLOOKUP: looking up a value from a table based on the value you pass in Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#-preparing-the-spreadsheet","text":"You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). Multi line =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), BMP<UserBMP, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay)), BACKLAY, \"\" ) Single line =IF(AND(OR(AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),BMP<UserBMP,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay)),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. =G9 Stake: It's completely up to you what staking approach you want to take. We've kept it simple, and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the SETTINGS worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the SETTINGS worksheet. The formula is referencing cell H2 in the SETTINGS worksheet where we can easily update all the formulas by changing this single value. It divides $10 by the current available best back odds (cell G9 for the first runner) minus one to get the stake required to win $10. This goes in column N (N9 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =stake/(G9-1)","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#-connecting-to-bet-angel","text":"","title":"- Connecting to Bet Angel"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#video-walk-through","text":"We've put together a litte video walk through to help make this process easier.","title":"Video walk through"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#-selecting-markets","text":"We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that BetSmart have provided ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature.","title":"- Selecting markets"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#-linking-the-spreadsheet","text":"Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins.","title":"- Linking the spreadsheet"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#bet-angel-features","text":"Here are some Bet Angel features that you'll need to consider.","title":"Bet Angel features"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#-multiple-betsclearing-status-cells","text":"The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use each worksheet over and over again for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs.","title":"- Multiple bets/clearing status cells"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#-turning-off-bet-confirmation","text":"Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. Bet Angel allows you to save different settings configurations as profiles. Depending what you are wanting to use Bet Angel for each time you open it up, you can select the appropriate setting profile to suit your needs without having to go through and change them every time.","title":"- Turning off bet confirmation"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#-editing-the-spreadsheet","text":"The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits.","title":"- Editing the spreadsheet"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts.","title":"Areas for improvement"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/BetAngelMultipleMarketRatings/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Disclaimer"},{"location":"thirdPartyTools/BfBotManagerAutomation/","text":"BF Bot Manager: Double or bust \u00b6 Dobbing using BF Bot Manager automation rules \u00b6 BF Bot Manager may not have Excel automation integration like Gruss or Bet Angel, but it does make up for it with an intuitive rule-based automation system which is great if you're starting out with automation in general. There are hundreds of ways to implement different strategies using rules in BF Bot Manager but if you have any specific tutorial ideas that you would like to see here, please get in contact with us at bdp@betfair.com.au - We'd love to hear from you. Are you Ready? \u00b6 - The plan \u00b6 In this tutorial, we're going over the implementation of a strategy called \"DOBBING\" which means Double Or Bust. The idea behind Dobbing is to place bets on the movement of prices within a market rather than the actual race result itself. More specifically, backing a horse and then hedging that bet \u2013 by laying it, to achieve an evens money return \u2013 no matter the outcome of the race. If you want to follow along and try this approach yourself you'll need to download BF Bot Manager and sign up for either a subscription or at least a test period. They have a 5 day free trial that's valuable for establishing whether this tool will do what you want it to for your specific strategy. Resources Additional information on Dobbing Tool: BF Bot Manager BF Bot Manager Manuals - Set up \u00b6 First up we need to make sure we've downloaded and installed BF Bot Manager, and signed in. Once you open the program up click on the 'STRATEGIES' tab and then 'Add new' button. This will prompt a pop-up window to appear where you choose the name of the strategy and select a colour for it. I've named my strategy \"Double or Bust\" and set the colour as orange to make it easily identifiable which is a good idea, especially if you have a range of different strategies. - Creating a new strategy \u00b6 Once the strategy has been created, several conditions will also be automatically applied. Under 'Market Conditions': 'Overrounds' will check to make sure that overrounds for Back are between 100% and 115% and 85% and 100% for lay as default. We will keep this as is for our strategy. 'Time to bet' will check that the time till jump is 5 minutes or less as default before placing bets. For our strategy, we will change this to 30 seconds by clicking the edit icon. Under 'Selection Conditions': 'Back and Lay price ratio' will check to see if the difference between the back and lay price is no more than 15% as default. Just like how we changed the 'Time to bet' condition above, we will change this condition to 50%. 'Min/Max Selection Price' will ensure that the lay price for any runner is within price range of 1.5 and 10. We will delete this condition completely and replace it with a condition called 'Bet on my favourites only' We will do this in the next section of the tutorial. All These conditions must be met before the strategy will trigger any bet. - Conditions \u00b6 First, we will create the condition 'Bet on \"My Selections\" Only'. Click the plus icon and set the condition group as 'Selection Conditions' and then choose the 'Bet on \"My Selections\" Only' and click add. Make sure that Back is selected from the options box that will appear. - Rules \u00b6 Next, we are going to add a price rule to our strategy. Using the same process that used to create the 'Bet on My Selections only' condition, select the 'Price Rules' rule from the 'Price Settings' group. In the set price rules option box, select the following options: The next rule we're going to create will be under the Staking category called 'Level/Initial stake'. You can choose whatever default stake you are comfortable starting out with. Finally, we need to create an after-bet rule that will allow us to trade out. This rule is called 'Trade out (second) bet' under the 'After bet rules' category. See below the options that I have selected for this rule. TIP: BF Bot Manager strategies manual We recommend taking a look at the strategies manual if you get stuck or aren't sure how any of the conditions / rules work. It's an extremely good resource that goes into detail about each of the various functions and abilities that are available. And you're set! \u00b6 Once you've set your rules up and got comfortable using BF Bot Manager it should only take number of seconds to load the markets up and choose your selections for the day. Make sure that the strategy is set to run, and BF Bot Manager will do the rest for you. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"BF Bot Manager - Double or Bust"},{"location":"thirdPartyTools/BfBotManagerAutomation/#bf-bot-manager-double-or-bust","text":"","title":"BF Bot Manager: Double or bust"},{"location":"thirdPartyTools/BfBotManagerAutomation/#dobbing-using-bf-bot-manager-automation-rules","text":"BF Bot Manager may not have Excel automation integration like Gruss or Bet Angel, but it does make up for it with an intuitive rule-based automation system which is great if you're starting out with automation in general. There are hundreds of ways to implement different strategies using rules in BF Bot Manager but if you have any specific tutorial ideas that you would like to see here, please get in contact with us at bdp@betfair.com.au - We'd love to hear from you.","title":"Dobbing using BF Bot Manager automation rules"},{"location":"thirdPartyTools/BfBotManagerAutomation/#are-you-ready","text":"","title":"Are you Ready?"},{"location":"thirdPartyTools/BfBotManagerAutomation/#-the-plan","text":"In this tutorial, we're going over the implementation of a strategy called \"DOBBING\" which means Double Or Bust. The idea behind Dobbing is to place bets on the movement of prices within a market rather than the actual race result itself. More specifically, backing a horse and then hedging that bet \u2013 by laying it, to achieve an evens money return \u2013 no matter the outcome of the race. If you want to follow along and try this approach yourself you'll need to download BF Bot Manager and sign up for either a subscription or at least a test period. They have a 5 day free trial that's valuable for establishing whether this tool will do what you want it to for your specific strategy. Resources Additional information on Dobbing Tool: BF Bot Manager BF Bot Manager Manuals","title":"- The plan"},{"location":"thirdPartyTools/BfBotManagerAutomation/#-set-up","text":"First up we need to make sure we've downloaded and installed BF Bot Manager, and signed in. Once you open the program up click on the 'STRATEGIES' tab and then 'Add new' button. This will prompt a pop-up window to appear where you choose the name of the strategy and select a colour for it. I've named my strategy \"Double or Bust\" and set the colour as orange to make it easily identifiable which is a good idea, especially if you have a range of different strategies.","title":"- Set up"},{"location":"thirdPartyTools/BfBotManagerAutomation/#-creating-a-new-strategy","text":"Once the strategy has been created, several conditions will also be automatically applied. Under 'Market Conditions': 'Overrounds' will check to make sure that overrounds for Back are between 100% and 115% and 85% and 100% for lay as default. We will keep this as is for our strategy. 'Time to bet' will check that the time till jump is 5 minutes or less as default before placing bets. For our strategy, we will change this to 30 seconds by clicking the edit icon. Under 'Selection Conditions': 'Back and Lay price ratio' will check to see if the difference between the back and lay price is no more than 15% as default. Just like how we changed the 'Time to bet' condition above, we will change this condition to 50%. 'Min/Max Selection Price' will ensure that the lay price for any runner is within price range of 1.5 and 10. We will delete this condition completely and replace it with a condition called 'Bet on my favourites only' We will do this in the next section of the tutorial. All These conditions must be met before the strategy will trigger any bet.","title":"- Creating a new strategy"},{"location":"thirdPartyTools/BfBotManagerAutomation/#-conditions","text":"First, we will create the condition 'Bet on \"My Selections\" Only'. Click the plus icon and set the condition group as 'Selection Conditions' and then choose the 'Bet on \"My Selections\" Only' and click add. Make sure that Back is selected from the options box that will appear.","title":"- Conditions"},{"location":"thirdPartyTools/BfBotManagerAutomation/#-rules","text":"Next, we are going to add a price rule to our strategy. Using the same process that used to create the 'Bet on My Selections only' condition, select the 'Price Rules' rule from the 'Price Settings' group. In the set price rules option box, select the following options: The next rule we're going to create will be under the Staking category called 'Level/Initial stake'. You can choose whatever default stake you are comfortable starting out with. Finally, we need to create an after-bet rule that will allow us to trade out. This rule is called 'Trade out (second) bet' under the 'After bet rules' category. See below the options that I have selected for this rule. TIP: BF Bot Manager strategies manual We recommend taking a look at the strategies manual if you get stuck or aren't sure how any of the conditions / rules work. It's an extremely good resource that goes into detail about each of the various functions and abilities that are available.","title":"- Rules"},{"location":"thirdPartyTools/BfBotManagerAutomation/#and-youre-set","text":"Once you've set your rules up and got comfortable using BF Bot Manager it should only take number of seconds to load the markets up and choose your selections for the day. Make sure that the strategy is set to run, and BF Bot Manager will do the rest for you. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/BfBotManagerAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au","title":"What next?"},{"location":"thirdPartyTools/BfBotManagerAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Disclaimer"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/","text":"MarketFeeder Pro: 2-6 Staking \u00b6 Automating a 2-6 Staking strategy using MarketFeeder Pro \u00b6 In other tutorials available on the automation hub, the primary focus has been on choosing which runner to place a bet on. For this tutorial, we're going to look into something a little different. We will go through the steps to automate our stakes based on the percentage of our bank. This is a good method to keep the stakes that we're placing on the exchange under control and is primarily intended for backing on favourites. Lets do this \u00b6 - The plan \u00b6 If you want to follow along and try this approach yourself you'll need to download MarketFeeder Pro and sign up for either a subscription or at least a test period. They have a 1 month free trial that's valuable for establishing whether this tool will do what you want it to for your specific strategy. Resources Tool: MarketFeeder Pro MarketFeeder Pro Forum The idea behind this staking strategy is to win 2 bets out of a sequence of 6 opportunities. Each bet in the sequence is multiplied by predefined values from a list: 1, 2, 4, 6, 8, 12 For example, if we start with $10, we are aiming to win the following amounts: $10, $20, $40, $60, $80, $120 If there are any losses after each of these bets, the loss is included in the following bet. Once 2 bets have been won, the automation cycle will restart and likewise, if we reach the end of the cycle and 2 bets haven\u2019t been won the cycle will also restart. After each restart, the stake is recalculated as a percentage of our current bank. Please Note This strategy is designed to work while betting on favourites (back bets only) - Set up \u00b6 First up we need to make sure we've downloaded and installed MarketFeeder Pro, and signed in. Once you open the program up click on the 'Triggers' icon and open the triggers window. Once the 'Active Triggers' window appears, click the triggers editor button which will open another window where we will create this strategy. Click 'New File' and we're ready to start creating our strategy. - Constants \u00b6 First, we will create the constants and enter the values that will be used by our strategy. Click the 'Add constant' button 3 times which will give us 3 variables that we can start editing. We want to have 3 variables: 'cycle_length', 'bet_target' and 'init_bet'. As covered above, the cycle length will be 6 (the 6 opportunities in our planned sequence), bet target will be 2 (aim to win 2 bets before the cycle is restarted) and initial bet will be 1 (relating to trying to win 10% of our bank). - Trigger 1: initialisation of variables \u00b6 Next, we're going to create the necessary triggers which will contain the variables necessary for our strategy to work. In the Triggers window that we already have open, click the 'Add Block' button and then 'Add Trigger' Change the action from 'Lay' by clicking on it and selecting 'Set user variable' from the drop down box that appears. Then Rename it to 'Cycle_Counter' and give the value of 1. Create a further 3 actions by clicking the 'Add action' button named 'wns_counter' with a value of 0, 'current_loss' with a value of 0, 'Current bet' entering the below formula as the value: formula for 'Current bet' action init_bet*IF(test_mode=1,test_funds,total_funds)/100 Change the trigger selections to 'First Matching Session' and Execute 'One time Only' TIP: Naming triggers It's a good idea to give meaningful names to your triggers that help identify what they are and what they do. For this first trigger that we've created, I've renamed it to 'Initialisation of Variables' - Trigger 2: incrementing the counter at the end of an event \u00b6 For our next trigger which we will name 'incrementing the counter at the end of an event', we will create two actions, both as 'Set user variables'. The first will be named 'cycle-counter' and contain a value as the formula: formula for 'cycle_counter' cycle_counter+1 The second will be named 'current_loss' with a value of: formula for 'cycle_loss' IF(current_loss - market_settled_pl > 0.5, current_loss - market_settled_pl, 0) Again, we will be setting the selections for this trigger as 'First matching selection' but the Execute will be set as 'Once Per market'. We will also be setting the Market status to 'Settled'. - Trigger 3: after Winning \u00b6 Our third trigger called 'After Winning' will only contain one action called 'wns_counter'. Follow the same process set above to complete the trigger. The formula we'll be using for this value will be: formula for 'wns_counter' wns_counter+1 - Trigger 4: resetting counters \u00b6 The fourth trigger will contain 4 actions: 'cycle_counter', 'wns_counter', 'current_loss' and 'current_bet'. The last action will be using the formula: formula for 'current_bet' init_bet*IF(test_mode=1, test_funds, total_funds)/100 - Trigger 5: backing \u00b6 Our fifth and final trigger called 'backing' is a little different from the previous triggers and actions. Instead of it being a 'Set user variable' it will be changed to 'back'. The back price will need to be changed to 'back_price' and the amount will need the following formula: formula for 'backing' (current_bet*IF(cycle_counter=1, multiplier1, IF(cycle_counter=2, multiplier2, IF(cycle_counter=3, multiplier3, IF(cycle_counter=4, multiplier4, IF(cycle_counter=5, multiplier5, multiplier6))))) + current_loss)/(back_price-1) Selections gets changed to 'Favourite, Execute changed to 'Once per Market' and market status to 'All Except Settled' Once all of the triggers and actions have been added, it should look like this: TIP: Reorder triggers If your triggers are not listed in the same order as shown above, you can click and drag the triggers to re-order them correctly. And you're set! \u00b6 Once you've set your rules up and got comfortable using MarketFeeder Pro it should only take number of seconds to load the markets up and choose your selections for the day. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Market Feeder - Pro 2-6 Staking"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#marketfeeder-pro-2-6-staking","text":"","title":"MarketFeeder Pro: 2-6 Staking"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#automating-a-2-6-staking-strategy-using-marketfeeder-pro","text":"In other tutorials available on the automation hub, the primary focus has been on choosing which runner to place a bet on. For this tutorial, we're going to look into something a little different. We will go through the steps to automate our stakes based on the percentage of our bank. This is a good method to keep the stakes that we're placing on the exchange under control and is primarily intended for backing on favourites.","title":"Automating a 2-6 Staking strategy using MarketFeeder Pro"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#lets-do-this","text":"","title":"Lets do this"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#-the-plan","text":"If you want to follow along and try this approach yourself you'll need to download MarketFeeder Pro and sign up for either a subscription or at least a test period. They have a 1 month free trial that's valuable for establishing whether this tool will do what you want it to for your specific strategy. Resources Tool: MarketFeeder Pro MarketFeeder Pro Forum The idea behind this staking strategy is to win 2 bets out of a sequence of 6 opportunities. Each bet in the sequence is multiplied by predefined values from a list: 1, 2, 4, 6, 8, 12 For example, if we start with $10, we are aiming to win the following amounts: $10, $20, $40, $60, $80, $120 If there are any losses after each of these bets, the loss is included in the following bet. Once 2 bets have been won, the automation cycle will restart and likewise, if we reach the end of the cycle and 2 bets haven\u2019t been won the cycle will also restart. After each restart, the stake is recalculated as a percentage of our current bank. Please Note This strategy is designed to work while betting on favourites (back bets only)","title":"- The plan"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#-set-up","text":"First up we need to make sure we've downloaded and installed MarketFeeder Pro, and signed in. Once you open the program up click on the 'Triggers' icon and open the triggers window. Once the 'Active Triggers' window appears, click the triggers editor button which will open another window where we will create this strategy. Click 'New File' and we're ready to start creating our strategy.","title":"- Set up"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#-constants","text":"First, we will create the constants and enter the values that will be used by our strategy. Click the 'Add constant' button 3 times which will give us 3 variables that we can start editing. We want to have 3 variables: 'cycle_length', 'bet_target' and 'init_bet'. As covered above, the cycle length will be 6 (the 6 opportunities in our planned sequence), bet target will be 2 (aim to win 2 bets before the cycle is restarted) and initial bet will be 1 (relating to trying to win 10% of our bank).","title":"- Constants"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#-trigger-1-initialisation-of-variables","text":"Next, we're going to create the necessary triggers which will contain the variables necessary for our strategy to work. In the Triggers window that we already have open, click the 'Add Block' button and then 'Add Trigger' Change the action from 'Lay' by clicking on it and selecting 'Set user variable' from the drop down box that appears. Then Rename it to 'Cycle_Counter' and give the value of 1. Create a further 3 actions by clicking the 'Add action' button named 'wns_counter' with a value of 0, 'current_loss' with a value of 0, 'Current bet' entering the below formula as the value: formula for 'Current bet' action init_bet*IF(test_mode=1,test_funds,total_funds)/100 Change the trigger selections to 'First Matching Session' and Execute 'One time Only' TIP: Naming triggers It's a good idea to give meaningful names to your triggers that help identify what they are and what they do. For this first trigger that we've created, I've renamed it to 'Initialisation of Variables'","title":"- Trigger 1: initialisation of variables"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#-trigger-2-incrementing-the-counter-at-the-end-of-an-event","text":"For our next trigger which we will name 'incrementing the counter at the end of an event', we will create two actions, both as 'Set user variables'. The first will be named 'cycle-counter' and contain a value as the formula: formula for 'cycle_counter' cycle_counter+1 The second will be named 'current_loss' with a value of: formula for 'cycle_loss' IF(current_loss - market_settled_pl > 0.5, current_loss - market_settled_pl, 0) Again, we will be setting the selections for this trigger as 'First matching selection' but the Execute will be set as 'Once Per market'. We will also be setting the Market status to 'Settled'.","title":"- Trigger 2: incrementing the counter at the end of an event"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#-trigger-3-after-winning","text":"Our third trigger called 'After Winning' will only contain one action called 'wns_counter'. Follow the same process set above to complete the trigger. The formula we'll be using for this value will be: formula for 'wns_counter' wns_counter+1","title":"- Trigger 3: after Winning"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#-trigger-4-resetting-counters","text":"The fourth trigger will contain 4 actions: 'cycle_counter', 'wns_counter', 'current_loss' and 'current_bet'. The last action will be using the formula: formula for 'current_bet' init_bet*IF(test_mode=1, test_funds, total_funds)/100","title":"- Trigger 4: resetting counters"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#-trigger-5-backing","text":"Our fifth and final trigger called 'backing' is a little different from the previous triggers and actions. Instead of it being a 'Set user variable' it will be changed to 'back'. The back price will need to be changed to 'back_price' and the amount will need the following formula: formula for 'backing' (current_bet*IF(cycle_counter=1, multiplier1, IF(cycle_counter=2, multiplier2, IF(cycle_counter=3, multiplier3, IF(cycle_counter=4, multiplier4, IF(cycle_counter=5, multiplier5, multiplier6))))) + current_loss)/(back_price-1) Selections gets changed to 'Favourite, Execute changed to 'Once per Market' and market status to 'All Except Settled' Once all of the triggers and actions have been added, it should look like this: TIP: Reorder triggers If your triggers are not listed in the same order as shown above, you can click and drag the triggers to re-order them correctly.","title":"- Trigger 5: backing"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#and-youre-set","text":"Once you've set your rules up and got comfortable using MarketFeeder Pro it should only take number of seconds to load the markets up and choose your selections for the day. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au","title":"What next?"},{"location":"thirdPartyTools/MarketFeeder2_6Staking/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Disclaimer"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/","text":"Bet Angel: Market favourite automation \u00b6 Automating a market favourite strategy using Bet Angel Pro \u00b6 Here we explore how to implement an automated strategy to place Betfair Starting Price (BSP) bets on the top two runners in the market. This lets you choose your selections based on market sentiment close to the jump, and not worry about current market price by using BSP to place your bets. You could equally use effectively the same approach if you wanted to lay the favourite(s) instead of back them. Building on our previous articles , we're using the spreadsheet functionality available in Bet Angel Pro to implement this strategy. If you haven't already we'd recommend going back and having a read of this article , as the concepts here do build on what we covered previously. As we've said before, there are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on bdp@betfair.com.au with your feedback and opinions. Lets Begin \u00b6 - The plan \u00b6 Given that we're simply choosing our selections based on the market we don't need any ratings for this strategy. The plan is to look at the market a couple of minutes before the scheduled jump and place BSP bets based on its formation. Our approach here, and how we've set up the accompanying spreadsheet, backs the top two runners in the market two minutes out from the scheduled start time using the Betfair Starting Price. Resources Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and approach Tool: Bet Angel Pro - Set up \u00b6 Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. - Writing your rules \u00b6 As with any automated strategy, one of the most important steps is deciding what logical approach you want to take, and writing rules that suit. We're using a customised version of the default Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on the favourites being shown in the market. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet \u00b6 In short, we want to back runners when: the selection's available to back price (the blue box on the Exchange) is either the lowest or second lowest in the market - the top two market favourites the scheduled event start time is less than 2 minutes away the event isn't in play - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Fav refers to cell H4 in the SETTINGS worksheet CurrentBMP refers to cell AI8 in the Bet Angel worksheet where the overrounds are calculated BMP refers to cell J3 in the SETTINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell E4 in the SETTINGS worksheet MinTime refers to cell I2 in the SETTINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners MaxTime refers to cell K2 in the SETTINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell G1 in the Bet Angel work sheet. Bet Angel will populate a status in this cell such as \"In Play\" or \"Suspended\" TakeSP refers to cell H5 in the SETTINGS worksheet which allows you to change a single value more easily, alongside other values for the global command This is our trigger on Excel formula: Multi line =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") Single line =IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1,TimeTillJump < MaxTime,TimeTillJump > MinTime,CurrentBMP<BMP,ISBLANK(InPlay)),\"BACK\",\"\") Stepping through each step: Finding the top two selections in the market: check each runner to see if they're one of the two market favourites - We're doing this by going through the best available to back (blue) price for each runner, ranking them in order (which sorts them from highest to lowest - which is the opposite of what we want) then subtracting that rank number from the total number of selections available to inverse the order. Finally, we plus one to the resulting rank - if we didn't do this then you'd have a rank order that started at 0, not 1, and we thought that would just confuse matters! Once it's established what each selection's rank is, we then check if that rank is less than three, and if it is we know that the runner in question is one of the top two in the market, based on the current available to back prices. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") Time < 2 mins and > 1 min: check whether the seconds left on the countdown are smaller than what is defined in the SETTINGS worksheet, cell K2 and greater than SETTINGS worksheet, cell I2, as we need to both place the bet and then convert it to a BSP bet before the off (more on this later). This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E4 of the SETTINGS sheet, where we've already done the calculations for you. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") BMP: checking whether the event overrounds are less than the specific value that can be changed from cell J3 in the SETTINGS worksheet =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") Not in play: checking whether the event has gone in play, as this is purely a pre-play strategy, though you could certainly take a similar approach to in-play markets. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") Result: if the statement above is true, the formula returns \"BACK\", at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") Convert bets to Betfair Starting Price: Bet Angel Pro doesn't offer the option to place straight BSP bets, so we've got around that here by placing the bets initially at odds of 1000 (which won't get matched for short favourites), and then at certain amount of seconds from the scheduled start using what Bet Angel calls a 'Global Command' to convert all unmatched bets to BSP. The exact number of seconds can be easily changed by updating cell H5 from the SETTINGS worksheet. This formula goes in cell L6, and once it's triggered the bets will automatically convert. =IF(TimeTillJump < TakeSP, \"TAKE_SP_ALL\", \"\") Excel functions IF statement: IF(if this is true, do this, else do this) AND statement: AND(this is true, and so is this, and so is this) - returns true or false COUNT function: returns number of cells in the range you pass in tha contain a number RANK function: returns the rank of a number in a list of numbers, with the smallest number returning the highest rank. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste these three formulas into the relevant cell on each green row - We copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). Multi line =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") Single line =IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1,TimeTillJump < MaxTime,TimeTillJump > MinTime,CurrentBMP<BMP,ISBLANK(InPlay)),\"BACK\",\"\") Odds: as we said we're putting the bet up initially at odds of 1000, so this is a simple one. 1000 Stake: it's completely up to you what staking approach you want to take. We're keeping it simple and using flat staking here, so will just place $10 on each runner. This goes in column N (N9 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. 10 Global Command: this is what triggers the open bets to convert to BSP, and only goes in one cell, L6. As soon as the countdown timer reaches less than 60 seconds this will fire. ''' =IF(TimeTillJump < TakeSP, \"TAKE_SP_ALL\", \"\") ''' - You know the drill \u00b6 The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Bet Angel Pro. - Selecting markets \u00b6 We used the markets menu in the Guardian tool to navigate to the tracks we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. If you wanted to include all horse or greyhound races for a day you could use the 'quick picks' tab to do this more efficiently. Once you've chosen the races you're interested in click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). - Linking the spreadsheet \u00b6 Open the 'Excel' tab in Guardian, then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: We appreciate it's obvious, but you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Bet Angel features \u00b6 Here are some Bet Angel features that you'll need to consider. - Multiple bets/clearing status cells \u00b6 The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as my check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use the same sheet for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status and global status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs. - Turning off bet confirmation \u00b6 Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. - Editing the spreadsheet \u00b6 The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits. Areas for improvement \u00b6 There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to bdp@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this. Also, if the market changes significantly in those last few minutes and a third selection shortens in past the two we've placed bets on you could end up with bets on more than the intended two runner. This is something you could check for in your bet rule if you wanted to ensure you were only backing a set number of runners. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Bet Angel - Market favourite automation"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#bet-angel-market-favourite-automation","text":"","title":"Bet Angel: Market favourite automation"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#automating-a-market-favourite-strategy-using-bet-angel-pro","text":"Here we explore how to implement an automated strategy to place Betfair Starting Price (BSP) bets on the top two runners in the market. This lets you choose your selections based on market sentiment close to the jump, and not worry about current market price by using BSP to place your bets. You could equally use effectively the same approach if you wanted to lay the favourite(s) instead of back them. Building on our previous articles , we're using the spreadsheet functionality available in Bet Angel Pro to implement this strategy. If you haven't already we'd recommend going back and having a read of this article , as the concepts here do build on what we covered previously. As we've said before, there are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on bdp@betfair.com.au with your feedback and opinions.","title":"Automating a market favourite strategy using Bet Angel Pro"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#lets-begin","text":"","title":"Lets Begin"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-the-plan","text":"Given that we're simply choosing our selections based on the market we don't need any ratings for this strategy. The plan is to look at the market a couple of minutes before the scheduled jump and place BSP bets based on its formation. Our approach here, and how we've set up the accompanying spreadsheet, backs the top two runners in the market two minutes out from the scheduled start time using the Betfair Starting Price. Resources Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and approach Tool: Bet Angel Pro","title":"- The plan"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-set-up","text":"Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.","title":"- Set up"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take, and writing rules that suit. We're using a customised version of the default Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on the favourites being shown in the market. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-trigger-to-place-bet","text":"In short, we want to back runners when: the selection's available to back price (the blue box on the Exchange) is either the lowest or second lowest in the market - the top two market favourites the scheduled event start time is less than 2 minutes away the event isn't in play","title":"- Trigger to place bet"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Fav refers to cell H4 in the SETTINGS worksheet CurrentBMP refers to cell AI8 in the Bet Angel worksheet where the overrounds are calculated BMP refers to cell J3 in the SETTINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell E4 in the SETTINGS worksheet MinTime refers to cell I2 in the SETTINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners MaxTime refers to cell K2 in the SETTINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell G1 in the Bet Angel work sheet. Bet Angel will populate a status in this cell such as \"In Play\" or \"Suspended\" TakeSP refers to cell H5 in the SETTINGS worksheet which allows you to change a single value more easily, alongside other values for the global command This is our trigger on Excel formula: Multi line =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") Single line =IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1,TimeTillJump < MaxTime,TimeTillJump > MinTime,CurrentBMP<BMP,ISBLANK(InPlay)),\"BACK\",\"\") Stepping through each step: Finding the top two selections in the market: check each runner to see if they're one of the two market favourites - We're doing this by going through the best available to back (blue) price for each runner, ranking them in order (which sorts them from highest to lowest - which is the opposite of what we want) then subtracting that rank number from the total number of selections available to inverse the order. Finally, we plus one to the resulting rank - if we didn't do this then you'd have a rank order that started at 0, not 1, and we thought that would just confuse matters! Once it's established what each selection's rank is, we then check if that rank is less than three, and if it is we know that the runner in question is one of the top two in the market, based on the current available to back prices. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") Time < 2 mins and > 1 min: check whether the seconds left on the countdown are smaller than what is defined in the SETTINGS worksheet, cell K2 and greater than SETTINGS worksheet, cell I2, as we need to both place the bet and then convert it to a BSP bet before the off (more on this later). This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E4 of the SETTINGS sheet, where we've already done the calculations for you. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") BMP: checking whether the event overrounds are less than the specific value that can be changed from cell J3 in the SETTINGS worksheet =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") Not in play: checking whether the event has gone in play, as this is purely a pre-play strategy, though you could certainly take a similar approach to in-play markets. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") Result: if the statement above is true, the formula returns \"BACK\", at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") Convert bets to Betfair Starting Price: Bet Angel Pro doesn't offer the option to place straight BSP bets, so we've got around that here by placing the bets initially at odds of 1000 (which won't get matched for short favourites), and then at certain amount of seconds from the scheduled start using what Bet Angel calls a 'Global Command' to convert all unmatched bets to BSP. The exact number of seconds can be easily changed by updating cell H5 from the SETTINGS worksheet. This formula goes in cell L6, and once it's triggered the bets will automatically convert. =IF(TimeTillJump < TakeSP, \"TAKE_SP_ALL\", \"\") Excel functions IF statement: IF(if this is true, do this, else do this) AND statement: AND(this is true, and so is this, and so is this) - returns true or false COUNT function: returns number of cells in the range you pass in tha contain a number RANK function: returns the rank of a number in a list of numbers, with the smallest number returning the highest rank. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste these three formulas into the relevant cell on each green row - We copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). Multi line =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump < MaxTime, TimeTillJump > MinTime, CurrentBMP<BMP, ISBLANK(InPlay)), \"BACK\", \"\") Single line =IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G67,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1,TimeTillJump < MaxTime,TimeTillJump > MinTime,CurrentBMP<BMP,ISBLANK(InPlay)),\"BACK\",\"\") Odds: as we said we're putting the bet up initially at odds of 1000, so this is a simple one. 1000 Stake: it's completely up to you what staking approach you want to take. We're keeping it simple and using flat staking here, so will just place $10 on each runner. This goes in column N (N9 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. 10 Global Command: this is what triggers the open bets to convert to BSP, and only goes in one cell, L6. As soon as the countdown timer reaches less than 60 seconds this will fire. ''' =IF(TimeTillJump < TakeSP, \"TAKE_SP_ALL\", \"\") '''","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-you-know-the-drill","text":"The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Bet Angel Pro.","title":"- You know the drill"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-selecting-markets","text":"We used the markets menu in the Guardian tool to navigate to the tracks we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. If you wanted to include all horse or greyhound races for a day you could use the 'quick picks' tab to do this more efficiently. Once you've chosen the races you're interested in click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up).","title":"- Selecting markets"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-linking-the-spreadsheet","text":"Open the 'Excel' tab in Guardian, then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins.","title":"- Linking the spreadsheet"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: We appreciate it's obvious, but you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#bet-angel-features","text":"Here are some Bet Angel features that you'll need to consider.","title":"Bet Angel features"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-multiple-betsclearing-status-cells","text":"The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as my check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use the same sheet for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status and global status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs.","title":"- Multiple bets/clearing status cells"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-turning-off-bet-confirmation","text":"Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings.","title":"- Turning off bet confirmation"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-editing-the-spreadsheet","text":"The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits.","title":"- Editing the spreadsheet"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to bdp@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this. Also, if the market changes significantly in those last few minutes and a third selection shortens in past the two we've placed bets on you could end up with bets on more than the intended two runner. This is something you could check for in your bet rule if you wanted to ensure you were only backing a set number of runners.","title":"Areas for improvement"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Disclaimer"},{"location":"thirdPartyTools/betAngelRatingsAutomation/","text":"Bet Angel: Ratings automation \u00b6 Automating a ratings based strategy using Bet Angel Pro \u00b6 Ratings are the basis for a lot of betting strategies, but they can be particularly painful and time-consuming to implement manually. This makes them ideal for automation, where you use a program to place bets on your behalf while you get on with other things. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on bdp@betfair.com.au with your feedback and opinions. Ready. Set. GO! \u00b6 - The plan \u00b6 We're using the Greyhound Ratings Model put together by some of our Data Scientists. This model creates ratings for Victorian greyhound races daily and is freely available on the Hub. It's pretty good at predicting winners, so we're going to place back bets on the dogs with shorter ratings where the market price is better than the model's rating. Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here. Here we'll step through how we went about getting Bet Angel Pro to place bets using the ratings from Betfair's Data Scientists' Greyhound Ratings Model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program running and be able to walk away. Resources Ratings: Betfair Data Scientists' Greyhound Ratings Model Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Bet Angel Pro - Set up \u00b6 Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. - Finding & formatting ratings \u00b6 Here we're using the ratings shared by our Data Scientists on the Hub . This makes for a bit of prep work, copying the list of runners and their rating into an Excel spreadsheet. As a minimum you'll need a list of runner names (including the runner number followed by a full stop, i.e. 1. Runner Name) in one column and their rating in another in an Excel sheet. If you have a list of ratings already in a spreadsheet that's even better - you'll be able to tweak the Excel formulas to work with whatever format your data is in. Wherever your ratings come from, you'll need to include them in the spreadsheet you're using to interact with Bet Angel. Here we're using a spreadsheet we edited for this strategy , and we've included a tab called RATINGS where you can copy in the runner names and ratings. - Writing your rules \u00b6 As with any automated strategy, one of the most important steps is deciding what logical approach you want to take, and writing rules that suit. We're using a customised version of the default Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet \u00b6 In short, we want to back runners when: the available to back price is better than the rating for that runner by a variable percentage they have a rating less than 5 the scheduled event start time is less than 2 minutes away the event isn't in play - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial MyRatings refers to the entire Column B in the Ratings worksheet MyRunners refers to the entire column A in the Ratings worksheet OddsMultiplier refers to the table in the Settings worksheet (C11 to D17) CurrentBMP refers to cell AI8 in the Bet Angel worksheet where the overrounds are calculated BMP refers to cell E2 in the RATINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell E3 in the Settings work sheet MyTime refers to cell E3 in the RATINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell G1 in the Bet Angel worksheet. Bet Angel will populate a status in this cell such as \"In Play\" or \"Suspended\" RatingThreshold refers to cell E4 in the RATINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners This is our trigger on Excel formula: Multi line =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") Single line =IF(AND(G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold,TimeTillJump < MyTime,CurrentBMP < BMP,ISBLAN(InPlay)),\"BACK\",\"\") Stepping through each step: Price > rating * percentage offset: check whether the available to back price is better than the runner's rating multiplied by a percentage - we do this by using the runner name in column B and looking up the corresponding rating for that runner from the RATINGS sheet. Percentage offset: There are lots of different approaches you can take to this. We're using a variable percentage offset, as we appreciate that we might want a different percentage better than the rating, depending on the price - i.e. 10% better than 2 ( 2 ( 2.20) is very different than 10% better than a 20 shot ( 20 shot ( 22.20), so here we're using a vlookup table to determine the percentage better than the rating that we want based on the current odds. Here are the 'ranges' of prices to percentage offset that we're using - you can disregard this and just change it to be a set percentage (i.e. *1.1 hardcoded into the formula) or just use your rating straight without an offset, or edit the ranges in the SETTINGS tab to suit your opinions. This table takes the 'min' odds for the range in the left column, and the number you want to multiply the odds by in the right column - so for 15% you'd multiply by 1.15 etc. Viewing your values: We've added columns (AF:AH) to show the rating, percentage offset and minimum acceptable odds for each runner, to add some reassurance that the spreadsheet is pulling the values we want it to. ODDS RANGE % MULTIPLIER 1 - 6 1.1 (10%) 6 - 9 1.15 (15%) 9 - 15 1.2 (20%) 15 - 20 1.3 (30%) 20 - 35 1.4 (40%) 35 + 1.5 (50%) Here are three different examples of formulas you can use here, depending on your approach: Variable percentage =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)),SETTINGS!$C$12:$D$17,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, ISBLANK(InPlay)), \"BACK\", \"\" ) Fixed percentage =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*1.1), INDEX(MyRatings,MATCH(B9,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, ISBLANK(InPlay)), \"BACK\", \"\" ) Pure rating =IF( AND( G9 > INDEX(MyRatings,MATCH(B9,MyRunners,0)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, ISBLANK(InPlay)), \"BACK\", \"\" ) Updating references to suit your ratings format If your ratings are formatted differently to our simple runner name | rating layout you can easily edit the formula to refence the relevant column directly, instead of changing your ratings to fit the formula. Let's say your ratings sheet is set out like this: race date | venue | runner name | last race time | weight | rating Here's the set up of the formula: MyRatings[your rating],MATCH(B9,MyRunners[runner name],0)) So your edited formula would be: RATINGS!F:F,MATCH(B9,RATINGS!C:C,0)) You need to make sure that you updated these references both in the this part of the formula, and in the next step too. RatingThreshold: check whether the runner's rating is less than the value you can specify in cell E4 in the RATINGS worksheet (because we only want to bet on the favourite few runners) =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") Time < 2 mins: check whether the seconds left on the countdown are smaller than 120 (2 minutes), as the majority of markets don't fully form until the last few minutes before the off. This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E4 of the SETTINGS sheet, where I've already done the calculations for you. =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") - BMP: checking whether the event overrounds are less than the specific value that can be changed from cell E2 in the RATINGS tab =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. We appreciate that greyhound races don't go in play, but we wanted this check in place anyway in case we (or you!) wanted to use a version of this strategy on horse racing in the future. =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") Result: if the statement above is true, the formula returns \"BACK\", at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") Excel functions IF statement: IF(if this is true, do this, else do this) AND statement: AND(this is true, and so is this, and so is this) - returns true or false VLOOKUP: looking up a value from a table based on the value you pass in Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste these three formulas into the relevant cell on each green row - We copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). Multi line =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") Single line =IF(AND(G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold,TimeTillJump < MyTime,CurrentBMP < BMP,ISBLANK(InPlay)),\"BACK\",\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this article. =G9 Stake: it's completely up to you what staking approach you want to take. We've kept it simple, and am just using a 'to win' strategy. Each bet aims to win $10 on that runner at the current odds. The formula divides $10 by the current available best back odds (cell G9 for the first runner) minus one to get the stake required to win $10. This goes in column N (N9 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =10/(G9-1) - Selecting markets \u00b6 We used the markets menu in the Guardian tool to navigate to the tracks we had ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. If you wanted to include all horse or greyhound races for a day you could use the 'quick picks' tab to do this more efficiently. Once you've chosen the races you're interested in click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). - Linking the spreadsheet \u00b6 Open the 'Excel' tab in Guardian, then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Bet Angel features \u00b6 Here are some Bet Angel features that you'll need to consider. - Multiple bets/clearing status cells \u00b6 The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use the same sheet for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs. - Turning off bet confirmation \u00b6 Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. - Editing the spreadsheet \u00b6 The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits. Areas for improvement \u00b6 There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to bdp@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - We missed some races because of this. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Bet Angel - Ratings automation"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#bet-angel-ratings-automation","text":"","title":"Bet Angel: Ratings automation"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#automating-a-ratings-based-strategy-using-bet-angel-pro","text":"Ratings are the basis for a lot of betting strategies, but they can be particularly painful and time-consuming to implement manually. This makes them ideal for automation, where you use a program to place bets on your behalf while you get on with other things. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on bdp@betfair.com.au with your feedback and opinions.","title":"Automating a ratings based strategy using Bet Angel Pro"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#ready-set-go","text":"","title":"Ready. Set. GO!"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-the-plan","text":"We're using the Greyhound Ratings Model put together by some of our Data Scientists. This model creates ratings for Victorian greyhound races daily and is freely available on the Hub. It's pretty good at predicting winners, so we're going to place back bets on the dogs with shorter ratings where the market price is better than the model's rating. Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here. Here we'll step through how we went about getting Bet Angel Pro to place bets using the ratings from Betfair's Data Scientists' Greyhound Ratings Model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program running and be able to walk away. Resources Ratings: Betfair Data Scientists' Greyhound Ratings Model Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Bet Angel Pro","title":"- The plan"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-set-up","text":"Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.","title":"- Set up"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-finding-formatting-ratings","text":"Here we're using the ratings shared by our Data Scientists on the Hub . This makes for a bit of prep work, copying the list of runners and their rating into an Excel spreadsheet. As a minimum you'll need a list of runner names (including the runner number followed by a full stop, i.e. 1. Runner Name) in one column and their rating in another in an Excel sheet. If you have a list of ratings already in a spreadsheet that's even better - you'll be able to tweak the Excel formulas to work with whatever format your data is in. Wherever your ratings come from, you'll need to include them in the spreadsheet you're using to interact with Bet Angel. Here we're using a spreadsheet we edited for this strategy , and we've included a tab called RATINGS where you can copy in the runner names and ratings.","title":"- Finding &amp; formatting ratings"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take, and writing rules that suit. We're using a customised version of the default Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-trigger-to-place-bet","text":"In short, we want to back runners when: the available to back price is better than the rating for that runner by a variable percentage they have a rating less than 5 the scheduled event start time is less than 2 minutes away the event isn't in play","title":"- Trigger to place bet"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial MyRatings refers to the entire Column B in the Ratings worksheet MyRunners refers to the entire column A in the Ratings worksheet OddsMultiplier refers to the table in the Settings worksheet (C11 to D17) CurrentBMP refers to cell AI8 in the Bet Angel worksheet where the overrounds are calculated BMP refers to cell E2 in the RATINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell E3 in the Settings work sheet MyTime refers to cell E3 in the RATINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell G1 in the Bet Angel worksheet. Bet Angel will populate a status in this cell such as \"In Play\" or \"Suspended\" RatingThreshold refers to cell E4 in the RATINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners This is our trigger on Excel formula: Multi line =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") Single line =IF(AND(G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold,TimeTillJump < MyTime,CurrentBMP < BMP,ISBLAN(InPlay)),\"BACK\",\"\") Stepping through each step: Price > rating * percentage offset: check whether the available to back price is better than the runner's rating multiplied by a percentage - we do this by using the runner name in column B and looking up the corresponding rating for that runner from the RATINGS sheet. Percentage offset: There are lots of different approaches you can take to this. We're using a variable percentage offset, as we appreciate that we might want a different percentage better than the rating, depending on the price - i.e. 10% better than 2 ( 2 ( 2.20) is very different than 10% better than a 20 shot ( 20 shot ( 22.20), so here we're using a vlookup table to determine the percentage better than the rating that we want based on the current odds. Here are the 'ranges' of prices to percentage offset that we're using - you can disregard this and just change it to be a set percentage (i.e. *1.1 hardcoded into the formula) or just use your rating straight without an offset, or edit the ranges in the SETTINGS tab to suit your opinions. This table takes the 'min' odds for the range in the left column, and the number you want to multiply the odds by in the right column - so for 15% you'd multiply by 1.15 etc. Viewing your values: We've added columns (AF:AH) to show the rating, percentage offset and minimum acceptable odds for each runner, to add some reassurance that the spreadsheet is pulling the values we want it to. ODDS RANGE % MULTIPLIER 1 - 6 1.1 (10%) 6 - 9 1.15 (15%) 9 - 15 1.2 (20%) 15 - 20 1.3 (30%) 20 - 35 1.4 (40%) 35 + 1.5 (50%) Here are three different examples of formulas you can use here, depending on your approach: Variable percentage =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)),SETTINGS!$C$12:$D$17,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, ISBLANK(InPlay)), \"BACK\", \"\" ) Fixed percentage =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*1.1), INDEX(MyRatings,MATCH(B9,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, ISBLANK(InPlay)), \"BACK\", \"\" ) Pure rating =IF( AND( G9 > INDEX(MyRatings,MATCH(B9,MyRunners,0)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, ISBLANK(InPlay)), \"BACK\", \"\" ) Updating references to suit your ratings format If your ratings are formatted differently to our simple runner name | rating layout you can easily edit the formula to refence the relevant column directly, instead of changing your ratings to fit the formula. Let's say your ratings sheet is set out like this: race date | venue | runner name | last race time | weight | rating Here's the set up of the formula: MyRatings[your rating],MATCH(B9,MyRunners[runner name],0)) So your edited formula would be: RATINGS!F:F,MATCH(B9,RATINGS!C:C,0)) You need to make sure that you updated these references both in the this part of the formula, and in the next step too. RatingThreshold: check whether the runner's rating is less than the value you can specify in cell E4 in the RATINGS worksheet (because we only want to bet on the favourite few runners) =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") Time < 2 mins: check whether the seconds left on the countdown are smaller than 120 (2 minutes), as the majority of markets don't fully form until the last few minutes before the off. This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E4 of the SETTINGS sheet, where I've already done the calculations for you. =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") - BMP: checking whether the event overrounds are less than the specific value that can be changed from cell E2 in the RATINGS tab =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. We appreciate that greyhound races don't go in play, but we wanted this check in place anyway in case we (or you!) wanted to use a version of this strategy on horse racing in the future. =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") Result: if the statement above is true, the formula returns \"BACK\", at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") Excel functions IF statement: IF(if this is true, do this, else do this) AND statement: AND(this is true, and so is this, and so is this) - returns true or false VLOOKUP: looking up a value from a table based on the value you pass in Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste these three formulas into the relevant cell on each green row - We copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). Multi line =IF( AND( G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)), OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, ISBLANK(InPlay)), \"BACK\", \"\") Single line =IF(AND(G9 > (INDEX(MyRatings,MATCH(B9,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(B9,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(B9,MyRunners,0)) <RatingThreshold,TimeTillJump < MyTime,CurrentBMP < BMP,ISBLANK(InPlay)),\"BACK\",\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this article. =G9 Stake: it's completely up to you what staking approach you want to take. We've kept it simple, and am just using a 'to win' strategy. Each bet aims to win $10 on that runner at the current odds. The formula divides $10 by the current available best back odds (cell G9 for the first runner) minus one to get the stake required to win $10. This goes in column N (N9 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =10/(G9-1)","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-selecting-markets","text":"We used the markets menu in the Guardian tool to navigate to the tracks we had ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. If you wanted to include all horse or greyhound races for a day you could use the 'quick picks' tab to do this more efficiently. Once you've chosen the races you're interested in click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up).","title":"- Selecting markets"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-linking-the-spreadsheet","text":"Open the 'Excel' tab in Guardian, then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins.","title":"- Linking the spreadsheet"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#bet-angel-features","text":"Here are some Bet Angel features that you'll need to consider.","title":"Bet Angel features"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-multiple-betsclearing-status-cells","text":"The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use the same sheet for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs.","title":"- Multiple bets/clearing status cells"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-turning-off-bet-confirmation","text":"Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings.","title":"- Turning off bet confirmation"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-editing-the-spreadsheet","text":"The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits.","title":"- Editing the spreadsheet"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to bdp@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - We missed some races because of this.","title":"Areas for improvement"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Disclaimer"},{"location":"thirdPartyTools/betAngelSimultaneousMarkets/","text":"Bet Angel: Automating simultaneous markets \u00b6 Dont miss out on a market with simultaneous automation \u00b6 If you have a concern of missing markets due to delays or unforeseen circumstances, Bet Angel is able to work off multiple worksheets for different meetings, all from the same workbook. For example, if we are wanting to automate markets at 1.45pm at Swan Hill, 1.55pm at Townsville and 2.02pm at Menangle, but the Swan Hill market is delayed, we don't want to miss out on the Townsville or Menangle markets. Click Here to download the spreadsheet that we have edited which will allow you to bet on multiple markets To set this up, we need to make sure that there are enough instances of the \u2018Bet Angel\u2019 worksheet to cover markets in the event of a delay or on track issue. In this context, three worksheets should be enough to cover the days markets if some of them are delayed for whatever reason. We've created a special Excel file with macros that will allow up to three markets to be linked at the same time. These macros automatically clear the contents of each sheet when they detect a market change and clear any errors that may occur. If you want to have more worksheets to be linked to different markets, you will need to update the macros accordingly so that they all work independently from one another. Simultaneous markets spreadsheet Please note that the above edited excel workbook does not include any automated strategies. You will need to add this in yourself or take a look at our Multiple market ratings tutorial Once this is done, save the file and close out of it completely (Bet Angel will open it back up for us automatically when we\u2019re ready to start our automation). In Bet Angel, follow the usual process of clicking the \u2018Guardian\u2019 Icon, select your markets and the usual \u2018Browse for file\u2019 button to locate your Excel file. The only thing you will need to do differently for Bet Angel is to simply allocate a specific worksheet to a particular market that you are betting on. As each market concludes, the assigned worksheet will then reset and then re-assign itself to the next market in the guaradian list. When you\u2019re ready for automation to take over, click the \u2018Connect\u2019 check box and then it will do its thing. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Bet Angel - simultaneous markets"},{"location":"thirdPartyTools/betAngelSimultaneousMarkets/#bet-angel-automating-simultaneous-markets","text":"","title":"Bet Angel: Automating simultaneous markets"},{"location":"thirdPartyTools/betAngelSimultaneousMarkets/#dont-miss-out-on-a-market-with-simultaneous-automation","text":"If you have a concern of missing markets due to delays or unforeseen circumstances, Bet Angel is able to work off multiple worksheets for different meetings, all from the same workbook. For example, if we are wanting to automate markets at 1.45pm at Swan Hill, 1.55pm at Townsville and 2.02pm at Menangle, but the Swan Hill market is delayed, we don't want to miss out on the Townsville or Menangle markets. Click Here to download the spreadsheet that we have edited which will allow you to bet on multiple markets To set this up, we need to make sure that there are enough instances of the \u2018Bet Angel\u2019 worksheet to cover markets in the event of a delay or on track issue. In this context, three worksheets should be enough to cover the days markets if some of them are delayed for whatever reason. We've created a special Excel file with macros that will allow up to three markets to be linked at the same time. These macros automatically clear the contents of each sheet when they detect a market change and clear any errors that may occur. If you want to have more worksheets to be linked to different markets, you will need to update the macros accordingly so that they all work independently from one another. Simultaneous markets spreadsheet Please note that the above edited excel workbook does not include any automated strategies. You will need to add this in yourself or take a look at our Multiple market ratings tutorial Once this is done, save the file and close out of it completely (Bet Angel will open it back up for us automatically when we\u2019re ready to start our automation). In Bet Angel, follow the usual process of clicking the \u2018Guardian\u2019 Icon, select your markets and the usual \u2018Browse for file\u2019 button to locate your Excel file. The only thing you will need to do differently for Bet Angel is to simply allocate a specific worksheet to a particular market that you are betting on. As each market concludes, the assigned worksheet will then reset and then re-assign itself to the next market in the guaradian list. When you\u2019re ready for automation to take over, click the \u2018Connect\u2019 check box and then it will do its thing.","title":"Dont miss out on a market with simultaneous automation"},{"location":"thirdPartyTools/betAngelSimultaneousMarkets/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/betAngelSimultaneousMarkets/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au","title":"What next?"},{"location":"thirdPartyTools/betAngelSimultaneousMarkets/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Disclaimer"},{"location":"thirdPartyTools/betAngelTippingAutomation/","text":"Bet Angel: Tipping automation \u00b6 Automating a (non-ratings based) tipping strategy using Bet Angel Pro \u00b6 We all love getting some good racing tips, but who has time to sit and place bets all day? Wouldn't it be easier if you could take those tips and get a program to automatically place the bets on your behalf? This is what we're going to explore here - we'll be using Bet Angel Pro to place bets automatically based on a set of tips. This is our first-time using Bet Angel for this approach and we are very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at bdp@betfair.com.au with your feedback and opinions. Lets Go! \u00b6 - The plan \u00b6 We have a set of tips that we've taken from our DataScientists' Racing Prediction Model, but this approach should work for any set of tips you may have. Our goal is to create an automated process which will let us choose our tips for the day, then walk away and the program do the leg work. Here we'll step through how we went about getting Bet Angel Pro to place bets on the favourite runner identified by Betfair's Data Scientists . There are no ratings associated with these tips, so we're happy to take Betfair's Starting Price instead of a price for these bets. Bet Angel Pro's 'Guardian' feature has the capacity to let you create rules that can then be applied to specific selections, which is how we're going to implement this strategy. After digging around on Bet Angel's forum and blog , we found an article on how to create a generic automation rule , which we found useful in helping us learn how to use the tool to automate these tips. We also found a rule that someone else had written to back or lay a selection at BSP , which is what we used as the basis for the rule we're using here . If you want to follow along and try this approach yourself you'll need to download Bet Angel Pro and sign up for either a subscription or at least a test period. They have a 14 day free trial that's valuable for establishing whether this tool will do what you want it to for your specific strategy. Resources Tips: Betfair Data Scientists' Racing Prediction Model Rules: rule file , based on Bet Angel Automation - Horse racing - Pre off : Back or Lay at Betfair SP Guardian Automation Bot Tool: Bet Angel Pro - Set up \u00b6 First up we need to make sure we've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. We've created an automated rule that you can download here, roughly based on the one discussed in this forum post (note: you will need to create a Bet Angel forum account to be able to download the rule). - Writing your rules \u00b6 open up the 'Automation' tab click 'import a rule' navigate to the rule file you just downloaded (probably in your Downloads folder by default) upload the rule click on the 'rules file name' and select the rule file you just uploaded You can choose to change the bet stake and other parts of the rule by: clicking on 'edit rules file', selecting the rule you want to change (i.e. back at SP) click on the 'parameters' tab change the stake as desired Turning off bet confirmation Unless you want to manually confirm each individual bet placed (which you definitely might want to leave turned on until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?'. You can then save these settings, but you'll need to go into the settings tab and choose the saved file each time you open the program. - Selecting your markets \u00b6 We decided we only wanted to bet on the gallops races at Geelong, so we used the markets menu to navigate to Geelong, then multi-selected all the win markets by holding down the control key and clicking on each win market. If you wanted to include all horse or greyhound races for a day you could use the 'quick picks' tab. Once you've chosen the races you're interested in click the 'add' button and you'll see them appear in the main body of the screen. - Choosing your selections \u00b6 The final step is to choose which selections you want to bet on. In this example we just chose the number 1 selection chosen by the Data Scientists in their Racing Prediction Model . Just click on the dropdown in the 'Automation Nomination Selection 1' column for each race and choose your selection. Back and lay bets: The runner selected in 'Automation Nomination Selection 1' will have a BSP back bet for your chosen stake placed. The runner selected in 'Automation Nomination Selection 2' will have a BSP lay bet for your chosen stake placed. Changing the rules This approach only allows you to back one runner and lay one runner per race. If your strategy bets on multiple runners per race you can edit the rules file, select 'copy rule', then edit the second rule to use the runner selected in 'Automation Nomination Selection 2 and delete the lay strategy. The beauty of this tool is that you can customise the rules until you have a set that do what you want them to. And you're set! \u00b6 Once you've set your rules up and got comfortable using Bet Angel Pro it should only take number of seconds to load the markets up and choose your selections for the day. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to us at bdp@betfair.com.au Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Bet Angel - Tipping automation"},{"location":"thirdPartyTools/betAngelTippingAutomation/#bet-angel-tipping-automation","text":"","title":"Bet Angel: Tipping automation"},{"location":"thirdPartyTools/betAngelTippingAutomation/#automating-a-non-ratings-based-tipping-strategy-using-bet-angel-pro","text":"We all love getting some good racing tips, but who has time to sit and place bets all day? Wouldn't it be easier if you could take those tips and get a program to automatically place the bets on your behalf? This is what we're going to explore here - we'll be using Bet Angel Pro to place bets automatically based on a set of tips. This is our first-time using Bet Angel for this approach and we are very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at bdp@betfair.com.au with your feedback and opinions.","title":"Automating a (non-ratings based) tipping strategy using Bet Angel Pro"},{"location":"thirdPartyTools/betAngelTippingAutomation/#lets-go","text":"","title":"Lets Go!"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-the-plan","text":"We have a set of tips that we've taken from our DataScientists' Racing Prediction Model, but this approach should work for any set of tips you may have. Our goal is to create an automated process which will let us choose our tips for the day, then walk away and the program do the leg work. Here we'll step through how we went about getting Bet Angel Pro to place bets on the favourite runner identified by Betfair's Data Scientists . There are no ratings associated with these tips, so we're happy to take Betfair's Starting Price instead of a price for these bets. Bet Angel Pro's 'Guardian' feature has the capacity to let you create rules that can then be applied to specific selections, which is how we're going to implement this strategy. After digging around on Bet Angel's forum and blog , we found an article on how to create a generic automation rule , which we found useful in helping us learn how to use the tool to automate these tips. We also found a rule that someone else had written to back or lay a selection at BSP , which is what we used as the basis for the rule we're using here . If you want to follow along and try this approach yourself you'll need to download Bet Angel Pro and sign up for either a subscription or at least a test period. They have a 14 day free trial that's valuable for establishing whether this tool will do what you want it to for your specific strategy. Resources Tips: Betfair Data Scientists' Racing Prediction Model Rules: rule file , based on Bet Angel Automation - Horse racing - Pre off : Back or Lay at Betfair SP Guardian Automation Bot Tool: Bet Angel Pro","title":"- The plan"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-set-up","text":"First up we need to make sure we've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. We've created an automated rule that you can download here, roughly based on the one discussed in this forum post (note: you will need to create a Bet Angel forum account to be able to download the rule).","title":"- Set up"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-writing-your-rules","text":"open up the 'Automation' tab click 'import a rule' navigate to the rule file you just downloaded (probably in your Downloads folder by default) upload the rule click on the 'rules file name' and select the rule file you just uploaded You can choose to change the bet stake and other parts of the rule by: clicking on 'edit rules file', selecting the rule you want to change (i.e. back at SP) click on the 'parameters' tab change the stake as desired Turning off bet confirmation Unless you want to manually confirm each individual bet placed (which you definitely might want to leave turned on until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?'. You can then save these settings, but you'll need to go into the settings tab and choose the saved file each time you open the program.","title":"- Writing your rules"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-selecting-your-markets","text":"We decided we only wanted to bet on the gallops races at Geelong, so we used the markets menu to navigate to Geelong, then multi-selected all the win markets by holding down the control key and clicking on each win market. If you wanted to include all horse or greyhound races for a day you could use the 'quick picks' tab. Once you've chosen the races you're interested in click the 'add' button and you'll see them appear in the main body of the screen.","title":"- Selecting your markets"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-choosing-your-selections","text":"The final step is to choose which selections you want to bet on. In this example we just chose the number 1 selection chosen by the Data Scientists in their Racing Prediction Model . Just click on the dropdown in the 'Automation Nomination Selection 1' column for each race and choose your selection. Back and lay bets: The runner selected in 'Automation Nomination Selection 1' will have a BSP back bet for your chosen stake placed. The runner selected in 'Automation Nomination Selection 2' will have a BSP lay bet for your chosen stake placed. Changing the rules This approach only allows you to back one runner and lay one runner per race. If your strategy bets on multiple runners per race you can edit the rules file, select 'copy rule', then edit the second rule to use the runner selected in 'Automation Nomination Selection 2 and delete the lay strategy. The beauty of this tool is that you can customise the rules until you have a set that do what you want them to.","title":"- Choosing your selections"},{"location":"thirdPartyTools/betAngelTippingAutomation/#and-youre-set","text":"Once you've set your rules up and got comfortable using Bet Angel Pro it should only take number of seconds to load the markets up and choose your selections for the day. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/betAngelTippingAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to us at bdp@betfair.com.au","title":"What next?"},{"location":"thirdPartyTools/betAngelTippingAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Disclaimer"},{"location":"thirdPartyTools/bfexplorerintro/","text":"Bfexplorer: Intorduction to automation in Bfexplorer \u00b6 Start automating in Bfexplorer \u00b6 Bfexplorer is an application that we've just been made aware of but has some great functionality when it comes to automating strategies that may not be possible using other tools. In this article, we'll go over some of the functionalities that may appeal to those who are looking to automate their strategies. Lets take a look \u00b6 - Bfexplorer Bots \u00b6 Once you have started Bfexplorer and chosen a market that you're interested in, you will be able to select from a range of already created bots that are ready for you to enter specific values that you would like these to operate to. These bots can be organized to run simultaneously or successively for more intricate automations. Out of the box ready to go Bots Place Bet Be the first in queue Fill or kill Close selection bet position Close selection bot position at odds Place bet and close selection bet position Tick offset Scratch trading Trailing stop loss Close market bet position Execute on selections Execute bots Execute on associated market Execute till target profit If then else Chain execution Concurrent execution Repeat until Execute trigger bot Football bot Tennis bot Stop market monitoring Trailing stop loss on market Like most third-party tools, Bfexplorer also includes a practice mode allows you to test your strategies without risking real money. - Bot Executor \u00b6 Bot executor is a tool that allows for any bots that you've set up to be executed at a certain time prior to a market event. - Bfexplorer additional features \u00b6 The program has a range of additional features that assist punters while automating. Users can open a web browser which can be used to look through the many examples from the Bfexplorer blog, access the community forum or browse through the Bfexplorer site, all from within the application itself. Bfexplorer users also have the ability to use an integrated spreadsheet functionality which can be used to record bet data from any automation that you run through the program. For the more tech savvy automators who may not want to fully dive into the Betfair API, Bfexplorer is an open platform that allows users to create their own plugins directly. Using F# and .Net languages, you can add any additional usability to Bfexplorer by simply placing a .fsx file into the Bfexplorer directory on your computer. This is a particularly powerful ability for users to have and is something that many other third-party tools don't include. - Learn More \u00b6 For more information on Bfexplorer, visit their blog which contains a lot of resources and examples of how to achieve different automations using their software. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Bfexplorer: Intorduction to automation in Bfexplorer"},{"location":"thirdPartyTools/bfexplorerintro/#bfexplorer-intorduction-to-automation-in-bfexplorer","text":"","title":"Bfexplorer: Intorduction to automation in Bfexplorer"},{"location":"thirdPartyTools/bfexplorerintro/#start-automating-in-bfexplorer","text":"Bfexplorer is an application that we've just been made aware of but has some great functionality when it comes to automating strategies that may not be possible using other tools. In this article, we'll go over some of the functionalities that may appeal to those who are looking to automate their strategies.","title":"Start automating in Bfexplorer"},{"location":"thirdPartyTools/bfexplorerintro/#lets-take-a-look","text":"","title":"Lets take a look"},{"location":"thirdPartyTools/bfexplorerintro/#-bfexplorer-bots","text":"Once you have started Bfexplorer and chosen a market that you're interested in, you will be able to select from a range of already created bots that are ready for you to enter specific values that you would like these to operate to. These bots can be organized to run simultaneously or successively for more intricate automations. Out of the box ready to go Bots Place Bet Be the first in queue Fill or kill Close selection bet position Close selection bot position at odds Place bet and close selection bet position Tick offset Scratch trading Trailing stop loss Close market bet position Execute on selections Execute bots Execute on associated market Execute till target profit If then else Chain execution Concurrent execution Repeat until Execute trigger bot Football bot Tennis bot Stop market monitoring Trailing stop loss on market Like most third-party tools, Bfexplorer also includes a practice mode allows you to test your strategies without risking real money.","title":"- Bfexplorer Bots"},{"location":"thirdPartyTools/bfexplorerintro/#-bot-executor","text":"Bot executor is a tool that allows for any bots that you've set up to be executed at a certain time prior to a market event.","title":"- Bot Executor"},{"location":"thirdPartyTools/bfexplorerintro/#-bfexplorer-additional-features","text":"The program has a range of additional features that assist punters while automating. Users can open a web browser which can be used to look through the many examples from the Bfexplorer blog, access the community forum or browse through the Bfexplorer site, all from within the application itself. Bfexplorer users also have the ability to use an integrated spreadsheet functionality which can be used to record bet data from any automation that you run through the program. For the more tech savvy automators who may not want to fully dive into the Betfair API, Bfexplorer is an open platform that allows users to create their own plugins directly. Using F# and .Net languages, you can add any additional usability to Bfexplorer by simply placing a .fsx file into the Bfexplorer directory on your computer. This is a particularly powerful ability for users to have and is something that many other third-party tools don't include.","title":"- Bfexplorer additional features"},{"location":"thirdPartyTools/bfexplorerintro/#-learn-more","text":"For more information on Bfexplorer, visit their blog which contains a lot of resources and examples of how to achieve different automations using their software.","title":"- Learn More"},{"location":"thirdPartyTools/bfexplorerintro/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au","title":"What next?"},{"location":"thirdPartyTools/bfexplorerintro/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Disclaimer"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/","text":"Cymatic Trader: Ratings automation \u00b6 Automating a ratings based strategy using Cymatic Trader \u00b6 Ratings are the basis for a lot of betting strategies, but they can be particularly painful and time-consuming to implement manually. This makes them ideal for automation, where you use a program to place bets on your behalf while you get on with other things. Just like Bet Angle and Gruss, Cymatic Trader has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what I've used here to automate these ratings. There are so many different ways to use this part of Cymatic Trader and I'm very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to me on bdp@betfair.com.au with your feedback and opinions. Lets Begin \u00b6 - The plan \u00b6 I'm using the Greyhound Ratings Model put together by some of my Data Scientist colleagues. This model creates ratings for Victorian greyhound races daily and is freely available on the Hub. It's pretty good at predicting winners, so I'm going to place back bets on the dogs with shorter ratings where the market price is better than the model's rating. Cymatic Trader has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what I've used for the automation here. Here I'll step through how I went about getting Cymatic Trader to place bets using the ratings from Betfair's Data Scientists' Greyhound Ratings Model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program running and be able to walk away. Resources Ratings: Betfair Data Scientists' Greyhound Ratings Model Rules: here's the spreadsheet I set up with my macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Cymatic Trader - Set up \u00b6 Make sure you've downloaded and installed Cymatic Trader, and signed in. Once you open the program, you will see an Excel icon which is where we will link our spreadsheet to Cymatic Trader - Finding & formatting ratings \u00b6 Here I'm using the ratings shared by our Data Scientists on the Hub . This makes for a bit of prep work, copying the list of runners and their rating into an Excel spreadsheet. As a minimum you'll need a list of runner names (including the runner number followed by a full stop, i.e. 1. Runner Name) in one column and their rating in another in an Excel sheet. If you have a list of ratings already in a spreadsheet that's even better - you'll be able to tweak the Excel formulas to work with whatever format your data is in. Wherever your ratings come from, you'll need to include them in the spreadsheet you're using to interact with Cymatic Trader. Here I'm using a spreadsheet I edited for this strategy , and I've included a tab called RUNNERS where you can copy in the runner names and ratings. - Writing your rules \u00b6 As with any automated strategy, one of the most important steps is deciding what logical approach you want to take, and writing rules that suit. I'm using a customised version of the default Cymatic Trader template Excel sheet to implement my strategy, so it can make betting decisions based on my ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how I used Excel to implement my set of rules. - Trigger to place bet \u00b6 In short, I want to back runners when: the available to back price is better than the rating for that runner by a variable percentage they have a rating less than 5 the scheduled event start time is less than 2 minutes away the event isn't in play This is my trigger on Excel formula: Multi line =IFERROR( IF( AND( A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)) <Rating, SecondsToOff < BetTime, InPlayCheck=\"FALSE\"), \"BACK\", \"\" ), \"\") Single line =IFERROR(IF(AND(A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)) <Rating,SecondsToOff < BetTime,InPlayCheck=\"FALSE\"),\"BACK\",\"\"),\"\") Stepping through each step: Price > rating * percentage offset: check whether the available to back price is better than the runner's rating multiplied by a percentage - I do this by using the runner name in column B and looking up the corresponding rating for that runner from the RUNNERS sheet. Percentage offset: There are lots of different approaches you can take to this. I'm using a variable percentage offset, as I appreciate that we might want a different percentage better than the rating, depending on the price - i.e. 10% better than 2 ( 2 ( 2.20) is very different than 10% better than a 20 shot ( 20 shot ( 22.20), so here I'm using a vlookup table to determine the percentage better than the rating that I want based on the current odds. Here are the 'ranges' of prices to percentage offset that I'm using - you can disregard this and just change it to be a set percentage (i.e. *1.1 hardcoded into the formula) or just use your rating straight without an offset, or edit the ranges in the SETTINGS tab to suit your opinions. This table takes the 'min' odds for the range in the left column, and the number you want to multiply the odds by in the right column - so for 15% you'd multiply by 1.15 etc. Viewing your values: I've added columns (AF:AH) to show the rating, percentage offset and minimum acceptable odds for each runner, to add some reassurance that the spreadsheet is pulling the values we want it to. ODDS RANGE % MULTIPLIER 1 - 6 1.1 (10%) 6 - 9 1.15 (15%) 9 - 15 1.2 (20%) 15 - 20 1.3 (30%) 20 - 35 1.4 (40%) 35 + 1.5 (50%) Rating < 5: check whether the runner's rating is less than 5 (because I only want to bet on the favourite few runners). Formula Reference: 'RunnerNames' refers to column A in the RUNNERS sheet. 'RunnerRatings' refers to column B in the RUNNERS sheet. 'Rating' refers to cell E6 in the RUNNERS sheet which when changed by the user, will update the formula for all runners in a market. =IFERROR( IF( AND( A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))<Rating, SecondsToOff < BetTime, InPlayCheck=\"FALSE\"), \"BACK\", \"\" ), \"\") Time < 2 mins: check whether the seconds left on the countdown are smaller than 120 (2 minutes), as the majority of markets don't fully form until the last few minutes before the off. This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E4 of the SETTINGS sheet, where I've already done the calculations for you. Cell E4 has been renamed to \"SecondsToOff\" to keep our formulas more simple and easier to read. Formula Reference: 'SecondsToOff' refers to cell E4 in the SETTINGS sheet. 'BetTime' refers to cell E2 in the RUNNERS sheet which when changed by the user, will update the formula for all runners in a market. =IFERROR( IF( AND( A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)) <Rating, SecondsToOff < BetTime, InPlayCheck=\"FALSE\"), \"BACK\", \"\" ), \"\") Not in play: Checking whether the event has gone in play - as odds change so much in the run I only want to use this strategy pre-play. If this cell is populated with a 'FALSE' message it means it's safe to place bets. I appreciate that greyhound races don't go in play, but I wanted this check in place anyway in case I (or you!) wanted to use a version of this strategy on horse racing in the future. Formula Reference: 'InPlayCheck' refers to cell E4 in the Cymatic sheet. =IFERROR( IF( AND( A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)) <Rating, SecondsToOff < BetTime, InPlayCheck=\"FALSE\"), \"BACK\", \"\" ), \"\") Result: if the statement above is true, the formula returns \"BACK\", at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IFERROR( IF( AND( A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)) <Rating, SecondsToOff < BetTime, InPlayCheck=\"FALSE\"), \"BACK\", \"\" ), \"\") TIP: Excel functions IFERROR: If an error is returned, then don't show anything IF statement: IF(if this is true, do this, else do this) AND statement: AND(this is true, and so is this, and so is this) - returns true or false VLOOKUP: looking up a value from a table based on the value you pass in Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste these three formulas into the relevant cell on each row in the 'Command' (BA) column. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column BA (BA8 for the first runner). Multi line =IFERROR( IF( AND( A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))<Rating, SecondsToOff < BetTime, InPlayCheck=\"FALSE\"), \"BACK\", \"\" ), \"\") Single line =IFERROR(IF(AND(A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))<Rating,SecondsToOff < BetTime,InPlayCheck=\"FALSE\"),\"BACK\",\"\"),\"\") Odds: initially I was using the runner's rating as the price, but I got a bet placement error for some of the selections - eventually I realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake I'm now just using the currently available back odds (cell I8 for the first runner). This goes in column BB (BB8 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send me through your formula and I'll add it to this article. =IF(10/(G10-1)<0,\"\",10/(G10-1)) Stake: it's completely up to you what staking approach you want to take. I've kept it simple, and am just using a 'to win' strategy. Each bet aims to win $10 on that runner at the curret odds. The formula divides $10 by the current available best back odds (cell I8 for the first runner) minus one to get the stake required to win $10. This goes in column BC (BC8 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(10/(I8-1)<0,\"\",10/(I8-1)) Note: The IF statement in here is purely to keep our document clean of clutter when there are cells that are not populated with runners. A similar effect to IFERROR, if Cymatic Trader hasn't populated cell I8 with a number higher than 0, then dont populate this cell at all (I8 will only be populated if there is an applicable runner in the market). - Selecting markets \u00b6 I used the Navigator menu in Cymatic Trader to navigate to the tracks I had ratings for. If you wanted to include all horse or greyhound races for a day you could use the 'autopilot' tool to do this more efficiently. Once you've chosen the races you're interested in tick the 'autopilot' button and Gruss will automatically cycle through each market for you. - Linking the spreadsheet \u00b6 Click the Excel icon in the main tool bar and then 'connect Excel' from the drop down menu. From here, you will be able to point Cymatic Trader in the direction of where your Excel sheet is located on your computer. Make sure 'Enable Trigger Commands' is selected and 'Clear status cells when selecting different market\" if you are automating a series of markets. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Cymatic Trader it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Areas for improvement \u00b6 There are parts of this approach that I'm still trying to get to work to my liking, and I'll update this article as I find better solutions. If you have any suggestions for improvements please reach out to bdp@betfair.com.au - I'd love to hear your thoughts. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Cymatic - Trader Ratings automation"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#cymatic-trader-ratings-automation","text":"","title":"Cymatic Trader: Ratings automation"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#automating-a-ratings-based-strategy-using-cymatic-trader","text":"Ratings are the basis for a lot of betting strategies, but they can be particularly painful and time-consuming to implement manually. This makes them ideal for automation, where you use a program to place bets on your behalf while you get on with other things. Just like Bet Angle and Gruss, Cymatic Trader has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what I've used here to automate these ratings. There are so many different ways to use this part of Cymatic Trader and I'm very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to me on bdp@betfair.com.au with your feedback and opinions.","title":"Automating a ratings based strategy using Cymatic Trader"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#lets-begin","text":"","title":"Lets Begin"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-the-plan","text":"I'm using the Greyhound Ratings Model put together by some of my Data Scientist colleagues. This model creates ratings for Victorian greyhound races daily and is freely available on the Hub. It's pretty good at predicting winners, so I'm going to place back bets on the dogs with shorter ratings where the market price is better than the model's rating. Cymatic Trader has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what I've used for the automation here. Here I'll step through how I went about getting Cymatic Trader to place bets using the ratings from Betfair's Data Scientists' Greyhound Ratings Model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program running and be able to walk away. Resources Ratings: Betfair Data Scientists' Greyhound Ratings Model Rules: here's the spreadsheet I set up with my macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Cymatic Trader","title":"- The plan"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-set-up","text":"Make sure you've downloaded and installed Cymatic Trader, and signed in. Once you open the program, you will see an Excel icon which is where we will link our spreadsheet to Cymatic Trader","title":"- Set up"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-finding-formatting-ratings","text":"Here I'm using the ratings shared by our Data Scientists on the Hub . This makes for a bit of prep work, copying the list of runners and their rating into an Excel spreadsheet. As a minimum you'll need a list of runner names (including the runner number followed by a full stop, i.e. 1. Runner Name) in one column and their rating in another in an Excel sheet. If you have a list of ratings already in a spreadsheet that's even better - you'll be able to tweak the Excel formulas to work with whatever format your data is in. Wherever your ratings come from, you'll need to include them in the spreadsheet you're using to interact with Cymatic Trader. Here I'm using a spreadsheet I edited for this strategy , and I've included a tab called RUNNERS where you can copy in the runner names and ratings.","title":"- Finding &amp; formatting ratings"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take, and writing rules that suit. I'm using a customised version of the default Cymatic Trader template Excel sheet to implement my strategy, so it can make betting decisions based on my ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how I used Excel to implement my set of rules.","title":"- Writing your rules"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-trigger-to-place-bet","text":"In short, I want to back runners when: the available to back price is better than the rating for that runner by a variable percentage they have a rating less than 5 the scheduled event start time is less than 2 minutes away the event isn't in play This is my trigger on Excel formula: Multi line =IFERROR( IF( AND( A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)) <Rating, SecondsToOff < BetTime, InPlayCheck=\"FALSE\"), \"BACK\", \"\" ), \"\") Single line =IFERROR(IF(AND(A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)) <Rating,SecondsToOff < BetTime,InPlayCheck=\"FALSE\"),\"BACK\",\"\"),\"\") Stepping through each step: Price > rating * percentage offset: check whether the available to back price is better than the runner's rating multiplied by a percentage - I do this by using the runner name in column B and looking up the corresponding rating for that runner from the RUNNERS sheet. Percentage offset: There are lots of different approaches you can take to this. I'm using a variable percentage offset, as I appreciate that we might want a different percentage better than the rating, depending on the price - i.e. 10% better than 2 ( 2 ( 2.20) is very different than 10% better than a 20 shot ( 20 shot ( 22.20), so here I'm using a vlookup table to determine the percentage better than the rating that I want based on the current odds. Here are the 'ranges' of prices to percentage offset that I'm using - you can disregard this and just change it to be a set percentage (i.e. *1.1 hardcoded into the formula) or just use your rating straight without an offset, or edit the ranges in the SETTINGS tab to suit your opinions. This table takes the 'min' odds for the range in the left column, and the number you want to multiply the odds by in the right column - so for 15% you'd multiply by 1.15 etc. Viewing your values: I've added columns (AF:AH) to show the rating, percentage offset and minimum acceptable odds for each runner, to add some reassurance that the spreadsheet is pulling the values we want it to. ODDS RANGE % MULTIPLIER 1 - 6 1.1 (10%) 6 - 9 1.15 (15%) 9 - 15 1.2 (20%) 15 - 20 1.3 (30%) 20 - 35 1.4 (40%) 35 + 1.5 (50%) Rating < 5: check whether the runner's rating is less than 5 (because I only want to bet on the favourite few runners). Formula Reference: 'RunnerNames' refers to column A in the RUNNERS sheet. 'RunnerRatings' refers to column B in the RUNNERS sheet. 'Rating' refers to cell E6 in the RUNNERS sheet which when changed by the user, will update the formula for all runners in a market. =IFERROR( IF( AND( A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))<Rating, SecondsToOff < BetTime, InPlayCheck=\"FALSE\"), \"BACK\", \"\" ), \"\") Time < 2 mins: check whether the seconds left on the countdown are smaller than 120 (2 minutes), as the majority of markets don't fully form until the last few minutes before the off. This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E4 of the SETTINGS sheet, where I've already done the calculations for you. Cell E4 has been renamed to \"SecondsToOff\" to keep our formulas more simple and easier to read. Formula Reference: 'SecondsToOff' refers to cell E4 in the SETTINGS sheet. 'BetTime' refers to cell E2 in the RUNNERS sheet which when changed by the user, will update the formula for all runners in a market. =IFERROR( IF( AND( A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)) <Rating, SecondsToOff < BetTime, InPlayCheck=\"FALSE\"), \"BACK\", \"\" ), \"\") Not in play: Checking whether the event has gone in play - as odds change so much in the run I only want to use this strategy pre-play. If this cell is populated with a 'FALSE' message it means it's safe to place bets. I appreciate that greyhound races don't go in play, but I wanted this check in place anyway in case I (or you!) wanted to use a version of this strategy on horse racing in the future. Formula Reference: 'InPlayCheck' refers to cell E4 in the Cymatic sheet. =IFERROR( IF( AND( A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)) <Rating, SecondsToOff < BetTime, InPlayCheck=\"FALSE\"), \"BACK\", \"\" ), \"\") Result: if the statement above is true, the formula returns \"BACK\", at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IFERROR( IF( AND( A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)) <Rating, SecondsToOff < BetTime, InPlayCheck=\"FALSE\"), \"BACK\", \"\" ), \"\") TIP: Excel functions IFERROR: If an error is returned, then don't show anything IF statement: IF(if this is true, do this, else do this) AND statement: AND(this is true, and so is this, and so is this) - returns true or false VLOOKUP: looking up a value from a table based on the value you pass in Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Trigger to place bet"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste these three formulas into the relevant cell on each row in the 'Command' (BA) column. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column BA (BA8 for the first runner). Multi line =IFERROR( IF( AND( A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))<Rating, SecondsToOff < BetTime, InPlayCheck=\"FALSE\"), \"BACK\", \"\" ), \"\") Single line =IFERROR(IF(AND(A8 > (INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))*VLOOKUP(INDEX(RunnerRatings,MATCH(A8,RunnerNames,0)),OddsAndMultipliers,2)), INDEX(RunnerRatings,MATCH(A8,RunnerNames,0))<Rating,SecondsToOff < BetTime,InPlayCheck=\"FALSE\"),\"BACK\",\"\"),\"\") Odds: initially I was using the runner's rating as the price, but I got a bet placement error for some of the selections - eventually I realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake I'm now just using the currently available back odds (cell I8 for the first runner). This goes in column BB (BB8 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send me through your formula and I'll add it to this article. =IF(10/(G10-1)<0,\"\",10/(G10-1)) Stake: it's completely up to you what staking approach you want to take. I've kept it simple, and am just using a 'to win' strategy. Each bet aims to win $10 on that runner at the curret odds. The formula divides $10 by the current available best back odds (cell I8 for the first runner) minus one to get the stake required to win $10. This goes in column BC (BC8 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(10/(I8-1)<0,\"\",10/(I8-1)) Note: The IF statement in here is purely to keep our document clean of clutter when there are cells that are not populated with runners. A similar effect to IFERROR, if Cymatic Trader hasn't populated cell I8 with a number higher than 0, then dont populate this cell at all (I8 will only be populated if there is an applicable runner in the market).","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-selecting-markets","text":"I used the Navigator menu in Cymatic Trader to navigate to the tracks I had ratings for. If you wanted to include all horse or greyhound races for a day you could use the 'autopilot' tool to do this more efficiently. Once you've chosen the races you're interested in tick the 'autopilot' button and Gruss will automatically cycle through each market for you.","title":"- Selecting markets"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-linking-the-spreadsheet","text":"Click the Excel icon in the main tool bar and then 'connect Excel' from the drop down menu. From here, you will be able to point Cymatic Trader in the direction of where your Excel sheet is located on your computer. Make sure 'Enable Trigger Commands' is selected and 'Clear status cells when selecting different market\" if you are automating a series of markets.","title":"- Linking the spreadsheet"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Cymatic Trader it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#areas-for-improvement","text":"There are parts of this approach that I'm still trying to get to work to my liking, and I'll update this article as I find better solutions. If you have any suggestions for improvements please reach out to bdp@betfair.com.au - I'd love to hear your thoughts.","title":"Areas for improvement"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Disclaimer"},{"location":"thirdPartyTools/grussRatingsAutomation/","text":"Gruss Betting Assistant: Ratings automation \u00b6 Automating a ratings based strategy using Gruss Betting Assistant \u00b6 Ratings are an ideal base for an automation strategy, as they're an easy metric to write a set of rule around, and are also pretty time consuming to implement manually if you're physically placing the bets yourself. We've previously explored using Bet Angel Pro to implement this style of strategy , now we're going to use Gruss Betting Assistant to look at a different tool for automating betting on ratings. Like Bet Angel Pro, Gruss Betting Assistant also has a spreadsheet functionality that lets you place bets using your own variables and information from the live market. Gruss is a complex tool, and there are lots of different ways to use it, and we'd love any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on bdp@betfair.com.au with your feedback and opinions. Lets do this \u00b6 - The plan \u00b6 We're using the Greyhound Ratings Model put together by some of our Data Scientists. This model creates ratings for Victorian greyhound races daily and is freely available on the Hub. It's pretty good at predicting winners, so we're going to place back bets on the dogs with shorter ratings where the market price is better than the model's rating. Gruss Betting Assistant's Excel triggered betting feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here. Here we'll step through how we went about getting Gruss to place bets using these ratings . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program running and be able to walk away. Obviously, you can use your own ratings and change the rules according to what your strategy is. Resources Ratings: Betfair Data Scientists' Greyhound Ratings Model Rules: here's the spreadsheet I set up with my rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Gruss Betting Assistant - Set up \u00b6 Make sure you've downloaded and installed Gruss Betting Assistant , and signed in. - Finding & formatting ratings \u00b6 Here we're using the Betfair's Data Scientists' Greyhound Ratings Model . This makes for a bit of prep work, copying the list of runners and their rating into an Excel spreadsheet. As a minimum you'll need a list of runner names (including the runner number followed by a full stop, i.e. 1. Runner Name) in one column and their rating in another in an Excel sheet. If you have a list of ratings already in a spreadsheet that's even better - you'll be able to tweak the Excel formulas to work with whatever format your data is in. Wherever your ratings come from, you'll need to include them in the spreadsheet you're using to interact with Gruss. Here we're using a spreadsheet we've edited for this strategy , and we've included a tab called RATINGS where you can copy in the runner names and ratings. - Writing a rule \u00b6 As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Gruss template Excel sheet to implement our strategy, so it can make betting decisions based on my ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. There are lots of posts on the Gruss Forum on the topic if you want to explore it more yourself. This is how we used Excel to implement our set of rules. - Trigger to place bet \u00b6 In short, we want to back runners when: the available to back price is better than the rating for that runner by a variable percentage, depending on the price they have a rating less than what we specify the scheduled event start time is less than what we specify the event isn't in play - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial MyRatings refers to the entire Column B in the Ratings worksheet MyRunners refers to the entire column A in the Ratings worksheet OddsMultiplier refers to the table in the Settings worksheet (C11 to D17) CurrentBMP refers to cell AC4 in the Market worksheet where the overrounds are calculated BMP refers to cell E3 in the RATINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell E3 in the Settings work sheet MyTime refers to cell E4 in the RATINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners NotInPlayCheck refers to cell E2 in the Market worksheet. Gruss will populate a status in this cell such as \"In Play\" MarketStatus refers to cell F2 in the Market worksheet. Gruss will populate a status in this cell such as \"Suspended\" RatingThreshold refers to cell E5 in the RATINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners This is our trigger on Excel formula: Multi line =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Single line =IF(AND(F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)),INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold,TimeTillJump < MyTime,CurrentBMP < BMP,NotInPlayCheck=\"Not In Play\",MarketStatus<>\"Suspended\"),\"BACK\",\"\") Stepping through each step: Price > rating: check whether the available to back price is better than the runner's rating multiplied by a percentage - We do this by using the runner name in column A and looking up the corresponding rating for that runner from the RATINGS sheet. Percentage offset: There are lots of different approaches you can take to this. We're using a variable percentage offset, which is the same approach we took in Bet Angel article implementing this same strategy , as we appreciate that we might want a different percentage better than the rating, depending on the price - i.e. 10% better than 2 ( 2 ( 2.20) is very different than 10% better than a 20 shot ( 20 shot ( 22.20), so here we're using a vlookup table to determine the percentage better than the rating that we want based on the current odds. Here are the 'ranges' of prices to percentage offset that we're using - you can disregard this and just change it to be a set percentage (i.e. *1.1 hardcoded into the formula) or just use your rating straight without an offset, or edit the ranges in the SETTINGS tab to suit your opinions. This table takes the 'min' odds for the range in the left column, and the number you want to multiply the odds by in the right column - so for 15% you'd multiply by 1.15 etc. Viewing your values: We've added columns (Y:AB) to show the rating, percentage offset and minimum acceptable odds for each runner, to add some reassurance that the spreadsheet is pulling the values we want it to. ODDS RANGE % MULTIPLIER 1 - 6 1.1 (10%) 6 - 9 1.15 (15%) 9 - 15 1.2 (20%) 15 - 20 1.3 (30%) 20 - 35 1.4 (40%) 35 + 1.5 (50%) =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Updating references to suit your ratings format If your ratings are formatted differently to our simple runner name | rating layout you can easily edit the formula to refence the relevant column directly, instead of changing your ratings to fit the formula. Let's say your ratings sheet is set out like this: race date | venue | runner name | last race time | weight | rating Here's the set up of the formula: RATINGS!B:B[your rating],MATCH(A5,RATINGS!A:A[runner name],0)) So your edited formula would be: RATINGS!F:F,MATCH(A5,RATINGS!C:C,0)) You need to make sure that you updated these references both in the this part of the formula, and in the next step too. Rating < 5: check whether the runner's rating is less than what we specify in cell E5 in the RATINGS worksheet (because I only want to bet on the favourite few runners) =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Time < what we specify check whether the seconds left on the countdown are smaller than what we specify in cell E4 in the RATINGS worksheet, as the majority of markets don't fully form until the last few minutes before the off. I've taken the countdown in D2 (which is in hours, minutes, seconds) and turned it into a second countdown on the settings sheet, for simplicity's sake. =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") BMP < what we specify checking whether the event overrounds are less than the specific value that can be changed from cell E3 in the RATINGS tab =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Not in play: checking whether the event has gone in play - as odds change so much in the run, we only want to use this strategy pre-play. We're checking two cells here, to make sure to make sure the market's not in play and also not suspended - suspended shouldn't be an issue except it will cause the program to produce errors. We appreciate that greyhound races don't go in play, but we wanted this check in place anyway in case we (or you!) wanted to use a version of this strategy on horse racing in the future. =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Result: if the statement above is true, the formula returns \"BACK\", at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Excel functions IF statement: IF(if this is true, do this, else do this) AND statement: AND(this is true, and so is this, and so is this) - returns true or false VLOOKUP: looking up a value from a table based on the value you pass in Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste these three formulas into the relevant cell on each runner - we did a few extra rows than the number of runners in the markets we were looking at, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column Q (Q5 for the first runner). Multi line =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Single line =IF(AND(F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)),INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold,TimeTillJump < MyTime,CurrentBMP < BMP,NotInPlayCheck=\"Not In Play\",MarketStatus<>\"Suspended\"),\"BACK\",\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell F5 for the first runner). This goes in column R (R5 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this article. =IF(F5=0,\"\",F5) Stake: it's completely up to you what staking approach you want to take. We've kept it simple, and am just using a 'to win' strategy. Each bet aims to win $10 on that runner at the curret odds. The formula divides $10 by the current available best back odds (cell R5 for the first runner) minus one to get the stake required to win $10. We've then used the ROUND function to make sure we get a valid stake size that won't be rejected by the Exchange. This goes in column S (S5 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(ROUND(10/(F5-1),2)=-10,\"\",ROUND(10/(F5-1),2)) - Selecting markets \u00b6 Gruss makes it really easy to select markets in bulk. You could go through an add each market you have in your ratings individually, but it's much easier to just use the quick Pick functionality to add all Australian racing win markets. This is safe, because bets will only fire when they link up with a runner in your RATINGS sheet. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps. - Linking the spreadsheet \u00b6 This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Then click OK and the sheet with be linked with the program. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Areas for improvement \u00b6 There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to bdp@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Gruss - Ratings automation"},{"location":"thirdPartyTools/grussRatingsAutomation/#gruss-betting-assistant-ratings-automation","text":"","title":"Gruss Betting Assistant: Ratings automation"},{"location":"thirdPartyTools/grussRatingsAutomation/#automating-a-ratings-based-strategy-using-gruss-betting-assistant","text":"Ratings are an ideal base for an automation strategy, as they're an easy metric to write a set of rule around, and are also pretty time consuming to implement manually if you're physically placing the bets yourself. We've previously explored using Bet Angel Pro to implement this style of strategy , now we're going to use Gruss Betting Assistant to look at a different tool for automating betting on ratings. Like Bet Angel Pro, Gruss Betting Assistant also has a spreadsheet functionality that lets you place bets using your own variables and information from the live market. Gruss is a complex tool, and there are lots of different ways to use it, and we'd love any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on bdp@betfair.com.au with your feedback and opinions.","title":"Automating a ratings based strategy using Gruss Betting Assistant"},{"location":"thirdPartyTools/grussRatingsAutomation/#lets-do-this","text":"","title":"Lets do this"},{"location":"thirdPartyTools/grussRatingsAutomation/#-the-plan","text":"We're using the Greyhound Ratings Model put together by some of our Data Scientists. This model creates ratings for Victorian greyhound races daily and is freely available on the Hub. It's pretty good at predicting winners, so we're going to place back bets on the dogs with shorter ratings where the market price is better than the model's rating. Gruss Betting Assistant's Excel triggered betting feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here. Here we'll step through how we went about getting Gruss to place bets using these ratings . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program running and be able to walk away. Obviously, you can use your own ratings and change the rules according to what your strategy is. Resources Ratings: Betfair Data Scientists' Greyhound Ratings Model Rules: here's the spreadsheet I set up with my rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Gruss Betting Assistant","title":"- The plan"},{"location":"thirdPartyTools/grussRatingsAutomation/#-set-up","text":"Make sure you've downloaded and installed Gruss Betting Assistant , and signed in.","title":"- Set up"},{"location":"thirdPartyTools/grussRatingsAutomation/#-finding-formatting-ratings","text":"Here we're using the Betfair's Data Scientists' Greyhound Ratings Model . This makes for a bit of prep work, copying the list of runners and their rating into an Excel spreadsheet. As a minimum you'll need a list of runner names (including the runner number followed by a full stop, i.e. 1. Runner Name) in one column and their rating in another in an Excel sheet. If you have a list of ratings already in a spreadsheet that's even better - you'll be able to tweak the Excel formulas to work with whatever format your data is in. Wherever your ratings come from, you'll need to include them in the spreadsheet you're using to interact with Gruss. Here we're using a spreadsheet we've edited for this strategy , and we've included a tab called RATINGS where you can copy in the runner names and ratings.","title":"- Finding &amp; formatting ratings"},{"location":"thirdPartyTools/grussRatingsAutomation/#-writing-a-rule","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Gruss template Excel sheet to implement our strategy, so it can make betting decisions based on my ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. There are lots of posts on the Gruss Forum on the topic if you want to explore it more yourself. This is how we used Excel to implement our set of rules.","title":"- Writing a rule"},{"location":"thirdPartyTools/grussRatingsAutomation/#-trigger-to-place-bet","text":"In short, we want to back runners when: the available to back price is better than the rating for that runner by a variable percentage, depending on the price they have a rating less than what we specify the scheduled event start time is less than what we specify the event isn't in play","title":"- Trigger to place bet"},{"location":"thirdPartyTools/grussRatingsAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial MyRatings refers to the entire Column B in the Ratings worksheet MyRunners refers to the entire column A in the Ratings worksheet OddsMultiplier refers to the table in the Settings worksheet (C11 to D17) CurrentBMP refers to cell AC4 in the Market worksheet where the overrounds are calculated BMP refers to cell E3 in the RATINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell E3 in the Settings work sheet MyTime refers to cell E4 in the RATINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners NotInPlayCheck refers to cell E2 in the Market worksheet. Gruss will populate a status in this cell such as \"In Play\" MarketStatus refers to cell F2 in the Market worksheet. Gruss will populate a status in this cell such as \"Suspended\" RatingThreshold refers to cell E5 in the RATINGS worksheet which allows you to change a single value that will automatically update the formulas for all runners This is our trigger on Excel formula: Multi line =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Single line =IF(AND(F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)),INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold,TimeTillJump < MyTime,CurrentBMP < BMP,NotInPlayCheck=\"Not In Play\",MarketStatus<>\"Suspended\"),\"BACK\",\"\") Stepping through each step: Price > rating: check whether the available to back price is better than the runner's rating multiplied by a percentage - We do this by using the runner name in column A and looking up the corresponding rating for that runner from the RATINGS sheet. Percentage offset: There are lots of different approaches you can take to this. We're using a variable percentage offset, which is the same approach we took in Bet Angel article implementing this same strategy , as we appreciate that we might want a different percentage better than the rating, depending on the price - i.e. 10% better than 2 ( 2 ( 2.20) is very different than 10% better than a 20 shot ( 20 shot ( 22.20), so here we're using a vlookup table to determine the percentage better than the rating that we want based on the current odds. Here are the 'ranges' of prices to percentage offset that we're using - you can disregard this and just change it to be a set percentage (i.e. *1.1 hardcoded into the formula) or just use your rating straight without an offset, or edit the ranges in the SETTINGS tab to suit your opinions. This table takes the 'min' odds for the range in the left column, and the number you want to multiply the odds by in the right column - so for 15% you'd multiply by 1.15 etc. Viewing your values: We've added columns (Y:AB) to show the rating, percentage offset and minimum acceptable odds for each runner, to add some reassurance that the spreadsheet is pulling the values we want it to. ODDS RANGE % MULTIPLIER 1 - 6 1.1 (10%) 6 - 9 1.15 (15%) 9 - 15 1.2 (20%) 15 - 20 1.3 (30%) 20 - 35 1.4 (40%) 35 + 1.5 (50%) =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Updating references to suit your ratings format If your ratings are formatted differently to our simple runner name | rating layout you can easily edit the formula to refence the relevant column directly, instead of changing your ratings to fit the formula. Let's say your ratings sheet is set out like this: race date | venue | runner name | last race time | weight | rating Here's the set up of the formula: RATINGS!B:B[your rating],MATCH(A5,RATINGS!A:A[runner name],0)) So your edited formula would be: RATINGS!F:F,MATCH(A5,RATINGS!C:C,0)) You need to make sure that you updated these references both in the this part of the formula, and in the next step too. Rating < 5: check whether the runner's rating is less than what we specify in cell E5 in the RATINGS worksheet (because I only want to bet on the favourite few runners) =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Time < what we specify check whether the seconds left on the countdown are smaller than what we specify in cell E4 in the RATINGS worksheet, as the majority of markets don't fully form until the last few minutes before the off. I've taken the countdown in D2 (which is in hours, minutes, seconds) and turned it into a second countdown on the settings sheet, for simplicity's sake. =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") BMP < what we specify checking whether the event overrounds are less than the specific value that can be changed from cell E3 in the RATINGS tab =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Not in play: checking whether the event has gone in play - as odds change so much in the run, we only want to use this strategy pre-play. We're checking two cells here, to make sure to make sure the market's not in play and also not suspended - suspended shouldn't be an issue except it will cause the program to produce errors. We appreciate that greyhound races don't go in play, but we wanted this check in place anyway in case we (or you!) wanted to use a version of this strategy on horse racing in the future. =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Result: if the statement above is true, the formula returns \"BACK\", at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Excel functions IF statement: IF(if this is true, do this, else do this) AND statement: AND(this is true, and so is this, and so is this) - returns true or false VLOOKUP: looking up a value from a table based on the value you pass in Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/grussRatingsAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste these three formulas into the relevant cell on each runner - we did a few extra rows than the number of runners in the markets we were looking at, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column Q (Q5 for the first runner). Multi line =IF( AND( F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)), INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold, TimeTillJump < MyTime, CurrentBMP < BMP, NotInPlayCheck=\"Not In Play\", MarketStatus<>\"Suspended\"), \"BACK\", \"\") Single line =IF(AND(F5 > (INDEX(MyRatings,MATCH(A5,MyRunners,0))*VLOOKUP(INDEX(MyRatings,MATCH(A5,MyRunners,0)),OddsMultiplier,2)),INDEX(MyRatings,MATCH(A5,MyRunners,0)) < RatingThreshold,TimeTillJump < MyTime,CurrentBMP < BMP,NotInPlayCheck=\"Not In Play\",MarketStatus<>\"Suspended\"),\"BACK\",\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell F5 for the first runner). This goes in column R (R5 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this article. =IF(F5=0,\"\",F5) Stake: it's completely up to you what staking approach you want to take. We've kept it simple, and am just using a 'to win' strategy. Each bet aims to win $10 on that runner at the curret odds. The formula divides $10 by the current available best back odds (cell R5 for the first runner) minus one to get the stake required to win $10. We've then used the ROUND function to make sure we get a valid stake size that won't be rejected by the Exchange. This goes in column S (S5 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(ROUND(10/(F5-1),2)=-10,\"\",ROUND(10/(F5-1),2))","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/grussRatingsAutomation/#-selecting-markets","text":"Gruss makes it really easy to select markets in bulk. You could go through an add each market you have in your ratings individually, but it's much easier to just use the quick Pick functionality to add all Australian racing win markets. This is safe, because bets will only fire when they link up with a runner in your RATINGS sheet. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps.","title":"- Selecting markets"},{"location":"thirdPartyTools/grussRatingsAutomation/#-linking-the-spreadsheet","text":"This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Then click OK and the sheet with be linked with the program.","title":"- Linking the spreadsheet"},{"location":"thirdPartyTools/grussRatingsAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/grussRatingsAutomation/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to bdp@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this.","title":"Areas for improvement"},{"location":"thirdPartyTools/grussRatingsAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au","title":"What next?"},{"location":"thirdPartyTools/grussRatingsAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Disclaimer"},{"location":"thirdPartyTools/grusslSimultaneousMarkets/","text":"Gruss: Automating simultaneous markets \u00b6 Don't miss out on a market with simultaneous automation \u00b6 If you have a concern of missing markets due to delays or unforeseen circumstances at a market, Gruss is able to work off multiple worksheets for different meetings, all from the same workbook. For example, we can have worksheet one from our spreadsheet work through the markets taking place in Flemington, while worksheet 2 will work through markets at Sandown, worksheet 3 works through markets at Bendigo etc. To Set this up, we need to do need to duplicate the main \u2018Market\u2019 worksheet and have enough instances of the sheet for each meeting. There are four meetings that I want to cover, so I have duplicated the worksheet four times and renamed them to help me keep track. After these changes have been made, restart Gruss so that the changes go into effect. Once Gruss is back open, as before, click \u2018Market\u2019, \u2018Add to Quick Pick List\u2019, \u2018Horse Racing\u2019 (Or any event you are automating for), The country that you would like to bet and then \u2018All Win\u2019. This should generate a bunch of tabs in Gruss, each tab containing all the markets for a specific meeting. Simply close the ones you don\u2019t want. Once you\u2019re ready, click the \u2018Excel\u2019 menu in Gruss, and \u2018Log Multiple sheets Quick link\u2019. Select your Excel Workbook, assign the worksheets, select \u2018Enable Triggered Betting\u2019, \u201cCLEAR trigger clears matched odds\u2019, \u2018Clear bet refs on auto select markets\u2019, \u2018Clear bet refs on manual select market\u2019 and finally \u2018Auto Select First Market\u2019. Your screen should look something like this: When you\u2019re ready for automation to take over, click \u2018Start logging\u2019 at the bottom of the window. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Gruss it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Gruss - simultaneous markets"},{"location":"thirdPartyTools/grusslSimultaneousMarkets/#gruss-automating-simultaneous-markets","text":"","title":"Gruss: Automating simultaneous markets"},{"location":"thirdPartyTools/grusslSimultaneousMarkets/#dont-miss-out-on-a-market-with-simultaneous-automation","text":"If you have a concern of missing markets due to delays or unforeseen circumstances at a market, Gruss is able to work off multiple worksheets for different meetings, all from the same workbook. For example, we can have worksheet one from our spreadsheet work through the markets taking place in Flemington, while worksheet 2 will work through markets at Sandown, worksheet 3 works through markets at Bendigo etc. To Set this up, we need to do need to duplicate the main \u2018Market\u2019 worksheet and have enough instances of the sheet for each meeting. There are four meetings that I want to cover, so I have duplicated the worksheet four times and renamed them to help me keep track. After these changes have been made, restart Gruss so that the changes go into effect. Once Gruss is back open, as before, click \u2018Market\u2019, \u2018Add to Quick Pick List\u2019, \u2018Horse Racing\u2019 (Or any event you are automating for), The country that you would like to bet and then \u2018All Win\u2019. This should generate a bunch of tabs in Gruss, each tab containing all the markets for a specific meeting. Simply close the ones you don\u2019t want. Once you\u2019re ready, click the \u2018Excel\u2019 menu in Gruss, and \u2018Log Multiple sheets Quick link\u2019. Select your Excel Workbook, assign the worksheets, select \u2018Enable Triggered Betting\u2019, \u201cCLEAR trigger clears matched odds\u2019, \u2018Clear bet refs on auto select markets\u2019, \u2018Clear bet refs on manual select market\u2019 and finally \u2018Auto Select First Market\u2019. Your screen should look something like this: When you\u2019re ready for automation to take over, click \u2018Start logging\u2019 at the bottom of the window.","title":"Don't miss out on a market with simultaneous automation"},{"location":"thirdPartyTools/grusslSimultaneousMarkets/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Gruss it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/grusslSimultaneousMarkets/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to bdp@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/grusslSimultaneousMarkets/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable. If you're implementing your own strategies, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Disclaimer"},{"location":"thirdPartyTools/overview/","text":"Third party tools overview \u00b6 There are many third party applications that can be used in conjunction with Betfair which can help improve your interactions with the Exchange from more detailed live information from selected markets to automating your strategies. If you haven\u2019t before used a third-party betting tool, below is a small overview of each program If you have any suggestions for new tutorials / improvements please reach out to bdp@betfair.com.au - We'd love to hear your thoughts and feedback. Links to third party Tools \u00b6 Bet Angel Pro // Gruss // Market Feeder // Bfexplorer // Cymatic Trader // BF Bot Manager Disclaimer \u00b6 Note that whilst we create tutorials for third party applications for educational use, Betfair do not own or have any control over how third party applications are created, developed and maintained. If you use any of the above third party applications, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Tools Overview"},{"location":"thirdPartyTools/overview/#third-party-tools-overview","text":"There are many third party applications that can be used in conjunction with Betfair which can help improve your interactions with the Exchange from more detailed live information from selected markets to automating your strategies. If you haven\u2019t before used a third-party betting tool, below is a small overview of each program If you have any suggestions for new tutorials / improvements please reach out to bdp@betfair.com.au - We'd love to hear your thoughts and feedback.","title":"Third party tools overview"},{"location":"thirdPartyTools/overview/#links-to-third-party-tools","text":"Bet Angel Pro // Gruss // Market Feeder // Bfexplorer // Cymatic Trader // BF Bot Manager","title":"Links to third party Tools"},{"location":"thirdPartyTools/overview/#disclaimer","text":"Note that whilst we create tutorials for third party applications for educational use, Betfair do not own or have any control over how third party applications are created, developed and maintained. If you use any of the above third party applications, please gamble responsibly and note that you are responsible for any winnings/losses incurred.","title":"Disclaimer"},{"location":"thirdPartyTools/BfBotManager/BfBotManager/","text":"Bf Bot Manager is software designed to help you automate your betting and trading at Betfair betting exchange. Bf Bot Manager allows you to run unlimited number of strategies/bots at the same time. You can have one strategy betting on favourites, second one trading on horse races, third one betting on tennis matches and several strategies betting on football events or tipster tips. You can set software to automatically download or import your or tipster betting tips and bet on them just few seconds before event start time. Your settings can be exported to file allowing you to easily share strategy with your friends or to create backup. Software also supports manual bet placement and has ladder and grid controls for one click betting. Hedge (green/red up) functionality can be set to execute automatically or on single click. Bf Bot Manager supports simulation mode, many staking plans, loss recovery and much more. It even has football statistics for major leagues allowing you to automatically bet only on teams with specific previous results. You can also order custom functionality that we can develop for you and adjust settings to suit your own requirements. BF BOT MANAGER KEY FEATURES Custom Bots Dutching Automation Greening Tools Simulation Mode One-click Betting To find out more and to sign up for a FREE 5 Day trial, head to the BF Bot Manager Website","title":"BF Bot Manager"},{"location":"thirdPartyTools/CymaticTrader/CymaticTrader/","text":"Taking trading to another level, this fully featured, super fast trading software has many unique features developed over a number of years. Intuitive throughout, it was designed for professional traders but is equally welcomed by novices. It was the first program to actually reveal your estimated position in the exchange order book queue (PIQ), helping you gain an advantage when trading manually or with the built in robot. PIQ is now an essential feature for many traders. You can open multiple markets simultaneously and perform one-click trading using customizable ladders or grids. Greening, tick-offset, stop-loss and a range of other features are included, as you\u2019d expect in a great low-latency trading app. The Excel integration capability enables you to view real-time prices in Excel, trigger orders from Excel and even create your own fully automated trading robot! The integrated advanced charting is capable of displaying candle-stick, bar or line charts, in various time frames, plus a huge range of technical analysis indicators. Zooming, panning and drawing magnetic trend lines are all made easy. Another unique feature is the \u2018API Monitor\u2019, it can instantly reveal errors or bottlenecks caused by the internet or the exchange \u2013 thus helping you avoid trading during bad periods that other traders may often be oblivious to. It also calculates real time statistics such as the latency of various types of calls to the Betfair API for your current trading session. Warning messages and statistical reports can even be automatically emailed to you by the software. The software is well suited to single or multiple monitor scenarios. Downloading/filtering of betting history and account statements is also included. GRUSS KEY FEATURES Rapid keyboard betting short cuts Advanced charting Football Odds predictor Ladder interface Excel integration Practice mode To find out more and to sign up for a FREE 14 Day trial, head to the Cymatic Trader Website .","title":"Cymatic Trader"},{"location":"thirdPartyTools/Gruss/Gruss/","text":"Gruss Software offers one-click betting in the standard or ladder type interface and also the ability to link into Excel using pre-defined triggers to place bets. Betting Assistant deploys a wealth of functionality such as green-up, fill or kill, stop loss, dutch betting, coupon market view and more. Gruss Software is a constantly evolving solution where the developers listen to the users and their needs. Users can communicate directly with the developers, make suggestions and exchange ideas on the Betting Assistant Forum. GRUSS KEY FEATURES One click betting Dutching tool Ladder interface One click betting Advanced market navigation Excel integration To find out more and to sign up for a FREE 30 Day trial, head to the Gruss Software Website .","title":"Gruss Betting Assistant"},{"location":"thirdPartyTools/MarketFeeder/MarketFeeder/","text":"Auto-Green-Up, Auto-Dutch and implement triggered betting with MarketFeeder Pro. As well as the standard manual betting tools you know and love, the auto-trading functions allow for stress-free betting with the triggered betting feature. A trigger is an effective way to maximise your time and safe-guard you from any errors. A short set of instructions can be set to perform one of 50 actions, including backing, laying, cancelling bets, slowing or speeding up market refresh, greening up and spreading the loss, alerting, emailing and indeed being your second hand on Betfair. MARKETFEEDER PRO KEY FEATURES Monitoring Simultanious Markets 0.3 Second Refresh Rate Automated Market Search tool Triggered Betting Excel Spreadsheet Interaction Automatic Greenup Automatic Dutching Ladder Interface for Scalping To find out more and to sign up for a FREE 1 month trial, head to the MarketFeeder Pro Website","title":"MarketFeeder Pro"},{"location":"thirdPartyTools/betAngel/betAngel/","text":"Bet Angel is one of the longest established API Products available. Bet Angel has three separate products available for Betfair customers. Bet Angel Basic is free and useful for placing occasional bets but has limited trading and professional tools. Bet Angel Trader contains the essential trading tools and Bet Angel Professional is the ultimate trading tool kit providing all the advanced tools and features required. Bet Angel Betting Applications offer a complete suite of tools all of which can be used with live or via a fully featured, risk free, practice mode. In a fully customisable interface, you can Back & Lay across multiple screens on a range of sports. The Guardian Advanced Automation feature allows you to determine your automation rules with ease. Enabling complex automated betting based on timing, price triggers, comparative odds conditions and more. Automatically Cash-Out when your desired profit level is achieved and assure yourself a level profit (greening) no matter the result. For those who wish to write their own automated betting or trading strategies, Bet Angel can link to Microsoft Excel offering automation opportunities that are limitless. BET ANGEL KEY FEATURES Practice Mode One Click Betting Ladder Interface Advanced Charting Dutching and Bookmaking tools Trigger based automation Excel Spreadhseet integration Cash-Out To find out more and to sign up for a FREE 14 Day trial, head to the Bet Angel Website","title":"Bet Angel"},{"location":"thirdPartyTools/bfexplorer/bfexplorer/","text":"Bfexplorer is a powerful program that gives unlimited opportunities to users to create a program that will be best for their needs. Being an open program, you can fully customize or add new functionality to the program that may be beneficial to you. With over 30 bots pre-packaged and ready to go, you can start automating using bfexoplorer straight away, or you can dive in the deep end and add functionality yourself to achieve your strategy. Bfexplorer can be as simple or sophisticated as you like. BFEXPLORER KEY FEATURES BfExplorer SDK is available Ladder Interface Grid Interface In built spreadsheet In built web browser Over 30 bots ready to go To find out more and to sign up for a FREE 7 Day trial, head to the Bfexplorer Website","title":"Bfexplorer"}]}