{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take away the pain! Processing TAR Files 101"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "So even though we've got some great tutorials up here on our Automation Hub, a common wish from many an automated customer, is for a simple plug-and-play tutorial and script for parsing the PRO TAR files into CSVs. So here it is, an easy-to-follow tutorial that makes use of existing tutorials by Tom Bishop and others to easily and painlessly process these JSON files into CSVs ready to be modelled.\n",
    "\n",
    "And don't worry, this tutorial has plenty of error-handling built in so (hopefully) you won't wake up in the morning to find the script failed at 1AM while you were dreaming of the final straight at Moonee Valley. \n",
    "\n",
    "This tutorial will concern itself primarily with racing, however, at the bottom of the page we have also included complete code suited for parsing sports TAR files including handling for line/handicap markets.\n",
    "\n",
    "So without further ado, let's jump into it!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheat sheet\n",
    "- This is presented as a Jupyter notebook as this format is interactive and lets you run snippets of code from wihtin the notebook. To use this functionality [you'll need to download a copy of the `ipynb` file locally](https://github.com/betfair-down-under/autoHubTutorials/blob/master/processingTarFiles101/tar.ipynb) and open it in a text editor (i.e. VS code).\n",
    "- If you're looking for the complete code [head to the bottom of the page](https://betfair-datascientists.github.io/historicData/processingTarFiles101/#complete-code) or [download the script from Github](https://github.com/betfair-down-under/autoHubTutorials/tree/master/processingTarFiles101).\n",
    "- To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using [VS code](https://code.visualstudio.com/download)), make sure you've navigated in the terminal to the folder you've saved the script in and then type `py main.py` (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. \n",
    "- Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the [Betfair historic data site](https://historicdata.betfair.com/#/help). We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. For more information on these files please reach out to us at [automation](mailto:automation@betfair.com.au)\n",
    "- We're using the betfairlightweight and betfair_data package to do the heavy lifting\n",
    "- We've also posted the completed code logic on the [`betfair-downunder` Github repo](https://github.com/betfair-down-under/autoHubTutorials/tree/master/processingTarFiles101)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.0 Setup\n",
    "### 1.1 Importing libraries\n",
    "\n",
    "Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language.\n",
    "\n",
    "You'll need `betfairlightweight` and `betfair_data` which you can install with something like `pip install betfairlightweight`.\n",
    "\n",
    "NOTE: Import 'betfair_data.bflw' could not be resolved from source - This is a known issue, the script should still run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import csv\n",
    "import tarfile\n",
    "import zipfile\n",
    "import bz2\n",
    "import glob\n",
    "import ast\n",
    "from unittest.mock import patch\n",
    "import betfairlightweight\n",
    "from betfairlightweight import StreamListener\n",
    "from betfair_data import bflw \n",
    "import pandas as pd\n",
    "from betfair_data import PriceSize\n",
    "import functools\n",
    "from typing import List, Tuple\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "from itertools import zip_longest\n",
    "from currency_converter import CurrencyConverter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Picking our intervals\n",
    "Here we'll define our working folder and the timestamps for which we want to pull all of over data. \n",
    "We've set the script to pull prices from 10 minutes out from the jump at 30 second intervals and then at 5 second intervals from 1 minute out. Feel free to change as you need.\n",
    "\n",
    "PRO TIP: copying your file path from windows explorer works really well as long as you replace the back-slashes with forward-slashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory= '' #INSERT FILE DIRECTORY WHERE TAR FILES ARE STORED\n",
    "\n",
    "log1_Start = 60 * 10 # Seconds before scheduled off to start recording data for data segment one\n",
    "log1_Step = 30       # Seconds between log steps for first data segment\n",
    "log2_Start = 60 * 1  # Seconds before scheduled off to start recording data for data segment two\n",
    "log2_Step = 5    # Seconds between log steps for second data segment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Filtering the markets\n",
    "We'll then define a market filter as well as a way to filter out the harness racing markets. The different filter variables are listed below and can be added or removed as required. Some of these aren't that useful but have been listed for the sake of completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting race name and returning the parts \n",
    "def split_anz_horse_market_name(market_name: str) -> Tuple[str, int, str]:\n",
    "    # return race no, length, race type\n",
    "    # input samples: \n",
    "    # 'R6 1400m Grp1' -> ('R6','1400m','grp1')\n",
    "    # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot')\n",
    "    # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace')\n",
    "    parts = market_name.split(' ')\n",
    "    race_no = parts[0] \n",
    "    race_len = parts[1].split('m')\n",
    "    race_len = race_len[0]\n",
    "    race_type = parts[2].lower() \n",
    "    return (race_no, race_len, race_type)\n",
    "\n",
    "# filtering markets to those that fit the following criteria\n",
    "def filter_market(market: bflw.MarketBook) -> bool: \n",
    "    d = market.market_definition\n",
    "    return (d != None\n",
    "        and d.country_code == 'AU' \n",
    "        and d.market_type == 'WIN'\n",
    "        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace' #strips out Harness Races\n",
    "        #and (c := split_anz_horse_market_name(d.name)[2]) == 'hcap'#can use this to filter by race type\n",
    "        #and (c := split_anz_horse_market_name(d.name)[1]) >= '1200' #can use this to filter by race length\n",
    "        )\n",
    "\n",
    "# Simply add the below variable name to the market filter function above with the filter value\n",
    "# Equals (== 'Value in Quotation' or True/False/None), Does Not Equal (!= 'Value in Quotation' or True/False/None) - FOR ALL TYPES\n",
    "# Greater than (>), Greater than or equal to (>=), Less than (<), Less than or equal to (<=) - FOR INT/FLOAT\n",
    "# For list of value 'in'\n",
    "\n",
    "# and d.betting_type: str - ODDS, ASIAN_HANDICAP_SINGLES, ASIAN_HANDICAP_DOUBLES or LINE\n",
    "# and d.bsp_market: bool - True, False\n",
    "# and d.country_code: str - list of codes can be found here: https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2 - Default value is 'GB' - Australia = 'AU', New Zealand = 'NZ'\n",
    "# and d.event_id: str - PARENT_EVENT_ID\n",
    "# and d.event_name: Optional[str] - Usually the name of the Match-Up (e.g. Bangladesh v Sri Lanka) or Race Meeting Name (e.g. Wangaratta (AUS) 1st Dec) - Note: Dictionaries don't support wildcard searches\n",
    "# and d.event_type_id: str - SportID [Horse Racing - 7, Greyhounds - 4339]\n",
    "# and d.market_base_rate: float - Market Commission Rate\n",
    "# and d.market_type: str - e.g. \"WIN\"\n",
    "# and d.name: Optional[str] - market name (e.g. R1 1170m Mdn)\n",
    "# and d.number_of_active_runners: int - number of horses/dogs in the race\n",
    "# and d.number_of_winners: int - Win market 1, Place markets 2+\n",
    "# and d.turn_in_play_enabled: bool - True, False\n",
    "# and d.venue: Optional[str] - Racing Only - Track"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Utility Functions\n",
    "Here we will define our trading and listener functions which will be able to handle the json file format as well as defining some more working directories. We've also defined a few functions to generate the required output.\n",
    "\n",
    "NOTE: Input credentials are not required here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading = betfairlightweight.APIClient(username = \"username\", password = \"password\", app_key=\"app_key\")\n",
    "listener = StreamListener(max_latency=None)\n",
    "\n",
    "stream_files = glob.glob(file_directory+\"*.tar\") \n",
    "selection_meta = file_directory+\"metadata.csv\"\n",
    "prices_path =  file_directory+\"preplay.csv\"\n",
    "\n",
    "# rounding to 2 decimal places or returning '' if blank\n",
    "def as_str(v) -> str:\n",
    "    return '%.2f' % v if (type(v) is float) or (type(v) is int) else v if type(v) is str else ''\n",
    "\n",
    "# returning smaller of two numbers where min not 0\n",
    "def min_gr0(a: float, b: float) -> float:\n",
    "    if a <= 0:\n",
    "        return b\n",
    "    if b <= 0:\n",
    "        return a\n",
    "\n",
    "    return min(a, b)\n",
    "\n",
    "# parsing price data and pulling out weighted avg price, matched, min price and max price\n",
    "def parse_traded(traded: List[PriceSize]) -> Tuple[float, float, float, float]:\n",
    "    if len(traded) == 0: \n",
    "        return (None, None, None, None)\n",
    "\n",
    "    (wavg_sum, matched, min_price, max_price) = functools.reduce(\n",
    "        lambda total, ps: (\n",
    "            total[0] + (ps.price * ps.size), # wavg_sum before we divide by total matched\n",
    "            total[1] + ps.size, # total matched\n",
    "            min(total[2], ps.price), # min price matched\n",
    "            max(total[3], ps.price), # max price matched\n",
    "        ),\n",
    "        traded,\n",
    "        (0, 0, 1001, 0) # starting default values\n",
    "    )\n",
    "\n",
    "    wavg_sum = (wavg_sum / matched) if matched > 0 else None # dividing sum of wavg by total matched\n",
    "    matched = matched if matched > 0 else None \n",
    "    min_price = min_price if min_price != 1001 else None\n",
    "    max_price = max_price if max_price != 0 else None\n",
    "\n",
    "    return (wavg_sum, matched, min_price, max_price)\n",
    "\n",
    "\n",
    "def load_markets(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        print(file_path)\n",
    "        print(\"__ Parsing Detailed Prices ___ \")\n",
    "        if os.path.isdir(file_path):\n",
    "            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n",
    "                f = bz2.BZ2File(path, 'rb')\n",
    "                yield f\n",
    "                f.close()\n",
    "        elif os.path.isfile(file_path):\n",
    "            ext = os.path.splitext(file_path)[1]\n",
    "            # iterate through a tar archive\n",
    "            if ext == '.tar':\n",
    "                with tarfile.TarFile(file_path) as archive:\n",
    "                    for file in archive:\n",
    "                        yield bz2.open(archive.extractfile(file))\n",
    "            # or a zip archive\n",
    "            elif ext == '.zip':\n",
    "                with zipfile.ZipFile(file_path) as archive:\n",
    "                    for file in archive.namelist():\n",
    "                        yield bz2.open(archive.open(file))\n",
    "\n",
    "    return None\n",
    "\n",
    "def slicePrice(l, n):\n",
    "    try:\n",
    "        x = l[n].price\n",
    "    except:\n",
    "        x = \"\"\n",
    "    return(x)\n",
    "\n",
    "def sliceSize(l, n):\n",
    "    try:\n",
    "        x = l[n].size\n",
    "    except:\n",
    "        x = \"\"\n",
    "    return(x)\n",
    "\n",
    "def pull_ladder(availableLadder, n = 5):\n",
    "        out = {}\n",
    "        price = []\n",
    "        volume = []\n",
    "        if len(availableLadder) == 0:\n",
    "            return(out)        \n",
    "        else:\n",
    "            for rung in availableLadder[0:n]:\n",
    "                price.append(rung.price)\n",
    "                volume.append(rung.size)\n",
    "\n",
    "            out[\"p\"] = price\n",
    "            out[\"v\"] = volume\n",
    "            return(out)\n",
    "\n",
    "def final_market_book(s):\n",
    "\n",
    "    with patch(\"builtins.open\", lambda f, _: f):\n",
    "\n",
    "        gen = s.get_generator()\n",
    "\n",
    "        for market_books in gen():\n",
    "            \n",
    "            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n",
    "\n",
    "            if ((evaluate_market := filter_market(market_books[0])) == False):\n",
    "                    return(None)\n",
    "            \n",
    "            for market_book in market_books:\n",
    "\n",
    "                last_market_book = market_book\n",
    "        \n",
    "        return(last_market_book)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Parsing Timestamped Prices\n",
    "Next up are the functions that enable us to read the timestamped prices from the json files and output them ready to be joined on the metadata. \n",
    "This will provide:\n",
    "- Market ID\n",
    "- Number of Active Runners in the market (this can help in the event of late scratchings)\n",
    "- Timestamp (will help us later to calculate time relative to scheduled off time)\n",
    "- Traded Volume on the selection so far\n",
    "- Weighted Average Price\n",
    "\n",
    "- Last Traded Price\n",
    "- Selection Status (this can help in the event of late scratchings)\n",
    "- Reduction Factor (affect on the market if the runner were to be scratched)\n",
    "- Back Ladder (pulls 5 best prices and volume available)\n",
    "- Lay Ladder (pulls 5 best prices and volume available)\n",
    "- BSP Near Price\n",
    "- BSP Far Price\n",
    "\n",
    "NOTE: Projected BSP displayed on the app/website is the Near Price - more information about Near/Far Prices and their accuracy can be found [here](https://betfair-datascientists.github.io/historicData/analysingAndPredictingBSP/)\n",
    "\n",
    "NOTE: The function is defined to stop pulling the prices once the market goes in-play. If in-play prices are required, then simply remove the condition for market_book.inplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_prices(s, o):\n",
    "\n",
    "    with patch(\"builtins.open\", lambda f, _: f):\n",
    "\n",
    "        gen = s.get_generator()\n",
    "\n",
    "        marketID = None\n",
    "        tradeVols = None\n",
    "        time = None\n",
    "\n",
    "        for market_books in gen():\n",
    "\n",
    "            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n",
    "\n",
    "            if ((evaluate_market := filter_market(market_books[0])) == False):\n",
    "                    break\n",
    "\n",
    "            for market_book in market_books:\n",
    "\n",
    "                # Time Step Management ++++++++++++++++++++++++++++++++++\n",
    "\n",
    "                if marketID is None:\n",
    "\n",
    "                    # No market initialised\n",
    "                    marketID = market_book.market_id\n",
    "                    time =  market_book.publish_time\n",
    "\n",
    "                elif market_book.inplay:\n",
    "\n",
    "                    # Stop once market inplay\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n",
    "\n",
    "                    if seconds_to_start > log1_Start:\n",
    "                        \n",
    "                        # Too early before off to start logging prices\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "                    \n",
    "                        # Update data at different time steps depending on seconds to off\n",
    "                        wait = np.where(seconds_to_start <= log2_Start, log2_Step, log1_Step)\n",
    "\n",
    "                        # New Market\n",
    "                        if market_book.market_id != marketID:\n",
    "                            marketID = market_book.market_id\n",
    "                            time =  market_book.publish_time\n",
    "                        # (wait) seconds elapsed since last write\n",
    "                        elif (market_book.publish_time - time).total_seconds() > wait:\n",
    "                            time = market_book.publish_time\n",
    "                        # fewer than (wait) seconds elapsed continue to next loop\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                # Execute Data Logging ++++++++++++++++++++++++++++++++++\n",
    "                                                \n",
    "                for runner in market_book.runners:\n",
    "\n",
    "                    try:\n",
    "                        selection_status = runner.status\n",
    "                        reduction_factor = runner.adjustment_factor\n",
    "                        atb_ladder = pull_ladder(runner.ex.available_to_back, n = 5)\n",
    "                        atl_ladder = pull_ladder(runner.ex.available_to_lay, n = 5)\n",
    "                        spn = runner.sp.near_price\n",
    "                        spf = runner.sp.far_price\n",
    "                    except:\n",
    "                        selection_status = None\n",
    "                        reduction_factor = None\n",
    "                        atb_ladder = {}\n",
    "                        atl_ladder = {}\n",
    "                        spn = None\n",
    "                        spf = None\n",
    "\n",
    "                    # Calculate Current Traded Volume + Traded WAP\n",
    "                    limitTradedVol = sum([rung.size for rung in runner.ex.traded_volume])\n",
    "                    if limitTradedVol == 0:\n",
    "                        limitWAP = \"\"\n",
    "                    else:\n",
    "                        limitWAP = sum([rung.size * rung.price for rung in runner.ex.traded_volume]) / limitTradedVol\n",
    "                        limitWAP = round(limitWAP, 2)\n",
    "\n",
    "                    o.writerow(\n",
    "                        (\n",
    "                            market_book.market_id,\n",
    "                            market_book.number_of_active_runners,\n",
    "                            runner.selection_id,\n",
    "                            market_book.publish_time,\n",
    "                            limitTradedVol,\n",
    "                            limitWAP,\n",
    "                            runner.last_price_traded or \"\",\n",
    "                            selection_status,\n",
    "                            reduction_factor,\n",
    "                            str(atb_ladder).replace(' ',''), \n",
    "                            str(atl_ladder).replace(' ',''),\n",
    "                            str(spn),\n",
    "                            str(spf)\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "\n",
    "def parse_prices(dir, out_file):\n",
    "    \n",
    "    with open(out_file, \"w+\") as output:\n",
    "\n",
    "        writer = csv.writer(\n",
    "            output, \n",
    "            delimiter=',',\n",
    "            lineterminator='\\r\\n',\n",
    "            quoting=csv.QUOTE_ALL\n",
    "        )\n",
    "        \n",
    "        writer.writerow((\"market_id\",\"active_runners\",\"selection_id\",\"time\",\"traded_volume\",\"wap\",\"ltp\",\"selection_status\",'reduction_factor',\"atb_ladder\",\"atl_ladder\",\"sp_near\",\"sp_far\"))\n",
    "\n",
    "        for file_obj in load_markets(dir):\n",
    "\n",
    "            stream = trading.streaming.create_historical_generator_stream(\n",
    "                file_path=file_obj,\n",
    "                listener=listener,\n",
    "            )\n",
    "\n",
    "            loop_prices(stream, writer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 The Good Stuff\n",
    "Now comes the really meaty chunk of code that will do the grunt work for us. We'll generate the timestamped prices for each TAR file separately and then loop over them again to generate the selection metadata (This is using a chunk of code from our JSON to CSV Revisited tutorial using the Betfair_Data package. This is super speedy because its mainly written in Rust rather than python)\n",
    "In our metadata files we will provide:\n",
    "- Market ID\n",
    "- Market Time\n",
    "- Country Code\n",
    "- Track/Venue\n",
    "- Market Name\n",
    "- Selection ID\n",
    "- Horse Name\n",
    "- Result\n",
    "- BSP\n",
    "- Preplay Minimum Matched Price\n",
    "- Preplay Maximum Matched Price\n",
    "- Preplay Weighted Average Matched Price\n",
    "- Preplay Last Traded Price\n",
    "- Preplay Matched Volume (not including SP volume)\n",
    "- BSP Matched Volume (Back Stake)\n",
    "- Inplay Minimum Matched Price\n",
    "- Inplay Maximum Matched Price\n",
    "- Inplay Weighted Average Matched Price\n",
    "- Inplay Last Traded Price\n",
    "- Inplay Matched Volume\n",
    "\n",
    "Note: For markets that don't go in play (like Greyhounds/Place markets), these fields will be empty\n",
    "\n",
    "Following this metadata generation, we'll then join the dataframes together and add the \"seconds to scheduled off\" field, as converting the GMT time in the TAR Files to Melbourne time (this is useful for when markets are timed for early morning before 10am or 11am during daylight savings causing the local event date to be the previous day in GMT) and converting the currency to Australian Dollars using a historical currency conversion.\n",
    "\n",
    "The final piece will be outputting the files to CSV. The code is setup to output a separate file for each race which reduces the number of rows per race and enables output to be easily opened and checked for completeness. Changing this to output one CSV file per TAR file or one CSV file per day is fairly straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over each TAR file\n",
    "for tar in stream_files:\n",
    "    parse_prices([tar], prices_path) #This is where the timestamped prices are generated\n",
    "    print(\"__ Parsing Market and Selection Data ___ \")\n",
    "\n",
    "    # record prices to a file\n",
    "    with open(selection_meta, \"w\") as output:\n",
    "    # defining column headers\n",
    "        output.write(\"market_id,event_date,country,track,event_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,bsp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume\\n\")\n",
    "\n",
    "        for i, g in enumerate(bflw.Files([tar])):\n",
    "            print(\"Market {}\".format(i), end='\\r')\n",
    "\n",
    "            def get_pre_post_final():\n",
    "                eval_market = None\n",
    "                prev_market = None\n",
    "                preplay_market = None\n",
    "                postplay_market = None       \n",
    "\n",
    "                for market_books in g:\n",
    "                    for market_book in market_books:\n",
    "                        # if market doesn't meet filter return out\n",
    "                        if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n",
    "                            return (None, None, None)\n",
    "\n",
    "                        # final market view before market goes in play\n",
    "                        if prev_market is not None and prev_market.inplay != market_book.inplay:\n",
    "                            preplay_market = prev_market\n",
    "\n",
    "                        # final market view at the conclusion of the market\n",
    "                        if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n",
    "                            postplay_market = market_book\n",
    "\n",
    "                        # update reference to previous market\n",
    "                        prev_market = market_book\n",
    "\n",
    "                return (preplay_market, postplay_market, prev_market) # prev is now final\n",
    "\n",
    "            (preplay_market, postplay_market, final_market) = get_pre_post_final()\n",
    "\n",
    "            # no price data for market\n",
    "            if postplay_market is None:\n",
    "                continue; \n",
    "\n",
    "            preplay_traded = [ (r.last_price_traded, r.ex.traded_volume) for r in preplay_market.runners ] if preplay_market is not None else None\n",
    "\n",
    "            postplay_traded = [ (\n",
    "                r.last_price_traded,\n",
    "                r.ex.traded_volume,\n",
    "                # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (bsp - 1))        \n",
    "                min_gr0(\n",
    "                    next((pv.size for pv in r.sp.back_stake_taken if pv.size > 0), 0),\n",
    "                    next((pv.size for pv in r.sp.lay_liability_taken if pv.size > 0), 0)  / ((r.sp.actual_sp if (type(r.sp.actual_sp) is float) or (type(r.sp.actual_sp) is int) else 0) - 1)\n",
    "                ) if r.sp.actual_sp is not None else 0,\n",
    "            ) for r in postplay_market.runners ]\n",
    "\n",
    "            runner_data = [\n",
    "            {\n",
    "                'selection_id': r.selection_id,\n",
    "                'selection_name': next((rd.name for rd in final_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n",
    "                'selection_status': r.status,\n",
    "                'sp': as_str(r.sp.actual_sp),\n",
    "            }\n",
    "            for r in final_market.runners \n",
    "            ]\n",
    "\n",
    "            # runner price data for markets that go in play\n",
    "            if preplay_traded is not None:\n",
    "                def runner_vals(r):\n",
    "                    (pre_ltp, pre_traded), (post_ltp, post_traded, sp_traded) = r\n",
    "\n",
    "                    inplay_only = list(filter(lambda ps: ps.size > 0, [\n",
    "                        PriceSize(\n",
    "                            price=post_ps.price, \n",
    "                            size=post_ps.size - next((pre_ps.size for pre_ps in pre_traded if pre_ps.price == post_ps.price), 0)\n",
    "                        )\n",
    "                        for post_ps in post_traded \n",
    "                    ]))\n",
    "\n",
    "                    (ip_wavg, ip_matched, ip_min, ip_max) = parse_traded(inplay_only)\n",
    "                    (pre_wavg, pre_matched, pre_min, pre_max) = parse_traded(pre_traded)\n",
    "\n",
    "                    return {\n",
    "                        'preplay_ltp': as_str(pre_ltp),\n",
    "                        'preplay_min': as_str(pre_min),\n",
    "                        'preplay_max': as_str(pre_max),\n",
    "                        'preplay_wavg': as_str(pre_wavg),\n",
    "                        'preplay_matched': as_str(pre_matched or 0),\n",
    "                        'bsp_matched': as_str(sp_traded or 0),\n",
    "                        'inplay_ltp': as_str(post_ltp),\n",
    "                        'inplay_min': as_str(ip_min),\n",
    "                        'inplay_max': as_str(ip_max),\n",
    "                        'inplay_wavg': as_str(ip_wavg),\n",
    "                        'inplay_matched': as_str(ip_matched),\n",
    "                    }\n",
    "\n",
    "                runner_traded = [ runner_vals(r) for r in zip_longest(preplay_traded, postplay_traded, fillvalue=PriceSize(0, 0)) ]\n",
    "\n",
    "            # runner price data for markets that don't go in play\n",
    "            else:\n",
    "                def runner_vals(r):\n",
    "                    (ltp, traded, sp_traded) = r\n",
    "                    (wavg, matched, min_price, max_price) = parse_traded(traded)\n",
    "\n",
    "                    return {\n",
    "                        'preplay_ltp': as_str(ltp),\n",
    "                        'preplay_min': as_str(min_price),\n",
    "                        'preplay_max': as_str(max_price),\n",
    "                        'preplay_wavg': as_str(wavg),\n",
    "                        'preplay_matched': as_str(matched or 0),\n",
    "                        'bsp_matched': as_str(sp_traded or 0),\n",
    "                        'inplay_ltp': '',\n",
    "                        'inplay_min': '',\n",
    "                        'inplay_max': '',\n",
    "                        'inplay_wavg': '',\n",
    "                        'inplay_matched': '',\n",
    "                    }\n",
    "\n",
    "                runner_traded = [ runner_vals(r) for r in postplay_traded ]\n",
    "\n",
    "            # printing to csv for each runner\n",
    "            for (rdata, rprices) in zip(runner_data, runner_traded):\n",
    "                # defining data to go in each column\n",
    "                output.write(\n",
    "                    \"{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                        postplay_market.market_id,\n",
    "                        postplay_market.market_definition.market_time,\n",
    "                        postplay_market.market_definition.country_code,\n",
    "                        postplay_market.market_definition.venue,\n",
    "                        postplay_market.market_definition.name,\n",
    "                        rdata['selection_id'],\n",
    "                        rdata['selection_name'],\n",
    "                        rdata['selection_status'],\n",
    "                        rdata['sp'],\n",
    "                        rprices['preplay_min'],\n",
    "                        rprices['preplay_max'],\n",
    "                        rprices['preplay_wavg'],\n",
    "                        rprices['preplay_ltp'],\n",
    "                        rprices['preplay_matched'],\n",
    "                        rprices['bsp_matched'],\n",
    "                        rprices['inplay_min'],\n",
    "                        rprices['inplay_max'],\n",
    "                        rprices['inplay_wavg'],\n",
    "                        rprices['inplay_ltp'],\n",
    "                        rprices['inplay_matched'],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "    #loading selection file and parsing dates\n",
    "    selection = pd.read_csv(selection_meta, dtype={'market_id': object, 'selection_id': object}, parse_dates = ['event_date'])\n",
    "\n",
    "    #loading price file and parsing dates\n",
    "    prices = pd.read_csv(\n",
    "        prices_path, \n",
    "        quoting=csv.QUOTE_ALL,\n",
    "        dtype={'market_id': 'string', 'selection_id': 'string', 'atb_ladder': 'string', 'atl_ladder': 'string'},\n",
    "        parse_dates=['time']\n",
    "    )\n",
    "\n",
    "    #creating the ladder as a dictionary\n",
    "    prices['atb_ladder'] = [ast.literal_eval(x) for x in prices['atb_ladder']]\n",
    "    prices['atl_ladder'] = [ast.literal_eval(x) for x in prices['atl_ladder']]\n",
    "\n",
    "    #merging the price and selection files\n",
    "    df = selection.merge(prices, on = ['market_id', 'selection_id'])\n",
    "    #assigning best prices available and calculating time relative to market start time\n",
    "    df = (\n",
    "        df\n",
    "        .assign(back_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atb_ladder']])\n",
    "        .assign(lay_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atl_ladder']])\n",
    "        .assign(seconds_before_scheduled_off = lambda x: round((x['event_date'] - x['time']).dt.total_seconds()))\n",
    "        .query('seconds_before_scheduled_off < @log1_Start')\n",
    "    )\n",
    "\n",
    "    #creating a unique list of market ids\n",
    "    marketids = df['market_id'].unique().tolist()\n",
    "\n",
    "    #writing each market to its own csv file\n",
    "    for market in marketids:\n",
    "        #create a dataframe and a naming convention for this market\n",
    "        pricing_data=df[(df['market_id']==market)]\n",
    "        if pricing_data.empty:\n",
    "            continue\n",
    "        race_track=pricing_data['track'].iloc[0]\n",
    "        market_name=pricing_data['event_name'].iloc[0]\n",
    "        market_time=pricing_data['event_date'].iloc[0]\n",
    "        off=market_time.strftime('%Y-%m-%d')\n",
    "        #write race details to the dataframe\n",
    "        pricing_data['race']=pricing_data['event_name'].str.split('R').str[1]\n",
    "        pricing_data['race']=pricing_data['race'].str.split(' ').str[0]\n",
    "        pricing_data['distance']=pricing_data['event_name'].str.split(' ').str[1]\n",
    "        pricing_data['distance']=pricing_data['distance'].str.split('m').str[0]\n",
    "        pricing_data['race_type']=pricing_data['event_name'].str.split('m ').str[1]\n",
    "        pricing_data['selection_name']=pricing_data['selection_name'].str.split('\\. ').str[1]\n",
    "        #convert GMT timezone to AEST/AEDT\n",
    "        pricing_data['event_date']=pricing_data['event_date'].astype('datetime64[ns]')\n",
    "        pricing_data['event_date']=pricing_data['event_date'].dt.tz_localize('UTC',ambiguous=False)\n",
    "        pricing_data['event_date']=pricing_data['event_date'].dt.tz_convert('Australia/Melbourne')\n",
    "        pricing_data['event_date']=pricing_data['event_date'].dt.tz_localize(None)\n",
    "        pricing_data['time']=pricing_data['time'].astype('datetime64[ns]')\n",
    "        pricing_data['time']=pricing_data['time'].dt.tz_localize('UTC',ambiguous=False)\n",
    "        pricing_data['time']=pricing_data['time'].dt.tz_convert('Australia/Melbourne')\n",
    "        pricing_data['time']=pricing_data['time'].dt.tz_localize(None)\n",
    "        #covert GBP to AUD\n",
    "        event_date=(pd.to_datetime(pricing_data['event_date']).dt.date).iloc[0]\n",
    "        conversion_rate=CurrencyConverter(fallback_on_missing_rate=True).convert(1,'GBP','AUD',date=event_date)\n",
    "        pricing_data['traded_volume']=pricing_data['traded_volume']*conversion_rate\n",
    "        pricing_data.loc[(pricing_data['traded_volume'] < 0), 'traded_volume'] = 0\n",
    "        pricing_data['traded_volume'] = pricing_data['traded_volume'].round(decimals=2)\n",
    "        #reorder the dataframe and write to csv\n",
    "        pricing_data=pricing_data[['event_date','country','track','race','distance','race_type','market_id','selection_id','selection_name',\"selection_status\",'reduction_factor','result','bsp','time','traded_volume','wap','ltp','atb_ladder','atl_ladder','back_best','lay_best','seconds_before_scheduled_off','sp_near','sp_far','pp_min','pp_max','pp_wap','pp_ltp','pp_volume','bsp_volume','ip_min','ip_max','ip_wap','ip_ltp','ip_volume']]\n",
    "        pricing_data.to_csv(file_directory+off+' - '+race_track+' - '+market_name+'.csv',index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Clean-Up\n",
    "The final piece is just to clean up and remove the intermediate files. If you're encountering errors with the final output, comment this code out so these intermediate files can be viewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing intermediate working documents to clean up\n",
    "os.remove(selection_meta)\n",
    "os.remove(prices_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Conclusion and Next Steps\n",
    "If you'd like to expand your datasets, here's a list of other accessible data sources that can add additional datapoints and are fairly simple to join:\n",
    "- Unsupported API endpoint for a day's racecard - `https://apigateway.betfair.com.au/hub/racecard?date=YYYY-MM-DD` (simply substitute your day at the end)\n",
    "- Unsupported API endpoint for market and runner metadata including Barrier, Trainer, Jockey and Best Tote Price - `https://apigateway.betfair.com.au/hub/raceevent/market_id` (simply substitute your market_id at the end including \"1.\") These ratings are available back to 18/2/21.\n",
    "- Carrot Cruncher Horse Racing Model (url=`https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date='+YYYY-MM-DD+'&presenter=RatingsPresenter&csv=true`) (simply substitute your date in the middle)\n",
    "\n",
    "Hopefully this tutorial has helped and provides a great resource to be able to process these powerful, if cumbersome, TAR files and assists in your data gathering!\n",
    "\n",
    "For further tutorials on how to handle these new CSV files to create a model - check out our suite of other tutorials in the [modelling section](https://betfair-datascientists.github.io/modelling/howToModel) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Complete Code\n",
    "### 2.1 Complete Code - Racing Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import csv\n",
    "import tarfile\n",
    "import zipfile\n",
    "import bz2\n",
    "import glob\n",
    "import ast\n",
    "from unittest.mock import patch\n",
    "import betfairlightweight\n",
    "from betfairlightweight import StreamListener\n",
    "from betfair_data import bflw\n",
    "import pandas as pd\n",
    "from betfair_data import PriceSize\n",
    "import functools\n",
    "from typing import List, Tuple\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "from itertools import zip_longest\n",
    "from currency_converter import CurrencyConverter\n",
    "\n",
    "file_directory= 'C:/Users/motykam/Documents/Thoroughbreds/' #INSERT FILE DIRECTORY WHERE TAR FILES ARE STORED\n",
    "\n",
    "log1_Start = 60 * 10 # Seconds before scheduled off to start recording data for data segment one\n",
    "log1_Step = 30       # Seconds between log steps for first data segment\n",
    "log2_Start = 60 * 1  # Seconds before scheduled off to start recording data for data segment two\n",
    "log2_Step = 1    # Seconds between log steps for second data segment\n",
    "\n",
    "# splitting race name and returning the parts \n",
    "def split_anz_horse_market_name(market_name: str) -> Tuple[str, int, str]:\n",
    "# return race no, length, race type\n",
    "# input samples: \n",
    "# 'R6 1400m Grp1' -> ('R6','1400m','grp1')\n",
    "# 'R1 1609m Trot M' -> ('R1', '1609m', 'trot')\n",
    "# 'R4 1660m Pace M' -> ('R4', '1660m', 'pace')\n",
    "parts = market_name.split(' ')\n",
    "race_no = parts[0] \n",
    "race_len = parts[1].split('m')\n",
    "race_len = race_len[0]\n",
    "race_type = parts[2].lower() \n",
    "return (race_no, race_len, race_type)\n",
    "\n",
    "# filtering markets to those that fit the following criteria\n",
    "def filter_market(market: bflw.MarketBook) -> bool: \n",
    "d = market.market_definition\n",
    "return (d != None\n",
    "and d.country_code == 'AU' \n",
    "and d.market_type == 'WIN'\n",
    "and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace' #strips out Harness Races\n",
    "#and (c := split_anz_horse_market_name(d.name)[2]) == 'hcap'\n",
    "and (c := split_anz_horse_market_name(d.name)[1]) >= '1200'\n",
    ")\n",
    "\n",
    "# Simply add the below variable name to the market filter function above with the filter value\n",
    "# Equals (== 'Value in Quotation' or True/False/None), Does Not Equal (!= 'Value in Quotation' or True/False/None) - FOR ALL TYPES\n",
    "# Greater than (>), Greater than or equal to (>=), Less than (<), Less than or equal to (<=) - FOR INT/FLOAT\n",
    "# For list of value 'in'\n",
    "\n",
    "# and d.betting_type: str - ODDS, ASIAN_HANDICAP_SINGLES, ASIAN_HANDICAP_DOUBLES or LINE\n",
    "# and d.bsp_market: bool - True, False\n",
    "# and d.country_code: str - list of codes can be found here: https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2 - Default value is 'GB' - Australia = 'AU', New Zealand = 'NZ'\n",
    "# and d.event_id: str - PARENT_EVENT_ID\n",
    "# and d.event_name: Optional[str] - Usually the name of the Match-Up (e.g. Bangladesh v Sri Lanka) or Race Meeting Name (e.g. Wangaratta (AUS) 1st Dec) - Note: Dictionaries don't support wildcard searches\n",
    "# and d.event_type_id: str - SportID [Horse Racing - 7, Greyhounds - 4339]\n",
    "# and d.market_base_rate: float - Market Commission Rate\n",
    "# and d.market_type: str - e.g. \"WIN\"\n",
    "# and d.name: Optional[str] - market name (e.g. R1 1170m Mdn)\n",
    "# and d.number_of_active_runners: int - number of horses/dogs in the race\n",
    "# and d.number_of_winners: int - Win market 1, Place markets 2+\n",
    "# and d.turn_in_play_enabled: bool - True, False\n",
    "# and d.venue: Optional[str] - Racing Only - Track\n",
    "\n",
    "trading = betfairlightweight.APIClient(username = \"username\", password = \"password\", app_key=\"app_key\")\n",
    "listener = StreamListener(max_latency=None)\n",
    "\n",
    "stream_files = glob.glob(file_directory+\"*.tar\") \n",
    "selection_meta = file_directory+\"metadata.csv\"\n",
    "prices_path =  file_directory+\"preplay.csv\"\n",
    "\n",
    "# rounding to 2 decimal places or returning '' if blank\n",
    "def as_str(v) -> str:\n",
    "return '%.2f' % v if (type(v) is float) or (type(v) is int) else v if type(v) is str else ''\n",
    "\n",
    "# returning smaller of two numbers where min not 0\n",
    "def min_gr0(a: float, b: float) -> float:\n",
    "if a <= 0:\n",
    "return b\n",
    "if b <= 0:\n",
    "return a\n",
    "\n",
    "return min(a, b)\n",
    "\n",
    "# parsing price data and pulling out weighted avg price, matched, min price and max price\n",
    "def parse_traded(traded: List[PriceSize]) -> Tuple[float, float, float, float]:\n",
    "if len(traded) == 0: \n",
    "return (None, None, None, None)\n",
    "\n",
    "(wavg_sum, matched, min_price, max_price) = functools.reduce(\n",
    "lambda total, ps: (\n",
    "    total[0] + (ps.price * ps.size), # wavg_sum before we divide by total matched\n",
    "    total[1] + ps.size, # total matched\n",
    "    min(total[2], ps.price), # min price matched\n",
    "    max(total[3], ps.price), # max price matched\n",
    "),\n",
    "traded,\n",
    "(0, 0, 1001, 0) # starting default values\n",
    ")\n",
    "\n",
    "wavg_sum = (wavg_sum / matched) if matched > 0 else None # dividing sum of wavg by total matched\n",
    "matched = matched if matched > 0 else None \n",
    "min_price = min_price if min_price != 1001 else None\n",
    "max_price = max_price if max_price != 0 else None\n",
    "\n",
    "return (wavg_sum, matched, min_price, max_price)\n",
    "\n",
    "\n",
    "def load_markets(file_paths):\n",
    "for file_path in file_paths:\n",
    "print(file_path)\n",
    "print(\"__ Parsing Detailed Prices ___ \")\n",
    "if os.path.isdir(file_path):\n",
    "    for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n",
    "        f = bz2.BZ2File(path, 'rb')\n",
    "        yield f\n",
    "        f.close()\n",
    "elif os.path.isfile(file_path):\n",
    "    ext = os.path.splitext(file_path)[1]\n",
    "    # iterate through a tar archive\n",
    "    if ext == '.tar':\n",
    "        with tarfile.TarFile(file_path) as archive:\n",
    "            for file in archive:\n",
    "                yield bz2.open(archive.extractfile(file))\n",
    "    # or a zip archive\n",
    "    elif ext == '.zip':\n",
    "        with zipfile.ZipFile(file_path) as archive:\n",
    "            for file in archive.namelist():\n",
    "                yield bz2.open(archive.open(file))\n",
    "\n",
    "return None\n",
    "\n",
    "def slicePrice(l, n):\n",
    "try:\n",
    "x = l[n].price\n",
    "except:\n",
    "x = \"\"\n",
    "return(x)\n",
    "\n",
    "def sliceSize(l, n):\n",
    "try:\n",
    "x = l[n].size\n",
    "except:\n",
    "x = \"\"\n",
    "return(x)\n",
    "\n",
    "def pull_ladder(availableLadder, n = 5):\n",
    "out = {}\n",
    "price = []\n",
    "volume = []\n",
    "if len(availableLadder) == 0:\n",
    "    return(out)        \n",
    "else:\n",
    "    for rung in availableLadder[0:n]:\n",
    "        price.append(rung.price)\n",
    "        volume.append(rung.size)\n",
    "\n",
    "    out[\"p\"] = price\n",
    "    out[\"v\"] = volume\n",
    "    return(out)\n",
    "\n",
    "def final_market_book(s):\n",
    "\n",
    "with patch(\"builtins.open\", lambda f, _: f):\n",
    "\n",
    "gen = s.get_generator()\n",
    "\n",
    "for market_books in gen():\n",
    "    \n",
    "    # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    if ((evaluate_market := filter_market(market_books[0])) == False):\n",
    "            return(None)\n",
    "    \n",
    "    for market_book in market_books:\n",
    "\n",
    "        last_market_book = market_book\n",
    "\n",
    "return(last_market_book)\n",
    "\n",
    "def loop_prices(s, o):\n",
    "\n",
    "with patch(\"builtins.open\", lambda f, _: f):\n",
    "\n",
    "gen = s.get_generator()\n",
    "\n",
    "marketID = None\n",
    "tradeVols = None\n",
    "time = None\n",
    "\n",
    "for market_books in gen():\n",
    "\n",
    "    # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    if ((evaluate_market := filter_market(market_books[0])) == False):\n",
    "            break\n",
    "\n",
    "    for market_book in market_books:\n",
    "\n",
    "        # Time Step Management ++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        if marketID is None:\n",
    "\n",
    "            # No market initialised\n",
    "            marketID = market_book.market_id\n",
    "            time =  market_book.publish_time\n",
    "\n",
    "        elif market_book.inplay:\n",
    "\n",
    "            # Stop once market inplay\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            \n",
    "            seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n",
    "\n",
    "            if seconds_to_start > log1_Start:\n",
    "                \n",
    "                # Too early before off to start logging prices\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "            \n",
    "                # Update data at different time steps depending on seconds to off\n",
    "                wait = np.where(seconds_to_start <= log2_Start, log2_Step, log1_Step)\n",
    "\n",
    "                # New Market\n",
    "                if market_book.market_id != marketID:\n",
    "                    marketID = market_book.market_id\n",
    "                    time =  market_book.publish_time\n",
    "                # (wait) seconds elapsed since last write\n",
    "                elif (market_book.publish_time - time).total_seconds() > wait:\n",
    "                    time = market_book.publish_time\n",
    "                # fewer than (wait) seconds elapsed continue to next loop\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        # Execute Data Logging ++++++++++++++++++++++++++++++++++\n",
    "                                        \n",
    "        for runner in market_book.runners:\n",
    "\n",
    "            try:\n",
    "                selection_status = runner.status\n",
    "                reduction_factor = runner.adjustment_factor\n",
    "                atb_ladder = pull_ladder(runner.ex.available_to_back, n = 5)\n",
    "                atl_ladder = pull_ladder(runner.ex.available_to_lay, n = 5)\n",
    "                spn = runner.sp.near_price\n",
    "                spf = runner.sp.far_price\n",
    "            except:\n",
    "                selection_status = None\n",
    "                reduction_factor = None\n",
    "                atb_ladder = {}\n",
    "                atl_ladder = {}\n",
    "                spn = None\n",
    "                spf = None\n",
    "\n",
    "            # Calculate Current Traded Volume + Traded WAP\n",
    "            limitTradedVol = sum([rung.size for rung in runner.ex.traded_volume])\n",
    "            if limitTradedVol == 0:\n",
    "                limitWAP = \"\"\n",
    "            else:\n",
    "                limitWAP = sum([rung.size * rung.price for rung in runner.ex.traded_volume]) / limitTradedVol\n",
    "                limitWAP = round(limitWAP, 2)\n",
    "\n",
    "            o.writerow(\n",
    "                (\n",
    "                    market_book.market_id,\n",
    "                    market_book.number_of_active_runners,\n",
    "                    runner.selection_id,\n",
    "                    market_book.publish_time,\n",
    "                    limitTradedVol,\n",
    "                    limitWAP,\n",
    "                    runner.last_price_traded or \"\",\n",
    "                    selection_status,\n",
    "                    reduction_factor,\n",
    "                    str(atb_ladder).replace(' ',''), \n",
    "                    str(atl_ladder).replace(' ',''),\n",
    "                    str(spn),\n",
    "                    str(spf)\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def parse_prices(dir, out_file):\n",
    "\n",
    "with open(out_file, \"w+\") as output:\n",
    "\n",
    "writer = csv.writer(\n",
    "    output, \n",
    "    delimiter=',',\n",
    "    lineterminator='\\r\\n',\n",
    "    quoting=csv.QUOTE_ALL\n",
    ")\n",
    "\n",
    "writer.writerow((\"market_id\",\"active_runners\",\"selection_id\",\"time\",\"traded_volume\",\"wap\",\"ltp\",\"selection_status\",'reduction_factor',\"atb_ladder\",\"atl_ladder\",\"sp_near\",\"sp_far\"))\n",
    "\n",
    "for file_obj in load_markets(dir):\n",
    "\n",
    "    stream = trading.streaming.create_historical_generator_stream(\n",
    "        file_path=file_obj,\n",
    "        listener=listener,\n",
    "    )\n",
    "\n",
    "    loop_prices(stream, writer)\n",
    "\n",
    "\n",
    "\n",
    "#loop over each TAR file\n",
    "for tar in stream_files:\n",
    "parse_prices([tar], prices_path) #This is where the timestamped prices are generated\n",
    "print(\"__ Parsing Market and Selection Data ___ \")\n",
    "\n",
    "# record prices to a file\n",
    "with open(selection_meta, \"w\") as output:\n",
    "# defining column headers\n",
    "output.write(\"market_id,event_date,country,track,event_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,bsp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume\\n\")\n",
    "\n",
    "for i, g in enumerate(bflw.Files([tar])):\n",
    "    print(\"Market {}\".format(i), end='\\r')\n",
    "\n",
    "    def get_pre_post_final():\n",
    "        eval_market = None\n",
    "        prev_market = None\n",
    "        preplay_market = None\n",
    "        postplay_market = None       \n",
    "\n",
    "        for market_books in g:\n",
    "            for market_book in market_books:\n",
    "                # if market doesn't meet filter return out\n",
    "                if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n",
    "                    return (None, None, None)\n",
    "\n",
    "                # final market view before market goes in play\n",
    "                if prev_market is not None and prev_market.inplay != market_book.inplay:\n",
    "                    preplay_market = prev_market\n",
    "\n",
    "                # final market view at the conclusion of the market\n",
    "                if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n",
    "                    postplay_market = market_book\n",
    "\n",
    "                # update reference to previous market\n",
    "                prev_market = market_book\n",
    "\n",
    "        return (preplay_market, postplay_market, prev_market) # prev is now final\n",
    "\n",
    "    (preplay_market, postplay_market, final_market) = get_pre_post_final()\n",
    "\n",
    "    # no price data for market\n",
    "    if postplay_market is None:\n",
    "        continue; \n",
    "\n",
    "    preplay_traded = [ (r.last_price_traded, r.ex.traded_volume) for r in preplay_market.runners ] if preplay_market is not None else None\n",
    "\n",
    "    postplay_traded = [ (\n",
    "        r.last_price_traded,\n",
    "        r.ex.traded_volume,\n",
    "        # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (bsp - 1))        \n",
    "        min_gr0(\n",
    "            next((pv.size for pv in r.sp.back_stake_taken if pv.size > 0), 0),\n",
    "            next((pv.size for pv in r.sp.lay_liability_taken if pv.size > 0), 0)  / ((r.sp.actual_sp if (type(r.sp.actual_sp) is float) or (type(r.sp.actual_sp) is int) else 0) - 1)\n",
    "        ) if r.sp.actual_sp is not None else 0,\n",
    "    ) for r in postplay_market.runners ]\n",
    "\n",
    "    runner_data = [\n",
    "    {\n",
    "        'selection_id': r.selection_id,\n",
    "        'selection_name': next((rd.name for rd in final_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n",
    "        'selection_status': r.status,\n",
    "        'sp': as_str(r.sp.actual_sp),\n",
    "    }\n",
    "    for r in final_market.runners \n",
    "    ]\n",
    "\n",
    "    # runner price data for markets that go in play\n",
    "    if preplay_traded is not None:\n",
    "        def runner_vals(r):\n",
    "            (pre_ltp, pre_traded), (post_ltp, post_traded, sp_traded) = r\n",
    "\n",
    "            inplay_only = list(filter(lambda ps: ps.size > 0, [\n",
    "                PriceSize(\n",
    "                    price=post_ps.price, \n",
    "                    size=post_ps.size - next((pre_ps.size for pre_ps in pre_traded if pre_ps.price == post_ps.price), 0)\n",
    "                )\n",
    "                for post_ps in post_traded \n",
    "            ]))\n",
    "\n",
    "            (ip_wavg, ip_matched, ip_min, ip_max) = parse_traded(inplay_only)\n",
    "            (pre_wavg, pre_matched, pre_min, pre_max) = parse_traded(pre_traded)\n",
    "\n",
    "            return {\n",
    "                'preplay_ltp': as_str(pre_ltp),\n",
    "                'preplay_min': as_str(pre_min),\n",
    "                'preplay_max': as_str(pre_max),\n",
    "                'preplay_wavg': as_str(pre_wavg),\n",
    "                'preplay_matched': as_str(pre_matched or 0),\n",
    "                'bsp_matched': as_str(sp_traded or 0),\n",
    "                'inplay_ltp': as_str(post_ltp),\n",
    "                'inplay_min': as_str(ip_min),\n",
    "                'inplay_max': as_str(ip_max),\n",
    "                'inplay_wavg': as_str(ip_wavg),\n",
    "                'inplay_matched': as_str(ip_matched),\n",
    "            }\n",
    "\n",
    "        runner_traded = [ runner_vals(r) for r in zip_longest(preplay_traded, postplay_traded, fillvalue=PriceSize(0, 0)) ]\n",
    "\n",
    "    # runner price data for markets that don't go in play\n",
    "    else:\n",
    "        def runner_vals(r):\n",
    "            (ltp, traded, sp_traded) = r\n",
    "            (wavg, matched, min_price, max_price) = parse_traded(traded)\n",
    "\n",
    "            return {\n",
    "                'preplay_ltp': as_str(ltp),\n",
    "                'preplay_min': as_str(min_price),\n",
    "                'preplay_max': as_str(max_price),\n",
    "                'preplay_wavg': as_str(wavg),\n",
    "                'preplay_matched': as_str(matched or 0),\n",
    "                'bsp_matched': as_str(sp_traded or 0),\n",
    "                'inplay_ltp': '',\n",
    "                'inplay_min': '',\n",
    "                'inplay_max': '',\n",
    "                'inplay_wavg': '',\n",
    "                'inplay_matched': '',\n",
    "            }\n",
    "\n",
    "        runner_traded = [ runner_vals(r) for r in postplay_traded ]\n",
    "\n",
    "    # printing to csv for each runner\n",
    "    for (rdata, rprices) in zip(runner_data, runner_traded):\n",
    "        # defining data to go in each column\n",
    "        output.write(\n",
    "            \"{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                postplay_market.market_id,\n",
    "                postplay_market.market_definition.market_time,\n",
    "                postplay_market.market_definition.country_code,\n",
    "                postplay_market.market_definition.venue,\n",
    "                postplay_market.market_definition.name,\n",
    "                rdata['selection_id'],\n",
    "                rdata['selection_name'],\n",
    "                rdata['selection_status'],\n",
    "                rdata['sp'],\n",
    "                rprices['preplay_min'],\n",
    "                rprices['preplay_max'],\n",
    "                rprices['preplay_wavg'],\n",
    "                rprices['preplay_ltp'],\n",
    "                rprices['preplay_matched'],\n",
    "                rprices['bsp_matched'],\n",
    "                rprices['inplay_min'],\n",
    "                rprices['inplay_max'],\n",
    "                rprices['inplay_wavg'],\n",
    "                rprices['inplay_ltp'],\n",
    "                rprices['inplay_matched'],\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "#loading selection file and parsing dates\n",
    "selection = pd.read_csv(selection_meta, dtype={'market_id': object, 'selection_id': object}, parse_dates = ['event_date'])\n",
    "\n",
    "#loading price file and parsing dates\n",
    "prices = pd.read_csv(\n",
    "prices_path, \n",
    "quoting=csv.QUOTE_ALL,\n",
    "dtype={'market_id': 'string', 'selection_id': 'string', 'atb_ladder': 'string', 'atl_ladder': 'string'},\n",
    "parse_dates=['time']\n",
    ")\n",
    "\n",
    "#creating the ladder as a dictionary\n",
    "prices['atb_ladder'] = [ast.literal_eval(x) for x in prices['atb_ladder']]\n",
    "prices['atl_ladder'] = [ast.literal_eval(x) for x in prices['atl_ladder']]\n",
    "\n",
    "#merging the price and selection files\n",
    "df = selection.merge(prices, on = ['market_id', 'selection_id'])\n",
    "#assigning best prices available and calculating time relative to market start time\n",
    "df = (\n",
    "df\n",
    ".assign(back_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atb_ladder']])\n",
    ".assign(lay_best = lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atl_ladder']])\n",
    ".assign(seconds_before_scheduled_off = lambda x: round((x['event_date'] - x['time']).dt.total_seconds()))\n",
    ".query('seconds_before_scheduled_off < @log1_Start')\n",
    ")\n",
    "\n",
    "#creating a unique list of market ids\n",
    "marketids = df['market_id'].unique().tolist()\n",
    "\n",
    "#writing each market to its own csv file\n",
    "for market in marketids:\n",
    "#create a dataframe and a naming convention for this market\n",
    "pricing_data=df[(df['market_id']==market)]\n",
    "if pricing_data.empty:\n",
    "    continue\n",
    "race_track=pricing_data['track'].iloc[0]\n",
    "market_name=pricing_data['event_name'].iloc[0]\n",
    "market_time=pricing_data['event_date'].iloc[0]\n",
    "off=market_time.strftime('%Y-%m-%d')\n",
    "#write race details to the dataframe\n",
    "pricing_data['race']=pricing_data['event_name'].str.split('R').str[1]\n",
    "pricing_data['race']=pricing_data['race'].str.split(' ').str[0]\n",
    "pricing_data['distance']=pricing_data['event_name'].str.split(' ').str[1]\n",
    "pricing_data['distance']=pricing_data['distance'].str.split('m').str[0]\n",
    "pricing_data['race_type']=pricing_data['event_name'].str.split('m ').str[1]\n",
    "pricing_data['selection_name']=pricing_data['selection_name'].str.split('\\. ').str[1]\n",
    "#convert GMT timezone to AEST/AEDT\n",
    "pricing_data['event_date']=pricing_data['event_date'].astype('datetime64[ns]')\n",
    "pricing_data['event_date']=pricing_data['event_date'].dt.tz_localize('UTC',ambiguous=False)\n",
    "pricing_data['event_date']=pricing_data['event_date'].dt.tz_convert('Australia/Melbourne')\n",
    "pricing_data['event_date']=pricing_data['event_date'].dt.tz_localize(None)\n",
    "pricing_data['time']=pricing_data['time'].astype('datetime64[ns]')\n",
    "pricing_data['time']=pricing_data['time'].dt.tz_localize('UTC',ambiguous=False)\n",
    "pricing_data['time']=pricing_data['time'].dt.tz_convert('Australia/Melbourne')\n",
    "pricing_data['time']=pricing_data['time'].dt.tz_localize(None)\n",
    "#covert GBP to AUD\n",
    "event_date=(pd.to_datetime(pricing_data['event_date']).dt.date).iloc[0]\n",
    "conversion_rate=CurrencyConverter(fallback_on_missing_rate=True).convert(1,'GBP','AUD',date=event_date)\n",
    "pricing_data['traded_volume']=pricing_data['traded_volume']*conversion_rate\n",
    "pricing_data.loc[(pricing_data['traded_volume'] < 0), 'traded_volume'] = 0\n",
    "pricing_data['traded_volume'] = pricing_data['traded_volume'].round(decimals=2)\n",
    "#reorder the dataframe and write to csv\n",
    "pricing_data=pricing_data[['event_date','country','track','race','distance','race_type','market_id','selection_id','selection_name',\"selection_status\",'reduction_factor','result','bsp','time','traded_volume','wap','ltp','atb_ladder','atl_ladder','back_best','lay_best','seconds_before_scheduled_off','sp_near','sp_far','pp_min','pp_max','pp_wap','pp_ltp','pp_volume','bsp_volume','ip_min','ip_max','ip_wap','ip_ltp','ip_volume']]\n",
    "pricing_data.to_csv(file_directory+off+' - '+race_track+' - '+market_name+'.csv',index=False)\n",
    "\n",
    "\n",
    "#removing intermediate working documents to clean up\n",
    "os.remove(selection_meta)\n",
    "os.remove(prices_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Complete Code - Sports Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import csv\n",
    "import tarfile\n",
    "import zipfile\n",
    "import bz2\n",
    "import glob\n",
    "import ast\n",
    "from unittest.mock import patch\n",
    "import betfairlightweight\n",
    "from betfairlightweight import StreamListener\n",
    "from betfair_data import bflw #\"Import \"betfair_data.bflw\" could not be resolved from source\" - This is a known issue, the script should still run\n",
    "import pandas as pd\n",
    "from currency_converter import CurrencyConverter\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "file_directory = ''# INSERT FILE DIRECTORY WHERE TAR FILES ARE STORED\n",
    "\n",
    "log1_start = 60 * 60  # seconds before scheduled off to start recording data for data segment one\n",
    "log1_step = 60  # seconds between log steps for first data segment\n",
    "log2_start = 60 * 10  # seconds before scheduled off to start recording data for data segment two\n",
    "log2_step = 10  # seconds between log steps for second data segment\n",
    "\n",
    "\n",
    "def filter_market(market: bflw.MarketBook) -> bool:\n",
    "    d = market.market_definition\n",
    "    return (\n",
    "        d is not None\n",
    "        # and d.country_code in ['ES']\n",
    "        # and d.market_type == 'MATCH_ODDS'\n",
    "        and d.name in ['Match Odds', '1st Innings 20 Overs Line']\n",
    "        # and d.betting_type == 'ODDS'\n",
    "    )\n",
    "\n",
    "# Simply add the below variable name to the market filter function above with the filter value\n",
    "# Equals (== 'Value in Quotation' or True/False/None), Does Not Equal (!= 'Value in Quotation' or True/False/None) - FOR ALL TYPES\n",
    "# Greater than (>), Greater than or equal to (>=), Less than (<), Less than or equal to (<=) - FOR INT/FLOAT\n",
    "# For list of value 'in'\n",
    "\n",
    "# and d.betting_type: str - ODDS, ASIAN_HANDICAP_SINGLES, ASIAN_HANDICAP_DOUBLES or LINE\n",
    "# and d.bsp_market: bool - True, False\n",
    "# and d.country_code: str - list of codes can be found here: https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2 - Default value is 'GB' - Australia = 'AU', New Zealand = 'NZ'\n",
    "# and d.event_id: str - PARENT_EVENT_ID\n",
    "# and d.event_name: Optional[str] - Usually the name of the Match-Up (e.g. Bangladesh v Sri Lanka) or Race Name (e.g. R6 1400m Grp1) - Note: Dictionaries don't support wildcard searches\n",
    "# and d.event_type_id: str - SportID [Soccer - 1, Tennis - 2, Golf - 3, Cricket - 4, AFL - 61420]\n",
    "# and d.market_base_rate: float - Market Commission Rate\n",
    "# and d.market_type: str - e.g. \"MATCH_ODDS\", \"1ST_INNINGS_RUNS\",\"TO_QUALIFY\" - always all caps with \"_\" replacing spaces\n",
    "# and d.name: Optional[str] - market name (e.g. Sri Lanka 1st Inns Runs)\n",
    "# and d.number_of_active_runners: int - Head-To-Heads markets will be 2\n",
    "# and d.number_of_winners: int - Odds Markets usually 1, Line/Handicap markets usually 0\n",
    "# and d.regulators: str - 'MR_INT' to remove ring-fenced exchange markets \n",
    "# and d.turn_in_play_enabled: bool - True, False\n",
    "\n",
    "\n",
    "trading = betfairlightweight.APIClient(username = \"username\", password = \"password\", app_key=\"app_key\")\n",
    "listener = StreamListener(max_latency=None)\n",
    "stream_files = glob.glob(file_directory+\"*.tar\") \n",
    "selection_meta = file_directory+\"metadata.csv\"\n",
    "prices_path =  file_directory+\"preplay.csv\"\n",
    "\n",
    "# rounding to 2 decimal places or returning '' if blank\n",
    "def as_str(v) -> str:\n",
    "    return '%.2f' % v if (type(v) is float) or (type(v) is int) else v if type(v) is str else ''\n",
    "\n",
    "# returning smaller of two numbers where min not 0\n",
    "def min_gr0(a: float, b: float) -> float:\n",
    "    if a <= 0:\n",
    "        return b\n",
    "    if b <= 0:\n",
    "        return a\n",
    "\n",
    "    return min(a, b)\n",
    "\n",
    "def load_markets(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        print(file_path)\n",
    "        print(\"__ Parsing Detailed Prices ___ \")\n",
    "        if os.path.isdir(file_path):\n",
    "            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n",
    "                f = bz2.BZ2File(path, 'rb')\n",
    "                yield f\n",
    "                f.close()\n",
    "        elif os.path.isfile(file_path):\n",
    "            ext = os.path.splitext(file_path)[1]\n",
    "            # iterate through a tar archive\n",
    "            if ext == '.tar':\n",
    "                with tarfile.TarFile(file_path) as archive:\n",
    "                    for file in archive:\n",
    "                        yield bz2.open(archive.extractfile(file))\n",
    "            # or a zip archive\n",
    "            elif ext == '.zip':\n",
    "                with zipfile.ZipFile(file_path) as archive:\n",
    "                    for file in archive.namelist():\n",
    "                        yield bz2.open(archive.open(file))\n",
    "\n",
    "    return None\n",
    "\n",
    "def slicePrice(l, n):\n",
    "    try:\n",
    "        x = l[n].price\n",
    "    except:\n",
    "        x = \"\"\n",
    "    return(x)\n",
    "\n",
    "def sliceSize(l, n):\n",
    "    try:\n",
    "        x = l[n].size\n",
    "    except:\n",
    "        x = \"\"\n",
    "    return(x)\n",
    "\n",
    "def pull_ladder(availableLadder, n = 3):\n",
    "        out = {}\n",
    "        price = []\n",
    "        volume = []\n",
    "        if len(availableLadder) == 0:\n",
    "            return(out)        \n",
    "        else:\n",
    "            for rung in availableLadder[0:n]:\n",
    "                price.append(rung.price)\n",
    "                volume.append(rung.size)\n",
    "\n",
    "            out[\"p\"] = price\n",
    "            out[\"v\"] = volume\n",
    "            return(out)\n",
    "\n",
    "def final_market_book(s):\n",
    "\n",
    "    with patch(\"builtins.open\", lambda f, _: f):\n",
    "\n",
    "        gen = s.get_generator()\n",
    "\n",
    "        for market_books in gen():\n",
    "            \n",
    "            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n",
    "\n",
    "            if ((evaluate_market := filter_market(market_books[0])) == False):\n",
    "                    return(None)\n",
    "            \n",
    "            for market_book in market_books:\n",
    "\n",
    "                last_market_book = market_book\n",
    "        \n",
    "        return(last_market_book)\n",
    "\n",
    "def loop_prices(s, o):\n",
    "\n",
    "    with patch(\"builtins.open\", lambda f, _: f):\n",
    "\n",
    "        gen = s.get_generator()\n",
    "\n",
    "        marketID = None\n",
    "        tradeVols = None\n",
    "        time = None\n",
    "\n",
    "        for market_books in gen():\n",
    "\n",
    "            # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++\n",
    "\n",
    "            if ((evaluate_market := filter_market(market_books[0])) == False):\n",
    "                    break\n",
    "\n",
    "            for market_book in market_books:\n",
    "\n",
    "                # Time Step Management ++++++++++++++++++++++++++++++++++\n",
    "\n",
    "                if marketID is None:\n",
    "\n",
    "                    # No market initialised\n",
    "                    marketID = market_book.market_id\n",
    "                    time =  market_book.publish_time\n",
    "\n",
    "                elif market_book.status == \"CLOSED\":\n",
    "\n",
    "                    # Stop once market settled\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    seconds_to_start = (market_book.market_definition.market_time - market_book.publish_time).total_seconds()\n",
    "\n",
    "                    if seconds_to_start > log1_start:\n",
    "                        \n",
    "                        # Too early before off to start logging prices\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "                    \n",
    "                        # Update data at different time steps depending on seconds to off\n",
    "                        wait = np.where(seconds_to_start <= log2_start, log2_step, log1_step)\n",
    "\n",
    "                        # New Market\n",
    "                        if market_book.market_id != marketID:\n",
    "                            marketID = market_book.market_id\n",
    "                            time =  market_book.publish_time\n",
    "                        # (wait) seconds elapsed since last write\n",
    "                        elif (market_book.publish_time - time).total_seconds() > wait:\n",
    "                            time = market_book.publish_time\n",
    "                        # fewer than (wait) seconds elapsed continue to next loop\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                # Execute Data Logging ++++++++++++++++++++++++++++++++++\n",
    "                                                \n",
    "                for runner in market_book.runners:\n",
    "\n",
    "                    try:\n",
    "                        atb_ladder = pull_ladder(runner.ex.available_to_back, n = 3)\n",
    "                        atl_ladder = pull_ladder(runner.ex.available_to_lay, n = 3)\n",
    "                    except:\n",
    "                        atb_ladder = {}\n",
    "                        atl_ladder = {}\n",
    "\n",
    "                    # Calculate Current Traded Volume + Traded WAP\n",
    "                    limitTradedVol = sum([rung.size for rung in runner.ex.traded_volume])\n",
    "                    if limitTradedVol == 0:\n",
    "                        limitWAP = \"\"\n",
    "                    else:\n",
    "                        limitWAP = sum([rung.size * rung.price for rung in runner.ex.traded_volume]) / limitTradedVol\n",
    "                        limitWAP = round(limitWAP, 2)\n",
    "\n",
    "                    #Use this section to write rows that are required to join the metadata OR that will change in time\n",
    "                    o.writerow(\n",
    "                        (\n",
    "                            market_book.market_id,\n",
    "                            runner.selection_id,\n",
    "                            market_book.publish_time,\n",
    "                            market_book.inplay,\n",
    "                            limitTradedVol,\n",
    "                            limitWAP,\n",
    "                            runner.last_price_traded or \"\",\n",
    "                            str(atb_ladder).replace(' ',''), \n",
    "                            str(atl_ladder).replace(' ','')\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "\n",
    "def parse_prices(dir, out_file):\n",
    "    \n",
    "    with open(out_file, \"w+\") as output:\n",
    "\n",
    "        writer = csv.writer(\n",
    "            output, \n",
    "            delimiter=',',\n",
    "            lineterminator='\\r\\n',\n",
    "            quoting=csv.QUOTE_ALL\n",
    "        )\n",
    "        \n",
    "        writer.writerow((\"market_id\",\"selection_id\",\"time\",\"inplay\",\"traded_volume\",\"wap\",\"ltp\",\"atb_ladder\",\"atl_ladder\"))\n",
    "\n",
    "        for file_obj in load_markets(dir):\n",
    "\n",
    "            stream = trading.streaming.create_historical_generator_stream(\n",
    "                file_path=file_obj,\n",
    "                listener=listener,\n",
    "            )\n",
    "\n",
    "            loop_prices(stream, writer)\n",
    "\n",
    "#loop over each TAR file\n",
    "for tar in stream_files:\n",
    "    parse_prices([tar], prices_path)\n",
    "    print(\"__ Parsing Market and Selection Data ___ \")\n",
    "\n",
    "    # record prices to a file\n",
    "    with open(selection_meta, \"w\") as output:\n",
    "        # defining column headers\n",
    "        output.write(\"market_id,market_time,market_type,event_name,market_name,selection_id,x,selection_name,y,result\\n\")\n",
    "        #loop over each market in the TAR file\n",
    "        for i, g in enumerate(bflw.Files([tar])):\n",
    "            print(\"Market {}\".format(i), end='\\r')\n",
    "\n",
    "            def get_pre_post_final():\n",
    "                eval_market = None\n",
    "                prev_market = None\n",
    "                preplay_market = None\n",
    "                postplay_market = None       \n",
    "\n",
    "                for market_books in g:\n",
    "                    for market_book in market_books:\n",
    "                        # if market doesn't meet filter return out\n",
    "                        if eval_market is None and ((eval_market := filter_market(market_book)) == False):\n",
    "                            return (None, None, None)\n",
    "\n",
    "                        # final market view before market goes in play\n",
    "                        if prev_market is not None and prev_market.inplay != market_book.inplay:\n",
    "                            preplay_market = prev_market\n",
    "\n",
    "                        # final market view at the conclusion of the market\n",
    "                        if prev_market is not None and prev_market.status == \"OPEN\" and market_book.status != prev_market.status:\n",
    "                            postplay_market = market_book\n",
    "\n",
    "                        # update reference to previous market\n",
    "                        prev_market = market_book\n",
    "\n",
    "                return (preplay_market, postplay_market, prev_market) # prev is now final\n",
    "\n",
    "            (preplay_market, postplay_market, final_market) = get_pre_post_final()\n",
    "            \n",
    "            # no price data for market\n",
    "            if postplay_market is None:\n",
    "                continue; \n",
    "\n",
    "            preplay_traded = [ (r.last_price_traded, r.ex.traded_volume) for r in preplay_market.runners ] if preplay_market is not None else None\n",
    "\n",
    "            postplay_traded = [ (\n",
    "                r.last_price_traded,\n",
    "                r.ex.traded_volume,\n",
    "                # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1))        \n",
    "                min_gr0(\n",
    "                    next((pv.size for pv in r.sp.back_stake_taken if pv.size > 0), 0),\n",
    "                    next((pv.size for pv in r.sp.lay_liability_taken if pv.size > 0), 0)  / ((r.sp.actual_sp if (type(r.sp.actual_sp) is float) or (type(r.sp.actual_sp) is int) else 0) - 1)\n",
    "                ) if r.sp.actual_sp is not None else 0,\n",
    "            ) for r in postplay_market.runners ]\n",
    "            \n",
    "            # generic selection data\n",
    "            for r in final_market.runners:\n",
    "                selection_id=r.selection_id,\n",
    "                selection_name=next((rd.name for rd in final_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n",
    "                selection_status=r.status\n",
    "            \n",
    "            # printing to csv for each selection\n",
    "                output.write(\n",
    "                    \"{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                        postplay_market.market_id,\n",
    "                        postplay_market.market_definition.market_time,\n",
    "                        postplay_market.market_definition.market_type,\n",
    "                        postplay_market.market_definition.event_name,\n",
    "                        postplay_market.market_definition.name,\n",
    "                        selection_id,\n",
    "                        selection_name,\n",
    "                        selection_status\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    #loading selection file, parsing dates and cleaning the table\n",
    "    # loading selection file, parsing dates and cleaning the table\n",
    "    selection = pd.read_csv(\n",
    "        selection_meta, dtype={'market_id': object, 'selection_id': object}, parse_dates=['market_time']\n",
    "    )\n",
    "    selection.set_axis(\n",
    "        [\n",
    "            'market_id',\n",
    "            'market_time',\n",
    "            'market_type',\n",
    "            'event_name',\n",
    "            'market_name',\n",
    "            'selection_id',\n",
    "            'x',\n",
    "            'selection_name',\n",
    "            'y',\n",
    "            'result'\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    selection = selection[['market_id','market_time','market_type','event_name','market_name','selection_id','selection_name','result']]\n",
    "    selection['selection_id'] = selection['selection_id'].str.split('\\(').str[1]\n",
    "    selection['selection_name'] = selection['selection_name'].str.split(\"\\('\").str[1]\n",
    "    selection['selection_name'] = selection['selection_name'].str.split(\"'\").str[0]\n",
    "\n",
    "    # loading price file and parsing dates\n",
    "    prices = pd.read_csv(\n",
    "        prices_path,\n",
    "        quoting=csv.QUOTE_ALL,\n",
    "        dtype={'market_id': 'string', 'selection_id': 'string', 'atb_ladder': 'string', 'atl_ladder': 'string'},\n",
    "        parse_dates=['time']\n",
    "    )\n",
    "\n",
    "    # creating the ladder as a dictionary\n",
    "    prices['atb_ladder'] = [ast.literal_eval(x) for x in prices['atb_ladder']]\n",
    "    prices['atl_ladder'] = [ast.literal_eval(x) for x in prices['atl_ladder']]\n",
    "\n",
    "    # merging the price and selection files\n",
    "    df = selection.merge(prices, on=['market_id', 'selection_id'])\n",
    "\n",
    "    # assigning best prices available and calculating time relative to market start time\n",
    "    df = (\n",
    "        df\n",
    "        .assign(back_best=lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atb_ladder']])\n",
    "        .assign(lay_best=lambda x: [np.nan if d.get('p') is None else d.get('p')[0] for d in x['atl_ladder']])\n",
    "        .assign(\n",
    "            seconds_before_scheduled_off=lambda x: round((x['market_time'] - x['time']).dt.total_seconds())\n",
    "        )\n",
    "        .query('seconds_before_scheduled_off < @log1_Start')\n",
    "    )\n",
    "\n",
    "    # Writing each processed market to its own csv file\n",
    "    market_ids = df['market_id'].unique().tolist()\n",
    "    for market in market_ids:\n",
    "        # Create a dataframe and a naming convention for this market\n",
    "        pricing_data = df[df['market_id'] == market]\n",
    "        fixture = pricing_data['event_name'].iloc[0]\n",
    "        market_name = pricing_data['market_name'].iloc[0]\n",
    "        market_time = pricing_data['market_time'].iloc[0]\n",
    "        off = market_time.strftime('%Y-%m-%d')\n",
    "        # Convert GBP to AUD\n",
    "        event_date = (pd.to_datetime(pricing_data['market_time']).dt.date).iloc[0]\n",
    "        conversion_rate = CurrencyConverter(fallback_on_missing_rate=True).convert(1, 'GBP', 'AUD', date=event_date)\n",
    "        pricing_data['traded_volume'] = pricing_data['traded_volume'] * conversion_rate\n",
    "        pricing_data.loc[pricing_data['traded_volume'] < 0, 'traded_volume'] = 0\n",
    "        pricing_data['traded_volume'] = pricing_data['traded_volume'].round(decimals=2)\n",
    "        pricing_data.to_csv(file_directory + off + ' - ' + fixture + ' - ' + market_name + '.csv', index=False)\n",
    "\n",
    "#removing intermediate working documents to clean up\n",
    "os.remove(selection_meta)\n",
    "os.remove(prices_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Disclaimer\n",
    "Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
